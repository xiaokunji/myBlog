[{"content":"equals\n比较的是值 , 和java一样,利用hashcode()方法进行比较 例如(\u0026quot;he\u0026quot;+\u0026quot;llo\u0026quot;)\neq\n比较的是引用,比较的对象的引用地址\nne\n是eq的反义\n==\n当要比较的值是否null,\n如果为null则使用eq,\n如果不为null,则使用equals\nhttps://blog.csdn.net/do_yourself_go_on/article/details/72758380 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/scala/equalseqne%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E7%94%A8%E6%B3%95.html","summary":"equals 比较的是值 , 和java一样,利用hashcode()方法进行比较 例如(\u0026quot;he\u0026quot;+\u0026quot;llo\u0026quot;) eq 比较","title":"==,equals,eq,ne的区别和用法"},{"content":"[toc]\n1.介绍 在Spring boot应用中，要实现可监控的功能，依赖的是 spring-boot-starter-actuator 这个组件。它提供了很多监控和管理你的spring boot应用的HTTP或者JMX端点，并且你可以有选择地开启和关闭部分功能。当你的spring boot应用中引入下面的依赖之后，将自动的拥有审计、健康检查、Metrics监控功能。\n基于springboot2.X版本\n链接：https://www.jianshu.com/p/1aadc4c85f51\n2.使用 // 先有parent和web包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后就能访问了,哈哈哈,是不是超简单,http://127.0.0.1:8767/actuator/health\n端口是项目的访问地址,即使是用Feign访问的server层\n返回值:\nup:服务正常 down:服务异常 它是号称可以做监控中心的插件,展示东西远不止这些\n加上配置:management.endpoint.health.show-details=always\n表示总是展示详细信息,因为还有个选择是when-authorized,默认是不展示(never)\n再次访问,会展示出项目中使用的插件,比如redis,数据库等使用情况还有磁盘的使用情况,基本上,用这个能很好的看到项目的当前状态.\n默认情况下(Http)还能访问一个链接,/info (http://127.0.0.1:8767/actuator/info),这个能获取应用程序的定制信息，这些信息由 info 打头的属性提供,比如:info.app.version=V1.0.0,访问地址就能得到这个样的json字符串\n{ \u0026#34;app\u0026#34;:{ \u0026#34;version\u0026#34;:\u0026#34;V1.0.0\u0026#34; } } 还有其他链接:\nHttp 方法 路径 描述 get /autoconfig 提供了一份自动配置报告，记录哪些自动配置条件通过了，哪些没通过 get /configprops 描述配置属性（包含默认值）如何注入 Bean get /beans 描述应用程序上下文里全部的 Bean，以及它们的关系 get /dump 获取线程活动的快照 get /env 获取全部环境属性 get /env/{name} 根据名称获取特定的环境属性值 get /info 获取应用程序的定制信息，这些信息由 info 打头的属性提供 get /mappings 描述全部的 URI 路径，以及它们和控制器（包含 Actuator 端点）的映射关系 get /metrics 报告各种应用程序度量信息，比如内存用量和 HTTP 请求计数 get /metrics/{name} 报告指定名称的应用程序度量值 post /shutdown 关闭应用程序，要求 endpoints.shutdown.enabled 设置为 true（默认为 false） get /trace 提供基本的 HTTP 请求跟踪信息（时间戳、HTTP 头等） 除了/info,/health默认是开启的,其他都是关闭的 management.endpoints.web.exposure.include=* 使用这个配置可以打开所有端点(这些链接官方叫endpoint,翻译过来叫端点) 还有一个配置,表示\u0026quot;除了\u0026quot;,一看就懂对吧 management.endpoints.web.exposure.exclude=env,beans\n3. 安全 在 Actuator 启用的情况下，如果没有做好相关权限控制，非法用户可通过访问默认的执行器端点（endpoints）来获取应用系统中的监控信息。\n3.1安全隐患: 认证字段的获取以证明可影响其他用户:\n这个主要通过访问/trace 路径获取用户认证字段信息，比如如下站点存在 actuator 配置不当漏洞，在其 trace 路径下，除了记录有基本的 HTTP 请求信息（时间戳、HTTP 头等），还有用户 token、cookie 字段：\n数据库账户密码泄露:\n通过其/env 路径, 由于 actuator 会监控站点 mysql、mangodb 之类的数据库服务，所以通过监控信息有时可以拿下 mysql、mangodb 数据库；\ngit 项目地址泄露:\n这个一般是在/health 路径，比如如下站点，访问其 health 路径可探测到站点 git 项目地址：\n后台用户账号密码泄露:\n这个一般是在/heapdump 路径下，访问/heapdump 路径，返回 GZip 压缩 hprof 堆转储文件。在 Android studio 打开，会泄露站点内存信息，很多时候会包含后台用户的账号密码（包含漏洞的图片暂时没得，大家记住思路就好了..），通过泄露的账号密码，然后进入后台一番轰炸也不错的。\n来自:https://www.freebuf.com/news/193509.html\n3.2 措施 引入spring-boot-starter-security依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-security\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 在application.properties中指定actuator的端口以及开启security功能，配置访问权限验证，这时再访问actuator功能时就会弹出登录窗口，需要输入账号密码验证后才允许访问。\nmanagement.port=8099 management.security.enabled=true security.user.name=admin security.user.password=admin ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/actuator.html","summary":"[toc] 1.介绍 在Spring boot应用中，要实现可监控的功能，依赖的是 spring-boot-starter-actuator 这个组件。它提供了很多监控和管理你的spring boot应用的HTTP","title":"Actuator"},{"content":"[toc]\n一、什么是Anaconda？ 1. 简介 Anaconda（官方网站 ）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。\n简单的说就是提供了一个python的环境,还有很多工具包,不需要自己单独安装python了,这个工具自带python,还可以通过虚拟环境,指定不同版本的python\n通过虚拟出来的环境,可以当成一个python用,要用时指定对应目录即可\n链接：https://www.jianshu.com/p/62f155eb6ac5\n2. Anaconda、conda、pip、virtualenv的区别 ① Anaconda Anaconda是一个包含180+的科学包及其依赖项的发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook等。 ② conda conda是包及其依赖项和环境的管理工具。\n适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。\n适用平台：Windows, macOS, Linux\n用途：\n快速安装、运行和升级包及其依赖项。 在计算机中便捷地创建、保存、加载和切换环境。 如果你需要的包要求不同版本的Python，你无需切换到不同的环境，因为conda同样是一个环境管理器。仅需要几条命令，你可以创建一个完全独立的环境来运行不同的Python版本，同时继续在你常规的环境中使用你常用的Python版本。——conda官方网站 conda为Python项目而创造，但可适用于上述的多种语言。\nconda包和环境管理器包含于Anaconda的所有版本当中。\n③ pip pip是用于安装和管理软件包的包管理器。 pip编写语言：Python。 Python中默认安装的版本： Python 2.7.9及后续版本：默认安装，命令为pip Python 3.4及后续版本：默认安装，命令为pip3 pip名称的由来：pip采用的是递归缩写进行命名的。其名字被普遍认为来源于2处： “Pip installs Packages”（“pip安装包”） “Pip installs Python”（“pip安装Python”） ④ virtualenv virtualenv：用于创建一个独立的Python环境的工具。 解决问题： 当一个程序需要使用Python 2.7版本，而另一个程序需要使用Python 3.6版本，如何同时使用这两个程序？ 如果将所有程序都安装在系统下的默认路径，如：/usr/lib/python2.7/site-packages，当不小心升级了本不该升级的程序时，将会对其他的程序造成影响。 如果想要安装程序并在程序运行时对其库或库的版本进行修改，都会导致程序的中断。 在共享主机时，无法在全局site-packages目录中安装包。 virtualenv将会为它自己的安装目录创建一个环境，这并不与其他virtualenv环境共享库；同时也可以选择性地不连接已安装的全局库。 ⑤ pip 与 conda 比较 → 依赖项检查 pip： 不一定会展示所需其他依赖包。 安装包时或许会直接忽略依赖项而安装，仅在结果中提示错误。 conda： 列出所需其他依赖包。 安装包时自动安装其依赖项。 可以便捷地在包的不同版本中自由切换。 → 环境管理 pip：维护多个环境难度较大。 conda：比较方便地在不同环境之间进行切换，环境管理较为简单。 → 对系统自带Python的影响 pip：在系统自带Python中包的**更新/回退版本/卸载将影响其他程序。 conda：不会影响系统自带Python。 → 适用语言 pip：仅适用于Python。 conda：适用于Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。 ⑥ conda与pip、virtualenv的关系 conda结合了pip和virtualenv的功能。 链接：https://www.jianshu.com/p/62f155eb6ac5\n二、Anaconda的安装步骤 2. Windows系统安装Anaconda 前往官方下载页面 下载。有两个版本可供选择：Python 3.6 和 Python 2.7，选择版之后根据自己操作系统的情况点击“64-Bit Graphical Installer”或“32-Bit Graphical Installer”进行下载。 剩下的一路下一步就行了,最后一步取消两个勾(弹广告的)\n链接：https://www.jianshu.com/p/62f155eb6ac5\n3. Linux系统安装Anaconda 没测过\n前往官方下载页面 下载。有两个版本可供选择：Python 3.6 和 Python 2.7。 启动终端，在终端中输入命令md5sum /path/filename或sha256sum /path/filename 注意：将该步骤命令中的/path/filename替换为文件的实际下载路径和文件名。其中，path是路径，filename为文件名。 强烈建议： 路径和文件名中不要出现空格或其他特殊字符。 路径和文件名最好以英文命名，不要以中文或其他特殊字符命名。 根据Python版本的不同有选择性地在终端输入命令： Python 3.6：bash ~/Downloads/Anaconda3-5.0.1-Linux-x86_64.sh\nPython 2.7：bash ~/Downloads/Anaconda2-5.0.1-Linux-x86_64.sh\n注意：\n首词bash也需要输入，无论是否用的Bash shell。 如果你的下载路径是自定义的，那么把该步骤路径中的~/Downloads替换成你自己的下载路径。 除非被要求使用root权限，否则均选择“Install Anaconda as a user”。 安装过程中，看到提示“In order to continue the installation process, please review the license agreement.”（“请浏览许可证协议以便继续安装。”），点击“Enter”查看“许可证协议”。\n在“许可证协议”界面将屏幕滚动至底，输入“yes”表示同意许可证协议内容。然后进行下一步。\n安装过程中，提示“Press Enter to accept the default install location, CTRL-C to cancel the installation or specify an alternate installation directory.”（“按回车键确认安装路径，按\u0026rsquo;CTRL-C\u0026rsquo;取消安装或者指定安装目录。”）如果接受默认安装路径，则会显示“PREFIX=/home//anaconda\u0026lt;2 or 3\u0026gt;”并且继续安装。安装过程大约需要几分钟的时间。\n建议：直接接受默认安装路径。 安装器若提示“Do you wish the installer to prepend the Anaconda\u0026lt;2 or 3\u0026gt; install location to PATH in your /home//.bashrc ?”（“你希望安装器添加Anaconda安装路径在/home//.bashrc文件中吗？”），建议输入“yes”。 注意： 路径/home//.bash_rc中“”即进入到家目录后你的目录名。 如果输入“no”，则需要手动添加路径，否则conda将无法正常运行。 当看到“Thank you for installing Anaconda\u0026lt;2 or 3\u0026gt;!”则说明已经成功完成安装。\n关闭终端，然后再打开终端以使安装后的Anaconda启动。或者直接在终端中输入source ~/.bashrc也可完成启动。\n验证安装结果。可选用以下任意一种方法：\n在终端中输入命令condal list，如果Anaconda被成功安装，则会显示已经安装的包名和版本号。\n在终端中输入python。这条命令将会启动Python交互界面，如果Anaconda被成功安装并且可以运行，则将会在Python版本号的右边显示“Anaconda custom (64-bit)”。退出Python交互界面则输入exit()或quit()即可。\n在终端中输入anaconda-navigator。如果Anaconda被成功安装，则Anaconda Navigator将会被启动。\n链接：https://www.jianshu.com/p/62f155eb6ac5\n四、管理conda 0. 写在前面 接下来均是以命令行模式进行介绍，Windows用户请打开“Anaconda Prompt”；macOS和Linux用户请打开“Terminal”（“终端”）进行操作。\nconda --version 测试是否正确安装\nconda update conda 可能需要更新一下conda\n五、管理环境 0. 写在前面 进入命令行后,默认在base环境下,(装好了很多包),\n在路径最前面,环境名用括号括起来\n1. 创建新环境 conda create --name \u0026lt;env_name\u0026gt; \u0026lt;package_names\u0026gt; 注意：\n\u0026lt;env_name\u0026gt;即创建的环境名。建议以英文命名，且不加空格，名称两边不加尖括号“\u0026lt;\u0026gt;”。\n\u0026lt;package_names\u0026gt;即安装在环境中的包名。名称两边不加尖括号“\u0026lt;\u0026gt;”。\n如果要安装指定的版本号，则只需要在包名后面以=和版本号的形式执行。如：conda create --name python2 python=2.7，即创建一个名为“python2”的环境，环境中安装版本为2.7的python。默认与Anaconda版本一样\n如果要在新创建的环境中创建多个包，则直接在``后以空格隔开，添加多个包名即可。如：conda create -n python3 python=3.5 numpy pandas，即创建一个名为“python3”的环境，环境中安装版本为3.5的python，同时也安装了numpy和pandas。默认不装包\n--name同样可以替换为-n。\n提示：默认情况下，新创建的环境将会被保存在/Users//anaconda3/env目录下，其中，\u0026lt;user_name\u0026gt;为当前用户的用户名。\n链接：https://www.jianshu.com/p/62f155eb6ac5\n2. 切换环境 ① Linux 或 macOS source activate \u0026lt;env_name\u0026gt; ② Windows activate \u0026lt;env_name\u0026gt; 3. 退出环境至root ① Linux 或 macOS source deactivate ② Windows deactivate 4. 显示已创建环境 # 三选一 conda info --envs conda info -e conda env list 结果中星号 “*” 所在行即为当前所在环境\n5. 复制环境 conda create --name \u0026lt;new_env_name\u0026gt; --clone \u0026lt;copied_env_name\u0026gt; 注意： \u0026lt;copied_env_name\u0026gt;即为被复制/克隆环境名。环境名两边不加尖括号“\u0026lt;\u0026gt;”。 \u0026lt;new_env_name\u0026gt;即为复制之后新环境的名称。环境名两边不加尖括号“\u0026lt;\u0026gt;”。 如：conda create --name py2 --clone python2，即为克隆名为“python2”的环境，克隆后的新环境名为“py2”。此时，环境中将同时存在“python2”和“py2”环境，且两个环境的配置相同。 6. 删除环境 conda remove --name \u0026lt;env_name\u0026gt; --all 注意：\u0026lt;env_name\u0026gt;为被删除环境的名称。环境名两边不加尖括号“\u0026lt;\u0026gt;”。 3. 安装包 ① 在指定环境中安装包 conda install --name \u0026lt;env_name\u0026gt; \u0026lt;package_name\u0026gt; 例如：conda install --name python2 pandas即在名为“python2”的环境中安装pandas包。\n② 在当前环境中安装包 conda install \u0026lt;package_name\u0026gt; 例如：conda install pandas即在当前环境中安装pandas包。\n③ 使用pip安装包 → 使用场景 当使用conda install无法进行安装时，可以使用pip进行安装。例如：see包。\n→ 命令 pip install \u0026lt;package_name\u0026gt; 如：pip install see即安装see包。\n→ 注意 pip只是包管理器，无法对环境进行管理。因此如果想在指定环境中使用pip进行安装包，则需要先切换到指定环境中，再使用pip命令安装包。 pip无法更新python，因为pip并不将python视为包。 pip可以安装一些conda无法安装的包；conda也可以安装一些pip无法安装的包。因此当使用一种命令无法安装包时，可以尝试用另一种命令。 ④ 从Anaconda.org安装包 在浏览器中输入：http://anaconda.org 通过界面的方式下载包\n链接：https://www.jianshu.com/p/62f155eb6ac5\n更换仓库源 在工具中使用python ","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/anaconda.html","summary":"[toc] 一、什么是Anaconda？ 1. 简介 Anaconda（官方网站 ）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。An","title":"Anaconda"},{"content":"[toc]\n1. 什么是analysis? analysis是Elasticsearch在文档发送之前对文档正文执行的过程，以添加到反向索引中（inverted index）。 在将文档添加到索引之前，Elasticsearch会为每个分析的字段执行许多步骤：\nCharacter filtering (字符过滤器): 使用字符过滤器转换字符 Breaking text into tokens (把文字转化为标记): 将文本分成一组一个或多个标记 Token filtering：使用标记过滤器转换每个标记(大小写转化/删除无用词等等) Token indexing：把这些标记存于index中 文本分词会发生在两个地方：\n创建索引:当索引文档字符类型为text时，在建立索引时将会对该字段进行分词。 搜索：当对一个text类型的字段进行全文检索时，会对用户输入的文本进行分词。 2. 主要组成 总体说来一个analyzer可以分为如下的几个部分：\n0个或1个以上的character filter\n接收原字符流，通过添加、删除或者替换操作改变原字符流。例如：去除文本中的html标签，或者将罗马数字转换成阿拉伯数字等。一个字符过滤器可以有零个或者多个。\n1个tokenizer\n简单的说就是将一整段文本拆分成一个个的词。例如拆分英文，通过空格能将句子拆分成一个个的词，但是对于中文来说，无法使用这种方式来实现。在一个分词器中,有且只有一个tokenizeer\n0个或1个以上的token filter\n将切分的单词添加、删除或者改变。例如将所有英文单词小写，或者将英文中的停词a删除等。在token filters中，不允许将token(分出的词)的position或者offset改变。同时，在一个分词器中，可以有零个或者多个token filters.\n单词的过滤顺序按照 filter过滤器的顺序\n3. 使用分析器 默认ES使用standard analyzer\n3.1 Elasticsearch的内置分析器 Analyzer\nStandard Analyzer - 默认分词器，按词切分，小写处理 Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理 Stop Analyzer - 小写处理，停用词过滤(the,a,is) Whitespace Analyzer - 按照空格切分，不转小写 Keyword Analyzer - 不分词，直接将输入当作输出 Patter Analyzer - 正则表达式，默认\\W+(非字符分割) Language - 提供了30多种常见语言的分词器 Customer Analyzer 自定义分词器 参考链接 Character Filter\ncharacter filter logical name description mapping char filter mapping 根据配置的映射关系替换字符 html strip char filter html_strip 去掉HTML元素 pattern replace char filter pattern_replace 用正则表达式处理字符串 Tokenizer\ntokenizer logical name description standard tokenizer standard edge ngram tokenizer edgeNGram keyword tokenizer keyword 不分词 letter analyzer letter 按单词分 lowercase analyzer lowercase letter tokenizer, lower case filter ngram analyzers nGram whitespace analyzer whitespace 以空格为分隔符拆分 pattern analyzer pattern 定义分隔符的正则表达式 uax email url analyzer uax_url_email 不拆分 url 和 email path hierarchy analyzer path_hierarchy 处理类似 /path/to/somthing样式的字符串 Token Filter\ntoken filter logical name description standard filter standard ascii folding filter asciifolding length filter length 去掉太长或者太短的 lowercase filter lowercase 转成小写 ngram filter nGram edge ngram filter edgeNGram porter stem filter porterStem 波特词干算法 shingle filter shingle 定义分隔符的正则表达式 stop filter stop 移除 stop words word delimiter filter word_delimiter 将一个单词再拆成子分词 stemmer token filter stemmer stemmer override filter stemmer_override keyword marker filter keyword_marker keyword repeat filter keyword_repeat kstem filter kstem snowball filter snowball phonetic filter phonetic 插件 synonym filter synonyms 处理同义词 compound word filter dictionary_decompounder, hyphenation_decompounder 分解复合词 reverse filter reverse 反转字符串 elision filter elision 去掉缩略语 truncate filter truncate 截断字符串 unique filter unique pattern capture filter pattern_capture pattern replace filte pattern_replace 用正则表达式替换 trim filter trim 去掉空格 limit token count filter limit 限制 token 数量 hunspell filter hunspell 拼写检查 common grams filter common_grams normalization filter arabic_normalization, persian_normalization 内置 Analyzer 一览表 3.2 分析器测试 可以通过_analyzerAPI来测试分词的效果。\nPOST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;The quick Brown Fox\u0026#34; } 结果: 按空格切分,所以会有四个词\n可以按照下面的规则组合使用：\n0个或者多个character filters 一个tokenizer 0个或者多个token filters POST _analyze { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;], \u0026#34;text\u0026#34;: \u0026#34;The quick Brown Fox\u0026#34; } 结果: 按空格切分,所以会有四个词,但每个词都是小写\n3.3 自定义分析器 当内置的分词器无法满足需求时，可以创建custom类型的分词器。\ntokenizer:内置或定制的tokenizer.(必须) char_filter:内置或定制的char_filter(非必须) filter:内置或定制的token filter(非必须) position_increment_gap:当值为文本数组时，设置改值会在文本的中间插入假空隙。设置该属性，对与后面的查询会有影响。默认该值为100. PUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_custom_analyzer\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;:\u0026#34;standard\u0026#34;, \u0026#34;char_filter\u0026#34;:[\u0026#34;html_strip\u0026#34;], \u0026#34;filter\u0026#34;:[\u0026#34;lowercase\u0026#34;,\u0026#34;asciifolding\u0026#34;] } } } } } 上面的示例中定义了一个名为my_custom_analyzer的分析器，该分析器的type为custom，tokenizer为standard，char_filter为hmtl_strip,filter定义了两个分别为：lowercase和asciifolding.\n测试一下:\nPOST my_index/_analyze { \u0026#34;text\u0026#34;: \u0026#34;Is this \u0026lt;b\u0026gt;déjà vu\u0026lt;/b\u0026gt;?\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34; } 结果:\n{ \u0026#34;tokens\u0026#34; : [ { \u0026#34;token\u0026#34; : \u0026#34;is\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 2, \u0026#34;type\u0026#34; : \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34; : 0 }, { \u0026#34;token\u0026#34; : \u0026#34;this\u0026#34;, \u0026#34;start_offset\u0026#34; : 3, \u0026#34;end_offset\u0026#34; : 7, \u0026#34;type\u0026#34; : \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34; : 1 }, { \u0026#34;token\u0026#34; : \u0026#34;deja\u0026#34;, \u0026#34;start_offset\u0026#34; : 11, \u0026#34;end_offset\u0026#34; : 15, \u0026#34;type\u0026#34; : \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34; : 2 }, { \u0026#34;token\u0026#34; : \u0026#34;vu\u0026#34;, \u0026#34;start_offset\u0026#34; : 16, \u0026#34;end_offset\u0026#34; : 22, \u0026#34;type\u0026#34; : \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34; : 3 } ] } 4. 分析器作用位置及使用 分析器的使用地方有两个：\n创建索引时 进行搜索时 4.1 创建索引时指定分析器 如果设置手动设置了分析器，ES将按照下面顺序来确定使用哪个分析器：\n先判断字段是否有设置分析器，如果有，则使用字段属性上的分析器设置 如果设置了analysis.analyzer.default，则使用该设置的分析器 如果上面两个都未设置，则使用默认的standard分析器 为字段指定分析器\nPUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;whitespace\u0026#34; } } } } 设置索引默认分析器\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;default\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;simple\u0026#34; } } } } } 4.2 搜索时如何确定分析器 在搜索时，通过下面参数依次检查搜索时使用的分词器：\n搜索时指定analyzer参数 创建mapping时指定字段的search_analyzer属性 创建索引时指定setting的analysis.analyzer.default_search 查看创建索引时字段指定的analyzer属性 如果上面几种都未设置，则使用默认的standard分词器。 搜索时指定analyzer查询参数\nGET my_index/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;Quick foxes\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;stop\u0026#34; } } } } 指定字段的seach_analyzer\nPUT my_index { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;whitespace\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;simple\u0026#34; } } } } // 上面指定创建索引时使用的默认分析器为whitespace分词器，而搜索的默认分词器为 simple分词器。 指定索引的默认搜索分词器\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;default\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;simple\u0026#34; }, \u0026#34;default_seach\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;whitespace\u0026#34; } } } } } // 上面指定创建索引时使用的默认分词器为simple分词器，而搜索的默认分词器为whitespace分词器。 5. 常用分析器 5.1 繁简转换分析器 简体和繁体的几个主要情况：\n字形差异，汉字字形本身存在着明显差异，如劉与刘，一般是单个字。\n词汇的差异，习惯、文化环境造成的用法差异，如滑鼠与鼠标，一般是多字词语。\n多种形态共存，有很多代表不同意思的繁体最后简化成了一个字，所以就出现了多音和多义的简体字，如：飆、飈、𩙪都对应于一个简体：飚。\n对应第一种，我们可以采用替换的办法，将所有的繁体都替换成对应的简体，在创建索引的时候，就进行标准化，比较好实现，(此文仅针对第一种)\n第二种我们可以采用收集相应的词汇转换关系，进行替换，不过因为是多字词语的替换，所以这里面还需要提取处理好分词，不然就有可能替换错误反而引起歧义的问题。\n第三种情况，因为繁体和简体是一对多的情况，我们将繁体通过映射表转成简体相对容易，反过来将简体转换成繁体将遇到调整，可能需要结合前后语义来选择正确的繁体。\n5.1.1 安装与验证 Github 上的地址是：https://github.com/medcl/elasticsearch-analysis-stconvert/releases\n下载之后解压到es的安装目录下的plugins 子目录,重启es即可 (必须选择对应版本)\n验证:\n如果你的集群里面有不止一个 Elasticsearch 的节点，你需要在每一台 Elasticsearch 的实例上执行相同的插件安装，并进行重启让其加载生效。\n5.1.2 插件介绍 STConvert 这个插件一共提供了 4 个不同的组件:\n一个名为 stconvert 的 Analyzer，可以将简体转换成繁体 一个名为 stconvert 的 Tokenizer，可以将简体转换成繁体 一个名为 stconvert 的 Token Filter，可以将简体转换成繁体 一个名为 stconvert 的 Char Filter，可以将简体转换成繁体 每个组件都可以有以下3个参数用来进行自定义配置，分别是：\n参数 convert_type 设置转换的方向，默认是 s2t，表示简体到繁体，如果要将繁体转换为简体，则设置为 t2s 参数 keep_both 用于设置是否保留转换之前的内容，一般来说保留原始内容可以提高我们的搜索命中率(也就是说可以同时搜索繁体和简体)，默认是 false，也就是不保留 参数 delimiter 主要是用于，当保留原始内容的时候，如何分割两部分内容，默认的值是逗号 , t: Traditional (传统的) =\u0026gt; Traditional Chinese (繁体)\ns: Simplified (简化的) =\u0026gt; Simplified Chinese (简体)\n5.1.3 简转繁 简转繁好做,因为插件默认就是做这个的\nGET /_analyze { \u0026#34;tokenizer\u0026#34; : \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34; : [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34; : [\u0026#34;stconvert\u0026#34;], \u0026#34;text\u0026#34; : \u0026#34;我爱China。\u0026#34; } 我们指定了 Tokenizer 为 standard，也指定了 lowercase 作为分词之后的 Filter，这样就跟标准的 Standard Analyzer 的分析效果一样了，我们还新增了一个 Char Filter 的参数设置，使用的是我们新安装的简繁体转换插件提供的 stconvert，也就是将简体转成繁体,其结果如下\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;我\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;IDEOGRAPHIC\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;愛\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;IDEOGRAPHIC\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;china\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 可以看到，除了和 Standard 分词效果一样的地方以外，我们还额外的将中文字符都转成了繁体，爱 字转成了 愛，而 我 没有变化，是因为它的繁体也是 我。建议我们在真正动手进行自定义分词之前，通过这样的方式先进行分词效果的测试，得到满意的结果之后再进行具体的自定义 Analyzer 的创建工作。\n5.1.4 繁转简 创建索引时指定分析器\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;], \u0026#34;char_filter\u0026#34;: [\u0026#34;tsconvert\u0026#34;] } }, \u0026#34;char_filter\u0026#34;: { \u0026#34;tsconvert\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;stconvert\u0026#34;, \u0026#34;delimiter\u0026#34; : \u0026#34;#\u0026#34;, \u0026#34;keep_both\u0026#34; : false, \u0026#34;convert_type\u0026#34; : \u0026#34;t2s\u0026#34; } } } } } // delimiter 和 keep_both 属性可以不写 测试:\nPOST /my_index/_analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;我愛China。\u0026#34; } 经过执行，得到的分析结果如下：\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;我\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;IDEOGRAPHIC\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 }, { \u0026#34;token\u0026#34;: \u0026#34;爱\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;IDEOGRAPHIC\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 1 }, { \u0026#34;token\u0026#34;: \u0026#34;china\u0026#34;, \u0026#34;start_offset\u0026#34;: 2, \u0026#34;end_offset\u0026#34;: 7, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 2 } ] } 5.2 Ik分析器 IK分析插件将Lucene IK分析器（http://code.google.com/p/ik-analyzer/）集成到elasticsearch中，支持自定义字典。\n5.2.1 安装与验证 Github 上的地址是：https://github.com/medcl/elasticsearch-analysis-ik/releases\n下载之后解压到es的安装目录下的plugins 子目录,重启es即可 (必须选择对应版本)\n5.2.2 插件介绍 Analyzer: ik_smart , ik_max_word , Tokenizer: ik_smart , ik_max_word ik_max_word 和 ik_smart 什么区别?\nik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合，适合 Term Query；\nik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”，适合 Phrase 查询。\n5.2.3 基本使用 使用analyzer:\n使用tokenizer\nPUT product { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34; ], \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } } } 5.2.4 自定义分词库 配置 在ik分析器的config目录下的 IKAnalyzer.cfg.xml\n内容如下:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE properties SYSTEM \u0026#34;http://java.sun.com/dtd/properties.dtd\u0026#34;\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;comment\u0026gt;IK Analyzer 扩展配置\u0026lt;/comment\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展字典 --\u0026gt; \u0026lt;entry key=\u0026#34;ext_dict\u0026#34;\u0026gt;custom/mydict.dic;custom/single_word_low_freq.dic\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展停止词字典--\u0026gt; \u0026lt;entry key=\u0026#34;ext_stopwords\u0026#34;\u0026gt;custom/ext_stopword.dic\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置远程扩展字典 --\u0026gt; \u0026lt;entry key=\u0026#34;remote_ext_dict\u0026#34;\u0026gt;location\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置远程扩展停止词字典--\u0026gt; \u0026lt;entry key=\u0026#34;remote_ext_stopwords\u0026#34;\u0026gt;http://xxx.com/xxx.dic\u0026lt;/entry\u0026gt; \u0026lt;/properties\u0026gt; 扩展词: 不想被分词的词: 比如, 乔碧罗, 神罗天征等\n停止词: 不想要的词,搜索时不会搜它, 比如: 个 ,啊, the , a 等\n远程词典具备热更新的能力, 满足以下前两点即可实现热更新\n其中 location 是指一个 url，比如 http://yoursite.com/getCustomDict，该请求只需满足以下两点即可完成分词热更新。\n该 http 请求需要返回两个头部(header)，一个是 Last-Modified，一个是 ETag，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。\n写个接口,返回修改时间和ETag即可\n或者用Ng转发静态文件, 会自动加上这个两个配置,当文件改动时,这两个值自动改变\n该 http 请求返回的内容格式是一行一个分词，换行符用 即可。\nurl返回一个UTF-8格式的文本即可\n如果有多个文件,用英文分号隔开;\n监控流程\n读取 IKAnalyzer.cfg.xml配置文件,并解析其中的配置\n向词库服务器发送Head请求\n从响应中获取Last- Modify、 Etags字段值，判断是否变化\n如果未变化，体眠1min,返回第0步\n​\t如果有变化，重新加戟词典\n休眠1min,返回第0步\n自定义词汇只会对新加的数据生效,不会对存量数据生效,需要更新一次才行\n数据在写入时就已分词存储,所以自定义词汇不会对存量数据生效\n官方 提供更新方式 POST product/_update_by_query?conflicts=proceed\nupdate_by_query原理\n开始时获取一个索引的快照，并且使用内部版本来号来进行更新。这意味着如果文档在获得快照后，对索引处理 过程中版本发生更改，将会发生版本冲突。当快照的版本和索引版本一直时则进行更新，并且递增文档版本号。\n当遇到冲突而导致整个更新过程失败时，更新过程是不会回滚的。如果不想因为冲突导致整个更新过程终止，可以在url中添加参数conflicts=proceed。或者在请求body中添加”conflicts”:”proceed”\n不光这里可以用,修改es文档也是用_update_by_query\n执行结果如图:(消耗时间单位 : 毫秒)\n格力的es集群,执行效率大概是2300条/s, 26万数据,大概花了1.7分钟\n更新可能耗时太久导致客户端连接超时,可以用命令查看任务执行情况(未结束的任务)\nGET _tasks?actions=*update*\u0026amp;detailed\n5.3 同义词分析器 配置同义词 5.4 拼音分析器 Pinyin Analysis for Elasticsearch 参考链接:\n分析器-简书 分析器-博客园 繁简转换 ES的同义词、扩展词、停止词热更新方案 ik分析器-github ik分析器-简书 ik分析器-csdn IK分词器原理与源码分析 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/analysis.html","summary":"[toc] 1. 什么是analysis? analysis是Elasticsearch在文档发送之前对文档正文执行的过程，以添加到反向索引中（invert","title":"analysis"},{"content":"[toc]\n1. 描述一下Spring AOP Spring AOP(Aspect Oriented Programming，面向切面编程)是OOPs(面向对象编程)的补充，它也提供了模块化。在面向对象编程中，关键的单元是对象，AOP的关键单元是切面，或者说关注点（可以简单地理解为你程序中的独立模块）。一些切面可能有集中的代码，但是有些可能被分散或者混杂在一起，例如日志或者事务。这些分散的切面被称为横切关注点。一个横切关注点是一个可以影响到整个应用的关注点，而且应该被尽量地集中到代码的一个地方，例如事务管理、权限、日志、安全等。\nAOP让你可以使用简单可插拔的配置，在实际逻辑执行之前、之后或周围动态添加横切关注点。这让代码在当下和将来都变得易于维护。如果你是使用XML来使用切面的话，要添加或删除关注点，你不用重新编译完整的源代码，而仅仅需要修改配置文件就可以了。\nSpring AOP通过以下两种方式来使用。但是最广泛使用的方式是Spring AspectJ 注解风格(Spring AspectJ Annotation Style)\n使用AspectJ 注解风格 使用Spring XML 配置风格 https://blog.csdn.net/dadiyang/article/details/82920139 2. AOP概念 首先让我们从一些重要的AOP概念和术语开始。这些术语不是Spring特有的。不过AOP术语并不是特别的直观，如果Spring使用自己的术语，将会变得更加令人困惑。\n切面（Aspect）：一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中一个关于横切关注点的很好的例子。在Spring AOP中，切面可以使用基于模式 ）或者基于@Aspect注解 的方式来实现。 连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是表示一个方法的执行。 通知（Advice）：在切面的某个特定的连接点上执行的动作。其中包括了“around”、“before”和“after”等不同类型的通知（通知的类型将在后面部分进行讨论）。许多AOP框架（包括Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 切入点（Pointcut）：匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 引入（Introduction）：用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个bean实现IsModified接口，以便简化缓存机制。 目标对象（Target Object）： 被一个或者多个切面所通知的对象。也被称做*被通知（advised）对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）*对象。 AOP代理（AOP Proxy）：AOP框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）：把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 通知类型：\n前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）。 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知。 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 环绕通知（Around Advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 环绕通知是最常用的通知类型。和AspectJ一样，Spring提供所有类型的通知，我们推荐你使用尽可能简单的通知类型来实现需要的功能。例如，如果你只是需要一个方法的返回值来更新缓存，最好使用后置通知而不是环绕通知，尽管环绕通知也能完成同样的事情。用最合适的通知类型可以使得编程模型变得简单，并且能够避免很多潜在的错误。比如，你不需要在JoinPoint上调用用于环绕通知的proceed()方法，就不会有调用的问题。\n在Spring 2.0中，所有的通知参数都是静态类型，因此你可以使用合适的类型（例如一个方法执行后的返回值类型）作为通知的参数而不是使用Object数组。\n通过切入点匹配连接点的概念是AOP的关键，这使得AOP不同于其它仅仅提供拦截功能的旧技术。 切入点使得通知可以独立对应到面向对象的层次结构中。例如，一个提供声明式事务管理 的环绕通知可以被应用到一组横跨多个对象的方法上（例如服务层的所有业务操作）。\nhttp://shouce.jb51.net/spring/aop.html AOP方式 机制 说明 静态织入 静态代理 直接修改原类，比如编译期生成代理类的 APT 静态织入 自定义类加载器 使用类加载器启动自定义的类加载器，并加一个类加载监听器，监听器发现目标类被加载时就织入切入逻辑，以 Javassist 为代表 动态织入 动态代理 字节码加载后，为接口动态生成代理类，将切面植入到代理类中，以 JDK Proxy 为代表 动态织入 动态字节码生成 字节码加载后，通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用织入逻辑。属于子类代理，以 CGLIB 为代表 会用就行了？你知道 AOP 框架的原理吗？ - 简书 (jianshu.com) 3. Spring AOP两种动态代理实现 为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介的作用。,动态代理则不需要写代理类,直接生成到内存中\nJdk代理：基于接口的代理，一定是基于接口，会生成目标对象的接口的子对象。\nCglib代理：基于类的代理，不需要基于接口，会生成目标对象的子对象。\njdk代理对象实质上是目标对象接口的实现类\nCglib代理对象实质上是目标对象的子类\n对接口创建代理优于对类创建代理，因为会产生更加松耦合的系统，所以spring默认是使用JDK代理。\n对类代理是让遗留系统或无法实现接口的第三方类库同样可以得到通知，这种方式应该是备用方案。\n来源: https://www.zhihu.com/question/34301445 3.1 jdk代理 三、jdk中的动态代理\n1、Java.lang.reflect.Proxy类可以直接生成一个代理对象\nProxy.newProxyInstance(ClassLoader loader, Class\u003c?\u003e[] interfaces, InvocationHandler h)生成一个代理对象\n参数1:ClassLoader loader 代理对象的类加载器 一般使用被代理对象的类加载器 参数2:Class\u003c?\u003e[] interfaces 代理对象的要实现的接口 一般使用的被代理对象实现的接口 参数3:InvocationHandler h (接口)执行处理类 InvocationHandler中的invoke(Object proxy, Method method, Object[] args)方法：调用代理类的任何方法，此方法都会执行\n参数3.1:代理对象(慎用) 参数3.2:当前执行的方法 参数3.3:当前执行的方法运行时传递过来的参数 返回值:当前方法执行的返回值 2、Proxy.newProxyInstance方法参数介绍\nClassLoader：类加载器！\n它是用来加载器的，把.class文件加载到内存，形成Class对象！ Class[]　interfaces：指定要实现的接口们 InvocationHandler：代理对象的所有方法(个别不执行，getClass())都会调用InvocationHandler的invoke()方法。 3、InvocationHandler方法参数介绍\npublic Object invoke(Object proxy, Method method, Object[] args);\n这个invoke()方法在什么时候被调用！在调用代理对象所实现接口中的方法时\nObject proxy：当前对象，即代理对象！在调用谁的方法！ Method method：当前被调用的方法（目标方法） Object[] args：实参！ AOP都把它用成工厂了,把要代理的类放入工厂,在设置工厂里的增强方法,就可以无限增强任意类,简直强无敌\nhttps://www.cnblogs.com/gdwkong/p/8035120.html 为什么jdk动态代理必须基于接口\n原因如下：\n1、生成的代理类继承了Proxy，由于java是单继承，所以只能实现接口，通过接口实现\n2、从代理模式的设计来说，充分利用了java的多态特性，也符合基于接口编码的规范\n当然，jdk在生成代理的参数中也说明了，需要传入对应接口\n3.2 cglib代理 JDK动态代理是基于接口的方式，换句话来说就是代理类和目标类都实现同一个接口，那么代理类和目标类的方法名就一样了，这种方式上一篇说过了；CGLib动态代理是代理类去继承目标类，然后重写其中目标类的方法啊，这样也可以保证代理类拥有目标类的同名方法；\nCGLIB底层：使用字节码处理框架ASM，来转换字节码并生成新的类。\nCGLIB（CODE GENERLIZE LIBRARY）代理是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的所有方法，所以该类或方法不能声明称final的。\n如果目标对象没有实现接口，则默认会采用CGLIB代理；\n如果目标对象实现了接口，可以强制使用java Proxy实现代理。\nhttps://www.cnblogs.com/wyq1995/p/10945034.html https://blog.csdn.net/upxiaofeng/article/details/79035493 启动的时候java Proxy快 ; 执行的时候cglib 快\n这是因为两者实现方式不同, java Proxy是用反射机制,执行的时候才创建出对象; cglib是用字节码方式,编译的时候就弄好了\nSpring AOP中的JDK和CGLib动态代理哪个效率更高？ - 知乎 (zhihu.com) 4. Spring AOP 和AspectJ AspectJ是一套独立的面向切面编程的解决方案, Spring AOP 只是简化的面向切面编程方案\nSpring aop 旨在提供一个跨 Spring IoC 的简单的 aop 实现, 以解决程序员面临的最常见问题。它不打算作为一个完整的 AOP 解决方案 —— 它只能应用于由 Spring 容器管理的 bean。\nAspectJ 是原始的 aop 技术, 目的是提供完整的 aop 解决方案。它更健壮, 但也比 Spring AOP 复杂得多。还值得注意是, AspectJ 可以在所有域对象中应用。\n虽然spring AOP 用了aspectj 的一些注解,但是 具体实现是spring自己写的\nSpring AOP AspectJ 在纯 Java 中实现 使用 Java 编程语言的扩展实现 不需要单独的编译过程 除非设置 LTW，否则需要 AspectJ 编译器 (ajc) 只能使用运行时织入 运行时织入不可用。支持编译时、编译后和加载时织入 功能不强-仅支持方法级编织 更强大 - 可以编织字段、方法、构造函数、静态初始值设定项、最终类/方法等\u0026hellip;\u0026hellip;。 只能在由 Spring 容器管理的 bean 上实现 可以在所有域对象上实现 仅支持方法执行切入点 支持所有切入点 代理是由目标对象创建的, 并且切面应用在这些代理上(动态代理) 在执行应用程序之前 (在运行时) 前, 各方面直接在代码中进行织入(动态字节码技术) 比 AspectJ 慢多了 更好的性能 易于学习和应用 相对于 Spring AOP 来说更复杂 Aspectj与Spring AOP比较 - 简书 (jianshu.com) Spring AOP,AspectJ, CGLIB 有点晕 - 简书 (jianshu.com) spring aop和AspectJ的区别 - 简书 (jianshu.com) 为什么Spring AOP 不像AspectJ一样采用静态织入的方式呢?\n它不是运行时植入,是spring启动时就生成好了字节\n不是它不想用,只是折中之后,成了现在这样\n用AspectJ 要引入acj编译器,让spring就太重了,强依赖AspectJ了\n用agent又不好做,得在启动参数上指定agent,(动态的attch也不好用)\n用Javassist ,它是改类加载器的,也不太好\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/aop.html","summary":"[toc] 1. 描述一下Spring AOP Spring AOP(Aspect Oriented Programming，面向切面编程)是OOPs(面向对象编程)的补充，它也提供了模块化。在面向对象编程中，","title":"aop"},{"content":"[toc]\n1. 前言 MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML(除了数据查询语句)语句，以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。一般来说开启二进制日志大概会有1%的性能损耗(参见MySQL官方中文手册 5.1.24版)。二进制有两个最重要的使用场景: 其一：MySQL Replication在Master端开启binlog，Mster把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 其二：自然就是数据恢复了，通过使用mysqlbinlog工具来使恢复数据。\n二进制日志包括两类文件：\n二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件，\n二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML(除了数据查询语句)语句事件。\nbinlog是一个二进制文件集合，每个binlog文件以一个4字节的魔数开头，接着是一组Events:\n魔数：0xfe62696e对应的是0xfebin；\nEvent：每个Event包含header和data两个部分；header提供了Event的创建时间，哪个服务器等信息，data部分提供的是针对该Event的具体信息，如具体数据的修改；\n第一个Event用于描述binlog文件的格式版本，这个格式就是event写入binlog文件的格式；\n其余的Event按照第一个Event的格式版本写入；\n最后一个Event用于说明下一个binlog文件；\nbinlog的索引文件是一个文本文件，其中内容为当前的binlog文件列表\n当遇到以下3种情况时，MySQL会重新生成一个新的日志文件，文件序号递增：\nMySQL服务器停止或重启时\n使用 flush logs 命令；\n当 binlog 文件大小超过 max_binlog_size 变量的值时；\nmax_binlog_size 的最小值是4096字节，最大值和默认值是 1GB (1073741824字节)。事务被写入到binlog的一个块中，所以它不会在几个二进制日志之间被拆分。\n因此，如果你有很大的事务，为了保证事务的完整性，不可能做切换日志的动作，只能将该事务的日志都记录到当前日志文件中，直到事务结束，你可能会看到binlog文件大于 max_binlog_size 的情况。\n写 Binlog 的时机\n对支持事务的引擎如InnoDB而言，是在prepare完commit前写的。binlog 什么时候刷新到磁盘跟参数 sync_binlog 相关。\n如果设置为0，则表示MySQL不控制binlog的刷新，由文件系统去控制它缓存的刷新；\n如果设置为不为0的值，则表示每 sync_binlog 次事务，MySQL调用文件系统的刷新操作刷新binlog到磁盘中。\n设为1是最安全的，在系统故障时最多丢失一个事务的更新，但是会对性能有所影响。\n如果 sync_binlog=0 或 sync_binlog大于1，当发生电源故障或操作系统崩溃时，可能有一部分已提交但其binlog未被同步到磁盘的事务会被丢失，恢复程序将无法恢复这部分事务。\n在MySQL 5.7.7之前，默认值 sync_binlog 是0，MySQL 5.7.7和更高版本使用默认值1，这是最安全的选择。一般情况下会设置为100或者0，牺牲一定的一致性来获取更好的性能。\n这涉及几个知识点\nredolog\n二段式提交\n当我们更新数据时，两阶段提交的具体流程：\n更新操作先写入redolog，这时候这条log的状态是prepared状态 再将逻辑日志写入binlog 最后在binlog写好之后，把redolog里的这条日志的状态改为commit redolog和undolog：innodb事务日志包括redo log和undo log。redo log是重做日志，提供前滚操作，undo log是回滚日志，提供回滚操作。\n三种日志的写入顺序 : undo log -\u0026gt; redo log(prepare阶段) -\u0026gt; bin log -\u0026gt; commit\n详见：https://www.cnblogs.com/f-ck-need-u/p/9010872.html\n2. 三种模式 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT，在 MySQL 5.7.7 及更高版本中，默认值是 ROW\n2.1 row level (默认) 记录的方式是行，即如果批量修改数据，记录的不是批量修改的SQL语句事件，而是每条记录被更改的SQL语句，因此，ROW模式的binlog日志文件会变得很“重”。\n优点：row level的binlog日志内容会非常清楚的记录下每一行数据被修改的细节。而且不会出现某些特定情况下存储过程或function，以及trigger的调用和触发器无法被正确复制的问题。\n缺点：row level下，所有执行的语句当记录到日志中的时候，都以每行记录的修改来记录，这样可能会产生大量的日志内容，产生的binlog日志量是惊人的。批量修改几百万条数据，那么记录几百万行……\n存的是具体的修改语句\n2.2 Statement level 记录每一条修改数据的SQL语句（批量修改时，记录的不是单条SQL语句，而是批量修改的SQL语句事件）。看上面的图解可以很好的理解row level和statement level两种模式的区别。\n存的是sql 语句\n优点：statement模式记录的更改的SQ语句事件，并非每条更改记录，所以大大减少了binlog日志量，节约磁盘IO，提高性能。\n缺点：statement level下对一些特殊功能的复制效果不是很好，比如：函数、存储过程的复制。由于row level是基于每一行的变化来记录的，所以不会出现类似问题\n行模式和语句模式的区别\n语句模式：\n​\t100万条记录,\t只需1条delete * from test；就可以删除100万条记录\nrow模式\n100万条记录 , 记录100万条删除命令\n2.3 Mixed 实际上就是前两种模式的结合。在Mixed模式下，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。\n为什么不推荐使用mixed模式，理由如下\n假设master有两条记录，而slave只有一条记录。\nmaster的数据为\n+----+------------------------------------------------------+ | id | n | +----+------------------------------------------------------+ | 1 | d24c2c7e-430b-11e7-bf1b-00155d016710 | | 2 | ddd | +----+------------------------------------------------------+ slave的数据为\n+----+-------------------------------------------------------+ | id | n | +----+-------------------------------------------------------+ | 1 | d24c2c7e-430b-11e7-bf1b-00155d016710 | +----+-------------------------------------------------------+ 当在master上更新一条从库不存在的记录时，也就是id=2的记录，你会发现master是可以执行成功的。而slave拿到这个SQL后，也会照常执行，不报任何异常，只是更新操作不影响行数而已。并且你执行命令show slave status，查看输出，你会发现没有异常。但是，如果你是row模式，由于这行根本不存在，是会报1062错误的。\n3 查看与配置 配置:\n[mysqld] #设置日志路径，注意路经需要mysql用户有权限写,这里可以写绝对路径,也可以直接写mysql-bin(后者默认就是在/var/lib/mysql目录下) log-bin = /data/mysql/logs/mysql-bin.log #配置serverid server-id=1 #设置日志三种格式：STATEMENT、ROW、MIXED 。 binlog_format = mixed #设置binlog清理时间 expire_logs_days = 7 #binlog每个日志文件大小 max_binlog_size = 100m #binlog缓存大小 binlog_cache_size = 4m #最大binlog缓存大小 max_binlog_cache_size = 512m // 配置前两个亦可 查看biglog是否开启\nSHOW VARIABLES LIKE 'log_bin';\n查看binlog模式\nshow global variables like \u0026quot;binlog%\u0026quot;;\nshow binlog events; #只查看第一个binlog文件的内容 show binlog events in \u0026#39;mysql-bin.000002\u0026#39;;#查看指定binlog文件的内容 show binary logs; #获取binlog文件列表 show master status； #查看当前正在写入的binlog文件 show variables like \u0026#39;%binlog_format%\u0026#39;; # 查看日志格式 mysqlbinlog工具\n/usr/local/mysql/bin/mysqlbinlog --start-datetime=\u0026quot;2013-03-01 00:00:00\u0026quot; --stop-datetime=\u0026quot;2014-03-21 23:59:59\u0026quot; /usr/local/mysql/var/mysql-bin.000007 -r test2.sql\n-r 输出到文件\n-v / -vv 查看binlog是row格式 -vv 展示更详细的sql信息\n-d 指定库名\n-h 指定ip\n-P 端口\n-u 指定用户名\n-p 指定密码\n\u0026ndash;server-id= 指定sever-id\n当前查询binlog时一个事务日志被截断了,使用 --include-gtids 指定事务标识\ngtid（Global Transaction ID） : 全局事务id; 组成: server_uuid：transaction_id\n输出案例\n/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/; /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/; DELIMITER /*!*/; # at 4 #190308 10:05:03 server id 1 end_log_pos 123 CRC32 0xff02e23d Start: binlog v 4, server v 5.7.22-log created 190308 10:05:03 # Warning: this binlog is either in use or was not closed properly. # at 123 #190308 10:05:03 server id 1 end_log_pos 154 CRC32 0xb81da4c5 Previous-GTIDs # [empty] # at 154 #190308 10:05:09 server id 1 end_log_pos 219 CRC32 0xfb30d42c Anonymous_GTID last_committed=0 sequence_number=1 rbr_only=yes /*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/; SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39;/*!*/; # at 219 ... ... # at 21019 #190308 10:10:09 server id 1 end_log_pos 21094 CRC32 0x7a405abc Query thread_id=113 exec_time=0 error_code=0 SET TIMESTAMP=1552011009/*!*/; BEGIN /*!*/; # at 21094 #190308 10:10:09 server id 1 end_log_pos 21161 CRC32 0xdb7a2b35 Table_map: `maxwell`.`positions` mapped to number 110 # at 21161 #190308 10:10:09 server id 1 end_log_pos 21275 CRC32 0xec3be372 Update_rows: table id 110 flags: STMT_END_F ### UPDATE `maxwell`.`positions` ### WHERE ### @1=1 ### @2=\u0026#39;master.000003\u0026#39; ### @3=20262 ### @4=NULL ### @5=\u0026#39;maxwell\u0026#39; ### @6=NULL ### @7=1552011005707 ### SET ### @1=1 ### @2=\u0026#39;master.000003\u0026#39; ### @3=20923 ### @4=NULL ### @5=\u0026#39;maxwell\u0026#39; ### @6=NULL ### @7=1552011009790 # at 21275 #190308 10:10:09 server id 1 end_log_pos 21306 CRC32 0xe6c4346d Xid = 13088 COMMIT/*!*/; SET @@SESSION.GTID_NEXT= \u0026#39;AUTOMATIC\u0026#39; /* added by mysqlbinlog */ /*!*/; DELIMITER ; # End of log file /*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 比较关注的就是里面的修改类语句\n截取其中的一段进行分析：\n# at 21019 #190308 10:10:09 server id 1 end_log_pos 21094 CRC32 0x7a405abc Query thread_id=113 exec_time=0 error_code=0 SET TIMESTAMP=1552011009/*!*/; BEGIN /*!*/; 上面输出包括信息：\nposition: 位于文件中的位置，即第一行的（# at 21019）,说明该事件记录从文件第21019个字节开始\ntimestamp: 事件发生的时间戳，即第二行的（#190308 10:10:09）\nserver id: 服务器标识（1）\nend_log_pos 表示下一个事件开始的位置（即当前事件的结束位置+1）\nthread_id: 执行该事件的线程id （thread_id=113）\nexec_time: 事件执行的花费时间\nerror_code: 错误码，0意味着没有发生错误\ntype:事件类型Query\n一些常见的事件类型\n事件类型 说明 QUERY_EVENT 执行更新语句时会生成此事件，包括：create，insert，update，delete； STOP_EVENT 当mysqld停止时生成此事件 ROTATE_EVENT 当mysqld切换到新的binlog文件生成此事件，切换到新的binlog文件可以通过执行flush logs命令或者binlog文件大于 max_binlog_size 参数配置的大小； INTVAR_EVENT 当sql语句中使用了AUTO_INCREMENT的字段或者LAST_INSERT_ID()函数；此事件没有被用在binlog_format为ROW模式的情况下 XID_EVENT 支持XA的存储引擎才有，本地测试的数据库存储引擎是innodb，所有上面出现了XID_EVENT；innodb事务提交产生了QUERY_EVENT的BEGIN声明，QUERY_EVENT以及COMMIT声明，如果是myIsam存储引擎也会有BEGIN和COMMIT声明，只是COMMIT类型不是XID_EVENT TABLE_MAP_EVENT 用在binlog_format为ROW模式下，将表的定义映射到一个数字，在行操作事件之前记录（包括：WRITE_ROWS_EVENT，UPDATE_ROWS_EVENT，DELETE_ROWS_EVENT） WRITE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 insert 操作 UPDATE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 update 操作 DELETE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 delete 操作 参考链接\nbinlog配置 binlog三种模式-博客园 查询binlog日志 mysql-binlog binlog和redolog ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/binlog.html","summary":"[toc] 1. 前言 MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML(除了数据查询语句)语句，以事件形式记录，还包含","title":"binlog"},{"content":"搭了config后,bus就很简单了\nhttps://www.jianshu.com/p/e48de30aab76 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/bus.html","summary":"搭了config后,bus就很简单了 https://www.jianshu.com/p/e48de30aab76","title":"Bus"},{"content":"centos\n6. 更换yum源 #下载wget yum install wget -y #备份原来的yum mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup #下载yum wget http://mirrors.163.com/.help/CentOS7-Base-163.repo #阿里的yum wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #更新缓存 yum makecache #查询源 yum -y update 来自 https://www.cnblogs.com/xjh713/p/7458437.html 安装自动提示包\nyum install -y bash-completion\n​ ​\n把某个命令做成系统服务\n可以把一些手动安装的应用, 统一纳入系统管理, 可以使用 systemctl 命令\n方案一:\n此方案是老的linux支持的方式, 不太建议\n在目录etc/init.d 下创建一个可执行文件 /redisd, redisd可以从redis中复制 cp /opt/software/redis-4.0.11/utils/redis_init_script , /etc/init.d/redisd , 比如nacos没有类似文件, 可以自己创建一个, 里面写执行命令(当成写一个sh脚本)\n在文件中加入如下命令 (别看它被注释了, 但是系统会读它)\n# chkconfig: 2345 10 90 # description: Start and Stop redisd (这里是服务的描述, 可以随便写) 增加可执行权限, chmod +x /etc/init.d/redisd\n加入系统服务, chkconfig --add redisd\n就可以用systemctl命令了, systemctl start redisd\n方案二\n新建执行文件, vim /lib/systemd/system/nacos.service\n添加内容\n[Unit] Description=nacos After=network.target [Service] Type=forking Environment=\u0026#34;JAVA_HOME=/usr/local/jdk1.8\u0026#34; #改成自己的jdk路径，因为服务脚本的环境和系统环境变量不能共享，所以还得设置才能生效。 ExecStart=/home/nacos/nacos/bin/startup.sh -m standalone #standalone 是单机，默认是集群cluster ExecReload=/home/nacos/nacos/bin/shutdown.sh \u0026amp;\u0026amp; /home/nacos/nacos/bin/startup.sh ExecStop=/home/nacos/nacos/bin/shutdown.sh PrivateTmp=true [Install] WantedBy=multi-user.target 就可以用systemctl命令了, systemctl start nacos\nLinux启动流程和服务管理(init.d和systemd)_init.d systemd_sunboychenll的博客-CSDN博客 可能是史上最全面易懂的 Systemd 服务管理教程！( 强烈建议收藏 )-腾讯云开发者社区-腾讯云 (tencent.com) ","permalink":"https://xiaokunji.com/zh/linux%E7%B3%BB%E7%BB%9F/centos/centos%E5%91%BD%E4%BB%A4.html","summary":"centos 6. 更换yum源 #下载wget yum install wget -y #备份原来的yum mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup #下载yum wget http://mirrors.163.com/.help/CentOS7-Base-163.repo #阿里的yum wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #更新缓存 yum makecache #查询源 yum -y update 来自 https://www.cnblogs.com/xjh713/p/7458437.html 安装自","title":"centos命令"},{"content":"[toc]\n介绍 可以理解为配置中心,把配置放在某个地方(jdbc,git,svn,vault),统一管理.\n和注册中心类似,分为客户端和服务端,服务端就是提供配置的,客户端就获取配置, 仅SpringCloudConfig不支持自动动态获取配置,需要结合bus(总线)来实现才行 使用配置中心后,会优先使用配置中心的地址,一旦配置中心连不通才会使用本地配置文件 以后再补充吧 使用 1. 基本使用 以snv为例\n服务端:\npom.xml\n\u0026lt;!-- 基础配置 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-config-server\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- svn --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.tmatesoft.svnkit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;svnkit\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动主类:\n@SpringBootApplication @EnableConfigServer //表示使用配置中心 public class SpringBootTestConfigApplication { public static void main(String[] args) { SpringApplication.run(SpringBootTestConfigApplication.class, args); } } application.properties\nserver.port=8999 server.servlet.context-path=/test spring.cloud.config.server.svn.uri=http://10.101.222.10/jht/标准项目/JSCP/code/trunk/jportal/jportal-server/src/main/ spring.cloud.config.server.svn.username=xiaokunqi spring.cloud.config.server.svn.password=xiaokunqi spring.cloud.config.server.default-label=resources # 具体某个文件夹下 spring.profiles.active=subversion 使用网址可以访问到配置: http://localhost:8999/test/application/jdbc 2. 客户端 pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 增加一个文件 bootstrap.properties\n#配置中心(服务端)地址+名字 spring.cloud.config.uri=http://localhost:8999/test spring.cloud.config.name=${spring.application.name} #取当前应用的名字 spring.cloud.config.profile=jdbc #查询格式: name-profile.properties = jportal-jdbc.properties bootstrap.properties 优先级会大于application.properties,或者说config只会读bootstrap.properties,我写在application中没有生效\n启动 先启动配置中心再启动客户端\n2. 使用jdbc配置 在生产中使用svn不太好,使用jdbc更好,就是把配置文件放在数据库中,config会默认使用\nSELECT KEY, VALUE from PROPERTIES where APPLICATION=? and PROFILE=? and LABEL=?\n查询数据,但是key和vaule在mysql中是关键字,如果使用默认sql,则如此:\nSELECT `KEY`, `VALUE` from PROPERTIES where APPLICATION=? and PROFILE=? and LABEL=? 有application,profile,label三个字段唯一标识一个文件(文件中就可以有很多配置了)\n客户端 bootstrap.properties #在基础配置中增加,这个配置在基础配置中可以不写 spring.cloud.config.label=dev # 可以理解为分支,其他数据一样,写成test,就能读取数据库中test的配置文件,就可以做到配置文件的切换了 服务端 pom.xml \u0026lt;!-- MYSQL --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; application.properties\nspring.profiles.active=jdbc spring.cloud.config.server.jdbc.sql=select keye , valuee from config_info where application=? and profile = ? and lable = ? # 指定sql,这里换了个名字,加了e, spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://10.10.203.10:3306/jplatdv?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;allowMultiQueries=true spring.datasource.username=test spring.datasource.password=Jht123456 spring.datasource.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.initialSize=5 spring.datasource.minIdle=5 spring.datasource.maxActive=20 spring.datasource.maxWait=60000 spring.cloud.config.server.jdbc=true 在数据库中创建表,用来存储配置\nCREATE TABLE config_info ( id varchar(255) NULL DEFAULT NULL, keye varchar(255) NULL DEFAULT NULL, valuee varchar(255) NULL DEFAULT NULL, application varchar(255) NULL DEFAULT NULL, profile varchar(255) NULL DEFAULT NULL, lable varchar(255) NULL DEFAULT NULL ) 插入一些例子\nINSERT INTO config_info VALUES (\u0026#39;1\u0026#39;, \u0026#39;spring.datasource.driver-class-name\u0026#39;, \u0026#39;com.mysql.jdbc.Driver\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;2\u0026#39;, \u0026#39;spring.datasource.url\u0026#39;, \u0026#39;jdbc:mysql://10.10.203.10:3306/jplatdv?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;allowMultiQueries=true\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;3\u0026#39;, \u0026#39;spring.datasource.username\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;4\u0026#39;, \u0026#39;spring.datasource.password\u0026#39;, \u0026#39;Jht123456\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;5\u0026#39;, \u0026#39;spring.datasource.type\u0026#39;, \u0026#39;com.alibaba.druid.pool.DruidDataSource\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;6\u0026#39;, \u0026#39;spring.datasource.initialSize\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;7\u0026#39;, \u0026#39;spring.datasource.minIdle\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;8\u0026#39;, \u0026#39;spring.datasource.maxActive\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;9\u0026#39;, \u0026#39;mybatis.mapper-locations\u0026#39;, \u0026#39;classpath*:mapper/mysql/*Mapper.xml\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jdbc\u0026#39;, \u0026#39;resource\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;10\u0026#39;, \u0026#39;jportal.server.name\u0026#39;, \u0026#39;jportal-server-xkj\u0026#39;, \u0026#39;jportal\u0026#39;, \u0026#39;constants\u0026#39;, \u0026#39;dev\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;11\u0026#39;, \u0026#39;business.server.name\u0026#39;, \u0026#39;business-server-xkj\u0026#39;, \u0026#39;jportal\u0026#39;, \u0026#39;constants\u0026#39;, \u0026#39;dev\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;12\u0026#39;, \u0026#39;logger.server.name\u0026#39;, \u0026#39;logger-server-xkj\u0026#39;, \u0026#39;jportal\u0026#39;, \u0026#39;constants\u0026#39;, \u0026#39;dev\u0026#39;); INSERT INTO config_info VALUES (\u0026#39;13\u0026#39;, \u0026#39;RAND_CODE_STRING\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;jportal\u0026#39;, \u0026#39;constants\u0026#39;, \u0026#39;dev\u0026#39;); 访问网址:http://localhost:8999/test/jportal-server-xkj/jdbc/dev就能看到配置了\n3.将配置中心加到注册中心 将配置中心加到注册中心,这样可以实现配置中心高可用,客户端还不用根据ip和端口,可以通过注册中心找到服务\n服务端 pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-eureka\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; application.properties\n#注册中心地址 eureka.client.serviceUrl.defaultZone=http://10.101.90.171:10001/eureka/ 客户端 注释掉spring.cloud.config.uri,因为不需要指定了. 添加如下配置: spring.cloud.config.discovery.enabled=true spring.cloud.config.discovery.serviceId=test // 配置中心的名字 eureka.client.serviceUrl.defaultZone=http://10.101.90.171:10001/eureka/ 有找到特别适用的场景,即使可以动态获取配置,感觉也很鸡肋,哪会轻易动配置,公司想要的场景是,开发-测试-运维能共同使用的配置中心,配置大家都能看到并使用,遗弃目前的人工交接配置\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/config.html","summary":"[toc] 介绍 可以理解为配置中心,把配置放在某个地方(jdbc,git,svn,vault),统一管理. 和注册中心类似,分为客户端和服务端,服务端就","title":"Config"},{"content":"[toc]\n一、前言 MySQL 保证数据不会丢的能力主要体现在两方面：\n能够恢复到任何时间点的状态； 能够保证MySQL在任何时间段突然奔溃，重启后之前提交的记录都不会丢失； 对于第一点将MySQL恢复到任何时间点的状态，相信很多人都知道，只要保留有足够的binlog，就能通过重跑binlog来实现。\n对于第二点的能力，也就是本文标题所讲的crash-safe。即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和unod log两个日志。\n因为crash-safe主要体现在事务执行过程中突然奔溃，重启后能保证事务完整性，所以在讲解具体原理之前，先了解下MySQL事务执行有哪些关键阶段，后面才能依据这几个阶段来进行解析。下面以一条更新语句的执行流程为例，话不多说，直接上图：\n从上图可以清晰地看出一条更新语句在MySQL中是怎么执行的，简单进行总结一下：\n从内存中找出这条数据记录，对其进行更新； 将对数据页的更改记录到redo log中； 将逻辑操作记录到binlog中； 对于内存中的数据和日志，都是由后台线程，当触发到落盘规则后再异步进行刷盘； 二、WAL机制 问题：为什么不直接更改磁盘中的数据，而要在内存中更改，然后还需要写日志，最后再落盘这么复杂？\nMySQL更改数据的时候，之所以不直接写磁盘文件中的数据，最主要就是性能问题。因为直接写磁盘文件是随机写，开销大性能低，没办法满足MySQL的性能要求。所以才会设计成先在内存中对数据进行更改，再异步落盘。但是内存总是不可靠，万一断电重启，还没来得及落盘的内存数据就会丢失，所以还需要加上写日志这个步骤，万一断电重启，还能通过日志中的记录进行恢复。\n写日志虽然也是写磁盘，但是它是顺序写，相比随机写开销更小，能提升语句执行的性能\n这个技术就是大多数存储系统基本都会用的WAL(Write Ahead Log)技术，也称为日志先行的技术，指的是对数据文件进行修改前，必须将修改先记录日志。保证了数据一致性和持久性，并且提升语句执行性能。\n三、核心日志模块 更新SQL执行过程中，总共涉及MySQL日志模块其中的三个核心日志，分别是redo log（重做日志）、undo log（回滚日志）、binlog（归档日志）。crash-safe的能力主要依赖的就是这三大日志。\n3.1、重做日志 redo log redo log也称为事务日志，由InnoDB存储引擎层产生。记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置，因为修改会覆盖之前的）。\n前面提到的WAL技术，redo log就是WAL的典型应用，MySQL在有事务提交对数据进行更改时，只会在内存中修改对应的数据页和记录redo log日志，完成后即表示事务提交成功，至于磁盘数据文件的更新则由后台线程异步处理。由于redo log的加入，保证了MySQL数据一致性和持久性（即使数据刷盘之前MySQL奔溃了，重启后仍然能通过redo log里的更改记录进行重放，重新刷盘），此外还能提升语句的执行性能（写redo log是顺序写），由此可见redo log是必不可少的。\nredo log是固定大小的，所以只能循环写，从头开始写，写到末尾就又回到开头，相当于一个环形。当日志写满了，就需要对旧的记录进行擦除，但在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。在redo log满了到擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求，所以有可能会导致MySQL卡顿。（所以针对并发量大的系统，适当设置redo log的文件大小非常重要！！！）\n3.2、回滚日志 undo log undo log顾名思义，主要就是提供了回滚的作用和多个行版本控制(MVCC)，保证事务的原子性。在数据修改的流程中，会记录一条与当前操作相反的逻辑日志到undo log中（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录），如果因为某些原因导致事务异常失败了，可以借助该undo log进行回滚，保证事务的完整性，所以undo log也必不可少。\n3.3、归档日志 binlog binlog在MySQL的server层产生，不属于任何引擎，主要记录用户对数据库操作的SQL语句（除了查询语句）。之所以将binlog称为归档日志，是因为binlog不会像redo log一样擦掉之前的记录循环写，而是一直记录（超过有效期(默认不删除)才会被清理），如果超过单日志的最大值（默认1G，可以通过变量 max_binlog_size 设置），则会新起一个文件继续记录。但由于日志可能是基于事务来记录的(如InnoDB表类型)，而事务是绝对不可能也不应该跨文件记录的，如果正好binlog日志文件达到了最大值但事务还没有提交则不会切换新的文件记录，而是继续增大日志，所以 max_binlog_size 指定的值和实际的binlog日志大小不一定相等。\n正是由于binlog有归档的作用，所以binlog主要用作主从同步和数据库基于时间点的还原。\n四、两阶段提交 问题：为什么redo log要分两步写，中间再穿插写binlog呢？\n从上面可以看出，因为redo log影响主库的数据，binlog影响从库的数据，所以redo log和binlog必须保持一致才能保证主从数据一致，这是前提。\nredo log和binlog其实就是很典型的分布式事务场景，因为两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理。而将redo log分成了两步，其实就是使用了两阶段提交协议（Two-phase Commit，2PC）。从图中可看出，事务的提交过程有两个阶段，就是将redo log的写入拆成了两个步骤：prepare和commit，中间再穿插写入binlog。\n两阶段提交虽然能够保证单事务两个日志的内容一致，但在多事务的情况下，却不能保证两者的提交顺序一致，比如下面这个例子，假设现在有3个事务同时提交：\nT1 (--prepare--binlog---------------------commit) T2 (-----prepare-----binlog----commit) T3 (--------prepare-------binlog------commit) 解析： redo log prepare的顺序：T1 --》T2 --》T3 binlog的写入顺序：T1 --》 T2 --》T3 redo log commit的顺序：T2 --》 T3 --》T1 结论：由于binlog写入的顺序和redo log提交结束的顺序不一致，导致binlog和redo log所记录的事务提交结束的顺序不一样，最终导致的结果就是主从数据不一致。 因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。所以在早期的MySQL版本中，通过使用prepare_commit_mutex锁来保证事务提交的顺序，在一个事务获取到锁时才能进入prepare，一直到commit结束才能释放锁，下个事务才可以继续进行prepare操作。\n通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。除了锁的竞争会影响到性能之外，还有一个对性能影响更大的点，就是每个事务提交都会进行两次fsync（写磁盘），一次是redo log落盘，另一次是binlog落盘。\n五、组提交 问题：针对通过在两阶段提交中加锁控制事务提交顺序这种实现方式遇到的性能瓶颈问题，有没有更好的解决方案呢？\n答案自然是有的，在MySQL 5.6 就引入了binlog组提交，即BLGC（Binary Log Group Commit）。binlog组提交的基本思想是，引入队列机制保证InnoDB commit顺序与binlog落盘顺序一致，并将事务分组，组内的binlog刷盘动作交给一个事务进行，实现组提交目的。具体如图：\n第一阶段（prepare阶段）：\n持有prepare_commit_mutex，并且write/fsync redo log到磁盘，设置为prepared状态，完成后就释放prepare_commit_mutex，binlog不作任何操作。\n第二个阶段（commit阶段）：这里拆分成了三步，每一步的任务分配给一个专门的线程处理：\nFlush Stage（写入binlog缓存）\n① 持有Lock_log mutex [leader持有，follower等待]\n② 获取队列中的一组binlog(队列中的所有事务)\n③ 写入binlog缓存\nSync Stage（将binlog落盘）\n①释放Lock_log mutex，持有Lock_sync mutex[leader持有，follower等待]\n②将一组binlog落盘（fsync动作，最耗时，假设sync_binlog为1）。\nCommit Stage（InnoDB commit，清楚undo信息）\n①释放Lock_sync mutex，持有Lock_commit mutex[leader持有，follower等待]\n② 遍历队列中的事务，逐一进行InnoDB commit\n③ 释放Lock_commit mutex\n六、数据恢复流程 问题：假设事务提交过程中，MySQL进程突然奔溃，重启后是怎么保证数据不丢失的？\n下图就是MySQL重启后，提供服务前会先做的事 \u0026ndash; 恢复数据的流程：\n对上图进行简单描述就是：奔溃重启后会检查redo log中是完整并且处于prepare状态的事务，然后根据XID（事务ID），从binlog中找到对应的事务，如果找不到，则回滚；找到并且事务完整则重新commit redo log，完成事务的提交。\n下面我们根据事务提交流程，在不同的阶段时刻，看看MySQL突然奔溃后，按照上述流程是如何恢复数据的。\n时刻A（刚在内存中更改完数据页，还没有开始写redo log的时候奔溃）：\n因为内存中的脏页还没刷盘，也没有写redo log和binlog，即这个事务还没有开始提交，所以奔溃恢复跟该事务没有关系；\n时刻B（正在写redo log或者已经写完redo log并且落盘后，处于prepare状态，还没有开始写binlog的时候奔溃）：\n恢复后会判断redo log的事务是不是完整的，如果不是则根据undo log回滚；如果是完整的并且是prepare状态，则进一步判断对应的事务binlog是不是完整的，如果不完整则一样根据undo log进行回滚；\n时刻C（正在写binlog或者已经写完binlog并且落盘了，还没有开始commit redo log的时候奔溃）：\n恢复后会跟时刻B一样，先检查redo log中是完整并且处于prepare状态的事务，然后判断对应的事务binlog是不是完整的，如果不完整则一样根据undo log回滚，完整则重新commit redo log；\n时刻D（正在commit redo log或者事务已经提交完的时候，还没有反馈成功给客户端的时候奔溃）：\n恢复后跟时刻C基本一样，都会对照redo log和binlog的事务完整性，来确认是回滚还是重新提交。\nMySQL 的 crash-safe 原理解析 (qq.com) MySQL slave 的 crash safe (youzan.com) mysql 几种日志-Java架构师必看 (javajgs.com) ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/crash-safe%E5%8E%9F%E7%90%86.html","summary":"[toc] 一、前言 MySQL 保证数据不会丢的能力主要体现在两方面： 能够恢复到任何时间点的状态； 能够保证MySQL在任何时间段突然奔溃，重启后之前提交的记录都","title":"crash-safe原理"},{"content":"[toc]\n前言 DDL(Data Definition Language) 众所周知，DDL定义了数据在数据库中的结构、关系以及权限等。比如CREATE，ALTER，DROP等等。\n几种算法实现 分别是：copy、inplace、instant。\ncopy 算法为最古老的算法，在 MySQL 5.5 及以下为默认算法。 从 MySQL 5.6 开始，引入了 inplace 算法并且默认使用。inplace 算法还包含两种类型：rebuild-table 和 not-rebuild-table。MySQL 使用 inplace 算法时，会自动判断，能使用 not-rebuild-table 的情况下会尽量使用，不能的时候才会使用 rebuild-table。当 DDL 涉及到主键和全文索引相关的操作时，无法使用 not-rebuild-table，必须使用 rebuild-table。其他情况下都会使用 not-rebuild-table。 从 MySQL 8.0.12 开始，引入了 instant 算法并且默认使用。目前 instant 算法只支持增加列等少量 DDL 类型的操作，其他类型仍然会默认使用 inplace。 copy 算法 较简单的实现方法，MySQL 会建立一个新的临时表，把源表的所有数据写入到临时表，在此期间无法对源表进行数据写入。MySQL 在完成临时表的写入之后，用临时表替换掉源表。这个算法主要被早期（\u0026lt;=5.5）版本所使用。\ninplace 算法 从 5.6 开始，常用的 DDL 都默认使用这个算法。inplace 算法包含两类：inplace-no-rebuild 和 inplace-rebuild，两者的主要差异在于是否需要重建源表。\ninplace 算法的操作阶段主要分为三个：\nPrepare阶段：- 创建新的临时 frm 文件(与 InnoDB 无关)。- 持有 EXCLUSIVE-MDL 锁，禁止读写。- 根据 alter 类型，确定执行方式（copy，online-rebuild，online-not-rebuild）。更新数据字典的内存对象。- 分配 row_log 对象记录数据变更的增量（仅 rebuild 类型需要）。- 生成新的临时ibd文件 new_table（仅rebuild类型需要）。 Execute 阶段：降级EXCLUSIVE-MDL锁，允许读写。扫描old_table聚集索引（主键）中的每一条记录 rec。遍历new_table的聚集索引和二级索引，逐一处理。根据 rec 构造对应的索引项。将构造索引项插入 sort_buffer 块排序。将 sort_buffer 块更新到 new_table 的索引上。记录 online-ddl 执行过程中产生的增量（仅 rebuild 类型需要）。重放 row_log 中的操作到 new_table 的索引上（not-rebuild 数据是在原表上更新）。重放 row_log 中的DML操作到 new_table 的数据行上。 Commit阶段：当前 Block 为 row_log 最后一个时，禁止读写，升级到 EXCLUSIVE-MDL 锁。重做 row_log 中最后一部分增量。更新 innodb 的数据字典表。提交事务（刷事务的 redo 日志）。修改统计信息。rename 临时 ibd 文件，frm文件。变更完成，释放 EXCLUSIVE-MDL 锁。 instant 算法 MySQL 8.0.12 才提出的新算法，目前只支持添加列等少量操作，利用 8.0 新的表结构设计，可以直接修改表的 metadata 数据，省掉了 rebuild 的过程，极大的缩短了 DDL 语句的执行时间。\n基于 instant算法 , 可以让 mysql大表加字段做到秒级, 案例:\nALTER TABLE person ADD COLUMN age INT ALGORITHM=instant;\nMySQL DDL实现机制和分类 - 知乎 (zhihu.com) MySQL 8.0.19亿级数据如何秒速增加字段？ - 腾讯云开发者社区-腾讯云 (tencent.com) ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/ddl%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%E5%92%8C%E5%88%86%E7%B1%BB.html","summary":"[toc] 前言 DDL(Data Definition Language) 众所周知，DDL定义了数据在数据库中的结构、关系以及权限等。比如CREATE，ALTER，DROP等等。 几种算法实现 分别是：co","title":"DDL实现机制和分类"},{"content":"[toc]\n1. 介绍 Docker Swarm 是一款用来管理多主机上的Docker容器的工具，可以负责帮你启动容器，监控容器状态，如果容器的状态不正常它会帮你重新帮你启动一个新的容器，来提供服务，同时也提供服务之间的负载均衡，而这些东西Docker-Compose 是做不到的\nKubernetes它本身的角色定位是和Docker Swarm 是一样的，也就是说他们负责的工作在容器领域来说是相同的部分，都是一个跨主机的容器管理平台，当然也有自己一些不一样的特点，k8s是谷歌公司根据自身的多年的运维经验研发的一款容器管理平台。而Docker Swarm则是由Docker 公司研发的。现在一般都用Kubernetes\ndocker、docker-compose、docker swarm和k8s的区别_似水流年的博客-CSDN博客 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/docker/docker_swarm.html","summary":"[toc] 1. 介绍 Docker Swarm 是一款用来管理多主机上的Docker容器的工具，可以负责帮你启动容器，监控容器状态，如果容器的状态不正常它会帮你重新帮你启动一个","title":"Docker_Swarm"},{"content":"[toc]\n1. 介绍 Docker-Compose 是用来管理你的容器的，有点像一个容器的管家，想象一下当你的Docker中有成百上千的容器需要启动，如果一个一个的启动那得多费时间。有了Docker-Compose你只需要编写一个文件，在这个文件里面声明好要启动的容器，配置一些参数，执行一下这个文件，Docker就会按照你声明的配置去把所有的容器启动起来，只需docker-compose up即可启动所有的容器，但是Docker-Compose只能管理当前主机上的Docker，也就是说不能去启动其他主机上的Docker容器\ndocker、docker-compose、docker swarm和k8s的区别_似水流年的博客-CSDN博客 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/docker/docker-compose.html","summary":"[toc] 1. 介绍 Docker-Compose 是用来管理你的容器的，有点像一个容器的管家，想象一下当你的Docker中有成百上千的容器需要启动，如果一个一个的启动那得多费时间。","title":"Docker-Compose"},{"content":"[toc]\n一.简易操作 1.安装docker,jdk 2.制作项目镜像 1.项目用mvn打包即可, (mvn install) 项目\n2.在 项目.jar 包同一个目录下新建(放一起方便管理嘛)\n3.Dockerfile文件,并写入:\n# Docker image for springboot file run # VERSION 0.0.1 # 基础镜像使用java # 下载jdk8 ,没有才下载 FROM java:8 # VOLUME 指定了临时文件目录为/tmp。 # 其效果是在主机 /var/lib/docker 目录下创建了一个临时文件，并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名为app.jar ADD SpringBootTest-0.0.1-SNAPSHOT.jar app.jar # 运行jar包 RUN bash -c \u0026#39;touch /app.jar\u0026#39; ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] 来自 https://blog.csdn.net/Sirius_hly/article/details/83685256 编译镜像 docker build -t spring-boot-docker . #末尾的点代表文件所在目录执行,指上下文目录,应该是可以指定目录的,虽然没试过 docker images # 可以查看制作好的镜像\n可以上传镜像,搭环境时只需要pull就行\n3.安装Mysql docker run -it --rm --name mysql -v /usr/conf.d:/usr/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -p 3306:3306 -d mysql 注: -i -t : 大致是运行镜像后能接受输入流(就是能往镜像里输东西,等会要初始化数据嘛) --rm : 如果名字叫mysql 的已存在,则删除 -p : 宿主端口:镜像端口 , 即 : 虚拟机的3306端口 对应到 镜像里的3306端口 -d : 后台启动并返回镜像ID -v : 本地文件夹:镜像文件夹, 将本地文件夹映射到镜像里,这样就能是使用外部配置文件了 4.配置数据库 #进入mysql容器 docker exec -it mysql bash\n#输入用户名密码 mysql -u root -p123456\n#设置外部网络访问mysql权限 ALTER user \u0026lsquo;root\u0026rsquo;@\u0026rsquo;%\u0026rsquo; IDENTIFIED WITH mysql_native_password BY \u0026lsquo;123456\u0026rsquo;; FLUSH PRIVILEGES;\n#创建用户 CREATE USER \u0026rsquo;test\u0026rsquo;@\u0026rsquo;%\u0026rsquo; IDENTIFIED BY \u0026lsquo;Jht123456\u0026rsquo;; #授权 GRANT ALL ON . TO \u0026rsquo;test\u0026rsquo;@\u0026rsquo;%\u0026rsquo;; FLUSH PRIVILEGES;\n#切换用户(不切换也行,貌似数据是共通的) exit \u0026amp;\u0026amp; exit mysql -u test -pJht123456\n#创建数据库 CREATE DATABASE IF NOT EXISTS jplatdvv default charset utf8;\n#切换数据库 use jplatdvv\n#创建表和插入数据 (linux 下mysql是大小写敏感的,可以在配置文件中指定不敏感) create table jpf_help_info\u0026hellip;..(略)\n5.启动项目 docker run \u0026ndash;name spring-boot-docker -d -p 8080:8080 \u0026ndash;link mysql:mysql spring-boot-docker # \u0026ndash;link 将两个镜像连接起来, mysql使用了别名(冒号后面是别名,虽然是本身),这样在数据库url中不用写ip,直接写mysql\n这样就能访问了,\nhttp://192.168.142.128:8998/test/getHelp\nSpringBootTest.zip 模拟代码 使用docker-compose工具能更好的使用docker\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/docker/docker+springboot.html","summary":"[toc] 一.简易操作 1.安装docker,jdk 2.制作项目镜像 1.项目用mvn打包即可, (mvn install) 项目 2.在 项目.jar 包同一个目录下新建(放一起方便","title":"docker+SpringBoot"},{"content":"[TOC]\n前言 到目前为止，ES有4种客户端，分别是：Jest client、Rest client、Transport client、Node client，相信大家在项目集成中选择客户端比较纠结，搜索案例的时候一会是这个客户端实现的，一会儿又是别的客户端实现的，自己又不了解每个客户端的优劣势，但又想集成最好的，下面就来说说各个客户端的区别，以及优劣势\nES支持两种协议\nHTTP协议，支持的客户端有Jest client和Rest client Native Elasticsearch binary协议，也就是Transport client和Node client Jest client和Rest client区别\n​ Jest client非官方支持，在ES5.0之前官方提供的客户端只有Transport client、Node client。在5.0之后官方发布Rest client，并大力推荐\nTransport client和Node client区别\n​ Transport client（7.0弃用）和Node client（2.3弃用）区别：最早的两个客户端，Transport client是不需要单独一个节点。Node client需要单独建立一个节点，连接该节点进行操作，ES2.3之前有独立的API，ES2.3之后弃用该API，推荐用户创建一个节点，并用Transport client连接进行操作\n综合：以上就是各个客户端现在的基本情况，可以看出Rest client目前是官方推荐的，但是springBoot默认支持的依然Transport client，这可能和ES更新速度有关\nSpringboot 2.3.x版本开始, Spring Data Elasticsearch 4.0.x 开始使用 restClient\n版本对应-官方文档 1. Rest client 此处仅使用HighLevelClient\n1. application.properties spring.elasticsearch.rest.uris=http://127.0.0.1:9200 2. pom.xml \u0026lt;!--rest--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.elasticsearch.client\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;elasticsearch-rest-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.4.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.elasticsearch.client\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;elasticsearch-rest-high-level-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.4.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 3. RestHighTest.java package com.rancho.demo.elasticsearch; import org.apache.http.HttpHost; import org.elasticsearch.action.delete.DeleteRequest; import org.elasticsearch.action.delete.DeleteResponse; import org.elasticsearch.action.get.GetRequest; import org.elasticsearch.action.get.GetResponse; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.index.IndexResponse; import org.elasticsearch.action.search.SearchRequest; import org.elasticsearch.action.search.SearchResponse; import org.elasticsearch.action.update.UpdateRequest; import org.elasticsearch.action.update.UpdateResponse; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.index.query.MatchPhrasePrefixQueryBuilder; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.search.builder.SearchSourceBuilder; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; import javax.annotation.Resource; import java.util.HashMap; import java.util.Map; @RunWith(SpringRunner.class) @SpringBootTest public class RestHighTest { @Resource private RestHighLevelClient restHighLevelClient; @Test public void add() { Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;String, Object\u0026gt;(); map.put(\u0026#34;id\u0026#34;, \u0026#34;20190909\u0026#34;); map.put(\u0026#34;name\u0026#34;, \u0026#34;测试\u0026#34;); map.put(\u0026#34;age\u0026#34;, 22); try { IndexRequest indexRequest = new IndexRequest(\u0026#34;content\u0026#34;, \u0026#34;doc\u0026#34;, map.get(\u0026#34;id\u0026#34;).toString()).source(map); IndexResponse indexResponse = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); System.out.println(indexResponse.toString()); } catch (Exception e) { e.printStackTrace(); } } @Test public void search() { SearchRequest searchRequest = new SearchRequest().indices(\u0026#34;content\u0026#34;).types(\u0026#34;doc\u0026#34;); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); MatchPhrasePrefixQueryBuilder mppqb = QueryBuilders.matchPhrasePrefixQuery(\u0026#34;name\u0026#34;, \u0026#34;测试\u0026#34;); sourceBuilder.query(mppqb); try { SearchResponse sr = this.restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); String result = sr.toString(); System.out.println(result); } catch (Exception e) { e.printStackTrace(); } } @Test public void update() { Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;String, Object\u0026gt;(); map.put(\u0026#34;id\u0026#34;, \u0026#34;20190909\u0026#34;); map.put(\u0026#34;name\u0026#34;, \u0026#34;测试-update\u0026#34;); map.put(\u0026#34;age\u0026#34;, 22); try { UpdateRequest request = new UpdateRequest(\u0026#34;content\u0026#34;, \u0026#34;doc\u0026#34;, map.get(\u0026#34;id\u0026#34;).toString()).doc(map); UpdateResponse updateResponse = restHighLevelClient.update(request, RequestOptions.DEFAULT); System.out.println(updateResponse.toString()); } catch (Exception e) { } } @Test public void get() { try { GetRequest request = new GetRequest(\u0026#34;content\u0026#34;, \u0026#34;doc\u0026#34;, \u0026#34;20190909\u0026#34;); GetResponse getResponse = this.restHighLevelClient.get(request, RequestOptions.DEFAULT); System.out.println(getResponse.toString()); } catch (Exception e) { } } @Test public void delete() { try { DeleteRequest request = new DeleteRequest(\u0026#34;content\u0026#34;, \u0026#34;doc\u0026#34;, \u0026#34;20190909\u0026#34;); DeleteResponse deleteResponse = this.restHighLevelClient.delete(request, RequestOptions.DEFAULT); System.out.println(deleteResponse.toString()); } catch (Exception e) { } } } 因为它是直接用了client,所以是没有线程池的,可以利用 apache的 GenericObjectPool类来实现线程池\n有文章说 : Rest Client是长连接，而且内部有默认的线程池管理，因此一般无需自定义线程池管理连接\n源码分析1 线程池是访问es的http连接池\n源码分析2 详情见search服务代码\n2. Springboot 方式 1.application.properties 文件增加以下内容\nspring.data.elasticsearch.cluster-name=elasticsearch spring.data.elasticsearch.cluster-nodes=127.0.0.1:9300 #java的es默认连接端口是9300，9200是http端口 spring.data.elasticsearch.repositories.enabled=true 2.pom.xml \u0026lt;!--spring整合elasticsearch包--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-elasticsearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 这里用的是springboot DB的方式,还有其他方式也可连接,springboot 对应的Es版本关系有严格匹配,这里用的是,springboot 2.0.6版,es是5.6.16版\n1、None of the configured nodes are available 或者org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream\n原因：spring data elasticSearch 的版本与Spring boot、Elasticsearch版本不匹配。 这是版本之间的对应关系。Spring boot 1.3.5默认的elasticsearch版本是5.2，此时启动7.2版本以下的Elasticsearch客户端连接正常。\n2、Caused by: java.lang.IllegalArgumentException: @ConditionalOnMissingBean annotations must specify at least one bean (type, name or annotation)\n原因：spring boot是1.3.x版本(es用的是1.X版本)，而es采用了boot2.x版本。在es的boot2.x版本去除了一些类，而这些类在spring boot的1.3.x版本中仍然被使用，导致此错误 3、es在5.x版本中一个索引允许有两个type,在6.x版本中一个索引只允许有一个type,在7.x版中支持不要type;(这是es的type移除规划)\n3.实体类 @Document(indexName = \u0026#34;jportal\u0026#34;, type = \u0026#34;platformLogin\u0026#34;) public class PlatformLogin { /** * serialVersionUID */ private static final long serialVersionUID = 448279746126620954L; @Id private String id; private Date operateTime; // es存时间存的是时间戳, private String operateTimeStr;// 用来格式化时间 private String loginAcc; private String loginName; private String loginIp; private String loginType; private String operateContent; // get,set方法省略 } Spring-data-elasticsearch为我们提供了@Document、@Field等注解，如果某个实体需要建立索引，只需要加上这些注解即可\n1.类上注解：@Document (相当于Hibernate实体的@Entity/@Table)(==必写==)，加上了@Document注解之后，++默认情况下这个实体中所有的属性都会被建立索引、并且分词。++\n类型 属性名 默认值 说明 String indexName 无 索引库的名称，建议以项目的名称命名(只能是小写) String type “” 类型，建议以实体的名称命名 short shards 5 默认分区数 short replica 1 每个分区默认的备份数 String refreshInterval “1s” 刷新间隔 String indexStoreType “fs” 索引文件存储类型 2.主键注解：@Id (相当于Hibernate实体的主键@Id注解)(==必写==) 只是一个标识，并没有属性。\n3.属性注解 @Field (相当于Hibernate实体的@Column注解) @Field默认是可以不加的，默认所有属性都会添加到ES中。加上@Field之后，@document默认把所有字段加上索引失效，只有加@Field 才会被索引(同时也看设置索引的属性是否为no)\n类型 属性名 默认值 说明 FieldType type FieldType.Auto(大部分是text类型) 自动检测属性的类型 FieldIndex index FieldIndex.analyzed 默认情况下分词 boolean store false 默认情况下不存储原文 String searchAnalyzer “” 指定字段搜索时使用的分词器 String indexAnalyzer “” 指定字段建立索引时指定的分词器 String[] ignoreFields {} 如果某个字段需要被忽略 4.代码 需某个类继承ElasticsearchRepository,因为是demo,这里直接用dao层继承了,上面controller中的业务逻辑也可以写在impl中.\ndao.java\n也可以像redis那样,使用 template\n@Resource private ElasticsearchTemplate elasticsearchTemplate; public interface IEsDao extends ElasticsearchRepository\u0026lt;PlatformLogin, String\u0026gt; { // 这种方式不用写具体实现,springboot会帮我们做,和访问mysql类型,反正项目中没用到 public PlatformLogin findByLoginNameLike(String string); } controller.java:\n@Autowired private IEsDao esDao; @RequestMapping(\u0026#34;search\u0026#34;) public String search( ) { //按标题进行搜索 // 这里要写keyword, 在之后的查询中使用loginName是将loginName作为text类型查询，而使用loginName.keyword则是将loginName作为keyword类型查询。前者会对查询内容做分词处理之后再匹配，而后者则是直接对查询结果做精确匹配。 PrefixQueryBuilder queryBuilder = QueryBuilders.prefixQuery(\u0026#34;loginName.keyword\u0026#34;, \u0026#34;肖坤\u0026#34;); ///ES的term query做的是精确匹配而不是分词查询，因此对text类型的字段做term查询将是查不到结果的（除非字段本身经过分词器处理后不变，未被转换或分词）。此时，必须使用foobar.keyword来对foobar字段以keyword类型进行精确匹配。 // 来自:https://segmentfault.com/q/1010000017312707 // es中存了两份数据,一份分词的(text类型),一份不分词的(keyword类型) //如果没写keyword则是用条件按分词之后查询 // 写了keyword则是对条件按整个文档搜索 //还有好多查询条件,慢慢看api //withfilter和withquery入参条件是一样的,但是es处理却是不一样的 //1.在查询(query)上下文中，查询会回答这个问题——“这个文档匹不匹配这个查询，它的相关度高么？”如何验证匹配很好理解，如何计算相关度呢？ES中索引的数据都会存储一个_score分值，分值越高就代表越匹配。另外关于某个搜索的分值计算还是很复杂的，因此也需要一定的时间。 //2. 在过滤器上下文中，查询会回答这个问题——“这个文档匹不匹配？”答案很简单，是或者不是。它不会去计算任何分值，也不会关心返回的排序问题，因此效率会高一点。过滤上下文 是在使用filter参数时候的执行环境，比如在bool查询中使用Must_not或者filter另外，经常使用过滤器，ES会自动的缓存过滤器的内容，这对于查询来说，会提高很多性能。 //原文：https://blog.csdn.net/qq_29580525/article/details/80908523 // QueryBuilder queryBuilder = QueryBuilders.termQuery(\u0026#34;这里是你要查询的字段\u0026#34;, searchContent); SearchQuery searchQuery =new NativeSearchQueryBuilder().withQuery(queryBuilder).build(); System.out.println(\u0026#34;查询的语句:\u0026#34; + searchQuery.getQuery().toString()); Page\u0026lt;PlatformLogin\u0026gt; searchPageResults = esDao.search(searchQuery); PlatformLogin jpfloginInfo = esDao.findByLoginNameLike(\u0026#34;肖\u0026#34;); logger.info(\u0026#34;返回结果:{}\u0026#34;,searchPageResults.getContent()); logger.info(\u0026#34;返回结果:{}\u0026#34;,jpfloginInfo); return LocalDateTime.now().toString(); } 参考链接:\n几种链接方式 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%88%86%E6%94%AF/es-springboot.html","summary":"[TOC] 前言 到目前为止，ES有4种客户端，分别是：Jest client、Rest client、Transport client、Node client","title":"ES-springboot"},{"content":"[toc]\n1. 集群规划 集群中有两个主要角色，Master Node和Data Node，其它如Tribe Node等节点可根据业务需要另行设立\n所以区分master和data节点,职责单一化(通过配置即可完成)\nMaster Node，整个集群的管理者，负有对index的管理、shards的分配，以及整个集群拓扑信息的管理等功能，众所周知，Master Node可以通过Data Node兼任\nData Node是数据的承载者，对索引的数据存储、查询、聚合等操作提供支持，这些操作严重消耗系统的CPU、内存、IO等资源\n更多集群知识见 集群模式.md 2. 分片指定 因为es有分片的概念,即数据会分布到不同的分片上,存储到分片上时是根据id分配的,若我们能自定义这个存放规则,取值时就能精确查找了(默认情况下会全部遍历)(有点像自定义HBASE Rowkey)\n例如: PUT my_index/my_type/1?routing=user1 { \u0026#34;title\u0026#34;: \u0026#34;This is a document\u0026#34; } GET my_index/my_type/1?routing=user1 在自定义routing的情况下,难免会出现量大的index:\n一种解决办法是单独为这些数据量大的渠道创建独立的index，\n另一种办法是指定index参数index.routing_partition_size，来解决最终可能产生群集不均衡的问题\n索引中的mappings必须指定_routing为\u0026quot;required\u0026quot;: true，另外mappings不支持parent-child父子关系。\n3. 索引拆分 当数据越来越多,则分片上的数据也越来越多,查询和存储的速度也越来越慢，更重要的是一个index其实是有存储上限的，如官方声明单个shard的文档数不能超过20亿, 可以借助 Rollover Api 和 Index Template,以时间维度拆分\n大致如下: 存储数据的时候,存到具有类似名称的索引上,这些索引类属于同一个别名,则查询的时间可以根据这个规则范围查询\n例如: logs-2020.04.12 , logs-2020.05.12 都类比于 logs-*\n那如果你想查询3天内的数据可以通过日期规则来匹配索引名，\nGET /\u0026lt;logs-{now/d}-*\u0026gt;,\u0026lt;logs-{now/d-1d}-*\u0026gt;,\u0026lt;logs-{now/d-2d}-*\u0026gt;/_search\n4. Hot-Warm架构 冷热架构，为了保证大规模时序索引实时数据分析的时效性，可以根据资源配置不同将Data Nodes进行分类形成分层或分组架构，一部分支持新数据的读写，另一部分仅支持历史数据的存储，存放一些查询发生机率较低的数据，即Hot-Warm架构\n思路:\n将Data Node根据不同的资源配比打上标签，如：host、warm 定义2个时序索引的index template，包括hot template和warm template，hot template可以多分配一些shard和拥有更好资源的Hot Node 用hot template创建一个active index名为active-logs-1，别名active-logs，支持索引切割，插入一定数据后，通过roller over api将active-logs切割，并将切割前的index移动到warm nodes上，如active-logs-1，并阻止写入 通过Shrinking API收缩索引active-logs-1为inactive-logs-1，原shard为5，适当收缩到2或3，可以在warm template中指定，减少检索的shard，使查询更快 通过force-merging api合并inactive-logs-1索引每个shard的segment，节省存储空间 删除active-logs-1 来自 : https://www.cnblogs.com/xguo/p/10558828.html?utm_source=tuicool\u0026utm_medium=referral 5. 集群配置优化 jvm配置 xms和xmx设置成一样，避免heap resize时卡顿\nxmx不要超过物理内存的50%\n因为es的lucene写入强依赖于操作系统缓存，需要预留加多的空间给操作系统\n最大内存不超过32G，但是也不要太小\n堆太小会导致频繁的小延迟峰值，并因不断的垃圾收集暂停而降低吞吐量；\n如果堆太大，应用程序将容易出现来自全堆垃圾回收的罕见长延迟峰值；\n将堆限制为略小于32GB可以使用jvm的指针压缩技术增强性能；\njvm使用server模式\n推荐采用g1垃圾回收器\n关闭jvm swapping\n集群节点数 es的节点提供查询的时候使用较多的内存来存储查询缓存，es的lucene写入到磁盘也会先缓存在内存中，我们开启设计这个es节点时需要根据每个节点的存储数据量来进行判断。这里有一个流行的推荐比例配置：\n搜索类比例：1:16(内存:磁盘) 日志类比例：1:48 ~ 1:96(内存:磁盘) 示例：\n有一个业务的数据量预估实际有1T，我们把副本设置1个，那么es中总数据量为2T。\n如果业务偏向于搜索\n每个节点31*16=496G。在加上其它的预留空间，每个节点有400G的存储空间。2T/400G，则需要5个es存储节点。\n如果业务偏向于写入日志型\n每个节点31*50=1550G，就只需要2个节点即可\n这里31G表示的是jvm设置不超过32g否则不会使用java的指针压缩优化了。\n6. 写入和查询优化 写入优化 写入的目标在于增大写入的吞吐量，这里主要从两个方面进行优化：\n客户端：\n写入数据不指定_id，让ES自动产生\n当用户显示指定id写入数据时，ES会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。\n这样每次写入时，ES都会耗费一定的资源做查询。\n如果用户写入数据时不指定doc，ES则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。\n进行多线程写入，最好的情况时动态调整，如果http429，此时可以少写入点，不是可以多写点\nserver：\n可靠性要求不高时，可以副本设置为0。\n减少不必要的分词，从而降低cpu和磁盘的开销。\n不要对字符串使用默认的dynmic mapping。会自动分词产生不必要的开销。\n设置30s refresh，降低lucene生成频次，资源占用降低提升写入性能，但是损耗实时性。\ntranslong落盘异步化，提升性能，损耗灾备能力。\nmerge并发控制。\nES的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。\n这里可以针对myindex索引优化的示例：\nPUT myindex { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34; :{ \u0026#34;refresh_interval\u0026#34; : \u0026#34;30s\u0026#34;,\u0026#34;number_of_shards\u0026#34; :\u0026#34;2\u0026#34; }, \u0026#34;routing\u0026#34;: { \u0026#34;allocation\u0026#34;: { \u0026#34;total_shards_per_node\u0026#34; :\u0026#34;3\u0026#34; } }, \u0026#34;translog\u0026#34; :{ \u0026#34;sync_interval\u0026#34; : \u0026#34;30s\u0026#34;, \u0026#34;durability\u0026#34; : \u0026#34;async\u0026#34; }, number_of_replicas\u0026#34; : 0 } \u0026#34;mappings\u0026#34;: { \u0026#34;dynamic\u0026#34; : false, \u0026#34;properties\u0026#34; :{} } } 查询优化 首先有几个原则我们需要清楚：\nElasticSearch不是关系型数据库，即使ElasticSearch支持嵌套、父子查询，但是会严重损耗ElasticSearch的性能，速度也很慢。\n尽量先将数据计算出来放到索引字段中，不要查询的时候再通过es的脚本来进行计算。\n尽量利用filter的缓存来查询\n设计上不要深度分页查询，否则可能会使得jvm内存爆满。\n可以通过profile、explain工具来分析慢查询的原因。\n严禁*号通配符为开头的关键字查询，我们可以利用不同的分词器进行模糊查询。\n分片数优化，避免每次查询访问每一个分片，可以借助路由字段进行查询。\n需要控制单个分片的大小：\n这个上面有提到：查询类：20GB以内；日志类：50G以内。\n读但是不写入文档的索引进行lucene段进行强制合并。\n优化数据模型、数据规模、查询语句。\n尽量少对text类型字段做聚合操作, 因为效果不佳,性能低下\n7. 问题诊断 监控状态 绿色代表集群的索引的所有分片（主分片和副本分片）正常分配了。 红色代表至少一个主分片没有分配。 黄色代表至少一个副本没有分配。 慢查询 profile api 在查询条件中设置profile为true的参数，将会显示查询经历的细节。\nGET trace_segment_record_202204291430/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;serviceIp\u0026#34; : \u0026#34;xxxxx\u0026#34; } }, \u0026#34;profile\u0026#34;: true } 其结果\n这里会返回一个shards列表。其中：\nid\n【nodeId】【shardId】\nquery\n主要包含了如下信息：\nquery_type\n展示了哪种类型的查询被触发。\nlucene\n显示启动的lucene方法\ntime\n执行lucene查询小号的时间\nbreakdown\n里面包含了lucene查询的一些细节参数\nrewrite_time\n多个关键字会分解创建个别查询，这个分解过程花费的时间。将重写一个或者多个组合查询的时间被称为”重写时间“\ncollector\n在Lucene中，收集器负责收集原始结果，并对它们进行组合、过滤、排序等处理。这里我们可以根据这个查看收集器里面花费的时间及一些参数。\nProfile API让我们清楚地看到查询耗时。提供了有关子查询的详细信息，我们可以清楚地知道在哪个环节查询慢，另外返回的结果中，关于Lucene的详细信息也让我们深入了解到ES是如何执行查询的。\n节点cpu过高 如果出现节点占用CPU很高，我们需要知道CPU在运行什么任务，一般通过线程堆栈来查看。\n这里有两种方式可以查看哪些线程CPU占用率比较高：\n使用ElasticSearch提供的hot_threads api查看； 使用jstack和top命令查看，针对java应用cpu过高问题的这个是通用做法。 这里推荐使用hot_threads api\nGET /_nodes/hot_threads GET /_nodes/\u0026lt;node_id\u0026gt;/hot_threads 更多详见: 深入解读 Elasticsearch 热点线程 hot_threads_铭毅天下的博客-CSDN博客_es hot_thread ElasticSearch部署架构和容量规划 (qq.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/es%E4%BC%98%E5%8C%96.html","summary":"[toc] 1. 集群规划 集群中有两个主要角色，Master Node和Data Node，其它如Tribe Node等节点可根据业务需要另行设立 所以区分mas","title":"es优化"},{"content":"[toc]\n前言 随着服务的越来越多，越来越杂，服务之间的调用会越来越复杂，越来越难以管理。而当某个服务发生了变化，或者由于压力性能问题，多部署了几台服务，怎么让服务的消费者知晓变化，就显得很重要了。不然就会存在调用的服务其实已经下线了，但调用者不知道等异常情况。\nEureka是Netflix开源的服务发现组件，本身是一个基于REST的服务。它包含Server和Client两部分。Spring Cloud将它集成在子项目Spring Cloud Netflix中，从而实现微服务的注册与发现。\n著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。在此Zookeeper保证的是CP, 而Eureka则是AP。Zookeeper保证CP\n来自 \u0026lt;https://blog.csdn.net/qq_38363255/article/details/80909731\n1. 基础架构 Eureka服务端\n​\t也称为注册中心，用于提供服务的注册与发现。支持高可用配置，依托与强一致性提供良好的服务实例可用性，可以应对多种不同的故障场景。\nEureka客户端\n​\t主要处理服务的注册与发现。客户端服务通过注解和参数配置方式，嵌入在客户端的应用程序代码中，在应用程序启动时，向注册中心注册自身提供的服务并周期性地发送心跳来更新它的服务租约。同时，它也能从服务端查询当前注册的服务信息并把它们缓存到本地并周期性地刷新服务状态。\n从这个简图中，可以看出，Eureka有三部分组成：\nService Provider： 暴露服务的提供方。 Service Consumer：调用远程服务的服务消费方。 EureKa Server： 服务注册中心和服务发现中心 2. 基本使用 服务端:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-eureka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; eureka.instance.instance-id=${spring.cloud.client.ipAddress}:${server.port} eureka.instance.prefer-ip-address=true server.port=8766 eureka.client.serviceUrl.defaultZone=http://10.101.90.171:10000/eureka/ # 指定服务注册中心地址 这里直接指向了本服务 #eureka.client.service-url.defaultZone=http://${eureka.instance.hostname}:${server.port}/eureka/ 注: 默认不写时，是注册至：DEFAULT_URL中,默认就是http://localhost:8761/eureka。\n/** * Eureka服务端 * @author oKong * */ @SpringBootApplication @EnableEurekaServer public class EureakServiceApplication { public static void main(String[] args) throws Exception { SpringApplication.run(EureakServiceApplication.class, args); log.info(\u0026#34;spring-cloud-eureka-service启动!\u0026#34;); } } 启动应用，访问：http://127.0.0.1:1000/, 就能看到注册中心界面(此时没有客户端,所以应用列表为空)\n客户端: (关于Eureka)配置与服务端一样,\n/** * 服务提供者示例-eureka客户端 * @author oKong * */ @SpringBootApplication //注意这里也可使用@EnableEurekaClient //springcloud是灵活的，注册中心支持eureka、consul、zookeeper等 //这样在替换注册中心时，只需要替换相关依赖即可。 @EnableDiscoveryClient public class EurekaClientApplication { public static void main(String[] args) throws Exception { SpringApplication.run(EurekaClientApplication.class, args); log.info(\u0026#34;spring-cloud-eureka-client启动!\u0026#34;); } } 注意：这里也可使用@EnableEurekaClient注解，但一般不这么用，直接使用@EnableDiscoveryClient实现自动发现。因为SpringCloud本身支持Eureka、Consul、zookeeper等实现注册中心功能，若写死了某个注册中心的相关注解，之后替换时，还需要修改注解类。源码中,注解@EnableEurekaClient上有@EnableDiscoveryClient注解,可以说基本就是EnableEurekaClient有@EnableDiscoveryClient的功能,但是注释写明@EnableEurekaClient是一个方便使用eureka的注解而已,至于为啥就不清楚了,\n来自 https://www.jianshu.com/p/f6db3117864f 来自 https://blog.lqdev.cn/2018/09/06/SpringCloud/chapter-two/ 3. Eureka自我保护模式 ​\t默认情况下，如果Eureka Server在一定时间(30s)内没有接收到某个微服务实例的心跳，Eureka Server将会注销该实例（默认90秒）。但是当网络分区故障发生时，微服务与Eureka Server之间无法正常通信，这就可能变得非常危险了，因为微服务本身是健康的，此时本不应该注销这个微服务。\nEureka Server通过“自我保护模式”来解决这个问题，当Eureka Server节点在短时间(15分钟)内丢失过多客户端(85%)时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。\n自我保护模式是一种对网络异常的安全保护措施。使用自我保护模式，而让Eureka集群更加的健壮、稳定。 开发阶段可以通过配置：``eureka.server.enable-self-preservation=false`关闭自我保护模式。\n具体的配置参数，可至官网查看：\nhttp://cloud.spring.io/spring-cloud-static/Finchley.RELEASE/single/spring-cloud.html#_appendix_compendium_of_configuration_properties 配置字段中文说明，大家可查看网站：微服务架构：Eureka参数配置项详解，里面很详细的说明了。\n4. Eureka 如何获取IP? 从eureka.instance.ip-address获取,如果没有设置则取自动获取EurekaInstanceConfigBean instance = new EurekaInstanceConfigBean(inetUtils); InetUtils工具类会获取所有网卡，依次进行遍历，取ip地址合理、ipv4地址,不是回环地址(127.0.0.1),索引值最小且不在忽略列表的网卡的ip地址作为结果。如果仍然没有找到合适的IP, 那么就将InetAddress.getLocalHost()做为最后的fallback方案 何谓合理?:\n设置了仅使用本地接口的或者是首选地址(支持正则)\n类: EurekaClientAutoConfiguration, InetUtils,org.springframework.cloud.commons.util.InetUtils#findFirstNonLoopbackAddress\nhttp://www.manongjc.com/detail/15-ziuyupkzzzamoqr.html https://www.cnblogs.com/orangesea/articles/11300266.html Eureka高可用详见: 来自 https://blog.lqdev.cn/2018/09/06/SpringCloud/chapter-two/ ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/eureka.html","summary":"[toc] 前言 随着服务的越来越多，越来越杂，服务之间的调用会越来越复杂，越来越难以管理。而当某个服务发生了变化，或者由于压力性能问题，多部署了几台服","title":"Eureka"},{"content":" 类似于spring的过滤器,只是引用方式不同,\nspring: 引用过滤器使用xml方式,在web.xml中引用,过滤器越先引用(写在前面)优先级越高\nspring boot: 使用一个FilterRegistrationBean类来引用过滤器\n过滤器书写与spring一致:\npublic class MyFilter implements Filter { //写了一个MyFilter 过滤器 Logger logger = LoggerFactory.getLogger(this.getClass()); @Override public void destroy() { } @Override public void doFilter(ServletRequest srequest, ServletResponse sresponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) srequest; //可以做登录验证等等….. logger.info(\u0026#34;this is MyFilter,url :\u0026#34;+request.getRequestURI()); filterChain.doFilter(srequest, sresponse); } @Override public void init(FilterConfig arg0) throws ServletException { } } 使用 过滤器:\n@Bean public FilterRegistrationBean\u0026lt;MyFilter\u0026gt; testFilterRegistration() { FilterRegistrationBean\u0026lt;MyFilter\u0026gt; registration = new FilterRegistrationBean\u0026lt;MyFilter\u0026gt;(); registration.setFilter(new MyFilter()); registration.addUrlPatterns(\u0026#34;/*\u0026#34;); //设置拦截url registration.addInitParameter(\u0026#34;paramName\u0026#34;, \u0026#34;paramValue\u0026#34;); registration.setName(\u0026#34;MyFilter\u0026#34;); registration.setOrder(1);//设置优先级 return registration; } ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/filter.html","summary":"类似于spring的过滤器,只是引用方式不同, spring: 引用过滤器使用xml方式,在web.xml中引用,过滤器越先引用(写在前面)优先级越高 spring boot: 使","title":"filter"},{"content":"Scribe是Facebook开源的分布式日志搜集系统，架构简单，日志格式灵活，且支持异步发送消息和队列\n对比项 Flume-NG Scribe 使用语言 Java c/c++ 容错性 Agent和Collector间，Collector和Store间都有容错性，且提供三种级别的可靠性保证； Agent和Collector间, Collector和Store之间有容错性； 负载均衡 Agent和Collector间，Collector和Store间有LoadBalance和Failover两种模式 无 可扩展性 好 好 Agent丰富程度 提供丰富的Agent，包括avro/thrift socket, text, tail等 主要是thrift端口 Store丰富程度 可以直接写hdfs, text, console, tcp；写hdfs时支持对text和sequence的压缩； 提供buffer, network, file(hdfs, text)等 代码结构 系统框架好，模块分明，易于开发 代码简单 来自* \u0026lt;http://www.aboutyun.com/thread-8317-1-1.html \u0026gt;\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/flume%E5%92%8Cscribe.html","summary":"Scribe是Facebook开源的分布式日志搜集系统，架构简单，日志格式灵活，且支持异步发送消息和队列 对比项 Flume-NG Scribe 使用语言 Java c/c++ 容错性 Agen","title":"flume和scribe"},{"content":"[toc]\n前言 它通过 「 分而治之 」 的方法尝试将所有可用的处理器内核使用起来帮助加速并行处理。\n在实际使用过程中，这种 「 分而治之 」的方法意味着框架首先要 fork ，递归地将任务分解为较小的独立子任务，直到它们足够简单以便异步执行。然后，join 部分开始工作，将所有子任务的结果递归地连接成单个结果，或者在返回 void 的任务的情况下，程序只是等待每个子任务执行完毕。\nFork/Join框架是一个实现了ExecutorService接口的多线程处理器，它专为那些可以通过递归分解成更细小的任务而设计，最大化的利用多核处理器来提高应用程序的性能。\n与其他ExecutorService相关的实现相同的是，Fork/Join框架会将任务分配给线程池中的线程。而与之不同的是，Fork/Join框架在执行任务时使用了工作窃取算法。\nForkJoin 有两大核心思想：\n分治算法； 工作密取：为了充分利用 cpu 资源，一个工作线程执行完自己队列的任务之后，不会空闲，而是从其它队列里寻找任务。 **使用场景: **\nForkJoinPool 不是为了替代 ExecutorService，而是它的补充，在某些应用场景下性能比 ExecutorService 更好。\nForkJoinPool 主要用于实现“分而治之”的算法，特别是分治之后递归调用的函数，例如 quick sort 等。\nForkJoinPool 最适合的是计算密集型的任务，如果存在 I/O，线程间同步，sleep() 等会造成线程长时间阻塞的情况时，最好配合使用 ManagedBlocker。\nManagedBlocker 它可以控制在阻塞时增加并行数, 这样就不会卡死了\n18 Fork/Join框架 · 深入浅出Java多线程 (redspider.group) 一文秒懂 Java Fork/Join - Java 一文秒懂 - 简单教程，简单编程 (twle.cn) 在Java8 parallelStream()中使用I/O + ManagedBlocker有什么问题吗? - IT宝库 (itbaoku.cn) 关于ForkJoinPool使用ManagedBlocker防线程阻塞而降低吞吐量的说明_heng_zou的博客-CSDN博客_forkjoin 阻塞 ManagedBlocker的使用和深入理解ForkJoin 有待提升\n1. 工作窃取算法 工作窃取算法指的是在多线程执行不同任务队列的过程中，某个线程执行完自己队列的任务后从其他线程的任务队列里窃取任务来执行。\n工作窃取流程如下图所示：\n值得注意的是，当一个线程窃取另一个线程的时候，为了减少两个任务线程之间的竞争，我们通常使用双端队列来存储任务。被窃取的任务线程都从双端队列的头部拿任务执行，而窃取其他任务的线程从双端队列的尾部执行任务。\n另外，当一个线程在窃取任务时要是没有其他可用的任务了，这个线程会进入阻塞状态以等待再次“工作”。\n工作窃取算法的优点： 充分利用线程进行并行计算，减少了线程间的竞争。\n工作窃取算法的缺点： 在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且该算法会消耗了更多的系统资源，比如创建多个线程和多个双端队列。\n2. 源码解释 2.1 ForkJoinTask ForkJoinTask代表运行在ForkJoinPool中的任务。\n主要方法：\nfork() 在当前线程运行的线程池中安排一个异步执行。简单的理解就是再创建一个子任务。 join() 当任务完成的时候返回计算结果。 invoke() 开始执行任务，如果必要，等待计算完成。 子类：\nRecursiveAction 一个递归无结果的ForkJoinTask（没有返回值） RecursiveTask 一个递归有结果的ForkJoinTask（有返回值） ForkJoinTask是一个类似普通线程的实体，但是比普通线程轻量得多。\nfork()方法:使用线程池中的空闲线程异步提交任务\n// 本文所有代码都引自Java 8 public final ForkJoinTask\u0026lt;V\u0026gt; fork() { Thread t; // ForkJoinWorkerThread是执行ForkJoinTask的专有线程，由ForkJoinPool管理 // 先判断当前线程是否是ForkJoin专有线程，如果是，则将任务push到当前线程所负责的队列里去 if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else // 如果不是则将线程加入队列 // 没有显式创建ForkJoinPool的时候走这里，提交任务到默认的common线程池中 ForkJoinPool.common.externalPush(this); return this; } 其实fork()只做了一件事，那就是把任务推入当前工作线程的工作队列里。\njoin()方法：等待处理任务的线程处理完毕，获得返回值。来看下join()的源码：\npublic final V join() { int s; // doJoin()方法来获取当前任务的执行状态 if ((s = doJoin() \u0026amp; DONE_MASK) != NORMAL) // 任务异常，抛出异常 reportException(s); // 任务正常完成，获取返回值 return getRawResult(); } /** * doJoin()方法用来返回当前任务的执行状态 **/ private int doJoin() { int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; // 先判断任务是否执行完毕，执行完毕直接返回结果（执行状态） return (s = status) \u0026lt; 0 ? s : // 如果没有执行完毕，先判断是否是ForkJoinWorkThread线程 ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? // 如果是，先判断任务是否处于工作队列顶端（意味着下一个就执行它） // tryUnpush()方法判断任务是否处于当前工作队列顶端，是返回true // doExec()方法执行任务 (w = (wt = (ForkJoinWorkerThread)t).workQueue). // 如果是处于顶端并且任务执行完毕，返回结果 tryUnpush(this) \u0026amp;\u0026amp; (s = doExec()) \u0026lt; 0 ? s : // 如果不在顶端或者在顶端却没未执行完毕，那就调用awitJoin()执行任务 // awaitJoin()：使用自旋使任务执行完成，返回结果 wt.pool.awaitJoin(w, this, 0L) : // 如果不是ForkJoinWorkThread线程，执行externalAwaitDone()返回任务结果 externalAwaitDone(); } 我们在之前介绍过说Thread.join()会使线程阻塞，而ForkJoinPool.join()会使线程免于阻塞，下面是ForkJoinPool.join()的流程图：\n2.2 ForkJoinWorkerThread ForkJoinWorkerThread代表ForkJoinPool线程池中的一个执行任务的线程。\n/** * Default ForkJoinWorkerThreadFactory implementation; creates a * new ForkJoinWorkerThread. */ static final class DefaultForkJoinWorkerThreadFactory implements ForkJoinWorkerThreadFactory { public final ForkJoinWorkerThread newThread(ForkJoinPool pool) { return new ForkJoinWorkerThread(pool); } } 2.3 ForkJoinPool ForkJoinPool是用于执行ForkJoinTask任务的执行（线程）池。\nForkJoinPool管理着执行池中的线程和任务队列，此外，执行池是否还接受任务，显示线程的运行状态也是在这里处理。\n我们来大致看下ForkJoinPool的源码：\n@sun.misc.Contended public class ForkJoinPool extends AbstractExecutorService { // 任务队列 volatile WorkQueue[] workQueues; // 线程的运行状态 volatile int runState; // 创建ForkJoinWorkerThread的默认工厂，可以通过构造函数重写 public static final ForkJoinWorkerThreadFactory defaultForkJoinWorkerThreadFactory; // 公用的线程池，其运行状态不受shutdown()和shutdownNow()的影响 static final ForkJoinPool common; // 私有构造方法，没有任何安全检查和参数校验，由makeCommonPool直接调用 // 其他构造方法都是源自于此方法 // parallelism: 并行度， // 默认调用java.lang.Runtime.availableProcessors() 方法返回可用处理器的数量 private ForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, // 工作线程工厂 UncaughtExceptionHandler handler, // 拒绝任务的handler int mode, // 同步模式 String workerNamePrefix) { // 线程名prefix this.workerNamePrefix = workerNamePrefix; this.factory = factory; this.ueh = handler; this.config = (parallelism \u0026amp; SMASK) | mode; long np = (long)(-parallelism); // offset ctl counts this.ctl = ((np \u0026lt;\u0026lt; AC_SHIFT) \u0026amp; AC_MASK) | ((np \u0026lt;\u0026lt; TC_SHIFT) \u0026amp; TC_MASK); } /** * Creates and returns the common pool, respecting user settings * specified via system properties. * jdk8 提供了一个简单的pool,(默认并行数是cpu核数-1), Lambda中(所有)的并行流用的就是这个方法,所以乱用并行流可能会导致线程阻塞卡死 */ private static ForkJoinPool makeCommonPool() { int parallelism = -1; ForkJoinWorkerThreadFactory factory = null; UncaughtExceptionHandler handler = null; try { // ignore exceptions in accessing/parsing properties String pp = System.getProperty (\u0026#34;java.util.concurrent.ForkJoinPool.common.parallelism\u0026#34;); String fp = System.getProperty (\u0026#34;java.util.concurrent.ForkJoinPool.common.threadFactory\u0026#34;); String hp = System.getProperty (\u0026#34;java.util.concurrent.ForkJoinPool.common.exceptionHandler\u0026#34;); if (pp != null) parallelism = Integer.parseInt(pp); if (fp != null) factory = ((ForkJoinWorkerThreadFactory)ClassLoader. getSystemClassLoader().loadClass(fp).newInstance()); if (hp != null) handler = ((UncaughtExceptionHandler)ClassLoader. getSystemClassLoader().loadClass(hp).newInstance()); } catch (Exception ignore) { } if (factory == null) { if (System.getSecurityManager() == null) factory = defaultForkJoinWorkerThreadFactory; else // use security-managed default factory = new InnocuousForkJoinWorkerThreadFactory(); } if (parallelism \u0026lt; 0 \u0026amp;\u0026amp; // default 1 less than #cores (parallelism = Runtime.getRuntime().availableProcessors() - 1) \u0026lt;= 0) parallelism = 1; if (parallelism \u0026gt; MAX_CAP) parallelism = MAX_CAP; return new ForkJoinPool(parallelism, factory, handler, LIFO_QUEUE,\u0026#34;ForkJoinPool.commonPool-worker-\u0026#34;); } } 2.4 WorkQueue 双端队列，ForkJoinTask们存放在这里。\n当工作线程在处理自己的工作队列时，会从队列首取任务来执行（FIFO）；如果是窃取其他队列的任务时，窃取的任务位于所属任务队列的队尾（LIFO）。\nForkJoinPool与传统线程池最显著的区别就是它维护了一个工作队列数组（volatile WorkQueue[] workQueues，ForkJoinPool中的每个工作线程都维护着一个工作队列）。\narray 初始容量 8192； 第一个任务放在 4096，似乎是因为操作系统内存的原因； 8191 的位置放入任务之后，还是会回到 0 的位置； 初始 base = top = 4096； 从上面放入一个任务 top + 1，不会从下面放入任务； LIFO 模式自己线程从上面取走任务 top - 1； FIFO 模式自己线程从下面取走任务 base + 1； 被其它线程从下面窃取任务，base + 1，其它线程不会从上面窃取任务； 数组 size 由 top - base 获得； 从 8191 回到 0 之后，top 和 base 会继续往上加，索引值通过取余获得。 static final int INITIAL_QUEUE_CAPACITY = 1 \u0026lt;\u0026lt; 13; // 2^13 = 8192 static final int MAXIMUM_QUEUE_CAPACITY = 1 \u0026lt;\u0026lt; 26; // 2^26 = 67108864 如果队列长度不够了,会自动两倍扩容的\n/** * Callback from ForkJoinWorkerThread constructor to establish and * record its WorkQueue. * * @param wt the worker thread * @return the worker\u0026#39;s queue */ final WorkQueue registerWorker(ForkJoinWorkerThread wt) { UncaughtExceptionHandler handler; wt.setDaemon(true); // configure thread if ((handler = ueh) != null) wt.setUncaughtExceptionHandler(handler); WorkQueue w = new WorkQueue(this, wt); int i = 0; // assign a pool index int mode = config \u0026amp; MODE_MASK; int rs = lockRunState(); try { WorkQueue[] ws; int n; // skip if no array if ((ws = workQueues) != null \u0026amp;\u0026amp; (n = ws.length) \u0026gt; 0) { int s = indexSeed += SEED_INCREMENT; // unlikely to collide int m = n - 1; i = ((s \u0026lt;\u0026lt; 1) | 1) \u0026amp; m; // odd-numbered indices if (ws[i] != null) { // collision int probes = 0; // step by approx half n int step = (n \u0026lt;= 4) ? 2 : ((n \u0026gt;\u0026gt;\u0026gt; 1) \u0026amp; EVENMASK) + 2; while (ws[i = (i + step) \u0026amp; m] != null) { if (++probes \u0026gt;= n) { // 这里用 copyOf 进行复制, 这段代码太难读了,以后再来分析吧 workQueues = ws = Arrays.copyOf(ws, n \u0026lt;\u0026lt;= 1); m = n - 1; probes = 0; } } } w.hint = s; // use as random seed w.config = i | mode; w.scanState = i; // publication fence ws[i] = w; } } finally { unlockRunState(rs, rs \u0026amp; ~RSLOCK); } wt.setName(workerNamePrefix.concat(Integer.toString(i \u0026gt;\u0026gt;\u0026gt; 1))); return w; } 2.5 runState ForkJoinPool的运行状态。SHUTDOWN状态用负数表示，其他用2的幂次表示。\n总结 对于一个 new ForkJoinPool()，执行任务全流程如下：\nForkJoinPool 初始化 parallelism size = cpu 逻辑核心数，没有队列，没有线程；\n向 ForkJoinPool 提交一个任务；\n初始化队列数组，容量为 2 * Max { parallelism size, 2 ^ n }；\n创建一个没有线程的队列，容量为 2 ^ 13，随机放在队列数组的某一个偶数索引处；\n任务存入这个队列索引值为 2 ^ 12 处；\n再创建一个有线程的队列，容量为 2 ^ 13，随机放在队列数组的某一个奇数索引处；\n线程启动；\n线程从随机一个队列开始，遍历所有队列，最终扫描找到前面提交的任务，并从其所在队列取出；\n线程执行任务，拆分出两个子任务；\n如果用 invokeAll 提交，则一个进入线程所在队列，另一个直接在线程里执行； 如果用 fork 提交，则两个都进入线程所在队列； 提交的子任务触发创建新的线程，及与其对应的队列，还是在奇数索引处；\n提交的子任务可能仍然被当前线程执行，可能被其它线程窃取；\n线程在子任务处 join，join 期间会尝试从窃取自己任务的线程那里窃取任务执行；\n优先窃取队列底部； 队列没有任务则窃取其正在 join 的任务； 还没有则阻塞自己等待被唤醒，在阻塞之前会补偿一个活跃线程； 提交的子任务不管被哪个线程执行，仍会重复上述拆分、提交、窃取、阻塞流程；\n当任务被拆分的足够细，则会真正开始计算；\n计算完成从递归一层一层返回；\n最终所有子任务都完成，得到结果；\n如果不再提交任务，所有线程扫描不到任务进入 inactive 状态；\n最终，所有线程销毁，所有奇数索引位的队列回收，ForkJoinPool 中只剩下一个最初创建的在偶数索引位的队列。\nthread_fork/join并发框架2 - dengzy - 博客园 (cnblogs.com) [笔记][Java7并发编程实战手册]5.Fork＼Join(Java1.7新特性)框架_代码有毒的博客-CSDN博客 Java并发系列（12）——ForkJoin框架源码解析（上） - 知乎 (zhihu.com) Java并发系列（12）——ForkJoin框架源码解析（下） - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/forkjoin.html","summary":"[toc] 前言 它通过 「 分而治之 」 的方法尝试将所有可用的处理器内核使用起来帮助加速并行处理。 在实际使用过程中，这种 「 分而治之 」的方法意味着框架首先要 fork","title":"ForkJoin"},{"content":"[toc]\n1.介绍 Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。\nSpring Cloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全、监控、埋点和限流等。\nSpring Cloud Gateway 的特征：\n基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0 动态路由 Predicates 和 Filters 作用于特定路由 集成 Hystrix 断路器 集成 Spring Cloud DiscoveryClient 易于编写的 Predicates 和 Filters 限流 路径重写 1.2术语 Route（路由）：这是网关的基本构建块。它由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配。-\u0026gt; 理解为分发(改变)路径 Predicate（断言）：这是一个 Java 8 的 Predicate。输入类型是一个 ServerWebExchange。我们可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。-\u0026gt; 按指定的路径拦截 Filter（过滤器）：这是org.springframework.cloud.gateway.filter.GatewayFilter的实例，我们可以使用它修改请求和响应。-\u0026gt; 拦截下的路径可以做一些操作 1.2流程: 客户端向 Spring Cloud Gateway 发出请求。然后在 Gateway Handler Mapping 中找到与请求相匹配的路由，将其发送到 Gateway Web Handler。Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑，然后返回。 过滤器之间用虚线分开是因为过滤器可能会在发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。\n请求会两次经过filter -\u0026gt; 进来时和出去时\n2.使用 pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-gateway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 注意,web和路由不能同时引进\n2.1 路由 路由是用来分发的,这是基础,后面的过滤器和断言器都是为路由服务的.\nSpring Cloud Gateway 网关路由有两种配置方式：\n在配置文件 yml 中配置 通过@Bean自定义 RouteLocator，在启动主类 Application 中配置 这两种方式是等价的，建议使用 yml 方式进配置。\napplication.yml\nserver: port: 8081 spring: cloud: gateway: routes: - id: neo_route uri: http://www.ityouknow.com predicates: - Path=/spring-cloud logging: level: org.springframework.cloud.gateway: trace org.springframework.http.server.reactive: debug org.springframework.web.reactive: debug reactor.ipc.netty: debug 各字段含义如下：\nid：我们自定义的路由 ID，保持唯一 uri：目标服务地址 predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。 filters：过滤规则，本示例暂时没用。 routes下可以有很多id,predicates下可以有很多过滤工厂,\n上面这段配置的意思是，配置了一个 id 为 neo_route 的路由规则，当访问地址 http://localhost:8080/spring-cloud时会自动转发到地址：http://www.ityouknow.com/spring-cloud。\n转发功能同样可以通过代码来实现，我们可以在启动类 GateWayApplication 中添加方法 customRouteLocator() 来定制转发规则。\n@SpringBootApplication public class GateWayApplication { public static void main(String[] args) { SpringApplication.run(GateWayApplication.class, args); } @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder builder) { return builder.routes() .route(\u0026#34;path_route\u0026#34;, r -\u0026gt; r.path(\u0026#34;/about\u0026#34;) .uri(\u0026#34;http://ityouknow.com\u0026#34;)) .build(); } } 上面配置了一个 id 为 path_route 的路由，当访问地址http://localhost:8080/about时会自动转发到地址：http://www.ityouknow.com/about和上面的转发效果一样，只是这里转发的是以项目地址/about格式的请求地址。\n上面两个示例中 uri 都是指向了我的个人网站，在实际项目使用中可以将 uri 指向对外提供服务的项目地址，统一对外输出接口。\n其实不一定写在application里,写到任意一个会被加载的类中都可以,@component 放在类上就会被加载\n2.2 断言器 Spring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求参数来匹配，多个 Predicates 工厂可以组合使用。\napplication.yml\nspring: cloud: gateway: routes: - id: host_foo_path_headers_to_httpbin uri: http://ityouknow.com predicates: - Host=**.foo.org - Path=/headers - After=2018-01-20T06:06:06+08:00[Asia/Shanghai] predicates下的配置使用的都是断言工厂\nHost -\u0026gt; HostRoutePredicateFactory\nPath -\u0026gt; PathRoutePredicateFactory 只需要写前缀,(后面的RoutePredicateFactory不用写,也不能写);\nspringcloud是把所有的断言工厂加载进来,然后得到类名,去掉后缀作为key,类对象作为vaule,然后拿着配置文件中的值去get出值\n源码: ############ # 得到key # org.springframework.cloud.gateway.support.NameUtils.class public static String normalizeRoutePredicateName(Class\u0026lt;? extends RoutePredicateFactory\u0026gt; clazz) { return removeGarbage(clazz.getSimpleName().replace(RoutePredicateFactory.class.getSimpleName(), \u0026#34;\u0026#34;)); } ############ # put进map,存起来 # org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator.class public RouteDefinitionRouteLocator(RouteDefinitionLocator routeDefinitionLocator, List\u0026lt;RoutePredicateFactory\u0026gt; predicates, List\u0026lt;GatewayFilterFactory\u0026gt; gatewayFilterFactories, GatewayProperties gatewayProperties, ConversionService conversionService) { this.routeDefinitionLocator = routeDefinitionLocator; this.conversionService = conversionService; initFactories(predicates); gatewayFilterFactories.forEach( factory -\u0026gt; this.gatewayFilterFactories.put(factory.name(), factory)); this.gatewayProperties = gatewayProperties; } ############ # 根据名字取值 # org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator.class @SuppressWarnings(\u0026#34;unchecked\u0026#34;) private List\u0026lt;GatewayFilter\u0026gt; loadGatewayFilters(String id, List\u0026lt;FilterDefinition\u0026gt; filterDefinitions) { List\u0026lt;GatewayFilter\u0026gt; filters = filterDefinitions.stream().map(definition -\u0026gt; { GatewayFilterFactory factory = this.gatewayFilterFactories .get(definition.getName()); if (factory == null) { throw new IllegalArgumentException( \u0026#34;Unable to find GatewayFilterFactory with name \u0026#34; + definition.getName()); } # ... 后面还有很多,不粘贴出来了 } 2.3 过滤器 Spring Cloud Gateway 的 Filter 的生命周期不像 Zuul 的那么丰富，它只有两个：“pre” 和 “post”。\nPRE： 这种过滤器在请求被路由++之前调用++。我们可利用这种过滤器实现身份验证、在集群中选择请求的微服务、记录调试信息等。\nPOST：这种过滤器在路由到微服务++以后执行++。这种过滤器可用来为响应添加标准的 HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等。 Spring Cloud Gateway 的 Filter 分为两种：\nGatewayFilter\nGlobalFilter\nGlobalFilter 会应用到所有的路由上，而 GatewayFilter 将应用到单个路由或者一个分组的路由上。GatewayFilteryer加上配置后也可以变成全局过滤(还不如直接写成全局过滤器呢)\n2.3.1 GlobalFilter 直接上自定义的\npackage com.example.demo.filter; import java.util.ArrayList; import java.util.List; import java.util.Optional; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.cloud.gateway.filter.GatewayFilterChain; import org.springframework.cloud.gateway.filter.GlobalFilter; import org.springframework.core.Ordered; import org.springframework.core.io.buffer.DataBuffer; import org.springframework.http.server.reactive.ServerHttpResponse; import org.springframework.stereotype.Component; import org.springframework.web.server.ServerWebExchange; import reactor.core.publisher.Mono; @Component // 让框架加载这个类 public class TestGlobalFilter implements GlobalFilter, Ordered { // 继承 GlobalFilter 就是全局过滤器了 private static final Logger log = LoggerFactory.getLogger(TestGlobalFilter.class); // 过滤器的优先级,数字越大优先级越小 @Override public int getOrder() { return 0; } // 可以在这里干很多很多事情,比如验证请求头信息 @Override public Mono\u0026lt;Void\u0026gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { log.info(\u0026#34;这是gateway-filter,{}\u0026#34;,exchange); String value = exchange.getRequest().getPath().value(); System.out.println(\u0026#34;过滤器....\u0026#34;+value); List\u0026lt;String\u0026gt; name = Optional.ofNullable(exchange.getRequest().getHeaders().get(\u0026#34;name\u0026#34;)).orElse(new ArrayList\u0026lt;String\u0026gt;()); System.out.println(\u0026#34;过滤器....头信息-name:\u0026#34;+name); if (name.contains(\u0026#34;xkj\u0026#34;)) { ServerHttpResponse response = exchange.getResponse(); response.getHeaders().add(\u0026#34;name\u0026#34;, \u0026#34;xkj\u0026#34;); byte[] datas = \u0026#34;非法请求\u0026#34;.getBytes(); DataBuffer buffer = response.bufferFactory().wrap(datas); return response.writeWith(Mono.just(buffer));// 不正常情况时返回 } // 这里只是对进来的链接处理了,在后面加then(mono)方法就可以处理出去时的链接,mono就是具体的操作,可以用来记录链接使用时间等 return chain.filter(exchange); } } 过滤通过后会经过路由拦截并转发啦\n2.3.2 GatewayFilter 直接上自定义的\npackage com.example.demo.filter; import java.util.ArrayList; import java.util.List; import java.util.Optional; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.cloud.gateway.filter.GatewayFilter; import org.springframework.cloud.gateway.filter.GatewayFilterChain; import org.springframework.cloud.gateway.filter.factory.StripPrefixGatewayFilterFactory; import org.springframework.cloud.gateway.route.RouteLocator; import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder; import org.springframework.context.annotation.Bean; import org.springframework.core.Ordered; import org.springframework.core.io.buffer.DataBuffer; import org.springframework.http.server.reactive.ServerHttpResponse; import org.springframework.stereotype.Component; import org.springframework.web.server.ServerWebExchange; import reactor.core.publisher.Mono; @Component // 继承GatewayFilter 就是过滤器了 public class TestFilter implements GatewayFilter, Ordered { private static final Logger log = LoggerFactory.getLogger(TestFilter.class); @Override public int getOrder() { return 0; } // 和全局的差不多 @Override public Mono\u0026lt;Void\u0026gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { log.info(\u0026#34;这是gateway-filter,{}\u0026#34;,exchange); String value = exchange.getRequest().getPath().value(); System.out.println(\u0026#34;过滤器....\u0026#34;+value); List\u0026lt;String\u0026gt; ages = Optional.ofNullable(exchange.getRequest().getHeaders().get(\u0026#34;age\u0026#34;)).orElse(new ArrayList\u0026lt;\u0026gt;()); if (ages.contains(\u0026#34;20\u0026#34;)) { ServerHttpResponse response = exchange.getResponse(); response.getHeaders().add(\u0026#34;name\u0026#34;, \u0026#34;xkj\u0026#34;); byte[] datas = \u0026#34;非法请求\u0026#34;.getBytes(); DataBuffer buffer = response.bufferFactory().wrap(datas); return response.writeWith(Mono.just(buffer)); } return chain.filter(exchange); } // 写好过滤器后,就要通过路由来生效了,和前面的路由案例一样 @Bean public RouteLocator customerRouteLocator(RouteLocatorBuilder builder) { log.info(\u0026#34;这是gateway-customerRouteLocator,{}\u0026#34;,builder); return builder.routes() .route(r -\u0026gt; r.path(\u0026#34;/jp1/**\u0026#34;) .filters(f -\u0026gt; f.rewritePath(\u0026#34;/jp1/\u0026#34;, \u0026#34;/jportal/\u0026#34;).filter(new TestFilter()) ) .uri(\u0026#34;http://localhost:5766\u0026#34;) .order(0) .id(\u0026#34;test_route1\u0026#34;) ) .build(); } } 2.3.3 自定义过滤工厂 前面说到,路由建议写配置,但是路由里只能写工厂,所以接下来介绍自定义过滤工厂\n过滤器工厂的顶级接口是GatewayFilterFactory，我们可以直接继承它的两个抽象类来简化开发AbstractGatewayFilterFactory和AbstractNameValueGatewayFilterFactory，这两个抽象类的区别就是前者接收一个参数（像StripPrefix），后者接收两个参数（像AddResponseHeader和我们这种）。\napplication.yml\nspring: cloud: gateway: routes: - id: jportal_route uri: http://localhost:5766/ predicates: - Path=/jpxkj/** filters: - Test=name,xkj # - StripPrefix=1 # - PrefixPath=/jportal - RewritePath=/jpxkj/, /jportal/ # - RewritePath=/jpxkj/(?\u0026lt;segment\u0026gt;.*), /jportal/${segment} 重点解释filters里面的\nfilters下面全是工厂\nTest是自己写的工厂,同样只能写前缀,原理和断言工厂一样\n- StripPrefix : 截断,会截断路径,数字表示截断一个(不懂就看源码) - prefixPath : 加前缀,会在uri后面加上这个前缀 - RewritePath : 重定向,把/jpxkj/的这段路径变成/jportal/,原理就是用了replaceAll()方法 两个RewritePath能实现同样的效果,官网案例是复杂的,我也不懂非得写复杂,还是两者有区别? StripPrefix,PrefixPath组合实现了RewritePath的功能 工厂类\npackage com.example.demo.filter; import org.springframework.cloud.gateway.filter.GatewayFilter; import org.springframework.cloud.gateway.filter.factory.AbstractNameValueGatewayFilterFactory; import org.springframework.http.server.reactive.ServerHttpRequest; import org.springframework.stereotype.Component; @Component public class TestGatewayFilterFactory extends AbstractNameValueGatewayFilterFactory { //这是接收两个参数的类 @Override public GatewayFilter apply(NameValueConfig config) { System.out.println(\u0026#34;----TestGatewayFilterFactory:\u0026#34;+config); return new TestFilter();// 这是前面写好的过滤器 // 也可以现场写一个 //\treturn (exchange, chain) -\u0026gt; { //\tServerHttpRequest request = exchange.getRequest().mutate() //\t.header(config.getName(), config.getValue()) //\t.build(); // //\treturn chain.filter(exchange.mutate().request(request).build()); //\t}; } } 2.4 加入注册中心 前面使用的路径全是指定的,在生产中,肯定要加入注册中心 application.yml\n# 增加配置 spring: application: name: gateway-server-test cloud: gateway: discovery: locator: enabled: true eureka: client: service-url: defaultZone: http://10.101.90.171:10000/eureka/ 加入配置后,网关中心会注册到注册中心,注册中心里所有的应用都会成为uri,名称都是大写,这个要注意\npom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动类加上注解@EnableDiscoveryClient\n路径用lb://${appName} 就可以了, application.yml就是这样了\nspring: cloud: gateway: discovery: locator: enabled: true # lowerCaseServiceId: true # 应用名变小写 routes: - id: jportal_route uri: lb://JPORTAL-XKJ predicates: - Path=/jpxkj/** filters: - Test1=name,xkj 3. 限流算法 1、计数器（固定窗口）算法 计数器算法是使用计数器在周期内累加访问次数，当达到设定的限流值时，触发限流策略。下一个周期开始时，进行清零，重新计数。\n此算法在单机还是分布式环境下实现都非常简单，使用redis的incr原子自增性和线程安全即可轻松实现。\n这个算法通常用于QPS限流和统计总访问量，对于秒级以上的时间周期来说，会存在一个非常严重的问题，那就是临界问题，如下图：\n假设1min内服务器的负载能力为100，因此一个周期的访问量限制在100，然而在第一个周期的最后5秒和下一个周期的开始5秒时间段内，分别涌入100的访问量，虽然没有超过每个周期的限制量，但是整体上10秒内已达到200的访问量，已远远超过服务器的负载能力，由此可见，计数器算法方式限流对于周期比较长的限流，存在很大的弊端。\n2、滑动窗口算法 滑动窗口算法是将时间周期分为N个小周期，分别记录每个小周期内访问次数，并且根据时间滑动删除过期的小周期。\n如下图，假设时间周期为1min，将1min再分为2个小周期，统计每个小周期的访问数量，则可以看到，第一个时间周期内，访问数量为75，第二个时间周期内，访问数量为100，超过100的访问则被限流掉了。\n由此可见，当滑动窗口的格子划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确。\n滑动窗口算法虽然解决了固定窗口的临界问题，但是一旦到达限流后，请求都会直接暴力被拒绝。酱紫我们会损失一部分请求，这其实对于产品来说，并不太友好。\n面试必备：4种经典限流算法讲解 - 知乎 (zhihu.com) 3、漏桶算法 漏桶算法是访问请求到达时直接放入漏桶，如当前容量已达到上限（限流值），则进行丢弃（触发限流策略）。漏桶以固定的速率进行释放访问请求（即请求通过），直到漏桶为空。\n面对突发流量的时候，漏桶算法还是循规蹈矩地处理请求，这就不是我们想看到的啦。流量变突发时，我们肯定希望系统尽量快点处理请求，提升用户体验嘛。而且流量的流入和流出无法控制, 面对突发流量处理能力不佳\n4、令牌桶算法 令牌桶算法是程序以r（r=时间周期/限流值）的速度向令牌桶中增加令牌，直到令牌桶满，请求到达时向令牌桶请求令牌，如获取到令牌则通过请求，否则触发限流策略\n并不能说明令牌桶一定比漏洞好，她们使用场景不一样。\n令牌桶算法，放在服务端，用来保护服务端（自己），主要用来对调用者频率进行限流，为的是不让自己被压垮。所以如果自己本身有处理能力的时候，如果流量突发（实际消费能力强于配置的流量限制=桶大小），那么实际处理速率可以超过配置的限制（桶大小）。 而漏桶算法，放在调用方，这是用来保护他人，也就是保护他所调用的系统。主要场景是，当调用的第三方系统本身没有保护机制，或者有流量限制的时候，我们的调用速度不能超过他的限制，由于我们不能更改第三方系统，所以只有在主调方控制。这个时候，即使流量突发，也必须舍弃。因为消费能力是第三方决定的。 Spring Cloud Gateway系列【12】 限流算法使用案例详解及源码分析_云烟成雨TD的博客-CSDN博客 常见限流算法介绍（漏桶算法、令牌桶算法）及实现\u0026ndash;待整理 - 月染霜华 - 博客园 (cnblogs.com) 扩展阅读: 高可用之限流-07-token bucket 令牌桶算法 | Echo Blog (houbb.github.io) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/gateway.html","summary":"[toc] 1.介绍 Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一","title":"Gateway"},{"content":"区别:\n关于切换分支\nGit 的切换分支更像(或者说就是)切换快照(head),因为可以在同一个文件夹下使用 git checkout 来切换分支(本地代码就切换了)\nsvn的切换分支更像是切换文件,(好像也可以在同一个文件夹下) 关于提交代码\nGit 是要先add,再commit最后push 才能把代码推送至远程\nsvn是直接commit就能把代码推送至远程(也可以先add再commit) svn提交代码后，本地的版本不是最新的，需要update才是最新版本；而git是提交后，本地和远程均为最新版本 关于删除/新增文件\nGit 删除/新增文件需要指明某一个文件删除/新增( 新增:git add 删除:git rm)\nSVN 只需要将删除/新增文件直接提交即可 关于优/劣势: 版本最重要的是历史版本,而不是当前版本(文件). svn一旦代码服务器数据丢失,将失去所有历史版本,(除非你本地有每个版本的文件) git是分布式的,每次pull或者生成新的分支都会保留历史版本,也就是说代码服务器数据丢失,历史版本也会保留(只要有人pull过就会保存) git是每个历史版本都存储完整的文件,便于恢复,svn是存储差异文件,历史版本不可恢复 git有一个本地仓库,所以即使你没有网络(包括局域网),你也能提交代码(到本地仓库),等有网了你再提交到代码服务器;而svn是保存文件,没有本地仓库的概念,所以没网的情况下不能提交 关于解决冲突: 当出现冲突后,均需要解决冲突,冲突上面部分是自己的代码,但是不同的是, git解决完冲突后直接commit就行; svn需要标记为解决(mark as merge)才能commit ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E5%92%8Csvn.html","summary":"区别: 关于切换分支 Git 的切换分支更像(或者说就是)切换快照(head),因为可以在同一个文件夹下使用 git checkout 来切换分支(本地代码就切换了) svn的","title":"git和svn"},{"content":"[toc]\n工作区（working tree）： 本地编辑器 暂存区（index）：git add操作后进入暂存区，可用git status查看 本地仓库（repository）：git commit 后进入本地仓库 git commit回退三种姿势_DiuDiu_yang的博客-CSDN博客_git 回退commit 1. 初始化git和github 先在github上新建一个项目\nssh-keygen -t rsa -C \u0026quot;1767822853@qq.com\u0026quot; //123 是你自己注册GitHub的邮箱 (一路回车即可)\n用户主目录里找到.ssh文件夹，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露，id_rsa.pub是公钥，可以公开。\n把公钥配置到github上\n验证\nssh -T git@github.com\n配置user和email\ngit config --global user.name \u0026#34;xkj\u0026#34; # 用户名 git config --global user.email \u0026#34;1767822853@qq.com\u0026#34; # 用户邮箱地址 连接git和github\ngit remote add origin git@github.com:flora0103/example.git\n剩下的就是走代码提交流程了\n连接流程 push时问题 2. 代码提交流程: git add .（后面有一个点，意思是将你本地所有修改了的文件添加到暂存区）\ngit commit -m \u0026quot;\u0026quot;\n(引号里面是你的介绍，就是你的这次的提交是什么内容，便于你以后查看，这个是将索引的当前内容与描述更改的用户和日志消息一起存储在新的提交中)\ngit pull origin master 这是下拉代码，将远程最新的代码先跟你本地的代码合并一下，如果确定远程没有更新，可以不用这个，最好是每次都执行以下，完成之后打开代码查看有没有冲突，并解决，如果有冲突解决完成以后再次执行1跟2的操作\ngit push origin master 将代码推至远程就可以了\n3. 文件操作: 新增文件: git add a.txt\n删除文件: git rm a.txt\n初始化项目时,github已存在文件时,会发生冲突 error: failed to push some refs to\n使用如下命令：git pull --rebase origin master 然后再进行上传: git push -u origin master ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E5%91%BD%E4%BB%A4.html","summary":"[toc] 工作区（working tree）： 本地编辑器 暂存区（index）：git add操作后进入暂存区，可用git status查看 本地仓库（rep","title":"git命令"},{"content":"git 的缺点：\nGit 没有严格的权限管理控制，一般通过系统设置文件读写权限的方式来做权限控制。\n就是那种,一个项目,前段只能看到前段代码,后端只能看到后端代码,git是做不到这种按路径授权, 如果允许按照路径授权，则各个克隆的关系将不再是平等的关系，有的内容多，有的内容少，分布式的理念被破坏\n工作目录只能是整个项目。比如 checkout，建分支，都是基于整个项目的。而 svn 可以基于项目中的某一个目录\n来自* \u0026lt;https://blog.csdn.net/hellow__world/article/details/72529022 \u0026gt;\nhttps://www.zhihu.com/question/22363313/answer/142703190 关于git 密钥:(主要用于免密登录)\ngit用的是密钥,由公钥和私钥组成( 公钥加密,私钥解密,所以说公钥给别人,私钥自己留 =\u0026gt; 两个人想通信,则公钥要互给 )\n我们要和github传代码,就要互相通信,我们把公钥上传到github上,就要能访问github了,第一次访问时github就将指纹系统保存下来了(仿佛就像将github的公钥保存在自己电脑上),以后就能互相通信了\n登录过程和使用 rlogin 或 telnet 建立的会话非常类似。 在连接时， SSH 会利用一个密钥指纹系统来验证服务器的真实性。 只有在第一次连接时， 用户会被要求输入 yes。 之后的连接将会验证预先保存下来的密钥指纹。 如果保存的指纹与登录时接收到的不符， 则将会给出警告。 指纹保存在 ~/.ssh/known_hosts 中， 对于 SSH v2 指纹， 则是 ~/.ssh/known_hosts2。\n来自 https://www.freebsd.org/doc/zh_CN/books/handbook/openssh.html 来自 https://www.cnblogs.com/dzblog/p/6930147.html 误解:以前以为git中的密钥是用来加密代码传输的,其实是用来验证电脑和github间的信任\n认证流程：\nClient端用户TopGun将自己的公钥存放在Server上，追加在文件authorized_keys中。\nServer收到登录请求后，随机生成一个字符串str1，并发送给Client。\nClient用自己的私钥对字符串str1进行加密。\n将加密后字符串发送给Server。\nServer用之前存储的公钥进行解密，比较解密后的str2和str1。\n根据比较结果，返回客户端登陆结果。\n来自* \u0026lt;https://www.cnblogs.com/dzblog/p/6930147.html \u0026gt;\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E6%8B%93%E5%B1%95.html","summary":"git 的缺点： Git 没有严格的权限管理控制，一般通过系统设置文件读写权限的方式来做权限控制。 就是那种,一个项目,前段只能看到前段代码,后端只能看到后","title":"git拓展"},{"content":"[toc]\n1. 为何HBase速度很快？ HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree(Log-Structured Merge-Tree) + HTable(region分区) + Cache决定——客户端可以直接定位到要查数据所在的HRegion server服务器，然后直接在服务器的一个region上查找要匹配的数据，并且这些数据部分是经过cache缓存的。\nHBase的写入速度快是因为它其实并不是真的立即写入文件中，而是先写入内存，随后异步刷入HFile。所以在客户端看来，写入速度很快。另外，写入时候将随机写入转换成顺序写，数据写入速度也很稳定。\n读取速度快是因为它使用了LSM树型结构，而不是B或B+树.HBase的存储结构导致它需要磁盘寻道时间在可预测范围内，并且读取与所要查询的rowkey连续的任意数量的记录都不会引发额外的寻道开销。而且，HBase读取首先会在缓存（BlockCache）中查找，它采用了LRU（最近最少使用算法），如果缓存中没找到，会从内存中的MemStore中查找，再去HFile查找\nhttps://zhuanlan.zhihu.com/p/83233850 2. hbase 实时查询的原理 实时查询，可以认为是从内存中查询，一般响应时间在 1 秒内。 HBase 的机制是数据先写入到内存中，当数据量达到一定的量（如 128M），再写入磁盘中， 在内存中，是不进行数据的更新或合并操作的，只增加数据，这使得用户的写操作只要进入内存中就可以立即返回，保证了 HBase I/O 的高性能。\n3. 列簇怎么创建比较好？(\u0026lt;=2) rowKey 最好要创建有规则的 rowKey，即最好是有序的。 HBase 中一张表最好只创建一到两个列族比较好，因为 HBase 不能很好的处理多个列族。\n4. 描述 HBase 中 scan 和 get 的功能以及实现的异同？ HBase 的查询实现只提供两种方式：\n按指定 RowKey 获取唯一一条记录，get 方法（org.apache.hadoop.hbase.client.Get）Get 的方法处理分两种 : 设置了 ClosestRowBefore 和没有设置 ClosestRowBefore 的rowlock。主要是用来保证行的事务性，即每个 get 是以一个 row 来标记的。一个 row 中可以有很多 family 和 column。\n按指定的条件获取一批记录，scan 方法(org.apache.Hadoop.hbase.client.Scan）实现条件查询功能使用的就是 scan 方式。\n原文链接：https://blog.csdn.net/shujuelin/article/details/89035272\n5. 简述 HBase 中 compact 用途是什么，什么时候触发，分为哪两种，有什么区别? 在 hbase 中每当有 memstore 数据 flush 到磁盘之后，就形成一个 storefile，当 storeFile的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。\nCompact 的作用：\n合并文件\n清除过期，多余版本的数据(删除的数据不会马上删除,只会被标记需要删除)\n提高读写数据的效率\nHBase 中实现了两种 compaction 的方式：minor and major. 这两种 compaction 方式的\n区别是：\nMinor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。\nMajor 操作是对 Region 下的 HStore 下的所有 StoreFile 执行合并操作，最终的结果是整理合并出一个文件。\n需要注意的是 major 合并的时候，只会对一个列族的 HFile 进行合并。\n原文链接：https://blog.csdn.net/shujuelin/article/details/89035272\n6. HBase rowlock 有什么用 在初始化 Put 对象的时候，如果需要频繁地重复修改某些行（加锁），用户有必要创建一个 RowLock 实例来防止其他客户端访问这些行。\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/hbase.html","summary":"[toc] 1. 为何HBase速度很快？ HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree(Log-Struct","title":"HBase"},{"content":"[toc]\n1.介绍 一句话(官方):分布式存储系统HDFS( Hadoop Distributed File System)。 其实就是一个文件系统，类似于linux的文件系统。有目录，目录下可以存 储文件。但它又是一个分布式的文件系统。\n基本原理\n将文件切分成等大的数据块，分别存储到多台机器上。 每个数据块存在多个备份。 将数据切分、容错、负载均衡等功能透明化。 可将HDFS看成是一个巨大、具有容错性的磁盘。 缺点\n不适合存储大量小文件。 不适合低延迟数据访问 。 不支持多用户写入及任意修改文件 。 2.HDFS实现原理 数据块\n每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之 上的文件系统通过磁盘块来管理该文件系统中的块，该文件系统块的大小可以是磁盘块的整数倍。 HDFS 同样也有块 (block) 的概念，但是大得多，默认为 128 MB 。与单一磁盘上的文件系统相似， HDFS 上的文件也被划分为块大小的多个分块 (chunk) ，作为独立的存储单元。但与其他 文件系统不同的是， HDFS 中小于一个块大小的文件不会占据整个块的空间。(好像老版本的会占用整个空间) 2.1 NameNode\nNameNode是HDFS架构中的主节点。 当NameNode启动时会从fsimage中读取数据,缩短启动时间,并会写入editlog,Namenode只有 在启动时候合并fsimage和edits log.\n功能\n管理各个从节点的状态(DataNode)。 记录存储在HDFS上的所有数据的元数据信息。例如:block存储的位置，文件大小，文件权限，文件层级等等。这些信息以两个文件形式永久保存在本地磁盘上。 命名空间镜像文件(FsImage): fsimage是HDFS文件系统存于硬盘中的元数据检查点，里面记录 了自最后一次检查点之前HDFS文件系统中所有目录和文件的序列化信息 编辑日志(edit-logs)文件:保存了自最后一次检查点之后所有针对HDFS文件系统的操作，比如: 增加文件、重命名文件、删除目录等等。 记录了存储在HDFS上文件的所有变化，例如文件被删除，namenode会记录到editlog中。\n接受DataNode的心跳和各个datanode上的block报告信息，确保DataNode是否存活。\n负责处理所有块的复制因子。\n如果DataNode节点宕机，NameNode会选择另外一个DataNode均衡复制因子，并做负载均衡。\n2.2 Sencondary NameNode S NameNode是NameNode的助手，不要将其理解成是NameNode的备份。Secondary NameNode 的整个目的在HDFS中提供一个Checkpoint Node，所以也被叫做checkpoint node。\n功能\n定时的从NameNode获取EditLogs,并更新到FsImage上。 一旦它有新的fsimage文件，它将其拷贝回NameNode上，NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。 关于NameNode是什么时候将改动 写到edit logs中的? 这个操作实际上是由 DataNode的写操作触发的，当我们往 DataNode写文件时，DataNode会跟 NameNode通信，告诉NameNode什么文 件的第几个block放在它那里，NameNode 这个时候会将这些元数据信息写到edit logs文件中。\n2.3 DataNode DataNode是HDFS架构的从节点，管理各自节点的Block信息。\n功能\n多个数据实际是存储到DataNode上面。 DataNode分别运行在独立的节点上。 DataNode执行客户端级别的读写请求。 向NameNode发送心跳(默认是3s)，报告各自节点的健康状况。 3. HDFS读写流程 3.1 写数据 client将数据分片 client请求namenode，要将多个块写入到HDFS。例如这里的Block A和Block B。 NameNode会给client赋予写权限，并为client提供可以写入数据的DataNode的IP地址。Namenode在选择可写入数据的dataNode的规则是结合了DN的健康状态、复制因子、机架感知等因素随机选择的DN。假如复制因子是3(默认值)，那么会为每个block返回三个IP地址。例如NN为client提供了以下的IP地址列表。 在写入数据之前，client首先 要确认namenode提供的ip列表 是否准备好了接收数据。Client通过连接各个块的ip列表来为 每个块创建流水线。（传递的方式） 流水线建立好以后，client将 会向流水线中写入数据（block 是并行写入的）。Client只会将block A向 DN1复制。其他节点复制是在DN 之间完成的。 当数据复制到所有的DN完成之 后，按照ip地址列表相反的方向，依次反馈写入成功的信息。 DN1将确认信息反馈给client，client再将确认信息反馈给NN， NN更新元数据信息，client关 闭pipline。 3.2 文件读取 Client请求namenode要读取exaple. txt文件。 NN根据自己的元数据信息，反馈给 client一个DataNode的列表(存储B lock A和B)。 Client连接DN，读取BlockA,Block B的数据。 Client合并block A和Block B的数 据。 4. HDFS balancer 集群磁盘数据不均衡导致的原因有很多情况。\n添加新的DataNode节点。 人为干预，修改block副本数。 各个机器磁盘大小不一致。 长时间运行大量的delete操作等。 5. HDFS快照 Hdfs的快照（snapshot）是在某一时间点对指定文件系统拷贝，快照采用只读模式，可以对重要数据进行恢复、防止用户错误性的操作。\n快照分两种：\n一种是：建立文件系统的索引，每次更新文件不会真正的改变文件，而是新开辟一个空间用来保存更改的文件，(hdfs属于该种)\n一种是：拷贝所有的文件系统\n和VM做快照差不多,相当于做了个备份,可以做回滚\n快照创建是瞬间的:成本是0(1)排除查找信息节点的时间 。 额外的内存使用仅仅当对快照进行修改时产生：内存使用时0(M),M是修改文件/目录的数量。 是一个只读的基于时间点文件系统拷贝。快照可以是整个文件系统的也可以是一部分。常用来作为数据备 份，防止用户错误和容灾。 在datanode 上面的blocks 不会复制，做Snapshot 的文件是纪录了block的列表和文件的大小，但是没有数据的复制 Snapshot 并不会影响HDFS 的正常操作: 修改会按照时间的反序记录，这样可以直接读取到最新的数据。 快照数据是 当前数据减去修改的部分计算出来的。 快照会存储在snapshottable的目录下。snapshottable下存储的snapshots 最多为65535个。没有限制snapshottable目录 的数量。管理员可以设置任何的目录成为snapshottable。如果snapshottable里面存着快照，那么文件夹不能删除或者 改名。 6. Hadoop 配额设置 6.1 什么是配额 ​\tHadoop 分布式文件系统(HDFS)允许管理员为每个目录设置配额,限制该目录下占用空间大小,和目录/文件数量.新建立的目录没有配额。最 大的配额是 Long.Max_Value。配额为 1 可以强制目录保持为空。\n​\t目录配额是对目录树上该目录下的名字数量做硬性限制。如果创建文件或目录时超过了配额， 该操作会失败。重命名不会改变该目录的配额;如果重命名操作会导致违反配额限制，该操作 将会失败。如果尝试设置一个配额而现有文件数量已经超出了这个新配额，则设置失败。\n所有超出配额的操作都会失败\n配额和 fsimage 保持一致。当启动时，如果 fsimage 违反了某个配额限制(也许 fsimage 被偷 偷改变了)，则启动失败并生成错误报告。设置或删除一个配额会创建相应的日志记录。\n6.2 配额种类 Name Quotas:设置某一个目录下文件总数 从文件数目上限制\nSpace Quotas:设置某一个目录下可使用空间大小 从空间上限制\n注意：这里需要特别注意的是“Space Quota”的设置所看的不是 Hdfs 的文件大小，而是写入 Hdfs 所有 block 块的大小。包含备份的部分，而且不足一个块也要按照一个块计算\n​\thdfs 的配额管理是跟着目录走，如果目录被重命名，配额依然有效。 麻烦的是，在设置完配额以后，如果超过限制，虽然文件不会写入到 hdfs，但是文件名依然会存在，只是文件 size 为 0。当加大配额设置后，还需要将之前的空文件删除才能进一步写入。如果新设置的 quota 值，小于该目录现有的 Name Quotas 及 Space Quotas，系统并不会给 出错误提示，但是该目录的配置会变成最新设置的 quota。\n设置\n启用设定:hadoop dfsadmin -setQuota 10000 /user/seamon 清除設定: hadoop dfsadmin -clrQuota /user/seamon 可以使用m,g,t 代表 MB,GB,TB 启用设定: hadoop dfsadmin -setSpaceQuota 1g /user/seamon/ 清除設定: hadoop dfsadmin -clrSpaceQuota /user/seamon 7. 复制因子 HDFS为我们提供了可靠的存储，就是因为这个复制因子。默认复制因子是3。\n复制因子是每个block备份的数目，nameNode保证每个block在集群中有复制因子数目的备份，不多也不少。每个block的备份分在不同的dataNode中。\n通常情况下，当复制因数是3时，HDFS的放置策略是一个数据块放置到本地机架的一个节点，另一份数据块放置到本地机架的另一个节点，最后一份数据块放置到不同机架的一个节点。这个策略减少了机架之间的通信而极大提高了写性能。机架发生故障的可能性远小于节点，这个策略并不影响数据的可靠性和可用性，但是，在读数据时，它确实降低了整体带宽，因为数据块仅放置在两个而不是三个机架上，这种策略下，一个文件的复制块没有在机架之间均匀分布，三分之一的复制块在一个节点上；三分之二的复制块在一个机架上，另三分之一的复制块均匀分布在另外的机架上。这个策略提高了写性能，没有降低数据可靠性或读性能。\n为减少全局的带宽和读延迟，HDFS尝试从离要求读的客户端最近的地方读取复制块，如果读者节点与复制块节点在同一机架上，则这个复制块优先用来满足读请求，如果HDFS集群有多个数据中心，则优先使用本地数据中心中的复制块，而不是远方的数据块。\n就近原则\nhttps://www.aboutyun.com/forum.php?mod=viewthread\u0026tid=17306 8. 安全模式 安全模式是hadoop的一种保护机制，用于保证集群中的数据块的安全性。如果HDFS处于安全模式，则表示HDFS是只读状态。\n当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。假设我们设置的副本数（即参数dfs.replication）是5，那么在datanode上就应该有5个副本存在，假设只存在3个副本，那么比例就是3/5=0.6。在配置文件hdfs-default.xml中定义了一个最小的副本的副本率0.999\ndfs.namenode.safemode.threshold-pct\n我们的副本率0.6明显小于0.99，因此系统会自动的复制副本到其他的dataNode,使得副本率不小于0.999.如果系统中有8个副本，超过我们设定的5个副本，那么系统也会删除多余的3个副本(估计是心跳时由NameNode抉择,指定DataNode去删除)。当副本率达标后,过30秒之后,NameNode自动退出安全模式\n查看安全模式状态：hdfs dfsadmin -safemode get 进入安全模式状态：hdfs dfsadmin -safemode enter 离开安全模式状态：hdfs dfsadmin -safemode leave https://www.cnblogs.com/TiePiHeTao/p/5105682165be5a124a01c2cd06a89c64.html 9.空间回收 ​\t如果一个文件被用户或应用程序删除了，它并不立刻从HDFS中删除，相反，HDFS首先改变它的名字，将其放到/trash目录中，只要它保留在/trash中，这个文件可以迅速恢复。当/trash中的文件到达其生命期后，NameNode从HDFS命名空间中删除这个文件。删除这个文件将释放与之关联的数据块的空间。需要注意的是，用户删除一个文件的时刻与HDFS增加剩余空间的时刻之间相比，有一个适当的滞后。\n只要这个文件还在/trash目录，用户可以恢复一个删除的文件。如果用户想恢复，则他/她可以转到/trash目录，恢复这个文件。/trash目录仅包含最新的被删除文件，/trash目录与其他目录相似，除了一点特征之外：HDFS施加特别的自动删除策略给这个目录下的文件。目前，这个缺省的策略是删除超过6个小时的文件\nhttps://www.aboutyun.com/forum.php?mod=viewthread\u0026tid=17306 https://blog.csdn.net/weixin_42297075/article/details/104365795 https://www.aboutyun.com/thread-18075-1-3.html https://www.cnblogs.com/nucdy/p/5684196.html https://www.aboutyun.com/thread-17301-1-3.html https://www.aboutyun.com/forum.php?mod=viewthread\u0026tid=17304 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/hdfs.html","summary":"[toc] 1.介绍 一句话(官方):分布式存储系统HDFS( Hadoop Distributed File System)。 其实就是一个文件系统，类似于linux的文件系统。有目录，目录下可以","title":"HDFS"},{"content":"[toc]\n1. Hive 的 sort by 和 order by 的区别 order by 会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。\nsort by不是全局排序，其在数据进入reducer前完成排序.\n因此，如果用sort by进行排序，并且设置mapred.reduce.tasks\u0026gt;1， 则sort by只保证每个reducer的输出有序，不保证全局有序。\n2. Hbase 和 hive 有什么区别hive 与 hbase 的底层存储是什么？hive是产生的原因是什么？habase是为了弥补hadoop的什么缺陷？\n答案\n​ 共同点：\nhbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储 区别：\nHive是建立在Hadoop之上为了减少MapReducejobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。\n象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。\nHive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。\nHive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。\n6. hive借用hadoop的MapReduce来完成一些hive中的命令的执行\n7. hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。\n8. hbase是列存储。\n9. hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。\n10. hive需要用到hdfs存储文件，需要用到MapReduce计算框架。\n3. 数据仓库hive中，启动hive服务器的命令有哪些？分别代表什么意思？内部表与外部表有啥区别？分区与分桶，指的是什么？ 命令：\nhive \u0026ndash;service metastore 启动元数据\nhive：本地运行hive命令\nhiveserver2：远程服务，开放默认端口 10000\n内部表：内部表删除表时，数据也会被删除，\n外部表：外部表在创建时需要加external，删除表时，表中的数据仍然会存储在hadoop中，不会丢失\n分区：分文件夹：分目录，把一个大的数据集根据业务需要分割成小的数据集\n分桶：分数据：分桶是将数据集分解成更容易管理的若干部分\n原文链接：https://blog.csdn.net/pingsha_luoyan/article/details/97750251\n4. hive中集合数据类型什么？有什么作用？什么情况下，hive需要使用集合类型？ 数据类型：\n6个基本类型：整数，布尔类型，浮点数，字符，时间类型。字节数组\n2个集合数据类型： struct，map，array\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/hive.html","summary":"[toc] 1. Hive 的 sort by 和 order by 的区别 order by 会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）只有一个reducer，会导","title":"hive"},{"content":"hive on spark/yarn使用与区别 使用:\n默认是使用yarn,通过设置 配置文件或者启动时设置可以指定spark, 但使用spark计算引擎很麻烦,要对应版本的spark,且不含hive jar包\n由于官方的spark都自带hive jar包,所以需要自己编码\n是不是cdh不需要?待检验\nhttps://www.jianshu.com/p/339da2b6d480 区别:\n其本质是替换了计算引擎,一个基于MapReduce,一个基于spark,于此他们计算/资源等等方式都有所不同\n小文件的场景,危害,怎么处理 场景:\n接收的就是小文件,需要自行处理 自己的代码造成了很多小文件 危害:\n占用过多空间(一个块存不满也占用那么多)\n文件越多,map也多,划不来\n处理:\n总得来说合并小文件\n通过参数方式,再写入一次做合并\n存储时指定可以压缩的文本格式(sequenceFile),还可以指定压缩格式\n对于不常用的数据,可以使用hadoop的archive归档\n详情见hive的\u0026laquo;小文件\u0026raquo;\nHive中order by，sort by，distribute by，cluster by的区别 一：order by\norder by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，然而只有一个Reducer，会导致当输入规模较大时，消耗较长的计算时间。关于order by的详细介绍请参考这篇文章：Hive Order by操作 。\n二：sort by\nsort by不是全局排序，其在数据进入reducer前完成排序，因此，如果用sort by进行排序，并且设置mapred.reduce.tasks\u0026gt;1，则sort by只会保证每个reducer的输出有序，并不保证全局有序。sort by不同于order by，它不受hive.mapred.mode属性的影响，sort by的数据只能保证在同一个reduce中的数据可以按指定字段排序。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定)，对输出的数据再执行归并排序，即可得到全部结果。\n三：distribute by\ndistribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。\n注：Distribute by和sort by的使用场景\n1.Map输出的文件大小不均。\n2.Reduce输出文件大小不均。\n3.小文件过多。\n4.文件超大。\n四：cluster by\ncluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒叙排序，不能指定排序规则为ASC或者DESC。\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive.html","summary":"hive on spark/yarn使用与区别 使用: 默认是使用yarn,通过设置 配置文件或者启动时设置可以指定spark, 但使用spark计算引擎很麻烦","title":"hive"},{"content":"[toc]\nhttp 2.0 新特性 二进制分帧\n在应用层(HTTP2.0)和传输层(TCP、UDP)新增的二进制分帧层。在这层中,数据会被分割成更小的消息和帧,然后可以无序发,最后组装就行\nhead压缩\n多路复用\n做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。\n因为请求都在同一个tcp连接上完成\n服务器推送\n服务器可以对一个客户端请求发送多个响应,之前是一个请求一个响应\n实现原理大致: 客户端发送一次请求,服务端的请求并不会关闭,发完第一次,接着发第二次\n目前NGINX的V1.13.9和tomcat已经支持,后端也能实现\n关键字是link\n服务器推送有一个很麻烦的问题。所要推送的资源文件，如果浏览器已经有缓存，推送就是浪费带宽。即使推送的文件版本更新，浏览器也会优先使用本地缓存。\n一种解决办法是，只对第一次访问的用户开启服务器推送。\n来源: 服务器推送实现 http2.0新特性 http各版本之间的区别 HTTP协议的Keep-Alive 可以看到里面的请求头部和响应头部都有一个key-valueConnection: Keep-Alive，这个键值对的作用是让HTTP保持连接状态(就是俗称的长链接)，因为HTTP 协议采用“请求-应答”模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP 协议为无连接的协议）；当使用 Keep-Alive 模式时，Keep-Alive 功能使客户端到服务器端的连接持续有效。\n在HTTP 1.1版本后，默认都开启Keep-Alive模式，只有加入加入 Connection: close才关闭连接，当然也可以设置Keep-Alive模式的属性，例如 Keep-Alive: timeout=5, max=100，表示这个TCP通道可以保持5秒，max=100，表示这个长连接最多接收100次请求就断开。\nKeep-Alive模式下如何知道某一次数据传输结束\n如果不是Keep-Alive模式，HTTP协议中客户端发送一个请求，服务器响应其请求，返回数据。服务器通常在发送回所请求的数据之后就关闭连接。这样客户端读数据时会返回EOF（-1），就知道数据已经接收完全了。 但是如果开启了 Keep-Alive模式，那么客户端如何知道某一次的响应结束了呢？\n以下有两个方法\n如果是静态的响应数据，可以通过判断响应头部中的Content-Length 字段，判断数据达到这个大小就知道数据传输结束了。 但是返回的数据是动态变化的，服务器不能第一时间知道数据长度，这样就没有 Content-Length 关键字了。这种情况下，服务器是分块传输数据的，Transfer-Encoding：chunk，这时候就要根据传输的数据块chunk来判断，数据传输结束的时候，最后的一个数据块chunk的长度是0。 使用HTTP建立长连接\n当需要建立 HTTP 长连接时，HTTP 请求头将包含如下内容：\nConnection: Keep-Alive\n如果服务端同意建立长连接，HTTP 响应头也将包含如下内容：\nConnection: Keep-Alive\n当需要关闭连接时，HTTP 头中会包含如下内容：\nConnection: Close\n慢速攻击：Http协议中规定，HttpRequest以\n结尾来表示客户端发送结束。攻击者打开一个Http 1.1的连接，将Connection设置为Keep-Alive， 保持和服务器的TCP长连接。然后始终不发送\n， 每隔几分钟写入一些无意义的数据流， 拖死机器。\ncc攻击 来源地址 强缓存和协商缓存 这里的服务器值是 资源服务器, 例如 NG\n当我们向服务器请求资源后，会根据情况将资源 copy 一份副本存在本地，以方便下次读取。它与本地存储 localStorage 、cookie 等不同，本地存储更多是数据记录，存储量较小，为了本地操作方便。而缓存更多是为了减少资源请求，多用于存储文件，存储量相对较大。\n就浏览器而言，一般缓存我们分为四类，按浏览器读取优先级顺序依次为：Memory Cache、Service Worker Cache、HTTP Cache、Push Cache。而本篇文章主要讲的就是 HTTP Cache HTTP Cache HTTP Cache 是我们开发中接触最多的缓存，它分为强缓存和协商缓存。优先级: 强缓存 \u0026gt; 协商缓存\n强缓存：直接从本地副本比对读取，不去请求服务器，返回的状态码是 200。\n协商缓存：会去服务器比对，若没改变才直接读取本地缓存，返回的状态码是 304。\n因为需要问 服务器 看资源是否过期, 所以叫协商缓存\n强缓存 强缓存主要包括 expires 和 cache-control。\nexpires expires 是 HTTP1.0 中定义的缓存字段。当我们请求一个资源，服务器返回时，可以在 Response Headers 中增加 expires 字段表示资源的过期时间。它是一个时间戳（准确点应该叫格林尼治时间），当客户端再次请求该资源的时候，会把客户端时间与该时间戳进行对比，如果大于该时间戳则已过期，否则直接使用该缓存资源。\n例如: expires: Thu, 03 Jan 2019 11:43:04 GMT\n但是，发送请求时是使用的客户端时间去对比。所以存在以下两个问题\n客户端和服务端时间可能快慢不一致 客户端的时间是可以自行修改的（比如浏览器是跟随系统时间的，修改系统时间会影响到），所以不一定满足预期。 cache-control 正由于上面说的可能存在的问题，HTTP1.1 新增了 cache-control 字段来解决该问题，所以当 cache-control 和 expires 都存在时，cache-control 优先级更高。该字段是一个时间长度，单位秒，表示该资源过了多少秒后失效。当客户端请求资源的时候，发现该资源还在有效时间内则使用该缓存，它不依赖客户端时间。\ncache-control 主要有 max-age 和 s-maxage、public 和 private、no-cache 和 no-store 等值。例如: cache-control: public, max-age=3600, s-maxage=3600\n属性值 值 备注 max-age 3600 例如值为3600，表示（当前时间+3600秒）内不与服务器请求新的数据资源, s-maxage 和max-age一样且优先级更高，在代理服务器中仍生效 private 内容只缓存到私有缓存中(仅客户端可以缓存，代理服务器不可缓存) public 所有内容都将被缓存(客户端和代理服务器都可缓存) no-store 不缓存任何数据, 直接向服务器请求最新 no-cache 表示的是不直接询问浏览器缓存情况，而是去向服务器验证当前资源是否更新（即走协商缓存） pragma 既然讲到了 no-cache 和 no-store，就顺便把 pragma 也讲了。他的值有 no-cache 和 no-store，表示意思同 cache-control，优先级高于 cache-control 和 expires，即三者同时出现时，先看 pragma -\u0026gt; cache-control -\u0026gt; expires。\npragma: no-cache\n那时候Cache-control（http1.1）还没出\n协商缓存 上面的 expires 和 cache-control 都会访问本地缓存直接验证看是否过期，如果没过期直接使用本地缓存，并返回 200。但如果设置了 no-cache 和 no-store 则本地缓存会被忽略，会去请求服务器验证资源是否更新，如果没更新才继续使用本地缓存，此时返回的是 304，这就是协商缓存。协商缓存主要包括last-modified和 etag。\n客户端携带上一次的 last-modified和 etag 值到服务端, 服务端进行比较这个两个值是否发生变化, 如果不携带,则默认为要获取新值\nif-modified-since 存上次访问返回的 last-modified 值\nif-none-match 存上次访问返回的 etag 值\nNGINX 默认是开启Etag和 last-modified 的, 所以es中自定义分词列表的热更新能力就是通过这两个字段实现的\nlast-modified last-modified 记录资源最后修改的时间。启用后，请求资源之后的响应头会增加一个 last-modified 字段，如下：\n例如: last-modified: Thu, 20 Dec 2018 11:36:00 GMT\n当再次请求该资源时，请求头中会带有 if-modified-since字段，值是之前返回的 last-modified 的值，如：if-modified-since:Thu, 20 Dec 2018 11:36:00 GMT。服务端会对比该字段和资源的最后修改时间，若一致则证明没有被修改，告知浏览器可直接使用缓存并返回 304；若不一致则直接返回修改后的资源，并修改 last-modified 为新的值。\n但 last-modified 有以下两个问题：\n只要编辑了，不管内容是否真的有改变，都会以这最后修改的时间作为判断依据，当成新资源返回，从而导致了没必要的请求响应 时间的精确度只能到秒，如果在一秒内的修改是检测不到更新的，仍会告知浏览器使用旧的缓存。 ETag (EntityTags) 为了解决last-modified 上述问题，有了 etag。 etag 会基于资源的内容编码生成一串唯一的标识字符串，只要内容不同，就会生成不同的 etag。启用 etag 之后，请求资源后的响应返回会增加一个 etag 字段，如下： etag: \u0026quot;FllOiaIvA1f-ftHGziLgMIMVkVw_\u0026quot;\n当再次请求该资源时，请求头会带有 if-none-match 字段，值是之前返回的 etag 值，如：if-none-match:\u0026quot;FllOiaIvA1f-ftHGziLgMIMVkVw_\u0026quot;。服务端会根据该资源当前的内容生成对应的标识字符串和该字段进行对比，若一致则代表未改变可直接使用本地缓存并返回 304；若不一致则返回新的资源（状态码200）并修改返回的 etag 字段为新的值。\n可以看出 etag 比 last-modified 更加精准地感知了变化，所以 etag 优先级也更高。不过从上面也可以看出 etag 存在的问题，就是每次生成标识字符串会增加服务器的开销。\n弱etag\nETag: W/\u0026quot;630c1e6c-3485\u0026quot; 在商城中看到的是带了W/的, 这表示弱校验 , 全称是 weak, 强校验是字节级别的, 弱校验是语义级别的\n在Apache服务器中 Apache默认通过FileEtag中FileEtag | Node Mtime Size的配置自动生成ETag (节点/修改时间/文件大小 三个因素), 如果文件在一秒内频繁修改,并不改内容, 且etag的生成规则改为 Mtime, 则etag在这一秒内的都是一样的, 此种情况下可以通过 弱etag 来减少刷新缓存 (毕竟强etag是全局唯一的)\netag 的生成规则 http并没有定义 , 因此具体的etag生成逻辑由服务端实现\n一文讲透HTTP缓存之ETag - 掘金 (juejin.cn) HTTP 条件请求 - HTTP |MDN (mozilla.org) 案例分析 假设当前有这么一个 index 页面，返回的响应信息如下：\ncache-control: max-age=72000 expires: Tue, 20 Nov 2018 20:41:14 GMT last-modified: Tue, 20 Nov 2018 00:41:14 GMT 标签进入、输入url回车进入\n这种情况下会根据实际设计的缓存策略去判断。\n由于该例没有设置 no-cache 和 no-store，所以默认先走强缓存路线。根据 cache-control （expires 优先级低）判断缓存是否过期，若没有过期则此时返回 200(from cache)。 若本地缓存已经过期再走协商缓存路线，根据之前的 last-modified 值去与服务器比对，若这个时间之后没有改过则去读取本地缓存，返回 304(not modified)。 否则返回新的资源，状态码 200(ok)，并更新返回响应的 last-modified 值。 按刷新按钮、F5 刷新、网页右键“重新加载”\n不使用强缓存,直接使用 协商缓存\nctrl + F5 强制刷新\n强缓存和协商缓存都不走, 直接获取最新资源\nHTTP中的强缓存与协商缓存 - 漫思 - 博客园 (cnblogs.com) 强缓存和协商缓存 - 简书 (jianshu.com) http3 HTTP3基于UDP协议重新定义了连接，在QUIC层实现了无序、并发字节流的传输，解决了队头阻塞问题（包括基于QPACK解决了动态表的队头阻塞）； HTTP3重新定义了TLS协议加密QUIC(Quick UDP Internet Connection)头部的方式，既提高了网络攻击成本，又降低了建立连接的速度（仅需1个RTT就可以同时完成建链与密钥协商）； HTTP3 将Packet、QUIC Frame、HTTP3 Frame分离，实现了连接迁移功能，降低了5G环境下高速移动设备的连接维护成本。 基于TCP实现的HTTP2遗留下3个问题：\n有序字节流引出的队头阻塞（Head-of-line blocking），使得HTTP2的多路复用能力大打折扣； TCP与TLS叠加了握手时延，建链时长还有1倍的下降空间； 基于TCP四元组确定一个连接，这种诞生于有线网络的设计，并不适合移动状态下的无线网络，这意味着IP地址的频繁变动会导致TCP连接、TLS会话反复握手，成本高昂。 预检请求 发生跨域请求时, 浏览器不知道当前请求是否被服务端允许, 所以需要发送一个请求去验证一下, 此为预检请求.(浏览器自动发起)\n预检请求长什么样?\n可以看到有两个一样的请求, post请求是我们正常的业务请求, 而options就是预检请求, 但预检请求是不带body,也不会修改服务器的资源,也不会返回响应体.\n什么时候会发生预检请求?\n发生了跨域请求\n协议 + 域名 + 端口 组成源, 当 起始源和目标源 不同时认为是跨域请求\n该请求是非简单请求\n浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。\n只要同时满足以下两大条件，就属于简单请求。\n请求方法是以下三种方法之一： HEAD GET POST HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 也就\n我们常用的业务接口, 一般是json格式传参, 或者加了自定义的请求头参数, 也就是说这些接口都是非简单请求\n预检缓存过期或者禁止了缓存 (浏览器控制台上可以禁用缓存)\n详解\n非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为\u0026quot;预检\u0026quot;请求（preflight）。\n浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。预请求实际上是对服务端的一种权限请求\n请求头\n\u0026ldquo;预检\u0026quot;请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，\n关键字段是Origin，表示请求来自哪个源。\nAccess-Control-Request-Method: 用来列出浏览器的CORS请求会用到哪些HTTP方法\nAccess-Control-Request-Headers: 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，例如X-Custom-Header,authorization,saas-auth\n响应头\nAccess-Control-Allow-Origin: 表示允许的来源, * 表示允许任意来源\nAccess-Control-Allow-Methods: 它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法(这样可以避免多次\u0026quot;预检\u0026quot;请求。)\nAccess-Control-Allow-Headers: 它是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在\u0026quot;预检\u0026quot;中请求的字段\nAccess-Control-Max-Age: 该字段可选，用来指定本次预检请求的有效期，单位为秒。\n预检请求 OPTIONS - 知乎 (zhihu.com) http CORS options请求（预检请求）详解 - 知乎 (zhihu.com) 5分钟看懂HTTP3_文化 \u0026amp; 方法_Mehdi Zed_InfoQ精选文章 http3.0初体验 - 掘金 (juejin.cn) HTTP协议-HTTP3 - 简书 (jianshu.com) 深入剖析HTTP3协议 - 知乎 (zhihu.com) HTTP3.0和QUIC协议那些事 - 思创斯聊编程 (ispacesoft.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/http.html","summary":"[toc] http 2.0 新特性 二进制分帧 在应用层(HTTP2.0)和传输层(TCP、UDP)新增的二进制分帧层。在这层中,数据会被分割成更小的消息和帧,然后可","title":"http"},{"content":"[toc]\n介绍 HTTPS（全称：Hyper Text Transfer Protocol over Secure Socket Layer 或 Hypertext Transfer Protocol Secure，超文本传输安全协议），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 它是一个URI scheme（抽象标识符体系），句法类同http :体系。用于安全的HTTP数据传输。https:URL表明它使用了HTTP，但HTTPS存在不同于HTTP的默认端口及一个加密/身份验证层（在HTTP与TCP之间）。\n基础知识 非对称加密 加密使用的密钥和解密使用的密钥是不相同的，分别称为：公钥、私钥，公钥和算法都是公开的，私钥是保密的。非对称加密算法性能较低，但是安全性超强，由于其加密特性，非对称加密算法能加密的数据长度也是有限的。 例如：RSA、DSA、ECDSA、 DH、ECDHE。\n既可以用公钥加密私钥解密（传输敏感信息场景），也可以用私钥加密公钥解密（用户认证场景）\n例如\n非对称加密利用成对的两个秘钥：K1 和 K2。小红用其中一个加密文本，小明可 以用另一个解密文本。比如，小红用 K1 加密，小明用 K2 解密：\n小红 : C = E(M, K1) 小明 : M = D(C, K2) 这样一来，双方中的一方（比如小红）可以生成 K1和K2，然后把其中一个秘钥 （比如K1）私藏，称为私钥；另一个（比如K2）公开，称为公钥。另一 方（比如小明）得到公钥之后，双方就可以通信。\n因为加密和解密的 秘钥值不一样, 所以是不可逆吧\nRSA 算法：该算法的命名以三位科学家的姓氏缩写组合得来，在计算机网络世界，一直是最广为使用的 “非对称加密算法”。\nECC 是非对称加密里的 “后起之秀”，它基于 “椭圆曲线离散对数” 的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。\n对称加密 有流式、分组两种，加密和解密都是使用的同一个密钥。 例如：DES、AES-GCM、ChaCha20-Poly1305等。\n因为加密和解密的 秘钥值是一样的, 所以也是可逆\n证书 证书是用来认证公钥持有者的身份的电子文档，防止第三方进行冒充。一个证书中包含了公钥、持有者信息、证明证书内容有效的签名以及证书有效期，还有一些其他额外信息。操作系统、浏览器本身会预装一些它们信任的根证书，如果其中有该CA机构的根证书，那就可以拿到它对应的可信公钥了。\n实际上证书之间的认证也可以不止一层，可以A信任B，B信任C，以此类推，我们把它叫做信任链或数字证书链，也就是一连串的数字证书，由根证书为起点，透过层层信任，使终端实体证书的持有者可以获得转授的信任，以证明身份。\nCA CA就是签发电子证书的实体。\n数字签名 数字签名技术是将摘要信息用发送者的私钥加密 (CA的私钥)，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。 数字签名是个加密的过程，数字签名验证是个解密的过程。 普通数字签名算法有RSA、ElGamal、Fiat-Shamir、Guillou- Quisquarter、Schnorr、Ong-Schnorr-Shamir数字签名算法、Des/DSA,椭圆曲线数字签名算法和有限自动机数字签名算法等。\nHTTPS使用CA证书的传输方式就是使用了数字签名，非对称加密，对称加密等混合加密技术。\n所以上面用私钥加密, 就是用的CA证书的私钥\nCA数字签名的做法:\n小红把自己的公钥和ID（身份证号码，或者域名）合为身份证申请（certificate signing request，CSR），小红把CSR发给一个信任的人小亮。（被称为 certificate authority，CA） 小亮用自己的私钥加密小红的 CSR，得到的密文被称为数字签名（digital signature）。 小亮把 signature 和 CSR 的明文合在一起称为 CA签署的身份证（CA signed certificate，CRT），发给小红。 每当其他人（比如小明）找小红聊天（建立HTTPS连接）的时候，小红出示自己的小亮签署的身份证。 拿到这个身份证的人，只要小明是相信小亮的——在自己机器上安装了小亮的身份证，就可以从小亮的身份证中的CSR里提取小亮的公钥；\n然后用小亮的公钥解密小红的身份证中小亮的signature，得到一个小红的CSR； 如果这个CSR\u0026rsquo;和小红身份证中的CSR明文一致，则说明“这个小红的身份证是小亮确认过并且签名的”。\nHTTPS的协议栈层级 SSL/TLS协议严格的说位于OSI-7层模型传输层(TCP, UDP)协议之上、应用层之下的会话层。\n如上图所示 HTTPS 相比 HTTP 多了一层 SSL/TLS\nSSL（Secure Socket Layer，安全套接字层）：1994年为 Netscape 所研发，SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。\nTLS（Transport Layer Security，传输层安全）：其前身是 SSL，它最初的几个版本（SSL 1.0、SSL 2.0、SSL 3.0）由网景公司开发，1999年从 3.1 开始被 IETF 标准化并改名，发展至今已经有 TLS 1.0、TLS 1.1、TLS 1.2 三个版本。SSL3.0和TLS1.0由于存在安全漏洞，已经很少被使用到。TLS 1.3 改动会比较大，目前还在草案阶段，目前使用最广泛的是TLS 1.1、TLS 1.2。\n现在不会直接使用SSL协议了, 只是TLS的前身是SSL,且大家习惯了, 所以才会说https的协议是SSL\nTLS协议是一个分层协议，本身可以分为上下两层：\n下层为TLS记录层协议（record layer protocal) 上层为TLS握手层协议（handshake layer protocal) TLS协议的组成如下：\nHTTPS交互消息 说明(四次挥手)：\n浏览器向服务器发送随机数 client_random，TLS 版本和供筛选的加密套件列表。 服务器接收到，立即返回 服务端随机数server_random，以及双方都支持的加密套件 以及数字证书 (证书中附带公钥 Public key certificate)。 浏览器接收，先验证数字证书。若通过，接着使用加密套件的密钥协商算法 RSA 算法生成另一个随机数 pre_random，并且用证书里的公钥加密，传给服务器。 服务器用私钥解密这个被加密后的 pre_random 现在浏览器和服务器都拥有三样相同的凭证：client_random、server_random 和 pre_random。两者都用筛好的加密套件中的加密方法混合这三个随机数，生成数据传输的密钥。\n所以: HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。\nHTTPS是先进行TCP三次握手，再进行TLS四次握手 , 但如果是TLS1.3版本,可以TCP和TLS握手同时进行, 因为TCP的第三次握手时已经安全,可以携带数据了 (如果是TLS1.3 只需要两次握手)\n给面试官上一课：HTTPS是先进行TCP三次握手，再进行TLS四次握手 - 知乎 (zhihu.com) HTTPS的握手 TLS 主要的两种握手方式，分别为：RSA 握手、DH 握手。再以 DH 握手 为基础继续演进优化，推出更安全、性能更佳的 TLS1.3 版本握手方式\nRSA握手 (TLS1.1/1.2) 具体流程如下：\n浏览器向服务器发送随机数 client_random，TLS 版本和供筛选的加密套件列表。 服务器接收到，立即返回 server_random，确认好双方都支持的加密套件 以及数字证书 (证书中附带公钥 Public key certificate)。 浏览器接收，先验证数字证书。若通过，接着使用加密套件的密钥协商算法 RSA 算法生成另一个随机数 pre_random，并且用证书里的公钥加密，传给服务器。 服务器用私钥解密这个被加密后的 pre_random 现在浏览器和服务器都拥有三样相同的凭证：client_random、server_random 和 pre_random。两者都用筛好的加密套件中的加密方法混合这三个随机数，生成最终的密钥。最后，浏览器和服务器使用相同的密钥进行通信，即使用 对称加密。\nDH 握手(TLS1.2) 具体流程如下:\n浏览器向服务器发送随机数 client_random，TLS 版本和供筛选的加密套件列表。 服务器接收到后，确认好双方都支持的加密套件以及数字证书 (证书中附带公钥)。同时服务器利用私钥将 client_random，server_random，server_params 签名，生成服务器签名。然后将签名和 server_params 以及 server_random发送给客户端。 这里的 server_params 为 DH 算法所需参数。 浏览器接收，先验证数字证书和 签名。若通过，将 client_params 传递给服务器。这里的 client_params 为 DH 算法所需参数。 现在客户端和服务器都有 client_params、server_params 两个参数，因 ECDHE 计算基于 “椭圆曲线离散对数”，通过这两个 DH 参数就能计算出 pre_random。 得益于算法机制, 第三个随机数不需要通过网络传输, 安全性更高\nDH 密钥交换协议\nDH 密钥交换协议，Diffile-Hellman key Exchange，简称 DH 或 DHE 。它可以让双方在完全没有对方任何预先信息的条件下通过一个不安全的信道创建一个密钥。 k这个秘钥值可以通过不同的参数求证而得\n其安全性是和RSA是一样的，其安全性依赖于大数因数分解，所以提高安全性只能靠增加位数来保证，这样就涉及大量的乘法运算。性能比较低下。\n为了解决上述DH的问题，引入了ECC椭圆曲线，进而进化为 ECDHE 算法\nTLS1.3 握手 TLS1.3 废除了原有的部分不安全的加密算法，其中甚至包括 RSA 算法。基于ECC(椭圆曲线算法)\nRSA 算法的废除不仅因为已经有大能将其破解，同时还缺少 前向安全性。\n流程梳理：\n浏览器向服务器发送 client_params，client_random，TLS 版本和供筛选的加密套件列表。 服务器返回：server_random、server_params、TLS 版本、确定的加密套件方法以及证书。浏览器接收，先验证数字证书和签名。现在双方都有 client_params、server_params，可以根据 ECDHE 计算出 pre_random 了。 最后，集齐三个参数，生成最终秘钥。\n如你所见，TLS1.3 客户端和服务器之间只需要一次往返就完成 (TLS1.2 需要两次往返来完成握手)，即 1-RTT 握手。\nRTT : Round-Trip Time \u0026ndash; 往返延时, 表示从发送端发送数据开始，到发送端收到来自接收端的确认\n—RTT（Round-Trip Time）_越来越胖的GuanRunwei的博客-CSDN博客 前向安全性: 指的是长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。前向安全能够保护过去进行的通讯不受密码或密钥在未来暴露的威胁\nRSA 没有向前安全性，也就是需要每次的对称加密密钥的传递都是基于 公钥加密，服务端私钥解密。如果服务端的私钥丢失了，那几年前的通信数据都有可能被解密。所以这是极度不安全的，私钥的地位太重了，如果每次的加解密都是临时生成的密码来解决安全性，才不会对私钥的安全性有如此强的依赖。\nTLS过程（DH 非对称加密） - 简书 (jianshu.com) TLS 详解握手流程 - 掘金 (juejin.cn) 证书链 在Chrome上随便打开一个https的网站 https://fmall.gree.com/distributionh5 ，我们会发现在地址栏的左侧有个绿色的小锁，点击这个小锁，然后就可以查看这个网站的证书信息。查看证书信息如下：\n除了HTTPS使用的 gree.com 证书，向上还有两级证书，证书有3类：\nend-user ：gree.com 包含用来加密传输数据的公钥的证书，是HTTPS中使用的证书 intermediates：CA用来认证公钥持有者身份的证书，即确认HTTPS使用的end-user证书是属于gree.com的证书。这类intermediates证书甚至可以有很多级。 root：用来认证intermediates证书是合法证书的证书。 简单来说，end-user证书上面几级证书都是为了保证end-user证书未被篡改，保证是CA签发的合法证书，进而保证end-user证书中的公钥未被篡改。\nCA组织 除了end-user之外，证书被分为root Certificates和intermediates Certificates。相应地，CA也分了两种类型：root CAs 和 intermediates CAs。首先，CA的组织结构是一个树结构，一个root CAs下面包含多个intermediates CAs，而intermediates又可以包含多个intermediates CAs。root CAs 和 intermediates CAs都可以颁发证书给用户，颁发的证书分别是root Certificates和intermediates Certificates，最终用户用来认证公钥的证书则被称为end-user Certificates。\n因为证书有 root 和 intermediates两种, 所以CA组织也有两种, 而且intermediates间也可以颁发\ncertificates(证书) 我们使用end-user certificates来确保加密传输数据的公钥(public key)不被篡改，而又如何确保end-user certificates的合法性呢？这个认证过程跟公钥的认证过程类似，首先获取颁布end-user certificates的CA的证书，然后验证end-user certificates的signature。\n就是套娃一样, 用end-user要保证数据安全, 用intermediate是来保证end-user的安全, 用root来保证intermediates的安全, 这样就形成了证书链\n一般来说，root CAs不会直接颁布end-user certificates的，而是授权给多个二级CA，而二级CA又可以授权给多个三级CA，这些中间的CA就是intermediates CAs，它们才会颁布end-user certificates。\n为什么需要证书链这么麻烦的流程？\nRoot CA为什么不直接版本证书，而是要搞那么多中间层级呢？找了一下，godaddy官方给了一个答案，为了确保root certificates的绝对安全性，https://sg.godaddy.com/en/help/what-is-an-intermediate-certificate-868 ，将根证书隔离地越严格越好。有点像设计模式中的最少知道原则\n了解了这个证书体系之后，才明白为什么百度/google这种公司也需要向第三方购买签名证书了，自签root证书推广起来非常困难，这也导致目前的证书市场基本上被 Symantec(VeriSign/GeoTrust) / Comodo / GoDaddy 垄断。百度使用的是Versign，google使用的是GeoTrust。目前HTTPS的推广已经不可避免，也已经有一些公益组织开始提供免费、自动化、开放的证书签发服务，例如：Let\u0026rsquo;s Encrypt 。详细使用可以参考 入门指南 - Let\u0026rsquo;s Encrypt - 免费的SSL/TLS证书 (letsencrypt.org) 总结 Q\u0026amp;A 中间人攻击是什么 CA颁发证书就是解决了中间人攻击.\n假设没有CA证书, 客户端和服务端直接连接交互,流程如下:\n服务器为每个客户端生成一个公钥，将公钥发送给客户端； 客户端选择一个加密算法，然后用公钥加密以后发送给服务器； 服务器收到这个公钥加密后的算法以后拿自己的私钥解密，然后就知道这个加密算法是哪个了。今后就一直用这个算法通信； 但如果有人, 在客户端和服务端做个拦截, 所有的交互都经过中间人, 从第一步开始就冒充了服务端, 把中间人的公钥给客户端, 这样中间人就可以代替服务端与客户端交互, 或者篡改数据后转发给服务端, 这就是中间人攻击\n为什么数据传输是用对称加密？ 首先：非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的。\n用了 HTTPS 会被抓包吗？ HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。\n但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求。因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理。通常 HTTPS 抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环。\n既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？\nHTTPS 可以防止用户在不知情的情况下通信链路被监听，对于主动授信的抓包操作是不提供防护的，因为这个场景用户是已经对风险知情。要防止被抓包，需要采用应用级的安全防护，例如采用私有的对称加密，同时做好移动端的防反编译加固，防止本地算法被破解。\nHTTPS必须在每次请求中都要先在SSL/TLS层进行握手传输密钥吗？ 这也是我当时的困惑之一，显然每次请求都经历一次密钥传输过程非常耗时，那怎么达到只传输一次呢？靠“session”。\n服务器会为每个浏览器（或客户端软件）维护一个session ID，在TSL握手阶段传给浏览器，浏览器生成好密钥传给服务器后，服务器会把该密钥存到相应的session ID下，之后浏览器每次请求都会携带session ID，服务器会根据session ID找到相应的密钥并进行解密加密操作，这样就不必要每次重新制作、传输密钥了！\nHTTPS协议原理和流程分析 - 腾讯云开发者社区-腾讯云 (tencent.com) 你知道，HTTPS用的是对称加密还是非对称加密？ - 知乎 (zhihu.com) HTTPS用的是对称加密还是非对称加密？ - Rogn - 博客园 (cnblogs.com) 数字签名是什么？ - 阮一峰的网络日志 (ruanyifeng.com) 证书链-Digital Certificates - 简书 (jianshu.com) Java调用https服务报错unable to find valid certification path to requested target的解决方法_wolf的技术博客_51CTO博客 HTTPS加密（握手）过程 - 简书 (jianshu.com) 给面试官上一课：HTTPS是先进行TCP三次握手，再进行TLS四次握手 - 知乎 (zhihu.com) TLS1.3系列文章（2）：TLS1.3协议 - 掘金 (juejin.cn) 一个请求两种握手-TLS握手与TCP握手 - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/https.html","summary":"[toc] 介绍 HTTPS（全称：Hyper Text Transfer Protocol over Secure Socket Layer 或 Hypertext Transfer Protocol Secure，超文本传输安全协议），是以安全为目标的HTTP通道，简单讲是HTTP的","title":"HTTPS"},{"content":"（1）功能\nZeppelin和Hue都能提供一定的数据可视化的功能，都提供了多种图形化数据表示形式。单从这点来说，个人认为功能类似，大同小异，Hue可以通过经纬度进行地图定位，这个功能我在Zeppelin 0.6.0上没有找到。 Zeppelin支持的后端数据查询程序较多，0.6.0版本缺省有18种，原生支持Spark。而Hue的3.9.0版本缺省只支持Hive、Impala、Pig和数据库查询。 Zeppelin只提供了单一的数据处理功能，包括前面提到的数据摄取、数据发现、数据分析、数据可视化等都属于数据处理的范畴。而Hue的功能相对丰富的多，除了类似的数据处理，还有元数据管理、Oozie工作流管理、作业管理、用户管理、Sqoop集成等很多管理功能。从这点看，Zeppelin只是一个数据处理工具，而Hue更像是一个综合管理工具。 （2）架构\nZeppelin采用插件式的翻译器，通过插件开发，可以添加任何后端语言和数据处理程序。相对来说更独立和开放。 Hue与Hadoop生态圈的其它组件密切相关，一般都与CDH一同部署。 （3）使用场景\nZeppelin适合单一数据处理、但后端处理语言繁多的场景，尤其适合Spark。 Hue适合与Hadoop集群的多个组件交互、如Oozie工作流、Sqoop等联合处理数据的场景，尤其适合与Impala协同工作。 来自 \u0026lt;http://blog.csdn.net/wzy0623/article/details/52370045 \u0026gt;\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hue/hue%E4%B8%8Ezeppelin.html","summary":"（1）功能 Zeppelin和Hue都能提供一定的数据可视化的功能，都提供了多种图形化数据表示形式。单从这点来说，个人认为功能类似，大同小异，","title":"Hue与Zeppelin"},{"content":"很明显，这五者之间主要的区别在于第一个单词，而aaS都是as-a-service（即服务）的意思，这五个模式都是近年来兴起的，且这五者都是云计算的落地产品.\nIaaS\n=\u0026gt; Infrastructure-as-a-Service 基础设施即是服务, 简单的不科学说法就是，人家买了一堆电脑租给你用。这是第一层。解决了之前企业要买服务器，装修机房，还要随时防止你没交物业给你停电的问题。服务器都在供应商那， 你不用管啥时候维护。牛逼的供应商很多，阿里云，网易，Azure，AWS-EC2，AWS中国（中国的AWS跟米国的不是一回事，很多服务没有哦）\nPaaS\n=\u0026gt;这个词不好理解，Platform-as-a-Service 平台即是服务, 简单的不科学说法就是，人家帮你装好操作系统了和基础软件啦。供应商不仅帮你装好了操作系统还有很多基础软件，例如JAVA/Python的环境，让你上传代码就可以启动服务了。 牛逼的供应商很多，GAE/BAE/SAE，Heroku，JCS（oracle JAVA-Cloud-Service）。PaaS 系统提供很多OOTB的功能，例如编译环境，CI/CD，SCM，LB，DNS，Log， DB， Cache。其实我发现很多时候SAAS和PAAS大家分也分不清楚，随缘啦。\nSaaS\n=\u0026gt; Software-as-a-Service 软件即是服务, 简单的不科学说法就是，人家帮你装好word啦.这里是Application层，相当发达，咱们小公司，还是在这里机会更多。例如你想要短信服务，阿里云去买。你想要直播服务，优酷去买。你想要CDN服务，各个大厂商都在卖（七牛，阿里云，AWS-S3）。\nCaaS\n=\u0026gt;这个次比较新，Container-as-a-Service 容器即是服务, 简单的不科学且不形象的说法（我也不知道怎么比喻）就是，人家帮你在你mac上装了10个IE（注意，CaaS 有很多中说法，有人说是Communications，有人说是Commerce还有人说是Cloud, 这里只讨论容器服务)。随着容器的迅速发展，开始有大量厂商提供容器服务，让企业成本（现金成本和管理成本）更低，他们一般都提供容器编排服务。流行度还没那么高，目测很多企业还是在ISSA的基础上使用容器编排软件来搞。比较流行的是Docker,Rancher,Kubernetes和Mesos。\nIaaS VS PaaS\n简单点说IaaS提供虚拟机，而PaaS提供了IDE和测试环境。用了IaaS你就可以装很多乱七八糟的软件了，而PaaS可能对你的行为进行限制，例如你在BAE上部署一个spring项目，你想读取磁盘的一个文件。PaaS会说NO，它已经帮你弄好了编译/运行时环境，限制也大，但是简单。如果你用IaaS，这是你的虚拟机，你可做任何你想要的，rm -rf / 也可以。应用级别的软件就要自己装自己搞了。\nPaaS VS CaaS\nPaaS针对特定的应用，提供了整套的解决方案，但是对于个性化需求限制太大，随着DevOps/Micro-Service的迅速发展，越来越多的公司期望通过环境的统一解决Dev/Test/Production的一致性问题，让部署更加自动化与统一化。容器来了，特别随着Docker的牛逼，这个神器让复杂的Micro-service问题降低了很大的门槛。当然CaaS也更便宜。\n听起来，Xxxx-as-a-Service 似乎感觉是继承关系。确实是，但不是绝对的。\n链接：https://www.jianshu.com/p/b27a9f4686f6\n更通俗的解释: https://www.zhihu.com/question/21641778/answer/62523535?from=groupmessage ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/iaaspaassaascaas.html","summary":"很明显，这五者之间主要的区别在于第一个单词，而aaS都是as-a-service（即服务）的意思，这五个模式都是近年来兴起的，且这五者都是云","title":"IaaS，PaaS，SaaS，CaaS"},{"content":"[toc]\n1.Impala和Hive的关系 Impala是基于Hive的大数据实时分析查询引擎，直接使用Hive的元数据库Metadata,意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断的完善中。\n与Hive的关系\nImpala 与Hive都是构建在Hadoop之上的数据查询工具各有不同的侧重适应面，但从客户端使用来看Impala与Hive有很多的共同之处，如数据表元数 据、ODBC/JDBC驱动、SQL语法、灵活的文件格式、存储资源池等。Impala与Hive在Hadoop中的关系如下图所示。\nHive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询，Impala给数据分析人员提供了快速实验、验证想法的大数 据分析工具。可以先使用hive进行数据转换处理，之后使用Impala在Hive处理后的结果数据集上进行快速的数据分析。\n2. Impala相对于Hive所使用的优化技术 1、没有使用 MapReduce进行并行计算，虽然MapReduce是非常好的并行计算框架，但它更多的面向批处理模式，而不是面向交互式的SQL执行。与 MapReduce相比：Impala把整个查询分成一执行计划树，而不是一连串的MapReduce任务，在分发执行计划后，Impala使用拉式获取 数据的方式获取结果，把结果数据组成按执行树流式传递汇集，减少的了把中间结果写入磁盘的步骤，再从磁盘读取数据的开销。Impala使用服务的方式避免 每次执行查询都需要启动的开销，即相比Hive没了MapReduce启动时间。 2、使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。 3、充分利用可用的硬件指令（SSE4.2）。 4、更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。 5、通过选择合适的数据存储格式可以得到最好的性能（Impala支持多种存储格式）。 6、最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。 3. Impala与Hive的异同 数据存储：使用相同的存储数据池都支持把数据存储于HDFS, HBase。 元数据：两者使用相同的元数据。 SQL****解释处理：比较相似都是通过词法分析生成执行计划。 执行计划：\nHive: 依赖于MapReduce执行框架，执行计划分成 map-\u0026gt;shuffle-\u0026gt;reduce-\u0026gt;map-\u0026gt;shuffle-\u0026gt;reduce…的模型。如果一个Query会 被编译成多轮MapReduce，则会有更多的写中间结果。由于MapReduce执行框架本身的特点，过多的中间过程会增加整个Query的执行时间。 Impala: 把执行计划表现为一棵完整的执行计划树，可以更自然地分发执行计划到各个Impalad执行查询，而不用像Hive那样把它组合成管道型的 map-\u0026gt;reduce模式，以此保证Impala有更好的并发性和避免不必要的中间sort与shuffle。 数据流：\nHive: 采用推的方式，每一个计算节点计算完成后将数据主动推给后续节点。 Impala: 采用拉的方式，后续节点通过getNext主动向前面节点要数据，以此方式数据可以流式的返回给客户端，且只要有1条数据被处理完，就可以立即展现出来，而不用等到全部处理完成，更符合SQL交互式查询使用。 内存使用：\nHive: 在执行过程中如果内存放不下所有数据，则会使用外存，以保证Query能顺序执行完。每一轮MapReduce结束，中间结果也会写入HDFS中，同样由于MapReduce执行架构的特性，shuffle过程也会有写本地磁盘的操作。 Impala: 在遇到内存放不下数据时，当前版本1.0.1是直接返回错误，而不会利用外存，以后版本应该会进行改进。这使用得Impala目前处理Query会受到一 定的限制，最好还是与Hive配合使用。Impala在多个阶段之间利用网络传输数据，在执行过程不会有写磁盘的操作（insert除外）。 调度：\nHive: 任务调度依赖于Hadoop的调度策略。 Impala: 调度由自己完成，目前只有一种调度器simple-schedule，它会尽量满足数据的局部性，扫描数据的进程尽量靠近数据本身所在的物理机器。调度器 目前还比较简单，在SimpleScheduler::GetBackend中可以看到，现在还没有考虑负载，网络IO状况等因素进行调度。但目前 Impala已经有对执行过程的性能统计分析，应该以后版本会利用这些统计信息进行调度吧。 容错：\nHive: 依赖于Hadoop的容错能力。 Impala: 在查询过程中，没有容错逻辑，如果在执行过程中发生故障，则直接返回错误（这与Impala的设计有关，因为Impala定位于实时查询，一次查询失败， 再查一次就好了，再查一次的成本很低）。但从整体来看，Impala是能很好的容错，所有的Impalad是对等的结构，用户可以向任何一个 Impalad提交查询，如果一个Impalad失效，其上正在运行的所有Query都将失败，但用户可以重新提交查询由其它Impalad代替执行，不 会影响服务。对于State Store目前只有一个，但当State Store失效，也不会影响服务，每个Impalad都缓存了State Store的信息，只是不能再更新集群状态，有可能会把执行任务分配给已经失效的Impalad执行，导致本次Query失败。 适用面：\nHive: 复杂的批处理查询任务，数据转换任务。 Impala：实时数据分析，因为不支持UDF，能处理的问题域有一定的限制，与Hive配合使用,对Hive的结果数据集进行实时分析。 来自* \u0026lt;https://www.cnblogs.com/zlslch/p/6785207.html?utm_source=itdadao\u0026amp;utm_medium=referral \u0026gt;\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/impala/impala%E4%B8%8Ehive.html","summary":"[toc] 1.Impala和Hive的关系 Impala是基于Hive的大数据实时分析查询引擎，直接使用Hive的元数据库Metadata,意味着im","title":"Impala与Hive"},{"content":"1. IoC是什么 **Ioc—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。**在Java开发中，**Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。****对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系.**如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下：\n●**谁控制谁，控制什么****：**传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对 象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）****。\n●**为何是反转，哪些方面反转了****：**有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。\n我们通常做事的方式，如果我们需要某个对象，一般都是采用这种直接创建的方式(new BeautifulGirl())，这个过程复杂而又繁琐，而且我们必须要面对每个环节，同时使用完成之后我们还要负责销毁它，在这种情况下我们的对象与它所依赖的对象耦合在一起。\n我们知道，我们依赖对象其实并不是依赖该对象本身(控制层调用实现层时,需要的是实现层里面内容)，而是依赖它所提供的服务，只要在我们需要它的时候，它能够及时提供服务即可，至于它是我们主动去创建的还是别人送给我们的，其实并不是那么重要。再说了，相比于自己千辛万苦去创建它还要管理、善后而言，直接有人送过来是不是显得更加好呢？\n这个给我们送东西的“人” 就是 IoC，在上面的例子中，它就相当于一个婚介公司，作为一个婚介公司它管理着很多男男女女的资料，当我们需要一个女朋友的时候，直接跟婚介公司提出我们的需求，婚介公司则会根据我们的需求提供一个妹子给我们，我们只需要负责谈恋爱，生猴子就行了。你看，这样是不是很简单明了。\n1.1 注入对象的方式 IOC Service Provider 为被注入对象提供被依赖对象也有如下几种方式：构造方法注入、stter方法注入、接口注入,静态工厂模式。\n1.1.1 构造器注入 构造器注入，顾名思义就是被注入的对象通过在其构造方法中声明依赖对象的参数列表，让外部知道它需要哪些依赖对象。\nYoungMan(BeautifulGirl beautifulGirl){ this.beautifulGirl = beautifulGirl; }\n构造器注入方式比较直观，对象构造完毕后就可以直接使用，这就好比你出生你家里就给你指定了你媳妇。\n1.1.2 setter 方法注入 对于 JavaBean 对象而言，我们一般都是通过 getter 和 setter 方法来访问和设置对象的属性。所以，当前对象只需要为其所依赖的对象提供相对应的 setter 方法，就可以通过该方法将相应的依赖对象设置到被注入对象中。如下：\npublic class YoungMan { private BeautifulGirl beautifulGirl; public void setBeautifulGirl(BeautifulGirl beautifulGirl) { this.beautifulGirl = beautifulGirl; } }\n相比于构造器注入，setter 方式注入会显得比较宽松灵活些，它可以在任何时候进行注入（当然是在使用依赖对象之前），这就好比你可以先把自己想要的妹子想好了，然后再跟婚介公司打招呼，你可以要林志玲款式的，赵丽颖款式的，甚至凤姐哪款的，随意性较强。\n1.1.3 接口方式注入 接口方式注入显得比较霸道，因为它需要被依赖的对象实现不必要的接口，带有侵入性。一般都不推荐这种方式。\n1.1.4 静态工厂模式 https://blog.csdn.net/shadow_zed/article/details/72566611 1.2 springIOC的实现过程 1，资源定位，找到对应的(xml)的位置\n2，将资源信息加载到BeanDefines的子类中，并不创建bean实例。\n3，注册BeanDefines到BeanFactory,这时候也可能还不创建Bean实例。\n4，将将Beandefines子类对象放入map ，判断是否为懒加载，如果非懒加载，则创建单例bean,并将所需要的对象进行依赖注入，若是懒加载，则在完成第三步的时候就进行实例话并进行依赖注入。\nhttps://www.cnblogs.com/chenssy/p/9576769.html https://www.cnblogs.com/xdp-gacl/p/4249939.html https://www.jianshu.com/p/14dffdfc330d 2. DI(依赖注入) IoC的一个重点是在系统运行中，动态的向某个对象提供它所需要的其他对象。这一点是通过DI（Dependency Injection，依赖注入）来实现的。比如对象A需要操作数据库，以前我们总是要在A中自己编写代码来获得一个Connection对象，有了 spring我们就只需要告诉spring，A中需要一个Connection，至于这个Connection怎么构造，何时构造，A不需要知道。在系统运行时，spring会在适当的时候制造一个Connection，然后像打针一样，注射到A当中，这样就完成了对各个对象之间关系的控制。A需要依赖 Connection才能正常运行，而这个Connection是由spring注入到A中的，依赖注入的名字就这么来的。那么DI是如何实现的呢？ Java 1.3之后一个重要特征是反射（reflection），它允许程序在运行的时候动态的生成对象、执行对象的方法、改变对象的属性，spring就是通过反射来实现注入的。\n我觉得IoC是一种思想，一种理论。DI是控制反转理论的一种实现，也许还有其他的实现方式。\nhttps://www.cnblogs.com/chenssy/p/9576769.html https://www.cnblogs.com/xdp-gacl/p/4249939.html ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/ioc%E6%8E%A7%E5%88%B6%E7%BF%BB%E8%BD%AC%E5%92%8Cdi%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5.html","summary":"1. IoC是什么 **Ioc—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。**在Java开发中，**Io","title":"Ioc(控制翻转)和DI(依赖注入)"},{"content":"[toc]\n前言 操作系统的一次IO过程\n应用程序发起的一次IO操作包含两个阶段：\nIO调用：应用程序进程向操作系统内核发起调用。\nIO执行：操作系统内核完成IO操作。\n操作系统内核完成IO操作还包括连个两个过程：\n准备数据阶段：数据从硬件拷贝到内核缓冲区，这里的硬件可以是磁盘，网卡等设备。\n拷贝数据阶段：将数据从内核缓冲区拷贝到用户空间缓冲区\n根据step1是否阻塞可以把IO操作划分为：\n阻塞IO\n非阻塞IO\n根据step2是否是否阻塞可以划分为：\n同步IO 异步IO 同步阻塞 IO (BIO) 服务端为了处理客户端的连接和请求的数据，写了如下代码。\nlistenfd = socket(); // 打开一个网络通信端口 bind(listenfd); // 绑定 listen(listenfd); // 监听 while(1) { connfd = accept(listenfd); // 阻塞建立连接 int n = read(connfd, buf); // 阻塞读数据 doSomeThing(buf); // 利用读到的数据做些什么 close(connfd); // 关闭连接，循环等待下一个连接 } 这段代码会执行得磕磕绊绊，就像这样。\n可以看到，服务端的线程阻塞在了两个地方，一个是 accept 函数，一个是 read 函数。\n如果再把 read 函数的细节展开，我们会发现其阻塞在了两个阶段。\n这就是传统的阻塞 IO。\n整体流程如下图。\n所以，如果这个连接的客户端一直不发数据，那么服务端线程将会一直阻塞在 read 函数上不返回，也无法接受其他客户端连接。\n这肯定是不行的。\n文件描述符:\n既然在Linux操作系统中，你将一切都抽象为了文件，那么对于一个打开的文件，我应用程序怎么对应上呢？\n文件描述符应运而生\n文件描述符：简称fd，当应用程序请求内核打开/新建一个文件时，内核会返回一个文件描述符用于对应这个打开/新建的文件，其fd本质上就是一个非负整数，读写文件也是需要使用这个文件描述符来指定待读写的文件的\n理解文件描述符 - 简书 (jianshu.com) 同步非阻塞 IO (NIO) 为了解决上面的问题，其关键在于改造这个 read 函数。\n有一种聪明的办法是，每次都创建一个新的进程或线程，去调用 read 函数，并做业务处理。\nwhile(1) { connfd = accept(listenfd); // 阻塞建立连接 pthread_create（doWork); // 创建一个新的线程 } void doWork() { int n = read(connfd, buf); // 阻塞读数据 doSomeThing(buf); // 利用读到的数据做些什么 close(connfd); // 关闭连接，循环等待下一个连接 } 这样，当给一个客户端建立好连接后，就可以立刻等待新的客户端连接，而不用阻塞在原客户端的 read 请求上。\n不过，这不叫非阻塞 IO，只不过用了多线程的手段使得主线程没有卡在 read 函数上不往下走罢了。操作系统为我们提供的 read 函数仍然是阻塞的。\n所以真正的非阻塞 IO，不能是通过我们用户层的小把戏，而是要恳请操作系统为我们提供一个非阻塞的 read 函数。\n这个 read 函数的效果是，如果没有数据到达时（到达网卡并拷贝到了内核缓冲区），立刻返回一个错误值（-1），而不是阻塞地等待。\n操作系统提供了这样的功能，只需要在调用 read 前，将文件描述符设置为非阻塞即可。\nfcntl(connfd, F_SETFL, O_NONBLOCK); int n = read(connfd, buffer) != SUCCESS); 这样，就需要用户线程循环调用 read，直到返回值不为 -1，再开始处理业务。\n这里我们注意到一个细节。\n非阻塞的 read，指的是在数据到达前，即数据还未到达网卡，或者到达网卡但还没有拷贝到内核缓冲区之前，这个阶段是非阻塞的。\n当数据已到达内核缓冲区，此时调用 read 函数仍然是阻塞的，需要等待数据从内核缓冲区拷贝到用户缓冲区，才能返回。\n整体流程如下图\nIO 多路复用 多路复用是NIO的一种\n系统给我们提供一类函数（如 select、poll、epoll函数），它们可以同时监控多个connfd的操作，任何一个返回内核数据就绪，应用进程再发起recvfrom系统调用。\n多路复用: 多路指的是多个IO请求(准备数据阶段) , 复用指的是 复制数据阶段用同一个线程处理\n为每个客户端创建一个线程，服务器端的线程资源很容易被耗光。\n当然还有个聪明的办法，我们可以每 accept 一个客户端连接后，将这个文件描述符（connfd）放到一个数组里。\nfdlist.add(connfd); 然后弄一个新的线程去不断遍历这个数组，调用每一个元素的非阻塞 read 方法。\nwhile(1) { for(fd \u0026lt;-- fdlist) { if(read(fd) != -1) { doSomeThing(); } } } 这样，我们就成功用一个线程处理了多个客户端连接。\n你是不是觉得这有些多路复用的意思？\n但这和我们用多线程去将阻塞 IO 改造成看起来是非阻塞 IO 一样，这种遍历方式也只是我们用户自己想出的小把戏，每次遍历遇到 read 返回 -1 时仍然是一次浪费资源的系统调用。\n在 while 循环里做系统调用，就好比你做分布式项目时在 while 里做 rpc 请求一样，是不划算的。\n所以，还是得恳请操作系统老大，提供给我们一个有这样效果的函数，我们将一批文件描述符通过一次系统调用传给内核，由内核层去遍历，才能真正解决这个问题。\nselect select 是操作系统提供的系统调用函数，通过它，我们可以把一个文件描述符的数组发给操作系统， 让操作系统去遍历，确定哪个文件描述符可以读写， 然后告诉我们去处理：\nselect系统调用的函数定义如下。\nint select( int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); // nfds:监控的文件描述符集里最大文件描述符加1 // readfds：监控有读数据到达文件描述符集合，传入传出参数 // writefds：监控写数据到达文件描述符集合，传入传出参数 // exceptfds：监控异常发生达文件描述符集合, 传入传出参数 // timeout：定时阻塞监控时间，3种情况 // 1.NULL，永远等下去 // 2.设置timeval，等待固定时间 // 3.设置timeval里时间均为0，检查描述字后立即返回，轮询 服务端代码，这样来写。\n首先一个线程不断接受客户端连接，并把 socket 文件描述符放到一个 list 里。\nwhile(1) { connfd = accept(listenfd); fcntl(connfd, F_SETFL, O_NONBLOCK); fdlist.add(connfd); } 然后，另一个线程不再自己遍历，而是调用 select，将这批文件描述符 list 交给操作系统去遍历。\nwhile(1) { // 把一堆文件描述符 list 传给 select 函数 // 有已就绪的文件描述符就返回，nready 表示有多少个就绪的 nready = select(list); ... } 不过，当 select 函数返回后，用户依然需要遍历刚刚提交给操作系统的 list。\n只不过，操作系统会将准备就绪的文件描述符做上标识，用户层将不会再有无意义的系统调用开销。\nwhile(1) { nready = select(list); // 用户层依然要遍历，只不过少了很多无效的系统调用 for(fd \u0026lt;-- fdlist) { if(fd != -1) { // 只读已就绪的文件描述符 read(fd, buf); // 总共只有 nready 个已就绪描述符，不用过多遍历 if(--nready == 0) break; } } } 正如刚刚的动图中所描述的，其直观效果如下。\n可以看出几个细节：\nselect 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的。（可优化为不复制）\nselect 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销。（内核层可优化为异步事件通知）\nselect 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。（可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历）\n整个 select 的流程图如下:\n可以看到，这种方式，既做到了一个线程处理多个客户端连接（文件描述符），又减少了系统调用的开销（多个文件描述符只有一次 select 的系统调用 + n 次就绪状态的文件描述符的 read 系统调用）。\npoll poll 也是操作系统提供的系统调用函数。\nint poll(struct pollfd *fds, nfds_tnfds, int timeout); struct pollfd { intfd; /*文件描述符*/ shortevents; /*监控的事件*/ shortrevents; /*监控事件中满足条件返回的事件*/ }; 它和 select 的主要区别就是，去掉了 select 只能监听 1024 个文件描述符的限制。\nepoll epoll 是最终的大 boss，它解决了 select 和 poll 的一些问题。流程图如下:\n还记得上面说的 select 的三个细节么？\nselect 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的。（可优化为不复制）\nselect 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销。（内核层可优化为异步事件通知）\nselect 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。（可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历）\n所以 epoll 主要就是针对这三点进行了改进。\n内核中保存一份文件描述符集合，无需用户每次都重新传入，只需告诉内核修改的部分即可。\n内核不再通过轮询的方式找到就绪的文件描述符，而是通过异步 IO 事件唤醒。\n内核仅会将有 IO 事件的文件描述符返回给用户，用户也无需遍历整个文件描述符集合。\n具体，操作系统提供了这三个函数。\n第一步，创建一个 epoll 句柄\nint epoll_create(int size); 第二步，向内核添加、修改或删除要监控的文件描述符。\nint epoll_ctl( int epfd, int op, int fd, struct epoll_event *event); 第三步，类似发起了 select() 调用\nint epoll_wait( int epfd, struct epoll_event *events, int max events, int timeout); 使用起来，其内部原理就像如下一般丝滑。\n总结 一切的开始，都起源于这个 read 函数是操作系统提供的，而且是阻塞的，我们叫它 阻塞 IO。\n为了破这个局，程序员在用户态通过多线程来防止主线程卡死。\n后来操作系统发现这个需求比较大，于是在操作系统层面提供了非阻塞的 read 函数，这样程序员就可以在一个线程内完成多个文件描述符的读取，这就是 非阻塞 IO。\n但多个文件描述符的读取就需要遍历，当高并发场景越来越多时，用户态遍历的文件描述符也越来越多，相当于在 while 循环里进行了越来越多的系统调用。\n后来操作系统又发现这个场景需求量较大，于是又在操作系统层面提供了这样的遍历文件描述符的机制，这就是 IO 多路复用。\n多路复用有三个函数，最开始是 select，然后又发明了 poll 解决了 select 文件描述符的限制，然后又发明了 epoll 解决 select 的三个不足。\n所以，IO 模型的演进，其实就是时代的变化，倒逼着操作系统将更多的功能加到自己的内核而已。\n如果你建立了这样的思维，很容易发现网上的一些错误。\n比如好多文章说，多路复用之所以效率高，是因为用一个线程就可以监控多个文件描述符。\n这显然是知其然而不知其所以然，多路复用产生的效果，完全可以由用户态去遍历文件描述符并调用其非阻塞的 read 函数实现。而多路复用快的原因在于，操作系统提供了这样的系统调用，使得原来的 while 循环里多次系统调用，变成了一次系统调用 + 内核层遍历这些文件描述符。\n就好比我们平时写业务代码，把原来 while 循环里调 http 接口进行批量，改成了让对方提供一个批量添加的 http 接口，然后我们一次 rpc 请求就完成了批量添加。\nselect、poll、epoll的对比\nselect poll epoll 底层数据结构 数组 链表 红黑树和双链表 获取就绪的fd 遍历 遍历 事件回调 获取就绪的fd 遍历 遍历 事件回调 最大连接数 1024 无限制 无限制 fd数据拷贝 每次调用select，需要将fd数据从用户空间拷贝到内核空间 每次调用poll，需要将fd数据从用户空间拷贝到内核空间 使用内存映射(mmap)，不需要从用户空间频繁拷贝fd数据到内核空间 图解IO模型——BIO，NIO，AIO - 简书 (jianshu.com) 彻底搞懂IO多路复用 (qq.com) 扩展阅读: 图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的! 看一遍就理解：IO模型详解 - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.html","summary":"[toc] 前言 操作系统的一次IO过程 应用程序发起的一次IO操作包含两个阶段： IO调用：应用程序进程向操作系统内核发起调用。 IO执行：操作系统内核完成","title":"IO多路复用"},{"content":"字节流: 大多以Stream结尾\n字符流:大多以Reader和Writer结尾\n字符流与字节流的区别 经过以上的描述，我们可以知道字节流与字符流之间主要的区别体现在以下几个方面：\n字节流操作的基本单元为字节；字符流操作的基本单元为Unicode码元。 字节流默认不使用缓冲区；字符流使用缓冲区。 字节流通常用于处理二进制数据，实际上它可以处理任意类型的数据，但它不支持直接写入或读取Unicode码元；字符流通常处理文本数据，它支持写入及读取Unicode码元。 参考地址:\n字符流和字节流简述 IO类图 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/io%E6%B5%81.html","summary":"字节流: 大多以Stream结尾 字符流:大多以Reader和Writer结尾 字符流与字节流的区别 经过以上的描述，我们可以知道字节流与字符流之间","title":"IO流"},{"content":"[toc]\n同步阻塞IO (BIO) 假设应用程序的进程发起IO调用，但是如果内核的数据还没准备好的话，那应用程序进程就一直在阻塞等待，一直等到内核数据准备好了，从内核拷贝到用户空间，才返回成功提示，此次IO操作，称之为阻塞IO。\n阻塞IO比较经典的应用就是阻塞socket、Java BIO。 阻塞IO的缺点就是：如果内核数据一直没准备好，那用户进程将一直阻塞，浪费性能，可以使用非阻塞IO优化。 同步非阻塞IO (NIO) 如果内核数据还没准备好，可以先返回错误信息给用户进程，让它不需要等待，而是通过轮询的方式再来请求。这就是非阻塞IO，流程图如下：\n非阻塞IO模型，简称NIO，Non-Blocking IO / New IO。它相对于阻塞IO，虽然大幅提升了性能，但是它依然存在性能问题，即频繁的轮询，导致频繁的系统调用，同样会消耗大量的CPU资源。可以考虑IO复用模型，去解决这个问题。\nIO多路复用模型 (NIO) IO复用模型核心思路：系统给我们提供一类函数（如我们耳濡目染的select、poll、epoll函数），它们可以同时监控多个fd的操作，任何一个返回内核数据就绪，应用进程再发起recvfrom系统调用。\n文件描述符fd(File Descriptor),它是计算机科学中的一个术语，形式上是一个非负整数。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。\nepoll函数的调用流程\n更多详见 - IO多路复用.md IO模型之信号驱动模型 信号驱动IO不再用主动询问的方式去确认数据是否就绪，而是向内核发送一个信号（调用sigaction的时候建立一个SIGIO的信号），然后应用用户进程可以去做别的事，不用阻塞。当内核数据准备好后，再通过SIGIO信号通知应用进程，数据准备好后的可读状态。应用用户进程收到信号之后，立即调用recvfrom，去读取数据。\n信号驱动IO模型，在应用进程发出信号后，是立即返回的，不会阻塞进程。它已经有异步操作的感觉了。但是你细看上面的流程图，发现数据复制到应用缓冲的时候，应用进程还是阻塞的。回过头来看下，不管是BIO，还是NIO，还是信号驱动，在数据从内核复制到应用缓冲的时候，都是阻塞的\nIO 模型之异步IO(AIO) 前面讲的BIO，NIO和信号驱动，在数据从内核复制到应用缓冲的时候，都是阻塞的，因此都不是真正的异步。AIO实现了IO全流程的非阻塞，就是应用进程发出系统调用后，是立即返回的，但是立即返回的不是处理结果，而是表示提交成功类似的意思。等内核数据准备好，将数据拷贝到用户进程缓冲区，发送信号通知用户进程IO操作执行完毕。\n看一遍就理解：IO模型详解 - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/io%E6%A8%A1%E5%9E%8B.html","summary":"[toc] 同步阻塞IO (BIO) 假设应用程序的进程发起IO调用，但是如果内核的数据还没准备好的话，那应用程序进程就一直在阻塞等待，一直等到内核数据准备好了，","title":"IO模型"},{"content":"[TOC]\n介绍 stream的优势:\n如表1-1中所示，Stream中的操作可以分为两大类：中间操作与结束操作，\n中间操作只是对操作进行了记录，只有结束操作才会触发实际的计算（即惰性求值），这也是Stream在迭代大集合时高效的原因之一。\n中间操作又可以分为无状态（Stateless）操作与有状态（Stateful）操作，前者是指元素的处理不受之前元素的影响；后者是指该操作只有拿到所有元素之后才能继续下去。\n结束操作又可以分为短路与非短路操作，这个应该很好理解，前者是指遇到某些符合条件的元素就可以得到最终结果；而后者是指必须处理所有元素才能得到最终结果。\n来自 http://www.cnblogs.com/Dorae/p/7779246.html 大概总结一下: 流式迭代集合操作,中间操作不会实际计算,而且会并行处理,(一个数据会同时被处理),等到了结束操作才会触发操作(和spark很像),java8的foreach还有并发处理,在数据量很大时foreach和流式的优势才会体现\n1. 案例 函数式接口(导读): 那些地方可以用,看流的入参就行了\nFunction =\u0026gt; 函数,有输入有输出 参入参数T , 返回 R (用得多)\npredicate =\u0026gt; 谓词/判定, 有输入,返回布尔值,主要作为一个谓词演算推导真假值存在 (用得多)\nconsumer =\u0026gt; 谓词/消费,有输入无输出,\nsupplier =\u0026gt; 提供, 无输入有输出,\n(特殊) Operator =\u0026gt; 继承于 BiFunction ,所以也属于function, 算子Operator包括：UnaryOperator和BinaryOperator。分别对应单元算子和二元算子。其中BinaryOperator 可用于reduce\n来自 https://blog.csdn.net/lz710117239/article/details/76192629 来自 http://www.sohu.com/a/123958799_465959 1.1 基本使用 // Function =\u0026gt; 就是一个函数,有输入输出 // 接收一个参数 Function\u0026lt;Integer, Integer\u0026gt; add = x -\u0026gt; x + 1; System.out.println(add.apply(4)); // 输出: 5 // 接收两个参数 BiFunction\u0026lt;String, Integer, Person\u0026gt; addxy2Str = (x, y) -\u0026gt; new Person(x, y); System.out.println(addxy2Str.apply(\u0026#34;xkj\u0026#34;, 6)); // 输出: 对象信息 String nameStr=\u0026#34;xkj,xkq,xkk,qoo\u0026#34;; List\u0026lt;String\u0026gt; nameList = Arrays.asList(nameStr.split(\u0026#34;,\u0026#34;)); List\u0026lt;Person\u0026gt; personList = new ArrayList\u0026lt;\u0026gt;(); Person p1=new Person(\u0026#34;xkj\u0026#34;, 4); personList.add(p1); // Predicate =\u0026gt; 可以用来过滤,查找 //功能:找到含 xkj 的字符串 =\u0026gt; 返回集合 || 返回布尔 Predicate\u0026lt;String\u0026gt; isConxkj= n-\u0026gt;n.contains(\u0026#34;xkj\u0026#34;); List\u0026lt;String\u0026gt; xkjInfo = nameList.stream().filter(isConxkj).collect(Collectors.toList());//按 isConxkj 过滤 boolean isHaveXkj = nameList.stream().anyMatch(isConxkj); // 按isConxkj 找到任意一个就行 System.out.println(xkjInfo); // 输出 : xkj System.out.println(isHaveXkj); // 输出: true // Consumer =\u0026gt; 只有入参没有返参,可以做一些数据处理 可以用在forEach中给list修改属性值, Consumer\u0026lt;Person\u0026gt; addPrefix = p-\u0026gt;p.setAge(30);; personList.forEach(System.out::println);//修改前 // 输出:Person [name=xkj, age=4] personList.stream().forEach(addPrefix); personList.forEach(System.out::println);//修改后 // 输出 : Person [name=xkj, age=30] //Supplier =\u0026gt; 可以用来取配置信息(还是定义变量来爽),还有new对象(我感觉还是直接new爽快) (暂时不知道场景) Supplier\u0026lt;Person\u0026gt; person=Person::new; Person p = person.get(); // 每个get都是个新的对象 System.out.println(p); //输出: Person [name=null, age=0] /** * 判断语句是否正确执行,返回语句的返回值,报错则为null * 比如,传入格式化时间语句,确定时间是否合法 */ public static \u0026lt;T\u0026gt; T of(Supplier\u0026lt;? extends T\u0026gt; supplier) { try { return supplier.get(); } catch (Throwable t) { return null; } } @Test public void testCollect() { initInfo(); Predicate\u0026lt;String\u0026gt; isConxkj= n-\u0026gt;n.contains(\u0026#34;xk\u0026#34;); //按 isConxkj 计数 Long xkjCount = nameList.stream().filter(isConxkj).collect(Collectors.counting()); System.out.println(xkjCount); //按 isConxkj 分区并对结果计数 (去掉后面的计数函数,就可以得到具体的值) Map\u0026lt;Boolean, Long\u0026gt; xkjCountMap = nameList.stream().collect(Collectors.partitioningBy(n-\u0026gt;n.contains(\u0026#34;xk\u0026#34;),Collectors.counting())); System.out.println(xkjCountMap); //按 isConxkj 过滤,得到的结果用,连接 String xkJoin = nameList.stream().filter(isConxkj).collect(Collectors.joining(\u0026#34;,\u0026#34;)); System.out.println(xkJoin); //按 isConxkj 过滤,得到的结果用,连接 Map\u0026lt;Object, Object\u0026gt; xkMap = nameList.stream().filter(isConxkj).collect(Collectors.toMap(k-\u0026gt;k+\u0026#34;_zz\u0026#34;, v-\u0026gt;v)); System.out.println(xkMap); //按 名字map,person为val Map\u0026lt;Object, Person\u0026gt; xkMapByName = personList.stream().collect(Collectors.toMap(p-\u0026gt;p.getName(),p-\u0026gt;p)); System.out.println(xkMapByName); //分组后去最大值 Map\u0026lt;Integer, Optional\u0026lt;Users\u0026gt;\u0026gt; collect = users.stream().collect(Collectors.groupingBy(Users::getAge, Collectors.maxBy(Comparator.comparing(Users::getId)))); // https://www.jianshu.com/p/21b20c375599 更多使用分组 // https://cloud.tencent.com/developer/article/1863390 // 按年龄分组 Map\u0026lt;Integer, List\u0026lt;Person\u0026gt;\u0026gt; groupMapByAge = personList.stream().collect(Collectors.groupingBy(p-\u0026gt;p.getAge())); System.out.println(groupMapByAge); // 用map操作数值类型时是有装箱处理的,多少有损耗,用mapToLong就是将值变成基本类型,然后用boxed变成普通流 Long decrease = ageList.stream().mapToLong(a-\u0026gt;a).boxed().reduce((x,y)-\u0026gt;x-y).orElse(0L); System.out.println(decrease); // 合并list List\u0026lt;ServiceOrder\u0026gt; serviceOrderList = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; projectNoAll = serviceOrderList.stream().flatMap(s-\u0026gt;s.getProjectNos().stream()).collect(Collectors.toList()); List\u0026lt;AClass\u0026gt; unionResult = Stream.of(serviceOrderList, serviceOrderList).flatMap(Collection::stream).collect(Collectors.toList()); @Test public void testMR() { initInfo(); BinaryOperator\u0026lt;String\u0026gt; lk = (x, y) -\u0026gt; x + y; Optional\u0026lt;String\u0026gt; reduceStr = nameList.stream().map(n -\u0026gt; n).reduce(lk);//将所有的值拼起来 System.out.println(reduceStr.get()); } T reduce(T identity, BinaryOperator\u0026lt;T\u0026gt; accumulator)\n相对于一个参数的方法来说，它多了一个T类型的参数；实际上就相当于需要计算的值在Stream的基础上多了一个初始化的值 eg.\nStream\u0026lt;String\u0026gt; s = Stream.of(\u0026#34;test\u0026#34;, \u0026#34;t1\u0026#34;, \u0026#34;t2\u0026#34;, \u0026#34;teeeee\u0026#34;, \u0026#34;aaaa\u0026#34;, \u0026#34;taaa\u0026#34;); System.out.println(s.reduce(\u0026#34;[value]:\u0026#34;, (s1, s2) -\u0026gt; s1.concat(s2))); // [value]:testt1t2teeeeeaaaataaa 来自 https://blog.csdn.net/icarusliu/article/details/79504602 1.2 其他应用 json 格式化使用, 值不为null的才输出\nreturn JSON.toJSONString(val,(PropertyFilter)((obj, name, value) -\u0026gt; value != null)); // NameFilter: 处理name字段,(比如对name加个后缀) // ValueFilter: 处理value字段,(比如对value加个后缀) // PropertyFilter: 处理obj字段 2. 高阶使用 2.1 受检函数式接口: package com.gree.ecommerce.utils.function; import java.util.Optional; /** * (检查)抛出lambda中异常情况 * * @author A80080 * @createDate 2021/4/21 */ public interface CheckFun { interface Function\u0026lt;T, R\u0026gt; { /** * Function类型 * * @param function 处理函数 * @param p 入参 * @return 处理结果, 报错返回 Optional.empty(); * @author A80080 * @createDate 2021/4/21 */ static \u0026lt;T, R\u0026gt; Optional\u0026lt;R\u0026gt; tryOf(Function\u0026lt;T, R\u0026gt; function, T p) { try { return Optional.ofNullable(function.apply(p)); } catch (Throwable t) { return Optional.empty(); } } /** * 参照 java.util.function.Function * * @param t 入参 * @return R 类型数据 * @throws Exception 异常 * @author A80080 * @createDate 2021/4/21 */ R apply(T t) throws Exception; } interface Supplier\u0026lt;T\u0026gt; { /** * Supplier 类型 * * @param supplier 处理函数 * @return 返回语句的返回值(optional), 报错则为Optional.empty() * @author A80080 */ static \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; tryOf(CheckFun.Supplier\u0026lt;? extends T\u0026gt; supplier) { try { return Optional.ofNullable(supplier.get()); } catch (Throwable t) { return Optional.empty(); } } /** * 参照 java.util.function.Supplier * * @return T 类型 * @throws Exception 异常 * @author A80080 * @createDate 2021/4/21 */ T get() throws Exception; } interface Consumer\u0026lt;T\u0026gt; { /** * consumer 类型 * \u0026lt;p\u0026gt; 报错将忽略 * * @param consumer 处理函数 * @param p 入参 * @author A80080 * @createDate 2021/4/21 */ static \u0026lt;T\u0026gt; void tryOf(CheckFun.Consumer\u0026lt;T\u0026gt; consumer, T p) { try { consumer.accept(p); } catch (Throwable ignored) { } } /** * 参照 java.util.function.Consumer * * @param t 入参 * @throws Exception 异常 * @author A80080 * @createDate 2021/4/21 */ void accept(T t) throws Exception; } interface Predicate\u0026lt;T\u0026gt; { /** * Predicate类型 * * @param predicate 处理函数 * @param p 入参 * @return 处理结果, 报错返回 false * @author A80080 * @createDate 2021/4/21 */ static \u0026lt;T\u0026gt; boolean tryOf(CheckFun.Predicate\u0026lt;T\u0026gt; predicate, T p) { try { return predicate.test(p); } catch (Throwable t) { return false; } } /** * 参照 java.util.function.Predicate * * @param t 入参 * @return boolean * @throws Exception 异常 * @author A80080 * @createDate 2021/4/21 */ boolean test(T t) throws Exception; } } // 使用案例 @Test public void test() { Stream.of(\u0026#34;uNameCookie\u0026#34;, \u0026#34;uidCookie\u0026#34;, \u0026#34;loginNameCookie\u0026#34;, \u0026#34;loginStatusCookie\u0026#34;, \u0026#34;mobileLoginTokenCookie\u0026#34;) .filter(c-\u0026gt;!CheckFun.Predicate.tryOf(a-\u0026gt;URLEncoder.encode(c, \u0026#34;utf-8\u0026#34;).equals(\u0026#34;\u0026#34;),c)) .peek(c -\u0026gt; c.concat(CheckFun.Supplier.tryOf(() -\u0026gt; URLEncoder.encode(c, \u0026#34;utf-8\u0026#34;)).orElse(c))) .map(c -\u0026gt; c.concat(CheckFun.Function.tryOf(a -\u0026gt; URLEncoder.encode(a, \u0026#34;utf-8\u0026#34;), c).orElse(c))) .peek(c -\u0026gt; CheckFun.Consumer.tryOf(x -\u0026gt; c.concat(URLEncoder.encode(x, \u0026#34;utf-8\u0026#34;)), c)) .forEach(System.out::println); } 2.2 根据类中某个字段去重以及分页处理 // 根据类中某个字段去重 @Test public void should_return_2_because_distinct_by_age() { userList = userList.stream() .filter(distinctByKey(User::getName)) .collect(Collectors.toList()); userList.forEach(System.out::println); assertEquals(2, userList.size()); } private static \u0026lt;T, R\u0026gt; Predicate\u0026lt;T\u0026gt; distinctByKey(Function\u0026lt;T, R\u0026gt; keyExtractor) { Set\u0026lt;R\u0026gt; seen = ConcurrentHashMap.newKeySet(); return t -\u0026gt; seen.add(keyExtractor.apply(t)); } // 链接：https://hacpai.com/article/1545321970124 divideBatchHandler(nameList,System.out::println); } /** * 运用场景:当数据量过大,需要分批处理时 * @author xkj * @param dataList 需要处理的数据 * @param consumer 处理函数 * @since */ public \u0026lt;T\u0026gt; void divideBatchHandler(List\u0026lt;T\u0026gt; dataList, Consumer\u0026lt;List\u0026lt;T\u0026gt;\u0026gt; consumer) { Optional.ofNullable(dataList).ifPresent(list -\u0026gt; IntStream.range(0, list.size()) .mapToObj(i -\u0026gt; new AbstractMap.SimpleImmutableEntry\u0026lt;\u0026gt;(i, list.get(i)))// 给数据编号 .collect(Collectors.groupingBy( e -\u0026gt; e.getKey() / 10, Collectors.mapping(Map.Entry::getValue, Collectors.toList()))) // 按编号分批并合并编号对应的值 .values() .parallelStream()// 并行处理 .forEach(consumer) // 执行处理函数 ); } 2.3 自定义Collector Collector主要包含五个参数，它的行为也是由这五个参数来定义的，如下所示：\nCollector实现的三个泛型具体是什么：\nT（输入的元素类型）：T A（累积结果的容器类型）： R（最终生成的结果类型）： public interface Collector\u0026lt;T, A, R\u0026gt; { // supplier参数用于生成结果容器，容器类型为A Supplier\u0026lt;A\u0026gt; supplier(); // accumulator用于消费元素，也就是归纳元素，这里的T就是元素，它会将流中的元素一个一个与结果容器A发生操作 BiConsumer\u0026lt;A, T\u0026gt; accumulator(); // combiner用于两个两个合并并行执行的线程的执行结果，将其合并为一个最终结果A BinaryOperator\u0026lt;A\u0026gt; combiner(); // finisher用于将之前整合完的结果R转换成为A Function\u0026lt;A, R\u0026gt; finisher(); // characteristics表示当前Collector的特征值，这是个不可变Set Set\u0026lt;Characteristics\u0026gt; characteristics(); } 枚举常量Characteristics 中共有三个特征值，它们的具体含义如下：\nCONCURRENT：表示结果容器只有一个（即使是在并行流的情况下）。只有在并行流且收集器不具备此特性的情况下，combiner()返回的lambda表达式才会执行（中间结果容器只有一个就无需合并）。设置此特性时意味着多个线程可以对同一个结果容器调用，因此结果容器必须是线程安全的。\nUNORDERED：表示流中的元素无序。\nIDENTITY_FINISH：表示中间结果容器类型与最终结果类型一致。设置此特性时finiser()方法不会被调用。\n/** 该案例作用: 对集合元素 求和并转成字符串 */ @Test public void Test(){ // 1. 第一个参数是临时值的容器 // 2. 第二个参数是把入参放到容器中 // 3. 第三个参数是把第二步的值做一个结合放到容器中 // 4. 第四个参数把容器中的值转化成结果类型 // 5. 第五个参数是一个特征值 BiConsumer\u0026lt;List\u0026lt;Long\u0026gt;, Long\u0026gt; accumulator = (x, y) -\u0026gt; { x.add(y); return; }; BinaryOperator\u0026lt;List\u0026lt;Long\u0026gt;\u0026gt; combiner = (x, y) -\u0026gt; { x.addAll(y); return x; }; Function\u0026lt;List\u0026lt;Long\u0026gt;, String\u0026gt; func = x -\u0026gt; String.valueOf(x.stream().mapToLong(y-\u0026gt; y).sum()); Collector\u0026lt;Long, List\u0026lt;Long\u0026gt;, String\u0026gt; longSupplier = Collector.of(ArrayList::new, accumulator, combiner,func, Collector.Characteristics.UNORDERED); String collect = Stream.of(1, 1, 2, 2) .map(Long::new) .collect(longSupplier); System.out.println(\u0026#34;collect = \u0026#34; + collect); } java8新特性(四) Collector（收集器）_戏流年的博客-CSDN博客_collector Java基础系列-Collector和Collectors - 简书 (jianshu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/java8.html","summary":"[TOC] 介绍 stream的优势: 如表1-1中所示，Stream中的操作可以分为两大类：中间操作与结束操作， 中间操作只是对操作进行了记录，只有结束操","title":"java8"},{"content":"[toc]\n1. java 历程 java8:增加lambda,流 java9:增加module系统(要用什么先声明) java10: 引入var,局部变量类型判断,不能用于方法上的参数 java11: 允许在lambda中使用var,增加了ZGC,(长期支持版本) java12: swtch增加,可以写表达式,可以(用break)写返回值 java13: swtch改变,用yield 返参,break不返,可以使用\u0026quot;\u0026ldquo;\u0026ldquo;code\u0026rdquo;\u0026ldquo;\u0026ldquo;\u0026ldquo;包含代码块(和py一样) 2. 网站如何提高并发量? 答:无法容纳高访问,是因为无法快速处理请求,大量请求被堆积,导致服务器崩溃.\n目前最耗时的是对数据库的访问,所有优化sql是必要的.\n还可以用各种缓存(Redis),各种服务器(CDN,文件服务器),各种分布式操作,加快对请求的处理\n3. 对象深拷贝与浅拷贝的区别 浅拷贝(影子克隆):只复制对象的基本类型,而对象类型仍属于原来的引用. 深拷贝(深度克隆):不仅复制对象的基本类型,同时也复制原对象中的对象.就是说完全是新对象产生的 4. multipart/form-data 与 application/x-www-form-urlencoded x-www-form-urlencoded\n传递数据是文本格式,form表单提交 默认就是这种格式\nform-data\n一般用于图片,视频等数据,传递数据使用二进制流传递,所以后台是不能接收到值的,所以需要用方面的类来处理,在jfw-front中,引入了文件上传的包,spring就自动解析了,所以jfw-front这两种格式都支持,当然也可以用特定方式来取form-data格式的值\n\u0026lt;!-- 上传文件拦截，设置最大上传文件大小 10M=10*1024*1024(B)=10485760 bytes --\u0026gt; \u0026lt;bean id=\u0026#34;multipartResolver\u0026#34; class=\u0026#34;org.springframework.web.multipart.commons.CommonsMultipartResolver\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;maxUploadSize\u0026#34; value=\u0026#34;10485760\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-web\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 这些格式是用于 http请求或者响应, 用于指明数据的格式, 称为 内容类型 (Content-Type)\nContent-Type 详解_leoss.H的博客-CSDN博客_content-type 5. jwt JSON Web Token（缩写 JWT）是目前最流行的跨域认证解决方案\nJWT 的原理是，服务器认证以后，生成一个 JSON 对象(唯一标识(用户id),到期时间,其他信息等)，发回给用户.\n以后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名（详见后文）。\n服务器就不保存任何 session 数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。\n来自 \u0026lt;http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html\u0026gt; 个人理解: 你登录完后,服务把需要的信息发送给你,以后你访问就带这个信息,我既能验证,又能获得信息(B门户中的组织树信息),用户数据都存在了客户端,服务端就不用存在,以后增加服务器也不用担心验证问题\n实际的 JWT 大概就像下面这样。\ndfesrhsrhtrsh. gaheiugvweiugnaqoweigqowe4e4waqegq. weughgbwggiqgwgru 它是一个很长的字符串，中间用点（.）分隔成三个部分。注意，JWT 内部是没有换行的，这里只是为了便于展示，将它写成了几行。\n来自 \u0026lt;http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html\u0026gt; JWT 的三个部分依次如下。\nHeader（头部） Payload（负载） Signature（签名） 来自 \u0026lt;http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html\u0026gt; Header 部分是一个 JSON 对象，描述 JWT 的元数据，通常是下面的样子。 { \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } 上面代码中，alg属性表示签名的算法（algorithm），默认是 HMAC SHA256（写成 HS256）；typ属性表示这个令牌（token）的类型（type），JWT 令牌统一写为JWT。\n最后，将上面的 JSON 对象使用 Base64URL 算法（详见后文）转成字符串。\n来自 \u0026lt;http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html\u0026gt; Payload 部分也是一个 JSON 对象，用来存放实际需要传递的数据。JWT 规定了7个官方字段，供选用。 iss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 除了官方字段，你还可以在这个部分定义私有字段，下面就是一个例子。\n{ \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;admin\u0026#34;: true } 注意，JWT 默认是不加密的，任何人都可以读到，所以不要把秘密信息放在这个部分。 这个 JSON 对象也要使用 Base64URL 算法转成字符串。\nSignature 部分是对前两部分的签名，防止数据篡改。\n首先，需要指定一个密钥（secret）。这个密钥只有服务器才知道，不能泄露给用户。然后，使用 Header 里面指定的签名算法（默认是 HMAC SHA256），按照下面的公式产生签名。\nHMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret) 算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用\u0026quot;点\u0026rdquo;（.）分隔，就可以返回给用户。\n前面提到，Header 和 Payload 串型化的算法是 Base64URL。这个算法跟 Base64 算法基本类似，但有一些小的不同。\nJWT 作为一个令牌（token），有些场合可能会放到 URL（比如 api.example.com/?token=xxx）。Base64 有三个字符+、/和=，在 URL 里面有特殊含义，所以要被替换掉：=被省略、+替换成-，/替换成_ 。这就是 Base64URL 算法。\n客户端收到服务器返回的 JWT，可以储存在 Cookie 里面，也可以储存在 localStorage。\n此后，客户端每次与服务器通信，都要带上这个 JWT。你可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP 请求的头信息Authorization字段里面。\nAuthorization: Bearer \u0026lt;token\u0026gt; 另一种做法是，跨域的时候，JWT 就放在 POST 请求的数据体里面。\nJWT 的几个特点\n（1）JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。\n（2）JWT 不加密的情况下，不能将秘密数据写入 JWT。\n（3）JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。\n（4）JWT 的最大缺点是，由于服务器不保存 session 状态，因此无法在使用过程中废止某个 token，或者更改 token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑。\n（5）JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT 的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。\n（6）为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。 来自 \u0026lt;http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html\u0026gt; 6. spring的解决循环依赖问题 Spring bean循环依赖即循环引用。是指2个或以上bean 互相持有对方，最终形成闭环。比如A依赖于B，B依赖A。\n产生循环依赖的方式有两种，一种是通过构造器注入形成的循环依赖，第二种是通过field属性注入形成的循环依赖。\nSpring通过特殊的bean生成机制解决了第二种方式产生的循环依赖问题，使得循环链的所有bean对象都能正确创建，而构造器注入方式阐释的循环依赖则会抛出异常。两者之间的差异能在bean创建机制中得到解释。\n总的来说，Spring解决循环依赖问题是通过结合bean实例化和bean属性填装分离，singletonObjects、earlySingletonObjects 、singletonFactories 三级缓存机制和引用提前暴露机制实现的。\n原文链接：https://blog.csdn.net/panda9527z/article/details/107359916\norg.springframework.beans.factory.support.DefaultSingletonBeanRegistry#getSingleton(java.lang.String, boolean) /** 一级缓存，保存singletonBean实例: bean name --\u0026gt; bean instance */ private final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(256); /** 二级缓存，保存创建好但没有初始化属性的Bean : bean name --\u0026gt; bean instance */ private final Map\u0026lt;String, Object\u0026gt; earlySingletonObjects = new HashMap\u0026lt;String, Object\u0026gt;(16); /** 三级缓存，保存singletonBean生产工厂: bean name --\u0026gt; ObjectFactory */ private final Map\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt; singletonFactories = new HashMap\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt;(16); protected Object getSingleton(String beanName, boolean allowEarlyReference) { // 查询一级缓存 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null \u0026amp;\u0026amp; isSingletonCurrentlyInCreation(beanName)) { synchronized (this.singletonObjects) { //若一级缓存内不存在，查询二级缓存 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null \u0026amp;\u0026amp; allowEarlyReference) { //若二级缓存内不存在，查询三级缓存 ObjectFactory\u0026lt;?\u0026gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { //若三级缓存中的，则通过工厂获得对象，并清除三级缓存，提升至二级缓存 singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return (singletonObject != NULL_OBJECT ? singletonObject : null); } 为什么要三级缓存呢?(一级或二级不行吗)\nspring处理循环依赖是把bean的实体化和属性装盘分开(就是 先生成对象,属性等会再设置),这样可以把实体类提前暴露出去,给其他bean使用,这样就解决了循环问题(我先给你用,装配时你只关心我,不关心我所拥有的属性)\n既然如此,我用一级缓存的也是可以,只要提前暴露就行,的确如此,但这样,会把完整的BEAN和不完整的BEAN(还没完成属性装配的BEAN)放一起,那去取的时候,无法区分,如果取到未装配的bean就凉凉了.\n如此,需要二级缓存,大部分情况下,二级缓存就足够用了,但是有种情况下会出问题,\n例如A-\u0026gt;b-\u0026gt;c-\u0026gt;A , 三者形成循环\n我对b类做了切面(切面会利用动态代理生成新的类),这样对bean池中就会两个类,b类和动态b类,但spring又是单例bean,所以会抛异常,这时需要再加一层缓存来保证这一种,spring利用工厂的方式来处理的.\nhttps://my.oschina.net/u/4340310/blog/4332450 https://blog.csdn.net/panda9527z/article/details/107359916 Spring是怎么解决循环依赖的？-Java面试题 (zwmst.com) 7.Future模式和Promise模式 Future:\n这个是是java的concurrent并发包里面的提供的类，他和Callable类常常一块使用。Future 表示异步计算的结果。其用于获取线程池执行callable后的结果，这个结果封装为Future类。\n值得一提的是，由于Future类获取返回结果的get方法是阻塞的，jdk1.8中加入completableFuture使得Future可以注册监听器从而实现类似JS中的callback功能。这也是异步编程的推荐方式：回调。\nPromise\n回调是一种我们推崇的异步调用方式，但也会遇到问题，也就是回调的嵌套。当需要多个异步回调一起书写时，就会出现下面的代码 (以 js 为例):\nasyncFunc1(opt, (...args1) =\u0026gt; { asyncFunc2(opt, (...args2) =\u0026gt; { asyncFunc3(opt, (...args3) =\u0026gt; { asyncFunc4(opt, (...args4) =\u0026gt; { // some operation }); }); }); }); 这样的代码不易读，嵌套太深修改也麻烦。于是 ES6 提出了 Promise 模式来解决回调地狱的问题。Promise 是一个 ECMAScript 6 提供的类，目的是更加优雅地书写复杂的异步任务。\nnew Promise(function (resolve, reject) { console.log(1111); resolve(2222); }).then(function (value) { console.log(value); return 3333; }).then(function (value) { console.log(value); throw \u0026#34;An error\u0026#34;; }).catch(function (err) { console.log(err); }); JDK8 中 completableFuture 类来解决嵌套回调问题, 使用thenCompose 和 whenComplete 这类处理\npublic class CompletableFutureDemo { public static void main(String[] args) throws InterruptedException { long l = System.currentTimeMillis(); CompletableFuture\u0026lt;Integer\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;在回调中执行耗时操作...\u0026#34;); timeConsumingOperation(); return 100; }); completableFuture = completableFuture.thenCompose(i -\u0026gt; { return CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;在回调的回调中执行耗时操作...\u0026#34;); timeConsumingOperation(); return i + 100; }); });//\u0026lt;1\u0026gt; completableFuture.whenComplete((result,e)-\u0026gt;{ System.out.println(\u0026#34;计算结果:\u0026#34; + result); }); System.out.println(\u0026#34;主线程运算耗时:\u0026#34; + (System.currentTimeMillis() - l) + \u0026#34; ms\u0026#34;); new CountDownLatch(1).await(); } static void timeConsumingOperation() { try { Thread.sleep(3000); } catch (Exception e) { e.printStackTrace(); } } } \u0026lt;1\u0026gt; : 使用 thenCompose 或者 thenComposeAsync 等方法可以实现回调的回调，且写出来的方法易于维护。\n简述 详解 8. 响应式和函数式 响应式编程有以下几个特点：\n异步编程：提供了合适的异步编程模型，能够挖掘多核CPU的能力、提高效率、降低延迟和阻塞等。 数据流：基于数据流模型，响应式编程提供一套统一的Stream风格的数据处理接口。和Java 8中的Stream相比，响应式编程除了支持静态数据流，还支持动态数据流，并且允许复用和同时接入多个订阅者。 变化传播：简单来说就是以一个数据流为输入，经过一连串操作转化为另一个数据流，然后分发给各个订阅者的过程。这就有点像函数式编程中的组合函数，将多个函数串联起来，把一组输入数据转化为格式迥异的输出数据。 函数式编程的特点：\n函数是\u0026quot;第一等公民\u0026rdquo;：所谓\u0026quot;第一等公民\u0026rdquo;（first class），指的是函数与其他数据类型一样，处于平等地位，可以赋值给其他变量，也可以作为参数，传入另一个函数，或者作为别的函数的返回值。 闭包和高阶函数：闭包是起函数的作用并可以像对象一样操作的对象。与此类似，FP 语言支持高阶函数。高阶函数可以用另一个函数（间接地，用一个表达式） 作为其输入参数，在某些情况下，它甚至返回一个函数作为其输出参数。这两种结构结合在一起使得可以用优雅的方式进行模块化编程，这是使用 FP 的最大好处。 递归: 用递归做为控制流程的机制。例如在Haskell的世界中，没有变量赋值，流程跳转，如果要实现一些简单的功能，比如求一个数组中的最大值，都需要借助递归实现。 惰性求值(Lazy Evaluation): 它表示为“延迟求值“和”最小化求值“。惰性求值使得代码具备了巨大的优化潜能。支持惰性求值的编译器会像数学家看待代数表达式那样看待函数式编程的程序：抵消相同项从而避免执行无谓的代码，安排代码执行顺序从而实现更高的执行效率甚至是减少错误。 惰性求值另一个重要的好处是它可以构造一个无限的数据类型，无需要担心由无穷计算所导致的 out-of-memory 错误。 没有\u0026quot;副作用\u0026rdquo;(side effect)：指的是函数内部与外部互动（最典型的情况，就是修改全局变量的值），产生运算以外的其他结果。函数式编程强调没有\u0026quot;副作用\u0026quot;，意味着函数要保持独立，所有功能就是返回一个新的值，没有其他行为，尤其是不得修改外部变量的值。 传统的面向对象编程通过抽象出的对象关系来解决问题。函数式编程通过function的组合来解决问题，响应式编程通过函数式编程的方式来解决回调地狱的问题。\n来源文章 8. JIT(即时编译) 定义: java代码需要经过编译形成字节码,然后被翻译执行, 翻译过程需要翻译器来操作的,而JIT是把字节码直接编译成机器码,再加上一些优化,这样使得java代码执行的整体速度加快了\n说JIT比解释快，其实说的是“执行编译后的代码”比“解释器解释执行”要快，并不是说“编译”这个动作比“解释”这个动作快。\nJIT 加快的是解释过程,而不是编译过程, 其中逃逸分析就是JIT的优化处理之一,(逃逸分析的作用之一是栈上分配对象内存)\n既然它快,为什么不全用JIT而是解释器+JIT混合使用呢?\nJIT编译再怎么快，至少也比解释执行一次略慢一些，而要得到最后的执行结果还得再经过一个“执行编译后的代码”的过程。所以，对“只执行一次”的代码而言，解释执行其实总是比JIT编译执行要快 编译后代码的大小相对于字节码的大小，膨胀比达到10x是很正常的。同上面说的时间开销一样，这里的空间开销也是，只有对执行频繁的代码才值得编译，如果把所有代码都编译则会显著增加代码所占空间，导致“代码爆炸” 来源 JIT原理 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/javaee.html","summary":"[toc] 1. java 历程 java8:增加lambda,流 java9:增加module系统(要用什么先声明) java10: 引入var,局部变量类型判断,不能用于方法上的","title":"javaEE"},{"content":"介绍: 该代码支持:\n填充work中的占位符(仅是填充,不能动态加格式之类) 将work转化为html 使用 pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;poi-ooxml\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${poi.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;poi-ooxml-schemas\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${poi.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ooxml-schemas\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;poi-scratchpad\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${poi.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;fr.opensagres.xdocreport\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;xdocreport\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; wordUtils\npublic static HWPFDocument replaceTables2003(String filePath, Map\u0026lt;String, Object\u0026gt; map) throws Exception { //logger.info(\u0026#34;替换word2003文档表格内容，文档路径:{}，要替换的内容:{}\u0026#34;, filePath, JsonUtil.toJSONString(map)); if (StringUtils.isBlank(filePath) || MapUtils.isEmpty(map)) { return null; } try (FileInputStream is = new FileInputStream(filePath)) { HWPFDocument document = new HWPFDocument(is); Range range = document.getRange(); for (Entry\u0026lt;String, Object\u0026gt; e : map.entrySet()) { String value = null == e.getValue() ? \u0026#34;\u0026#34; : (String) e.getValue(); range.replaceText(e.getKey(), value); } return document; } catch (Exception e) { logger.error(\u0026#34;替换word2003文档表格内容失败:\u0026#34; + e.getMessage(), e); throw e; } } public static XWPFDocument replaceTables2007(String filePath, Map\u0026lt;String, Object\u0026gt; map) throws Exception { //logger.info(\u0026#34;替换word2007文档表格内容，文档路径:{}，要替换的内容:{}\u0026#34;, filePath, JsonUtil.toJSONString(map)); if (StringUtils.isBlank(filePath) || MapUtils.isEmpty(map)) { return null; } XWPFDocument document = new XWPFDocument(POIXMLDocument.openPackage(filePath)); /* 替换段落中的指定文字 */ Iterator\u0026lt;XWPFParagraph\u0026gt; itPara = document.getParagraphsIterator(); while (itPara.hasNext()) { XWPFParagraph paragraph = itPara.next(); Set\u0026lt;String\u0026gt; set = map.keySet(); for (String key : set) { List\u0026lt;XWPFRun\u0026gt; run = paragraph.getRuns(); for (XWPFRun xwpfRun : run) { if (xwpfRun.getText(xwpfRun.getTextPosition()) != null \u0026amp;\u0026amp; xwpfRun.getText(xwpfRun.getTextPosition()).contains(key)) { //参数0表示生成的文字是要从哪一个地方开始放置,设置文字从位置0开始 就可以把原来的文字全部替换掉了 String text = xwpfRun.getText(xwpfRun.getTextPosition()); String value = null == map.get(key) ? \u0026#34;\u0026#34; : (String) map.get(key); if (key.contains(\u0026#34;#\u0026#34;) || key.contains(\u0026#34;$\u0026#34;)) { text = text.replaceAll(Pattern.quote(key), value); } else { text = text.replaceAll(key, value); } xwpfRun.setText(text, 0); } } } } // 表格内容 Iterator\u0026lt;XWPFTable\u0026gt; it = document.getTablesIterator(); while (it.hasNext()) { XWPFTable table = it.next(); int rcount = table.getNumberOfRows(); for (int i = 0; i \u0026lt; rcount; i++) { XWPFTableRow row = table.getRow(i); List\u0026lt;XWPFTableCell\u0026gt; cells = row.getTableCells(); for (XWPFTableCell cell : cells) { for (Entry\u0026lt;String, Object\u0026gt; e : map.entrySet()) { if (cell.getText().equals(e.getKey())) { // 删除原来内容 cell.removeParagraph(0); if (e.getValue() != null) { // 写入新内容 cell.setText((String) e.getValue()); } } } } } } return document; } public static String docToHtml(String wordPath, String fileName) { logger.info(\u0026#34;将word2003转换成html，word文件路径:{}，word文件名:{}\u0026#34;, wordPath, fileName); InputStream input = null; ByteArrayOutputStream outStream = null; try { input = new FileInputStream(wordPath + File.separator + fileName); // 初始化转换器 WordToHtmlConverter converter = new WordToHtmlConverter(DocumentBuilderFactory.newInstance().newDocumentBuilder().newDocument()); // 保存文档中的图片 converter.setPicturesManager((content, pictureType, suggestedName, widthInches, heightInches) -\u0026gt; { // 设定图片路径 // return srcPath + File.separator + suggestedName; return suggestedName; }); // word2003 HWPFDocument wordDocument = new HWPFDocument(input); converter.processDocument(wordDocument); // 设定图片 List\u0026lt;Picture\u0026gt; pics = wordDocument.getPicturesTable().getAllPictures(); if (CollectionUtils.isNotEmpty(pics)) { // 存储图片，根据给定的名称 for (Picture pic : pics) { // 将文件直接写出去 pic.writeImageContent(new FileOutputStream(new File(wordPath, pic.suggestFullFileName()))); } } Document docHtml = converter.getDocument(); DOMSource domSource = new DOMSource(docHtml); outStream = new ByteArrayOutputStream(); StreamResult streamResult = new StreamResult(outStream); TransformerFactory tf = TransformerFactory.newInstance(); Transformer serializer = tf.newTransformer(); serializer.setOutputProperty(OutputKeys.ENCODING, Constant.FILE_ENCODING); serializer.setOutputProperty(OutputKeys.INDENT, \u0026#34;yes\u0026#34;); serializer.setOutputProperty(OutputKeys.METHOD, \u0026#34;html\u0026#34;); serializer.transform(domSource, streamResult); // 文件名前缀 String prefix = CommonUtils.getFilePrefix(fileName); String htmlPath = wordPath + File.separator + prefix + \u0026#34;.html\u0026#34;; logger.info(\u0026#34;将word2003转换成html，html路径为:{}\u0026#34;, htmlPath); File target = new File(htmlPath); FileUtils.writeStringToFile(target, new String(outStream.toByteArray()), Constant.FILE_ENCODING); logger.info(\u0026#34;将word2003转换成html---成功\u0026#34;); return htmlPath; } catch (Exception e) { logger.error(\u0026#34;将word2003转换成html失败:\u0026#34; + e.getMessage(), e); throw new RuntimeException(\u0026#34;失败\u0026#34;); } finally { IOUtils.closeOutputStream(outStream); IOUtils.closeInputStream(input); } } public static String docxToHtml(String wordPath, String fileName) { logger.info(\u0026#34;将word2007转换成html，word文件路径:{}，word文件名:{}\u0026#34;, wordPath, fileName); OutputStreamWriter writer = null; FileInputStream fis = null; try { String sourceFileName = wordPath + File.separator + fileName; // 文件名前缀 String prefix = CommonUtils.getFilePrefix(fileName); String targetFileName = wordPath + File.separator + prefix + \u0026#34;.html\u0026#34;; logger.info(\u0026#34;将word2007转换成html，html路径为:{}\u0026#34;, targetFileName); // 图片存放路径 String imagePathStr = wordPath + \u0026#34;/image/\u0026#34;; fis = new FileInputStream(sourceFileName); XWPFDocument document = new XWPFDocument(fis); XHTMLOptions options = XHTMLOptions.create(); File imgFile = new File(imagePathStr); if (!imgFile.exists()) { imgFile.mkdirs(); } // 存放图片的文件夹 options.setExtractor(new FileImageExtractor(imgFile)); // html中图片的路径 options.URIResolver(new BasicURIResolver(\u0026#34;image\u0026#34;)); options.setIgnoreStylesIfUnused(false); options.setFragment(true); writer = new OutputStreamWriter(new FileOutputStream(targetFileName), Constant.FILE_ENCODING); XHTMLConverter xhtmlConverter = (XHTMLConverter) XHTMLConverter.getInstance(); xhtmlConverter.convert(document, writer, options); logger.info(\u0026#34;将word2007转换成html---成功\u0026#34;); return targetFileName; } catch (Exception e) { logger.error(\u0026#34;将word2007转换成html失败:\u0026#34; + e.getMessage(), e); throw new RuntimeException(\u0026#34;失败\u0026#34;); } finally { IOUtils.closeWriter(writer); IOUtils.closeInputStream(fis); } } ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E4%BB%A3%E7%A0%81%E7%B1%BB/java%E6%93%8D%E4%BD%9Cword%E6%96%87%E6%A1%A3.html","summary":"介绍: 该代码支持: 填充work中的占位符(仅是填充,不能动态加格式之类) 将work转化为html 使用 pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;poi-ooxml\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${poi.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;poi-ooxml-schemas\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${poi.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ooxml-schemas\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.poi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;poi-scratchpad\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${poi.version}\u0026lt;/version\u0026gt;","title":"java操作word文档"},{"content":"[TOC]\n1.抽象和接口 区别:\n设计角度:\n抽象是事物的对象,即对类抽象; 接口是对行为的抽象 抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象(继承只能是单继承嘛,所以是对类整体的抽象) 语法角度:\n1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法(java8的默认方法可以写实现)；\n2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的；\n3）抽象类可以有静态代码块和静态方法,而接口中不能含有静态代码块(java8可以写静态方法)；\n4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。\nhttps://www.cnblogs.com/dolphin0520/p/3811437.html 2.Java内存模型是什么(JMM)？ Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。比如，先行发生关系确保了：\n线程内的代码能够按先后顺序执行，这被称为程序次序规则。 对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。 前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。 一个线程内的任何操作必需在这个线程的start()调用之后，也叫作线程启动规则。 一个线程的所有操作都会在线程终止之前，线程终止规则。 一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。 可传递性 Java内存模型围绕着三个特征建立起来的。分别是：原子性，可见性，有序性\n注意这里是java的内存模型, 堆栈那些分布是指jvm内存模型\n3. JMM中内存操作流程 8 种基本操作，如图：\nlock 将对象变成线程独占的状态 unlock 将线程独占状态的对象的锁释放出来 read 从主内存读数据 load 将从主内存读取的数据写入工作内存 use 工作内存使用对象 assign 对工作内存中的对象进行赋值 store 将工作内存中的对象传送到主内存当中 write 将对象写入主内存当中，并覆盖旧值 对于有些操作lock和unlock是没有的,比如volatile,毕竟加了这两个东西线程就安全了\nJMM对8种内存交互操作制定的规则吧：\n不允许read、load、store、write操作之一单独出现，也就是read操作后必须load，store操作后必须write。 不允许线程丢弃他最近的assign操作，即工作内存中的变量数据改变了之后，必须告知主存。 不允许线程将没有assign的数据从工作内存同步到主内存。 一个新的变量必须在主内存中诞生，不允许工作内存直接使用一个未被初始化的变量。就是对变量实施use、store操作之前，必须经过load和assign操作。 一个变量同一时间只能有一个线程对其进行lock操作。多次lock之后，必须执行相同次数unlock才可以解锁。 如果对一个变量进行lock操作，会清空所有工作内存中此变量的值。在执行引擎使用这个变量前，必须重新load或assign操作初始化变量的值。 如果一个变量没有被lock，就不能对其进行unlock操作。也不能unlock一个被其他线程锁住的变量。 一个线程对一个变量进行unlock操作之前，必须先把此变量同步回主内存。 为什么volatile也无法保证线程安全_IT农场-CSDN博客_volatile线程安全吗 Java内存模型原理，你真的理解吗？ - 知乎 (zhihu.com) 面试官问我什么是JMM - 知乎 (zhihu.com) 3. Java中的volatile 变量是什么 可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。也就是一个线程修改的结果。另一个线程马上就能看到。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是一个非原子操作，也就是这个操作同样存在线程安全问题。\nJava语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他线程。当把变量声明为volatile类型后，编译器与运行时都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存操作一起重排序。volatile变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比sychronized关键字更轻量级的同步机制。\n当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU缓存中。如果计算机有多个CPU，每个线程可能在不同的CPU上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。\nvolatile必须满足两个条件：\n对变量的写操作不依赖当前值，如多线程下执行a++，是无法通过volatile保证结果准确性的； 该变量没有包含在具有其它变量的不变式中 java volatile关键字解惑 - 简书 (jianshu.com) 当一个变量定义为 volatile 之后，将具备两种特性：\n保证此变量对所有的线程的可见性;\n禁止指令重排序优化。\n有volatile修饰的变量，赋值后多执行了一个load addl $0x0, (%esp)操作，这个操作相当于一个内存屏障（指令重排序时不能把后面的指令重排序到内存屏障之前的位置），只有一个CPU访问内存时，并不需要内存屏障；（什么是指令重排序：是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理）。\nvolatile 和 内存屏障 - 哈哈呵h - 博客园 (cnblogs.com) 面试官问我什么是JMM - 知乎 (zhihu.com) JSL(Java Language Specification, java语言规范)中表示,long和double分为高32位和低32位,在操作时不是原子性的,可能出现线程安全的问题,所以JSL推荐用volatile修饰一下\nJava 并发编程：volatile的使用及其原理 - liuxiaopeng - 博客园 (cnblogs.com) volatile变量提供了一把弱锁,比如用boolean类型做标识符\nvolatile的正确使用姿势 - 知乎 (zhihu.com) 3. volatile的内存屏障 为什么会有内存屏障\n每个CPU都会有自己的缓存（有的甚至L1,L2,L3），缓存的目的就是为了提高性能，避免每次都要向内存取。但是这样的弊端也很明显：不能实时的和内存发生信息交换，分在不同CPU执行的不同线程对同一个变量的缓存值不同。 用volatile关键字修饰变量可以解决上述问题，那么volatile是如何做到这一点的呢？那就是内存屏障，内存屏障是硬件层的概念，不同的硬件平台实现内存屏障的手段并不是一样，java通过屏蔽这些差异，统一由jvm来生成内存屏障的指令。Lock是软件指令。 硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。\n内存屏障有两个作用：\n阻止屏障两侧的指令重排序； 强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效。 其中第二点依赖与计算机的MESI协议\n【并发编程】MESI\u0026ndash;CPU缓存一致性协议 - 风动静泉 - 博客园 (cnblogs.com) volatile写的内存屏障\n在保守策略下，volatile写插入内存屏障后生成的指令序列示意图：\n上图中StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存(不然两个写可能混乱,最终导致数据错误)。 这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面 是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方 法立即return或者再来一个volatile写）。\n为了保证能正确 实现volatile的内存语义，JMM在采取了保守策略：在每个volatile写的后面，或者在每个volatile 读的前面插入一个StoreLoad屏障。从整 体执行效率的角度考虑，JMM最终选择了在每个 volatile写的后面插入一个StoreLoad 屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM 在实现上的一个特点：首先确保正确性，然后再去追求执行效率。\nvolatile读的内存屏障\n下图是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图：\n上图中LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore 屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。\n内存屏障的优化 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不 改变 volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。\n就是只要屏障用不着就可以不用, 也就是说 上述读/写屏障中的不一定会加上loadStore等屏障\n比如: volatile读后面如果没有普通读,就不会加loadload屏障, 如果没有普通写,就不会加loadStore屏障\npublic class VolitileBarrierDemo { int a; volatile int v1 = 1; volatile int v2 = 2; void readWrite() { int i = v1; // 第一个volitile读 分两步 读volitile变量v1, 给i赋值 这两步是有序的 int j = v2; // 第二个volitile读 分两步 读volitile变量v2, 给j赋值 这两步是有序的 a = i + j; // 普通写 分两步 读 i 和 j的值 v1 = i + 1; // 第一个volitile写 两步 读i加1 给volitile v1赋值 volitile写 v2 = j * 2; // 第二个volitile写 两步 读j*2 给volitile v2赋值 volitile写 } } 内存屏障如图:\n注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见， 编译器通常会在这里插入一个StoreLoad屏障。\nLoadLoad 屏障：对于这样的语句Load1，LoadLoad，Load2。在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 StoreStore屏障：对于这样的语句Store1， StoreStore， Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 LoadStore 屏障：对于这样的语句Load1， LoadStore，Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 StoreLoad 屏障：对于这样的语句Store1， StoreLoad，Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。 volatile 和 内存屏障 - 哈哈呵h - 博客园 (cnblogs.com) 面试官问我什么是JMM - 知乎 (zhihu.com) 3. 为什么加了volatile还是线程不安全的? 我们知道CPU的处理速度和主存的读写速度不是一个量级的，为了平衡这种巨大的差距，每个CPU都会有缓存。因此，共享变量会先放在主存中，每个线程都有属于自己的工作内存，并且会把位于主存中的共享变量拷贝到自己的工作内存，之后的读写操作均使用位于工作内存的变量副本，并在某个时刻将工作内存的变量副本写回到主存中去\nVolatile的第一个语义就是保证此线程的可见性，一个线程对此变量的更改其他线程是立即可知的。也就是说 assign,store,write这三个操作是原子的，中间不会中断，会马上同步回主存，就好像直接操作主存一样，并通过缓存一致性通知其他缓存中的副本过期.\n但是内存间操作还有load和use,这两步不是安全的,所以volatile不是安全的\nhttps://www.jianshu.com/p/d52fea0d6ba5 为什么volatile也无法保证线程安全_IT农场-CSDN博客_volatile线程安全吗 4. Java中什么是竞态条件？ 举个例子说明。 当某个计算正确性取决于多个线程的交替执行时序时, 就会发生静态条件,即争取的结果要取决于运气, 最常见的静态条件就是\u0026quot;先检查后执行\u0026quot;,通过一个可能失效的观测结果来决定下一步的动作.\n例如:\nclass Counter { protected long count = 0; public void add(long value) { this.count = this.count + value; } } 观察线程A和B交错执行会发生什么，两个线程分别加了2和3到count变量上，两个线程执行结束后count变量的值应该等于5。然而由于两个线程是交叉执行的，两个线程从内存中读出的初始值都是0。然后各自加了2和3，并分别写回内存。最终的值并不是期望的5，而是最后写回内存的那个线程的值，上面例子中最后写回内存的是线程A，但实际中也可能是线程B。如果没有采用合适的同步机制，线程间的交叉执行情况就无法预料。add()方法就是一个临界区,它会产生竞态条件。\n5. 一个线程运行时发生异常会怎样 所以这里存在两种情形：\n如果该异常被捕获或抛出，则程序继续运行。 如果异常没有被捕获该线程将会停止执行 6. 为什么wait, notify 和 notifyAll这些方法不在thread类里面？ 一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。\n7. 什么是ThreadLocal变量？ ThreadLocal一般称为线程本地变量，它是一种特殊的线程绑定机制，将变量与线程绑定在一起，为每一个线程维护一个独立的变量副本。通过ThreadLocal可以将对象的可见范围限制在同一个线程内。\n没有ThreadLocal的时候，一个线程在其声明周期内，可能穿过多个层级，多个方法，如果有个对象需要在此线程周期内多次调用，且是跨层级的（线程内共享），通常的做法是通过参数进行传递；而ThreadLocal将变量绑定在线程上，在一个线程周期内，无论“你身处何地”，只需通过其提供的get方法就可轻松获取到对象。极大地提高了对于“线程级变量”的访问便利性。\n8. 如何避免死锁？ 死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件：\n互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。\n9. Java中活锁和死锁有什么区别？ 指事物1可以使用资源，但它让其他事物先使用资源；事物2可以使用资源，但它也让其他事物先使用资源，于是两者一直谦让，都无法使用资源。\n解决:\n在锁让出的时候时候添加随机睡眠时间, 约定线程优先级 10. Vector、ArrayList、LinkedList Vector、ArrayList、LinkedList均为线型的数据结构，但是从实现方式与应用场景中又存在差别。\n底层实现方式 ArrayList内部用数组来实现；LinkedList内部采用双向链表实现；Vector内部用数组实现。\n读写机制 ArrayList在执行插入元素是超过当前数组预定义的最大值时，数组需要扩容，扩容过程需要调用底层System.arraycopy()方法进行大量的数组复制操作；在删除元素时并不会减少数组的容量（如果需要缩小数组容量，可以调用trimToSize()方法）；在查找元素时要遍历数组，对于非null的元素采取equals的方式寻找。\nLinkedList在插入元素时，须创建一个新的Entry对象，并更新相应元素的前后元素的引用；在查找元素时，需遍历链表；在删除元素时，要遍历链表，找到要删除的元素，然后从链表上将此元素删除即可。\nVector与ArrayList仅在插入元素时容量扩充机制不一致。对于Vector，默认创建一个大小为10的Object数组，并将capacityIncrement设置为0；当插入元素数组大小不够时，如果capacityIncrement大于0，则将Object数组的大小扩大为现有size+capacityIncrement；如果capacityIncrement\u0026lt;=0,则将Object数组的大小扩大为现有大小的2倍。\n读写效率\nArrayList对元素的增加和删除都会引起数组的内存分配空间动态发生变化。因此，对其进行插入和删除速度较慢，但检索速度很快。\nLinkedList由于基于链表方式存放数据，增加和删除元素的速度较快，但是检索速度较慢。\n线程安全性\nArrayList、LinkedList为非线程安全；Vector是基于synchronized实现的线程安全的ArrayList。\n需要注意的是：单线程应尽量使用ArrayList，Vector因为同步会有性能损耗；即使在多线程环境下，我们可以利用Collections这个类中为我们提供的synchronizedList(List list)方法返回一个线程安全的同步列表对象\n11. 三次握手和四次挥手 三次握手 client发送消息给server: 我要和你发送消息了 server做好数据接收准备并反馈消息给client: 我收到你的通知了,\u0026ldquo;我在你的基础上+1\u0026rdquo; client反馈消息给server:我收到你的反馈了,\u0026ldquo;我再加点标识符,证明是咱俩准备交互\u0026rdquo; 四次挥手 client和server说:我没有新消息要给你了 server收到后进行反馈:我先告诉你我收到你的通知了,但是可能我还没有接收完消息,你等会 client接收后,进入等待状态,server处理完后告诉client:我处理完了,你可以关闭了 client给server发消息:我收到你的通知了,我关了 四次挥手中最后一次,client给server发送最后确认消息后,client并不会马上关闭,进入TIME_WAIT状态,此状态下会等2MSL(两个最大报文段生存时间),因为怕网络不好,server收不到,所以等一会再关(如果server没收到还会发第三次挥手的信息); 在高并发的情况下,可能造成部分正常请求出现TIME_WAIT情况,这时可以适当减少TIME_WAIT的等待时间来处理\nTCP的TIME_WAIT状态为什么要等待2MSL的时长 因为有三次握手/四次挥手的机制保证了TCP的通信可靠\nMSL: 即Maximum Segment Lifetime，一个数据分片（报文）在网络中能够生存的最长时间, 即超过两分钟即认为这个报文已经在网络中被丢弃了, 现在一般是60s . Linux和Windows系统修改MSL的值_Han的小站-CSDN博客_linux msl值 更为复杂的TCP交互: TCP 连接状态详解_Han的小站-CSDN博客_tcp 连接状态 为什么握手三次, 挥手要四次-CSDN博客 12. HashMap HashMap采用Entry数组来存储key-value对，每一个键值对组成了一个Entry实体，Entry类实际上是一个单向的链表结构，它具有Next指针，可以连接下一个Entry实体。 只是在JDK1.8中，链表长度大于等于8且数组长度大于64时的时候，链表会转成红黑树。\n为什么用数组+链表+红黑树(jdk1.8)？\n数组是用来确定桶的位置，利用元素的key的hash值对数组长度取模得到.用数据会比较快,且扩容比较好,\n链表是用来解决hash冲突问题，当出现hash值一样的情形，就在数组上的对应位置形成一条链表。当链表元素超过8时会转换成红黑树(元素少用链表,多则用红黑树是因为效率快),当元素减少到6时,会从树变成链表(中间7这个数值做缓冲)\n这里的hash值并不是指hashcode，而是将hashcode高低十六位异或过的。\n解决hash冲突的方式比较出名的有四种\n开放定址法 链地址法 (HashMap) 再哈希法 公共溢出区域法 一般用什么作为HashMap的key?\n一般用Integer、String这种不可变类当HashMap当key，而且String最为常用。因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算\n来自:https://www.cnblogs.com/flyuz/p/11378491.html#1-hashmap%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86\n在jdk1.7中，在多线程环境下，扩容时会造成环形链(由于头插法造成)或数据丢失。\n在jdk1.8中，在多线程环境下，会发生数据覆盖的情况。\nhttps://blog.csdn.net/qq9808/article/details/80850498 https://www.jianshu.com/p/e2f75c8cce01 13. java8 元空间 元空间是Java8开始才出现的内存区域,取代了之前的永久区\n元空间中存储的是类的元数据信息（metadata），只不过不再是存储在连续的堆空间上，而是移动到叫做“Metaspace”的本地内存（Native memory）中,\n原本永久区存放的static所引用的对象这类的存放在堆中\nstatic的变量存放在方法区 https://segmentfault.com/q/1010000020746567 为什么要将永久代替换成Metaspace？\n字符串存在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。(1.8之前,永久代和老年代会一起被GC,无论哪个满了,都会触发Full GC) 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 Oracle 可能会将HotSpot 与 JRockit 合二为一。 原文链接：https://blog.csdn.net/yjp198713/article/details/78759933/\n链接：https://www.jianshu.com/p/93e35781eebc\n14. 单例模式-双检锁 代码\npublic class Test { private volatile static Test instance; private Test() { } public static Test getInstance() { if (instance == null) { synchronized (Test.class) { if (instance == null) { instance = new Test(); } } } return instance; } } 解析\n第一个注意点：使用私有的构造函数，确保正常情况下该类不能被外部初始化（非正常情况比如通过反射初始化，一般使用反射之后单例模式也就失去效果了）。 第二个注意点：getInstance方法中第一个判空条件，逻辑上是可以去除的，去除之后并不影响单例的正确性，但是去除之后效率低。因为去掉之后，不管instance是否已经初始化，都会进行synchronized操作，而synchronized是一个重操作消耗性能。加上之后，如果已经初始化直接返回结果，不会进行synchronized操作。 第三个注意点：加上synchronized是为了防止多个线程同时调用getInstance方法时，各初始化instance一遍的并发问题。 第四个注意点：getInstance方法中的第二个判空条件是不可以去除，如果去除了，并且刚好有两个线程a和b都通过了第一个判空条件。此时假设a先获得锁，进入synchronized的代码块，初始化instance，a释放锁。接着b获得锁，进入synchronized的代码块，也直接初始化instance，instance被初始化多遍不符合单例模式的要求~。加上第二个判空条件之后，b获得锁进入synchronized的代码块，此时instance不为空，不执行初始化操作。 第五个注意点：instance的声明有一个voliate关键字，如果不用该关键字，有可能会出现异常。因为instance = new Test();并不是一个原子操作，会被编译成三条指令，如下所示。 1.给Test的实例分配内存 2.初始化Test的构造器 3.将instance对象指向分配的内存空间（注意 此时instance就不为空） 然后咧，java会指令重排序，JVM根据处理器的特性，充分利用多级缓存，多核等进行适当的指令重排序，使程序在保证业务运行的同时，充分利用CPU的执行特点，最大的发挥机器的性能！简单来说就是jvm执行上面三条指令的时候，不一定是1-2-3这样执行，有可能是1-3-2这样执行。\n如果jvm是按照1-3-2来执行的话，当1-3执行完2还没执行的时候，如果另外一个线程调用getInstance()，因为3执行了此时instance不为空，直接返回instance。问题是2还没执行，此时instance相当于什么都没有，肯定是有问题的。然后咧，voliate有一个特性就是禁止指令重排序，上面的三条指令是按照1-2-3执行的，这样就没有问题了。\n15. 同步集合和并发集合 同步集合:\nvector：就比arraylist多了个同步化机制（线程安全），因为效率较低，现在已经不太建议使用。在web应用中，特别是前台页面，往往效率（页面响应速度）是优先考虑的。 statck：堆栈类，先进后出。 hashtable：就比hashmap多了个线程安全。 enumeration：枚举，相当于迭代器。 并发集合:\nConcurrentHashMap：线程安全的HashMap的实现 CopyOnWriteArrayList：线程安全且在读操作时无锁的ArrayList CopyOnWriteArraySet：基于CopyOnWriteArrayList，不添加重复元素 ArrayBlockingQueue：基于数组、先进先出、线程安全，可实现指定时间的阻塞读写，并且容量可以限制 LinkedBlockingQueue：基于链表实现，读写各用一把锁，在高并发读写操作都多的情况下，性能优于ArrayBlockingQueue CopyOnWrite集合即写时复制的集合\n通俗的理解是当我们往一个集合添加元素的时候，不直接往当前集合添加，而是先将当前集合进行Copy，复制出一个新的集合，然后新的集合里添加元素，添加完元素之后，再将原集合的引用指向新的集合。这样做的好处是我们可以对CopyOnWrite集合进行并发的读，而不需要加锁，因为当前集合不会添加任何元素。所以CopyOnWrite集合也是一种读写分离的思想，读和写不同的集合。 (所以这种集合适合读多写少的场景)\nConcurrentHashMap\n使用分段锁来提高并发,对里面的数据进行分批加锁,而hashtable是一把大锁,直接把整个map锁住,有点像数据库中的表锁和行锁\n原文链接：https://blog.csdn.net/yuruixin_china/article/details/82082195 原文链接：https://blog.csdn.net/qq_41701956/article/details/103253168\n16. 线程池 16.1 创建线程池有哪几种方式？ newFixedThreadPool(int nThreads)\n创建一个固定长度的线程池，每当提交一个任务就创建一个线程，直到达到线程池的最大数量，这时线程规模将不再变化，当线程发生未预期的错误而结束时，线程池会补充一个新的线程。但它的队列可以长度是Integer.MAX_VALUE,有可能因为排队数造成OOM\nnewCachedThreadPool()\n创建一个可缓存的线程池，如果线程池的规模超过了处理需求，将自动回收空闲线程，而当需求增加时，则可以自动添加新线程，线程池的规模不存在任何限制。但它会创建很多核心线程,最大数是Integer.MAX_VALUE,所以有可能因为线程数过多造成OOM\nnewSingleThreadExecutor()\n这是一个单线程的Executor，它创建单个工作线程来执行任务，如果这个线程异常结束，会创建一个新的来替代它；它的特点是能确保依照任务在队列中的顺序来串行执行。和newFixedThreadPool一样,使用同一种队列类型-LinkedBlockingQueue,可能会因排队数造成OOM\nnewScheduledThreadPool(int corePoolSize)\n创建了一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似于Timer。和newSingleThreadExecutor一样都是单线程执行,它的最大线程数是Integer.MAX_VALUE,可能因为线程数过多而OOM\nnewWorkStealingPool()\n(JDK1.8) 意为窃取线程池,抢占式执行,所以不保证执行的顺序,其底层用的是ForkJoinPool类,可指定并发量,默认是当前处理器的数量\n16.2 执行逻辑 上述都是通过Executors类写好的,但它们都存在一定缺点,alibaba并不推荐使用它,它们底层都是调用了ThreadPoolExecutor方法(除了newWorkStealingPool),它有几个参数,构造函数参数说明：\ncorePoolSize =\u0026gt; 线程池核心线程数量 maximumPoolSize =\u0026gt; 线程池最大数量 keepAliveTime =\u0026gt; 空闲线程存活时间 unit =\u0026gt; 时间单位 workQueue =\u0026gt; 线程池所使用的缓冲队列 threadFactory =\u0026gt; 线程池创建线程使用的工厂 handler =\u0026gt; 线程池对拒绝任务的处理策略 判断核心线程数是否已满，核心线程数大小和corePoolSize参数有关，未满则创建线程执行任务 若核心线程池已满，判断队列是否满，队列是否满和workQueue参数有关，若未满则加入队列中 若队列已满，判断线程池是否已满，线程池是否已满和maximumPoolSize参数有关，若未满创建(非核心)线程执行任务 若线程池已满，则采用拒绝策略处理无法执执行的任务，拒绝策略和handler参数有关 原文链接：https://blog.csdn.net/damokelisijian866/article/details/102982390\nhttps://blog.csdn.net/qq_41701956/article/details/103253168 https://blog.csdn.net/tjbsl/article/details/98480843 https://blog.csdn.net/qq_31615049/article/details/80756781 16.3 线程池中线程是如何复用和回收的? 原理\n线程池的优点就是提高对线程的管理，提高资源的利用率，控制线程的数量。\n在线程池中，线程可以从阻塞队列 中不断 getTask() 新任务来执行，其核心原理在于线程池用Worker对Thread进行了封装，每调用一个 addWorker 就是等于新开一个线程，并不是每次执行任务都会调用 Thread.start() 来创建新线程，而是让每个线程去轮询，在这个轮询中，不停地检查是否还有任务等待被执行，如果有则直接去执行这个任务，也就是调用任务的 run() 方法，把 run() 方法当作和普通方法一样的地位去调用，相当于把每个任务的 run() 方法串联了起来，所以线程数量并不增加。\n一. 线程如何复用？\nThreadPoolExecutor 在创建线程时，会将线程封装成工作线程 Worker ,并放入工作线程组中，然后这个 Worker 反复从阻塞队列中拿任务去执行。\n通过取 Worker 的 firstTask 或者通过 getTask 方法从 workQueue 中获取待执行的任务。 直接调用 task 的 run 方法来执行具体的任务（而不是新建线程） 二. 线程如何回收？\n获取不到任务时，回收自己 将worker移出线程池 线程池状态置为TERMINATED 线程池 | 线程如何复用？_小郭的博客-CSDN博客_线程池如何复用 12 线程池原理 · 深入浅出Java多线程 (redspider.group) 更多知识见 线程池.html 16.4 execute和submit的区别 execute只能提交Runnable类型的任务，无返回值。submit既可以提交Runnable类型的任务，也可以提交Callable类型的任务，会有一个类型为Future的返回值，但当任务类型为Runnable时，返回值为null。 execute在执行任务时，如果遇到异常会直接抛出，而submit不会直接抛出，只有在使用Future的get方法获取返回值时，才会抛出异常。 异常也是打印到控制台, 如果业务代码没有捕获, 异常相当于就丢失了(log不会收集控制台), 在Theard类中uncaughtExceptionHandler变量存储了线程异常时的处理, 默认情况下使用了ThreadGroup类的实现, ThreadGroup默认打印到控制台了\njava.lang.ThreadGroup#uncaughtException public void uncaughtException(Thread t, Throwable e) { if (parent != null) { parent.uncaughtException(t, e); } else { Thread.UncaughtExceptionHandler ueh = Thread.getDefaultUncaughtExceptionHandler(); if (ueh != null) { ueh.uncaughtException(t, e); } else if (!(e instanceof ThreadDeath)) { System.err.print(\u0026#34;Exception in thread \\\u0026#34;\u0026#34; + t.getName() + \u0026#34;\\\u0026#34; \u0026#34;); e.printStackTrace(System.err); } } } 17. synchronized的升级原理是什么？ 在Java中，锁共有4种状态，级别从低到高依次为：无状态锁，偏向锁，轻量级锁和重量级锁 状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级。\n锁分级别原因：\n没有优化以前，synchronized是重量级锁（悲观锁），使用 wait 和 notify、notifyAll 来切换线程状态非常消耗系统资源；线程的挂起和唤醒间隔很短暂，这样很浪费资源，影响性能。所以 JVM 对synchronized 关键字进行了优化，把锁分为 无锁、偏向锁、轻量级锁、重量级锁 状态。\n无锁：没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功，其他修改失败的线程会不断重试直到修改成功。\n偏向锁：对象的代码一直被同一线程执行，不存在多个线程竞争，该线程在后续的执行中自动获取锁，降低获取锁带来的性能开销(因为大部分情况下还是只有一个线程在运行过来了)。偏向锁，指的就是偏向第一个加锁线程，该线程是不会主动释放偏向锁的，只有当其他线程尝试竞争偏向锁才会被释放。 (重点是只有一个线程,如果有线程来争抢就会锁升级)\n偏向锁的撤销，需要在某个时间点上没有字节码正在执行时，先暂停拥有偏向锁的线程，然后判断锁对象是否处于被锁定状态。如果线程不处于活动状态，则将对象头设置成无锁状态，并撤销偏向锁；\n如果线程处于活动状态，升级为轻量级锁的状态。\n轻量级锁：轻量级锁是指当锁是偏向锁的时候，被第二个线程 B 所访问，此时偏向锁就会升级为轻量级锁，线程 B 会通过自旋的形式尝试获取锁，线程不会阻塞，从而提高性能。\n当前只有一个等待线程，则该线程将通过自旋进行等待。但是当自旋超过一定的次数时，轻量级锁便会升级为重量级锁；当一个线程已持有锁，另一个线程在自旋，而此时又有第三个线程来访时，轻量级锁也会升级为重量级锁。\n重量级锁：指当有一个线程获取锁之后，其余所有等待获取该锁的线程都会处于阻塞状态。\nCAS是compare and swap的缩写，即比较后(比较内存中的旧值与预期值)交换(将旧值替换成预期值)\n多次尝试CAS操作直至成功或失败，这个过程叫做自旋。\nCAS利用cpu原语操作保证了线程安全,所以CAS保证锁升级时的线程安全,但它会存在ABA问题,当然有解决方案,用加版本号的方式解决(java已提供AtomicStampedReference对象处理),在JUC(java.util.concurrent)下用了很多这个,特别是atomic系列\n自旋次数默认是10次,由-Xx:PreBloackSpin 控制, 且有适应性自旋(很久没拿到锁的,等的时间短甚至阻塞,怕你死锁,反之可以等时间长点)\n详见:https://blog.csdn.net/aaa_bbb_ccc_123_456/article/details/103551391\nhttps://blog.csdn.net/qq_43948583/article/details/104725206 自旋锁与适应性自旋锁_JustinNeil的博客-CSDN博客_适应性自旋锁 原文链接：https://blog.csdn.net/meism5/article/details/90321826\nhttps://blog.csdn.net/always_younger/article/details/79462684 18. OSI 的网络模型都有哪些？ 应用层：网络服务与最终用户的一个接口。常用的协议包括DNS，HTTP，FTP等。 表示层：数据的表示、安全、压缩。 会话层：建立、管理、终止会话。 SSL/TLS协议 传输层：定义传输数据的协议端口号，以及流控和差错校验。TCP协议和UDP协议。 网络层：进行逻辑地址寻址，实现不同网络之间的路径选择。 数据链路层：建立逻辑连接、进行硬件地址寻址、差错校验等功能。 物理层：建立、维护、断开物理连接。 原文链接：https://blog.csdn.net/qq_41701956/article/details/103253168\n19. http(超文本传输协议): 请求: 请求行:请求方法 请求资源 请求版本 头部: 属性: 属性值 主体(有请求主体时,必须添加实体首部 Content-Type,Content-Length,就是post请求时) 响应: 响应行:协议版本 响应码 响应信息(OK 或者 NOT FOUND等等) 头部: 属性: 属性值 主体 20. hashCode()和 equals(): 在哈希表中判断两个元素是否重复要使用到 hashCode()和 equals()。\nhashCode 决定数据在表中的存储位置，而 equals 判断是否存在相同数据。\n当向集合 Set 中增加对象时，首先集合计算要增加对象的 hashCode 码，根据该值来得到一个位置用来存放当前对象，如在该位置没有一个对象存在的话，那么集合 Set 认为该对象在集合中不存在，直接增加进去。如果在该位置有一个对象存在的话，接着将准备增加到集合中的对象与该位置上的对象进行 equals 方法比较，如果该 equals 方法返回 false,那么集合认为集合中不存在该对象，在进行一次散列，将该对象放到散列后计算出的新地址里。如果 equals 方法返回 true，那么集合认为集合中已经存在该对象了，不会再将该对象增加到集合中了。\n21. 内存溢出和内存泄露 两者都是基于jvm来说的\n内存泄漏 memory leak\n是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄漏似乎不会有大的影响，但内存泄漏堆积后的后果就是内存溢出。\n对jvm来说,有些内存给了你,我无法回收了,那这些内存相当于泄露了\n内存溢出 out of memory :\n指程序申请内存时，没有足够的内存供申请者使用，\n对jvm来说,你问我要Long型的内存大小,但是我只能给了Int类型的内存大小了,这时你放进去,内存放不下就溢出了\n内存溢出原因：\n内存中加载的数据量过于庞大，如一次从数据库取出过多数据； 集合类中有对对象的引用，使用完后未清空，使得JVM不能回收； 代码中存在死循环或循环产生过多重复的对象实体； 使用的第三方软件中的BUG； 启动参数内存值设定的过小 内存溢出的解决方案：\n第一步，修改JVM启动参数，直接增加内存。(-Xms，-Xmx参数一定不要忘记加。)\n第二步，检查错误日志，查看“OutOfMemory”错误前是否有其 它异常或错误。\n第三步，对代码进行走查和分析，找出可能发生内存溢出的位置。\n重点排查以下几点：\n检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。 检查代码中是否有死循环或递归调用。 检查是否有大循环重复产生新对象实体。 检查List、MAP等集合对象是否有使用完后，未清除的问题。List、Map等集合对象会始终存有对对象的引用，使得这些对象不能被GC回收。 第四步，使用内存查看工具动态查看内存使用情况\n原文链接：https://blog.csdn.net/ruiruihahaha/java/article/details/70270574\n22. 公平锁和非公平锁(重入锁) 重入锁: 同一个人,可以重复拥有这把锁(锁的次数就会增加),它是基于公平锁和非公平锁来实现的,默认是非公平锁\n公平锁: 获得锁的几率大家都一样,通过 先进先出队列 控制\n非公平锁: 获得锁的几率不一样,对某些线程来说就是不公平的,新来的线程可以插队,优先获得锁\n它们两者的区别:\n1、公平锁能保证：老的线程排队使用锁，新线程仍然排队使用锁。\n2、非公平锁保证：老的线程排队使用锁；新线程抢占已经在排队的线程的锁。(基本可以这样理解)\n公平锁的优缺点:\n优点：所有的线程都能得到资源，不会饿死在队列中。 缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大 非公平锁的优缺点:\n优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。 缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。 公平锁获得锁之前,走\u0026quot;尝试加锁\u0026quot;函数,函数中需要判断队列是否有值,有值就等待,而非公平锁直接进行CAS,这样就有可能抢夺锁了,如果失败就走\u0026quot;尝试加锁\u0026quot;函数,此时不如需要判断队列(所以本质上的区别,两者去获得锁之前,会不会去判断队列) java.util.concurrent.locks.ReentrantLock.NonfairSync\n23 cpu密集型和IO密集型 cpu密集型,又称计算密集型，顾名思义就是应用需要非常多的CPU计算资源，对于计算密集型的应用，完全是靠CPU的核数来工作，所以为了让它的优势完全发挥出来，避免过多的线程上下文切换，比较理想方案是 线程数= CPU核数+1 或者在jdk1.8的forkjoin中线程数 = CPU内核线程数 * 2\n比方说，如果系统的主要任务是计算 Hash 值，那么这时选用更高性能的 Hash 算法就可以大大提升系统的性能。发现这类问题的主要方式，是通过一些 Profile 工具来找到消耗 CPU 时间最多的方法或者模块，比如 Linux 的 perf、eBPF等\nIO密集型: 就是对IO处理比较多,分为网络IO和磁盘IO. 一旦发生IO，线程就会处于等待状态，当IO结束，数据准备好后，线程才会继续执行,所以IO密集型更适合用多线程. 对于IO密集型应用：线程数= CPU核心数 / (1-阻塞系数), 系数一般在0.8~0.9\n我们熟知的系统大部分都属于 IO 密集型，比如数据库系统、缓存系统、Web 系统\nhttps://www.jianshu.com/p/f8b2e2869372 https://www.bilibili.com/video/BV1B7411L7tE?p=24 24. 进程和线程的区别 根本区别\n进程：资源调度最小单位。\n线程：CPU调度最小单位。\n地址空间\n进程：进程有自己独立的地址空间，每启动一个进程，系统都会为其分配地址空间，建立数据表来维护代码段、堆栈段和数据段。\n线程：线程没有独立的地址空间，同一进程的线程共享本进程的地址空间。\n内存和files共享\n进程：当创建一个进程的时候，mm_struct会指向另外一块地址，使用copy-on-write进行复制。\n线程：而创建一个线程的时候，mm_struct会指向父进程的同一块虚拟内存区域，所以会有资源冲突问题。\n不论线程和进程，在linux中的创建都是很快速的。\n块\n进程：进程控制块PCB。一个进程用ProcessControlBlock上的一个entry记录其基本信息（pid，state，priority等），进程会被操作系统分配一个内存逻辑地址空间，即其拥有一段内存空间供使用。\n线程：线程控制块TCB。线程是进程内负责执行一项任务的单元，这个单元用ThreadControlBlock上的一个entry记录其基本信息（tid，state，priority，counter，register info等），这个单元有着自己的stack来用于任务执行。\n系统开销\n进程：进程执行开销大。\n线程：线程执行开销小。\n切换速度 进程：切换相对慢。\n线程：切换相对快。\n24. Synchronized和Lock 类别 synchronized Lock 存在层次 Java的关键字，在jvm层面上 是一个接口类 锁的释放 1、以获取锁的线程执行完同步代码，释放锁 2、线程执行发生异常，jvm会让线程释放锁(自动释放锁) 在finally中必须释放锁，不然容易造成线程死锁(手动释放) 锁的获取 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待 分情况而定，Lock有多个锁获取的方式，具体下面会说道，大致就是可以尝试获得锁，线程可以不用一直等待 作用范围 代码块,变量,方法,类 写到代码中 锁类型 可重入 不可中断 非公平 可重入 可判断 可公平（两者皆可） 性能 少量同步(重量级锁性能差) 大量同步(性能好) 底层原理 底层使用指令码方式来控制锁的，映射成字节码指令就是增加来两个指令：monitorenter和monitorexit。当线程执行遇到monitorenter指令时会尝试获取内置锁，如果获取锁则锁计数器+1，如果没有获取锁则阻塞；当遇到monitorexit指令时锁计数器-1，如果计数器为0则释放锁 层是CAS乐观锁，依赖AbstractQueuedSynchronizer类，把所有的请求线程构成一个CLH队列。而对该队列的操作均通过Lock-Free（CAS）操作。 来自: https://blog.csdn.net/u012403290/article/details/64910926 https://www.jianshu.com/p/b343a9637f95 25. CMS 和G1 的区别 区别一： 回收内存范围\nCMS收集器是老年代的收集器，可以配合新生代的Serial和ParNew收集器一起使用\nG1收集器收集范围是老年代和新生代。不需要结合其他收集器使用\nG1 把老年代和新生代化成大小相等的独立区域, 消除了新/老代的物理隔离, 仍保留了逻辑隔离\n区别二： STW的时间\nCMS收集器以最小的停顿时间为目标的收集器。 G1收集器可预测垃圾回收的停顿时间（建立可预测的停顿时间模型） 区别三： 垃圾回收算法\nCMS收集器是使用“标记-清除”算法进行的垃圾回收，容易产生内存碎片 G1收集器使用的是“标记-复制”算法，进行了空间整合，降低了内存空间碎片。 区别四： 垃圾回收的过程不一样\njdk8提供了G1回收器, jdk9默认使用了G1回收器\nCMS 和G1 的区别 - 简书 (jianshu.com) 新一代垃圾回收器ZGC的探索与实践 - 美团技术团队 (meituan.com) 26. 什么是三色标记法? 我们要进行垃圾回收，就需要弄明白哪些对象是需要回收的，哪些对象是不需要回收的。\n计数法: 就是每个对象都有一个计数器，被引用了加一，移除引用减一。 根可达算法: 从根节点开始扫描, 看对象是否可达, 不可达则可以删除了, 整个过程必须「Stop the World」。这就导致整个应用程序必须停止，不能做任何改变 三色标记算法: 三色标记算法指的是将所有对象分为白色、黑色和灰色三种类型。黑色表示从 GCRoots 开始，已扫描过它全部引用的对象，灰色指的是扫描过对象本身，还没完全扫描过它全部引用的对象，白色指的是还没扫描过的对象。 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是 白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。\n黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象。\n灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过。\n因为是并发标记, 则会有两个问题：一个是错标，标记过不是垃圾的，变成了垃圾（也叫浮动垃圾）；第二个是错杀, 本来已经当做垃圾了，但是又有新的引用指向它。\n错标不怕, 影响不是很大，可能就是暂时的浪费一点内存，它肯定抗不过下一轮GC。\n错杀问题很大 , 把要用的 对象清除了, 程序就挂了\n当且仅当以下两个条件同时满足时，会产生“对象消失”的问题，即原本应该是黑色的对象被误标为白色：\n赋值器插入了一条或多条从黑色对象到白色对象的新引用； 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。(删除了多个灰色对象指向白色对象的引用) 因此，我们要解决并发扫描时的对象消失问题，只需破坏这两个条件的任意一个即可。由此分别 产生了三种解决方案：写屏障+增量更新（Incremental Update）和写屏障+原始快照（Snapshot At The Beginning， SATB）和 读屏障\n增量更新要破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了。\n原始快照要破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。\n读屏障, 当读取成员变量时，一律记录下来, 在条件一中【黑色对象 重新引用了 该白色对象】，重新引用的前提是：得获取到该白色对象，此时已经读屏障就发挥作用了。\n以上无论是对引用关系记录的插入还是删除，虚拟机的记录操作都是通过写屏障实现的。在HotSpot虚拟机中，增量更新和原始快照这两种解决方案都有实际应用，譬如，CMS是基于增量更新来做并发标记的，G1、Shenandoah则是用原始快照来实现, ZGC：读屏障\n三色标记法与读写屏障 - 简书 (jianshu.com) JVM 三色标记算法，原来是这么回事！_肥肥技术宅的博客-CSDN博客 27. GC的分代策略 针对HotSpot VM的实现，它里面的GC其实准确分类只有两大种：\nPartial GC：并不收集整个GC堆的模式\nYoung GC / Minor GC：只收集young gen的GC , 使用复制-回收算法\n当young gen中的eden区分配满的时候触发。注意young GC中有部分存活对象会晋升到old gen，所以young GC后old gen的占用量通常会有所升高。\nOld GC：只收集old gen的GC。只有CMS的concurrent collection是这个模式, 使用 标记-清除算法\nMixed GC：收集整个young gen以及部分old gen的GC。只有G1有这个模式\nFull GC：收集整个堆，包括young gen(年轻代)、old gen(老年代)、perm gen(永久代)（如果存在的话）等所有部分的模式。\n当准备要触发一次young GC时，如果发现统计数据说之前young GC的平均晋升大小比目前old gen剩余的空间大，则不会触发young GC而是转为触发full GC (老年代/永久代 空间不够了)\nG1的FullGC将是采用Serial收集器进行。这将会导致STW发生，这个时间直到收集完成为止。因此要注意G1的退化情况。调优的目的是尽量保证退化的情况不出现。\nMajor GC通常是跟full GC是等价的，收集整个GC堆。但这个词目前用的很混淆，当有人说“major GC”的时候一定要问清楚他想要指的是上面的full GC还是old GC。\nMajor GC和Full GC的区别是什么？触发条件呢？ - 知乎 (zhihu.com) 27-YongGC、MinorGC、 Major GC、FullGC傻傻分不清 - 知乎 (zhihu.com) java8 各种GC的总结 - 简书 (jianshu.com) 28. 为什么Switch支持枚举但不能用类名的方式写(限定名称) enum Foobar { FOO, BAR; } class Test { static int test(Foobar var0) { switch (var0) { case FOO: return 1; case BAR: return 2; case Foobar.BAR: // 这样写就会报错,提示: 不能是限定名称 return 2; default: return 0; } } } 限定名称: 带了路径去表示一个类, 例如: java.lang.String\nJava中限定类名和非限定类名的区别_Sam.Shi的博客-CSDN博客 switch本质只支持int类型的值, 所以支持int, Integer, char, 枚举,string 等等, 但不支持Long, double这类\n支持String是把字符串通过hashCode()转成整型,\nLong型转成int会丢失精度, 所以jvm不会自动转换,(可以自己手动强转)\n枚举比较特殊, switch是调用 ordinal() 方法来做判断 (表示枚举的顺序)\n进入正题: 枚举类型在JVM中是以类的形式表示的，每个枚举常量都是该类的一个实例, 枚举常量可以有自己的字段和方法, 所以当用限定名称指定枚举后, 就变成了一个类 (类无法转成整型), 如果只写枚举, 就是一个实例,\n原理:\nswitch语句要使用tableswitch和lookupswitch这两个指令，这两个指令只针对int类型进行操作\ntableswitch case值连续的场景使用\nlookupswitch case较为稀疏的场景使用\n截止于 jdk1.8\n深入JVM字节码探索switch指令、字符串、枚举 - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E9%9D%A2%E8%AF%95.html","summary":"[TOC] 1.抽象和接口 区别: 设计角度: 抽象是事物的对象,即对类抽象; 接口是对行为的抽象 抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对","title":"java常见面试题"},{"content":"1.java是值传递还是引用传递 ​ 答: 全部均为值传递!!!!\n以前我认为基本类型是值传递,而对象是引用传递,这是错误的!!!!\n有时候我会觉得传对象时里面的属性值变化了,\n(瞎猜:)其实是属性值的地址变化了,值也就改变了,而原来的值还在,指向原来的地址,只是没人用而已(等待被回收)\n来自* \u0026lt;https://blog.csdn.net/bjweimengshu/article/details/79799485 \u0026gt;\n2.尾递归 有一种特殊的递归方式叫尾递归。如果函数中的递归调用都是尾调用，则该函数是尾递归函数。尾递归的特性使得递归调用不需要额外的空间 . 不能有任何操作, 如果有f(n)+1这类的操作,都不算尾递归(因为还是保存了函数的返回值)\n来自:https://www.ibm.com/developerworks/cn/java/j-understanding-functional-programming-2/index.html?ca=drs-#icomments\nhttps://www.cnblogs.com/bellkosmos/p/5280619.html // 实现斐波那契数列 // 普通递归 int FibonacciRecursive(int n) { if( n \u0026lt; 2) return n; return FibonacciRecursive(n-1)+FibonacciRecursive(n-2); } // 尾递归 int FibonacciTailRecursive(int n,int ret1,int ret2) { if(n==0) return ret1; return FibonacciTailRecursive(n-1,ret2,ret1+ret2); } 来自: https://blog.csdn.net/mengxiangjia_linxi/article/details/78158819 尾递归效率高的原理:\n尾递归就是从最后开始计算, 每递归一次就算出相应的结果, 也就是说, 函数调用出现在调用者函数的尾部, 因为是尾部, 所以根本没有必要去保存任何局部变量. 直接让被调用的函数返回时越过调用者, 返回到调用者的调用者去。精髓：尾递归就是把当前的运算结果（或路径）放在参数里传给下层函数 , 也不用开辟新的栈空间,直接用上一个栈\n尾递归优化得益于编译器的支持,恰巧java不支持尾递归优化,但是上面那种省栈省开销还是有的(应该吧).一般函数式语言都是支持的,比如scala\n但是可以利用lambda的懒加载来实现尾递归优化,\n详情: https://www.cnblogs.com/invoker-/p/7723420.html#autoid-3-0-0 3. java中的CAS和AQS CAS : Conmpare And Swap (比较和交换) 是用于实现多线程同步的原子指令。 它将内存位置的内容与给(期望)定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。 这是作为单个原子操作完成的。\nAQS: 抽象队列同步器(AbstractQueuedSynchronizer) , 是用来构建锁或者其他同步组件的基础框架，它使用一个int成员表示同步状态，通过内部的FIFO队列来完成资源获取线程的排序工作。ReentrantLock、Semaphore、CountDownLatch、CyclicBarrier等并发类均是基于AQS来实现的，\nhttps://blog.csdn.net/yanghan1222/article/details/80247844 https://www.jianshu.com/p/0f876ead2846 https://www.cnblogs.com/fsmly/p/11274572.html 4. jvm中的记忆集(Remembered Set)是什么? jvm的堆内存分为了 年轻代和老年代, 分别使用各自的垃圾回收器回收\n如果年轻代中的对象引用了老年代的对象, 当youngGC时就不能只扫描年轻代, 还得扫描老年代看对象是否有引用, 这种情况下, 就会变成扫描整个堆 \u0026mdash;\u0026ndash; 称为 跨代引用假说\n所以jvm在年轻代中存了一个全局的数据结构, 称为记忆集. 这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生Minor GC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描\n5. 运行时常量池和字符串常量池 运行时常量池 类文件中除了包含类的版本、字段、方法、接口等描述信息, 这些最开始都是保存在常量池中，他们都是静态信息,\n当程序运行时被加载到内存后，这些符号才有对应的内存地址信息。这些常量一旦被转入内存就会变成运行时常量池。运行时常量池在方法区中。\n比如 math.compute() , 在代码中写的是字符串, 那具体怎么调用的呢?\ncompute()这个符号引用在运行时就会被转变为compute()方法具体代码在内存中的地址，主要通过对象头里的类型指针去转换直接引用。\ncompute()方法被加载到内存以后，就有了自己的地址，原来调用computer()方法的符号引用，现在就变成对compute()地址的直接引用，这个直接引用是存在对象头里的，通过指针来指向直接引用\n所以 运行时常量池 存的类和方法等一些描述信息, 用于程序调用执行, 存在方法区中, jdk8后放入元空间了\n字符串常量池 字符串常量池存的是字符串信息 , 存在于堆中 , 因为字符串用的多, 所以用个池单独存起来, 并实现值的复用\n比如:\nString c = \u0026#34;abcdefg\u0026#34; String d = \u0026#34;abcdefg\u0026#34; c和d 都指向字符串常量池, 且为同一个内存地址,\njdk8后 静态成员变量也放在堆中了\n两个池的存放位置 深刻理解运行时常量池、字符串常量池 - 掘金 (juejin.cn) java的字符串存储在堆中还是常量池中_编码大神经的博客-CSDN博客 6. java 虚拟线程 虚拟线程（Virtual Thread-）是 JDK 而不是 OS 实现的轻量级线程(Lightweight Process，LWP），许多虚拟线程共享同一个操作系统线程，虚拟线程的数量可以远大于操作系统线程的数量。\n近几十年来，我们一直依靠上述多线程模型来解决 Java 中的并发编程问题。为了提高系统的吞吐量，我们必须不断增加线程的数量，但是机器的线程很昂贵，可用线程的数量是有限的。尽管我们使用各种线程池来最大限度地提高线程的成本效益，但在 CPU、网络或内存资源被耗尽之前，线程往往成为我们应用程序性能的瓶颈，无法释放硬件应具有的最大性能。\n我们常见的Java线程与系统内核线程是一一对应的，系统内核线程调度器负责调度Java线程。\nJava19 引入了虚拟线程。在 Java19 中，我们以前使用的线程称为平台线程，仍然与系统内核线程一一对应。大量 (M个) 的虚拟线程，运行在少量 (N个) 的平台线程上（与 OS 线程一一对应）（M:N 调度）。JVM调度多个虚拟线程在特定平台线程上执行，并且在平台线程上一次只执行一个虚拟线程。\n直接用虚拟线程池代替线程池。如果您的项目使用CompletableFuture，您也可以直接将执行异步任务的线程池替换为Executors.newVirtualThreadPerTaskExecutor(). 消除池化机制。虚拟线程非常轻量级，不需要池化。 synchronized更改为ReentrantLock减少固定到平台线程的虚拟线程。 Java19 虚拟线程原理介绍及实现_lew-yu的博客-CSDN博客 Java 19 新特性概览 | JavaGuide ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/java%E5%9F%BA%E7%A1%80.html","summary":"1.java是值传递还是引用传递 ​ 答: 全部均为值传递!!!! 以前我认为基本类型是值传递,而对象是引用传递,这是错误的!!!! 有时候我会觉得传","title":"java基础"},{"content":"首先了解JPA是什么？ JPA(Java Persistence API)是Sun官方提出的Java持久化规范。它为Java开发人员提供了一种对象/关联映射工具来管理Java应用中的关系数据.\n他的出现主要是为了简化现有的持久化开发工作和整合ORM技术，结束现在Hibernate，TopLink，JDO等ORM框架各自为营的局面。值得注意的是，JPA是在充分吸收了现有Hibernate，TopLink，JDO等ORM框架的基础上发展而来的，具有易于使用，伸缩性强等优点。\n从目前的开发社区的反应上看，JPA受到了极大的支持和赞扬，其中就包括了Spring与EJB3.0的开发团队。\n注意:JPA是一套规范，不是一套产品，那么像Hibernate,TopLink,JDO他们是一套产品，如果说这些产品实现了这个JPA规范，那么我们就可以叫他们为JPA的实现产品。\nspring data jpa Spring Data JPA 是 Spring (所以这不是spring boot的)基于 ORM 框架、JPA 规范的基础上封装的一套JPA应用框架，可使开发者用极简的代码即可实现对数据的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展！学习并使用 Spring Data JPA 可以极大提高开发效率！ spring data jpa让我们解脱了DAO层的操作，基本上所有CRUD都可以依赖于它来实现\n来自 http://www.ityouknow.com/springboot/2016/08/20/spring-boo-jpa.html 注:在项目中应该用得不多,毕竟项目中都是复杂查询,不会用简单的,即使有也不用(要保持代码风格一致),一些用sql一些用jpa,阅读会有障碍,虽然jpa支持一定的复杂查询,但还是对sql比较熟悉\n导入包:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.6.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 继承JpaRepository public interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt; { } 实体类 import javax.persistence.Entity; import javax.persistence.Id; import javax.persistence.Table; @Entity @Table(name=\u0026#34;person\u0026#34;) public class User { @Id private String id; @Column(name = \u0026#34;name\u0026#34;)//映射数据库字段名 private String userName; private String password; // set , get ... } 使用默认方法 @Test public void testBaseQuery() throws Exception { User user=new User(); userRepository.findAll(); userRepository.findOne(1l); userRepository.save(user); userRepository.delete(user); userRepository.count(); userRepository.exists(1l); // ... } 来自 http://www.ityouknow.com/springboot/2016/08/20/spring-boo-jpa.html 自定义的简单查询\n根据方法名来自动生成SQL，主要的语法是findXXBy,readAXXBy,queryXXBy,countXXBy, getXXBy后面跟属性名称： (其实还是要根据它的规则来命名)\nimport org.springframework.data.jpa.repository.JpaRepository; import com.xkj.demo.entity.User; public interface UserRepository extends JpaRepository\u0026lt;User, Long\u0026gt;{ User findByUserName(String UserName);// } 来自 http://www.ityouknow.com/springboot/2016/08/20/spring-boo-jpa.html ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/jpa.html","summary":"首先了解JPA是什么？ JPA(Java Persistence API)是Sun官方提出的Java持久化规范。它为Java开发人员提供了一种对象/关联映射工具来管理Java应用中","title":"jpa"},{"content":"JVM 中类的装载是由类加载器（ ClassLoader）和它的子类来实现的， Java 中的类加载器是一个重要的 Java 运行时系统组件，它负责在运行时查找和装入类文件中的类。\n当 Java 程序需要使用某个类时， JVM 会确保这个类已经被加载、连接（验证、准备和解析）和初始化。类的加载是指把类的.class 文件中的数据读入到内存中，通常是创建一个字节数组读入.class 文件，然后产生与所加载类对应的 Class 对象。此时的类还不可用。\n当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。\n最后 JVM 对类进行初始化，包括：\n1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；\n2)如果类中存在初始化语句，就依次执行这些初始化语句。\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm_%E5%8A%A0%E8%BD%BD_class_%E6%96%87%E4%BB%B6%E7%9A%84%E5%8E%9F%E7%90%86%E6%9C%BA%E5%88%B6.html","summary":"JVM 中类的装载是由类加载器（ ClassLoader）和它的子类来实现的， Java 中的类加载器是一个重要的 Java 运行时系统组件，它负责在运行时查找和装入类","title":"JVM_加载_class_文件的原理机制"},{"content":"[toc]\njps jps 命令类似与 linux 的 ps 命令，但是它只列出系统中所有的 Java 应用程序。 通过 jps 命令可以方便地查看 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息。\n-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数\n-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null\n-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名\n-v 输出传递给JVM的参数\njinfo jinfo 是 JDK 自带的命令，可以用来查看正在运行的 java 应用程序的扩展参数，包括Java System属性和JVM命令行参数；也可以动态的修改正在运行的 JVM 一些参数。当系统崩溃时，jinfo可以从core文件里面知道崩溃的Java应用程序的配置信息\njinfo Pid 输出进程的基本信息。\njstat 用于监控虚拟机各种运行状态信息的命令行工具。他可以显示本地或远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据，在没有GUI图形的服务器上，它是运行期定位虚拟机性能问题的首选工具。\njstat -gcutil Pid 5s：每5秒输出一次GC情况。\njstack Jstat是JDK自带的一种堆栈跟踪工具。全称“Java Virtual Machine statistics monitoring tool”，可以用于生成java虚拟机当前时刻的线程快照\njstack -l Pid \u0026gt; /data/jstack.txt：将指定进行的线程情况进行输出到指定文件中。\njmap 命令jmap是一个多功能的命令。它可以生成 java 程序的 dump 文件， 也可以查看堆内对象示例的统计信息、查看 ClassLoader 的信息以及 finalizer 队列。\n-histo Pid \u0026gt; /data/histo.txt：将堆中对象统计信息输出到指定文件中。\njmap -heap Pid \u0026gt; /data/jmap_heap.txt：输出堆内存信息到指定文件中。\njmap -J-d64 -dump:format=b,file=/data/heap_dump.bin Pid：输出JVM的堆内容到指定文件中。\njhat：Java堆分析工具 jhat（Java堆分析工具），是一个用来分析java的堆情况的命令。使用jmap 可以生成Java堆的Dump文件。生成转储文件之后就可以用jhat命令，将转储文件转成html的形式，然后通过http访问可以查看堆情况。\njhat命令解析会Java堆dump并启动一个web服务器，然后就可以在浏览器中查看堆的dump文件了。\nJVM常用命令 - 简书 (jianshu.com) JVM命令 - acehm - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html","summary":"[toc] jps jps 命令类似与 linux 的 ps 命令，但是它只列出系统中所有的 Java 应用程序。 通过 jps 命令可以方便地查看 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息。 -q 只显","title":"jvm常用命令"},{"content":"前言 无论是YGC或是FullGC，都会导致stop-the-world，即整个程序停止一些事务的处理，只有GC进程允许以进行垃圾回收，因此如果垃圾回收时间较长，部分web或socket程序，当终端连接的时候会报connetTimeOut或readTimeOut异常，\n从JVM调优的角度来看，我们应该尽量避免发生YGC或FullGC，或者使得YGC和FullGC的时间足够的短。\n所谓调优,就是找到适合自己程序的配置(jvm配置,比如,老年代大小,新生代大小,线程数,垃圾回收器等等)\n原文链接：https://blog.csdn.net/Javazhoumou/article/details/99298624\nGC日志 GC日志分类 Minor GC/Young GC，表示新生代GC，指发生在新生代的垃圾收集动作，所有的Minor GC都会触发全世界的暂停（stop-the-world），停止应用程序的线程，不过这个过程非常短。 Major GC/ Full GC：老年代GC，指发生在老年代的 GC，也称之为 Full GC。 日志分析 Young GC回收日志:\n日志内容如下: [GC (Allocation Failure) [PSYoungGen: 637744K-\u0026gt;43513K(620544K)] 714874K-\u0026gt;144918K(730624K), 0.0652514 secs] [Times: user=0.20 sys=0.05, real=0.07 secs]\nGC (Allocation Failure) : 表示发送了GC类型, 以及GC的原因, 此处是 内存分配失败\nFull GC回收日志:\n日志内容如下: [Full GC (Ergonomics) [PSYoungGen: 43513K-\u0026gt;23126K(620544K)] [ParOldGen: 101404K-\u0026gt;109897K(221184K)] 144918K-\u0026gt;133024K(841728K), [Metaspace: 77276K-\u0026gt;76763K(1120256K)], 0.3217926 secs] [Times: user=1.55 sys=0.06, real=0.32 secs]\nGC原因类型 Allocation Failure\n内存分配失败导致的GC，常见于年轻代当中。\nGCLocker Initiated GC\n使用JNI临界区的方式操作数组或者字符串时，为了防止GC过程中jarray或者jstring发生位移，而导致数组指针失效，需要保持它们在JVM Heap中的地址在JNI Critical过程中保持不变。于是JVM实现了GC_locker，用于JNI Critical内阻止其他GC的发生。\n当GCLocker被激活且需要发生GC的时候（这里是否需要GC是各种GC发生时，调用GCLocker::check_active_before_gc()函数check并设置_needs_gc = true的），就会阻塞其他线程进入JNI临界区；并且在最后一个位于JNI临界区的线程退出临界区时，发起一次CGCause为_gc_locker的GC。这里解释了GCLocker Initiated GC发生的原委。\nErgonomics\n在JVM中的垃圾收集器中的Ergonomics就是负责自动的调解gc暂停时间和吞吐量之间的平衡，使你的虚拟机性能更好的一种做法。\n简单说就是内存在进行分配的时候，会通过一些算法，预估是否会出现无法分配的问题。如果符合无法分配预估值，会提前进行一次gc。\nMetadata GC Threshold\n这个gc主要发生的条件是元空间，也就是Metadata的参数设置问题。如果不设置元空间的大小，会有一个默认值是21M。此时可以适当加大元空间内存, -XX:MetaspaceSize=128M\n几种常见GC简介 - 简书 (jianshu.com) 工具篇 使用工具: (java 自带分析工具)\njmap：生成虚拟机的内存转储快照（heapdump文件）,使用mat软件打开并分析\nJConsole：JMX的可视化管理工具\n这个工具相比较前面几个工具，使用率比较高，很重要。它是一个java GUI监视工具，可以以图表化的形式显示各种数据。并可通过远程连接监视远程的服务器VM。用java写的GUI程序，用来监控VM，并可监控远程的VM\nVisualVM：多合一故障管理工具 这个工具也很牛bility。它同jconsole都是一个基于图形化界面的、可以查看本地及远程的JAVA GUI监控工具，Jvisualvm同jconsole的使用方式一样，直接在命令行打入jvisualvm即可启动，jvisualvm界面更美观一些\nhttps://baijiahao.baidu.com/s?id=1639024706303844305\u0026wfr=spider\u0026for=pc 有时间就玩一下吧\n服务启动后, 这里会自动检测到,并显示出来 此处各种监控, 其中较为重要的是监控tab页; 其次是抽样器和线程, 抽样器能看到对象内存大小的变化 监控tab中,分为cpu/内存/类/线程 的变化 导出堆进行分析, (当前软件就能分析, 可导出多次) 20.GC日志详解及日志分析工具 - 盛开的太阳 - 博客园 (cnblogs.com) 在线GC日志分析 JVM系列（四）GC日志分析 - 知乎 (zhihu.com) 阿里P8技术官：搞 Java开发的，看懂JVM的GC日志真的很重要 - 知乎 (zhihu.com) JVM实战：GC日志解析 - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm%E8%B0%83%E4%BC%98.html","summary":"前言 无论是YGC或是FullGC，都会导致stop-the-world，即整个程序停止一些事务的处理，只有GC进程允许以进行垃圾回收，因此如","title":"jvm调优相关"},{"content":"[toc]\n1.kafka的message包括哪些信息 一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，比如是否压缩、压缩格式等等)；如果magic的值为0，那么不存在attributes属性body是由N个字节构成的一个消息体，包含了具体的key/value消息\n2. 怎么解决kafka的数据丢失 producer端： 宏观上看保证数据的可靠安全性，肯定是依据分区数做好数据备份，设立副本数。\nbroker端： topic设置多分区，分区自适应所在机器，为了让各分区均匀分布在所在的broker中，分区数要大于broker数。\n分区是kafka进行并行读写的单位，是提升kafka速度的关键。\nConsumer端 consumer端丢失消息的情形比较简单：如果在消息处理完成前就提交了offset，那么就有可能造成数据的丢失。由于Kafka consumer默认是自动提交位移的，所以在后台提交位移前一定要保证消息被正常处理了，因此不建议采用很重的处理逻辑，如果处理耗时很长，则建议把逻辑放到另一个线程中去做。为了避免数据丢失，现给出两点建议：\nenable.auto.commit=false 关闭自动提交位移\n在消息被完整处理之后再手动提交位移\n3.为什么Kafka不支持读写分离？ 也就是说: kafka对某一个主题的读写是在一个节点完成的,分区的从节点都是用来同步的\n既然是在一个节点,为什么能做到高并发?(大概是因为: 对主题的读写本质是对主分区的读写,只要主分区尽可能的分配均匀至每个节点,就能分散压力)\n因为这样有两个明显的缺点：\n数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。 延时问题。数据从写入主节点到同步至从节点中的过程需要经历网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。 对于Kafka来说，必要性不是很高，因为在Kafka集群中，如果存在多个副本，经过合理的配置，可以让leader副本均匀的分布在各个broker上面，使每个 broker 上的读写负载都是一样的。\n4. Kafka中的延迟队列 在发送延时消息的时候并不是先投递到要发送的真实主题（real_topic）中，而是先投递到一些 Kafka 内部的主题（delay_topic）中，这些内部主题对用户不可见，然后通过一个自定义的服务拉取这些内部主题中的消息，并将满足条件的消息再投递到要发送的真实的主题中，消费者所订阅的还是真实的主题。\n5. Kafka为什么吞吐量大、速度快？ 一、顺序读写\n众所周知Kafka是将消息记录持久化到本地磁盘中的，一般人会认为磁盘读写性能差，可能会对Kafka性能如何保证提出质疑。实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，一些情况下磁盘顺序读写性能甚至要高于内存随机读写。\n二、Page Cache\n为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：\n1避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。\n2避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题\n相比于使用JVM或in-memory cache等数据结构，利用操作系统的Page Cache更加简单可靠。首先，操作系统层面的缓存利用率会更高，因为存储的都是紧凑的字节结构而不是独立的对象。其次，操作系统本身也对于Page Cache做了大量优化，提供了 write-behind、read-ahead以及flush等多种机制。再者，即使服务进程重启，系统缓存依然不会消失，避免了in-process cache重建缓存的过程。\n通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。\n三、零拷贝\nlinux操作系统 “零拷贝” 机制使用了sendfile方法， 允许操作系统将数据从Page Cache 直接发送到网络，只需要最后一步的copy操作将数据复制到 NIC 缓冲区， 这样避免重新复制数据 。示意图如下：\n![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA3LzA4L1pyNTF6Vi5wbmc) 通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。\n四、分区分段+索引\nKafka的message是按topic分类存储的，topic中的数据又是按照一个一个的partition即分区存储到不同broker节点。每个partition对应了操作系统上的一个文件夹，partition实际上又是按照segment分段存储的。这也非常符合分布式系统分区分桶的设计思想。\n通过这种分区分段的设计，Kafka的message消息实际上是分布式存储在一个一个小的segment中的，每次文件操作也是直接操作的segment。为了进一步的查询优化，Kafka又默认为分段后的数据文件建立了索引文件，就是文件系统上的.index文件。这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。\n五、批量读写\nKafka数据读写也是批量的而不是单条的。\n除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多。\n六、批量压缩\n在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，对于需要在广域网上的数据中心之间发送消息的数据流水线尤其如此。进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑。\n如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩\nKafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩\nKafka支持多种压缩协议，包括Gzip和Snappy压缩协议\nKafka速度的秘诀在于，它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络IO损耗，通过mmap提高I/O速度，写入数据的时候由于单个Partion是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/kafka.html","summary":"[toc] 1.kafka的message包括哪些信息 一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成head","title":"kafka"},{"content":" kafka 的 server.properties 中加入 host.name=10.1.21.37, 不然springBoot连不上kafka\n1.pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.application.properties\n#============== kafka =================== # 指定kafka 代理地址，可以多个 spring.kafka.bootstrap-servers=10.1.21.37:9092 #=============== provider ======================= spring.kafka.producer.retries=2 # 每次批量发送消息的数量 spring.kafka.producer.batch-size=16384 spring.kafka.producer.buffer-memory=33554432 # 指定消息key和消息体的编解码方式 spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer #=============== consumer ======================= # 指定默认消费者group id spring.kafka.consumer.group-id=test spring.kafka.consumer.auto-offset-reset=earliest spring.kafka.consumer.enable-auto-commit=true spring.kafka.consumer.auto-commit-interval=100 # 指定消息key和消息体的编解码方式 spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer topic=msg.topic 代码 @Autowired private KafkaTemplate\u0026lt;String, Object\u0026gt; kafka; // 生产者 @RequestMapping(\u0026#34;/sendMsg\u0026#34;) public LocalDateTime sendMsg(String helpNo) throws Exception { HelpInfo helpInfo=new HelpInfo(); LocalDateTime now = LocalDateTime.now(); helpInfo.setContent(\u0026#34;content\u0026#34;); helpInfo.setHelpNo(helpNo); helpInfo.setTitle(\u0026#34;title\u0026#34;); helpInfo.setUpdateName(now.format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)); // 指定主题, 计算分区, json类型的内容 SendResult\u0026lt;String,Object\u0026gt; sendResult = kafka.send(topic,Math.abs(helpNo.hashCode())%3,helpInfo.getHelpNo(), JSON.toJSONString(helpInfo)).get(); long hasOffset = sendResult.getRecordMetadata().offset();// 得到偏移量 , 捷顺是通过偏移量不等于-1来判断是否发送成功 System.out.println(\u0026#34;生产数据偏移量:\u0026#34;+hasOffset); System.out.println(sendResult); return now; } // 消费这个主题下所有分区数据 @KafkaListener(topics= \u0026#34;${topic}\u0026#34; ) public Object consumeMsg(String msg) { // msg就是生产者的数据 System.out.println(\u0026#34;消费的数据:\u0026#34;+msg); return msg; } // 在消费群组topicPartition 下 消费 ${topic} 主题的 2,0 分区,其中0分区从偏移量为2开始消费 @KafkaListener(id=\u0026#34;topicPartition\u0026#34;,topicPartitions= { @TopicPartition(topic = \u0026#34;${topic}\u0026#34;, // 指定主题 partitions= {\u0026#34;2\u0026#34;}, // 指定分区消费 partitionOffsets= @PartitionOffset(initialOffset = \u0026#34;2\u0026#34;, partition = \u0026#34;0\u0026#34;))})// 指定分区并指定偏移量 public Object consumeMsgOfPart(String msg) { System.out.println(\u0026#34;指定消费的数据:\u0026#34;+msg); // 一般情况会直接消费整个主题,毕竟分区是用来加快速度的,配上批量消费(得自己写消费工厂),简直棒棒哒 return msg; // 用ConsumerRecord接收,能接收都更多数据,比如分区,偏移量等等 @KafkaListener(id=\u0026#34;consumeMsgGetMorn\u0026#34;,topics=\u0026#34;${topic}\u0026#34;) public Object consumeMultMsg(ConsumerRecord\u0026lt;String, Object\u0026gt; record) { System.out.println(\u0026#34;消费的数据,更多返回值:\u0026#34;+record); return record; } // 使用Ack机制确认消费\n//使用Kafka的Ack机制比较简单，只需简单的三步即可：\n设置ENABLE_AUTO_COMMIT_CONFIG=false，禁止自动提交 设置AckMode=MANUAL_IMMEDIATE 监听方法加入Acknowledgment ack 参数 配置文件加入如下: spring.kafka.consumer.enable-auto-commit=false spring.kafka.listener.ack-mode=MANUAL_IMMEDIATE @KafkaListener(id=\u0026#34;consumeMsgGetMorn\u0026#34;,topics=\u0026#34;${topic}\u0026#34;) public Object consumeMultMsg(ConsumerRecord\u0026lt;String, Object\u0026gt; record, Acknowledgment ack) { System.out.println(\u0026#34;ack消费的数据值:\u0026#34;+record); // 如果不手动确认,则偏移量会不改变 ack.acknowledge(); return record; } 来自: https://www.jianshu.com/p/a64defb44a23 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%88%86%E6%94%AF/kafka-springboot.html","summary":"kafka 的 server.properties 中加入 host.name=10.1.21.37, 不然springBoot连不上kafka 1.pom.xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.application.properties #============== kafka =================== # 指定kafka 代理地址，可以多个 spring.kafka.bootstrap-servers=10.1.21.37:9092 #=============== provider ======================= spring.kafka.producer.retries=2 # 每次批量发送消息的数量","title":"Kafka-springBoot"},{"content":"[toc]\n1.1. 目标 本快速入门指南的目标是提供与KafkaStreams的第一个应用程序示例。我们将演示在你的第一个示例程序中，如果使用Kafka Streams库和演示一个简单的端到端的数据流。 值得注意的是，这种快速入门只涵盖了KafkaStreams的表面，这篇文档的剩余部分将会提供更多的细节，我们将在快速入门指南中为你指明方向。\n1.2. 我们想做什么 在这个快速入门中，我们将运行包含Apachekafka的一个wordcount演示应用程序。下面代码的关键在于使用Java8的lambda表达式，易于阅读。(摘自WordCountLambdaExample):\n[java] view plain copy //序列化/反序列化Sting和Long类型 final Serde\u0026lt;String\u0026gt; stringSerde = Serdes.String(); final Serde\u0026lt;Long\u0026gt; longSerde = Serdes.Long(); //通过指定输入topic “mystream”来构造KStream实例，\n//输入数据就以文本的形式保存在topic “mystream” 中。\n//(在本示例中，我们忽略所有消息的key.)\nKStream\u0026lt;String, String\u0026gt; textLines = builder.stream(stringSerde, stringSerde, \u0026quot;mystream\u0026quot;); KStream\u0026lt;String, Long\u0026gt; wordCounts = textLines\n//以空格为分隔符，将每行文本数据拆分成多个单词。\n//这些文本行就是从输入topic中读到的每行消息的Value。\n//我们使用flatMapValues方法来处理每个消息Value，而不是更通用的flatMap .flatMapValues(value -\u0026gt; Arrays.asList(value.toLowerCase().split(\u0026quot;\\W+\u0026quot;)))\n//我们随后将调用countByKey来计算每个单词出现的次数\n//所以我们将每个单词作为map的key。\n.map((key, value) -\u0026gt; new KeyValue\u0026lt;\u0026gt;(value, value)) //通过key来统计每个单词的次数\n//\n//这会将流类型从KStream\u0026lt;String,String\u0026gt;转为KTable\u0026lt;String,Long\u0026gt; (word-count).\n//因此我们必须提供String和long的序列化反序列化方法。\n//\n.countByKey(stringSerde, \u0026quot;Counts\u0026quot;) //转化KTable\u0026lt;String,Long\u0026gt;到KStream\u0026lt;String,Long\u0026gt;\n.toStream();\n//将KStream\u0026lt;String,Long\u0026gt;写入到输出topic中。\nwordCounts.to(stringSerde, longSerde, \u0026quot;streams-wordcount-output\u0026quot;); 在上面的代码执行过程中，我们将执行如下步骤：\n1、 启动一台kafka集群 2、 使用Kafkaconsole producer命令行生产者客户端往Kafka Topic中写入示例输入数据 3、 在Java应用程序中使用kafkaStream库来处理输入数据。这里，我们使用了一个包含kafka的WordCount示例程序。 4、 使用Kafkaconsole consumer命令行消费者客户端检查应用程序的输出。 5、 停止Kafka集群 1.3. 启动Kafka 集群 在本章节中，我们会在一台机器上安装并启动Kafka集群。该集群有一个单节点Kafka(只有一个Broker)外加一个单节点Zookeeper构成。在wordcount演示程序中，这种集群依赖是必须的。我们假定kafka broker运行地址为localhost:9092, Zookeeper本地地址为localhost:2181。 首先，安装Oracle JRE或JDK 1.7及以上版本 然后，下载和安装包含Kafka Streams的新版本Apache Kafka. 为此，我们使用Confluent Platform 3.0.0版本。 (下面操作比较简单，所以不翻译了。)\n[plain] view plain copy # Download and install Confluent Platform 3.0.0 from ZIP archive $ wget http://packages.confluent.io/archive/3.0/confluent-3.0.0-2.11.zip $ unzip confluent-3.0.0-2.11.zip # *** IMPORTANT STEP **** # The subsequent paths and commands used throughout this quickstart assume that # your are in the following working directory: $ cd confluent-3.0.0/ # Note: If you want to uninstall the Confluent Platform at the end of this quickstart, # run the following commands. # # $ rm -rf confluent-3.0.0/ # $ rm -rf /var/lib/kafka # Data files of Kafka # $ rm -rf /var/lib/kafka-streams # Data files of Kafka Streams # $ rm -rf /var/lib/zookeeper # Data files of ZooKeeper 提示：可以通过Installationvia ZIP and TAR archives 和ConfluentPlatform Quickstart 获取更进一步信息。 我们首先启动ZooKeeper实例。该实例将监听本地2181端口。由于这是一个长期运行的服务，你应该在自己的终端中运行。\n[plain] view plain copy # Start ZooKeeper. Run this command in its own terminal. $ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties 接下来，我们启动Kakfa的Broker，这将监听本地9092端口，然后连接到我们刚刚启动的Zookeeper实例。这也是一个长期运行的服务，也应该在终端中运行它。\n[plain] view plain copy # Start Kafka. Run this command in its own terminal $ ./bin/kafka-server-start ./etc/kafka/server.properties 现在，我们的单节点kafka集群已经完全运转起来了，我们就可以着手准备输入数据，运行我们的第一个kafka Streams示例程序。\n1.4. 准备输入数据 提示：在本章节中，我们将使用内置的命令行工具来输入kakfa数据。在实际使用中，你应该通过其他方式将数据写入Kafka中，比如通过你自己应用程序中的Kafka客户端。 现在，我们将一些输入数据发送到Kafka的topic中，然后由Kafka Streams的应用程序做后续处理。 首先，我们要创建名称为mystream的topic：\n[plain] view plain copy $ ./bin/kafka-topics.sh --create --zookeeper slave01:2181,slave02:2181,slave03:2181 --replication-factor 1 --partitions 1 --topic mystream 下一步，我们生成一些输入数据并保存在本地文件/tmp/file-input.txt中。\n[plain] view plain copy $ echo -e \u0026#34;all streams lead to kafka hello kafka streams join kafka summit\u0026#34; \u0026gt; /tmp/file-input.txt 生成的文件将包含如下内容：\n[plain] view plain copy all streams lead to kafka hello kafka streams join kafka summit 最后，我们发送这些数据到input topic\n[plain] view plain copy $ cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list master:9092,slave01:9092,slave02:9092,slave03:9092 --topic mystream Kafka console-producer从stdin中读取数据，并将每一行作为单独的消息发送到kafka的输入流中。该消息的key是null，消息是每行内容，使用字符串编码。\n注意: 你可能想知道这样一步步的快速启动和真实流处理系统的差异，在大型的实时的流处理系统中，数据总是在移动的，快速入门的目的仅仅是做功能证明。简单来说，一个端到端的数据管道建立在Kafka和Kafka Streams的各个方面。出于说教的原因，我们故意将快速入门清楚地拆分成一系列分开连续的步骤。\n但在实践中，这些步骤通常会看起来有些不同并且会有并发的存在。比如输入数据可能不会来源于本地文件，而是直接从分布式系统中发送的，并且数据将被连续的写入Kafka。类似的，流处理应用程序可能在第一行数据发送之前就已经启动并运行。\n1.5. 在KafkaStreams中处理输入数据 现在，我们已经生成了一些输入数据，我们可以运行我们的第一个基于Kafka Streams的java应用程序。\n我们将运行WordCount演示应用程序，它使用了ApacheKafka。它实现了WordCount算法，从输入文本来计算直方图。然而和其他你之前见过的操作被绑定在数据上的WordCount实例程序不同的是，这个示例程序是数据无界，无限流动的。和有界算法的变体类似，他是一个有状态的算法，跟踪并更新word的计数器。然后因为它必须接受无界的输入数据，它会周期性的输出其当前状态和计算结果，同时继续处理更多的数据，因为它不知道是否已经处理了所有的数据。这就是他和Hadoop 的Mapreduce算法之间的典型差异。一旦我们了解这种差异，检查了实际的输出数据之后，会更容易接受它。 由于wordCount示例程序与kafka打包在一起，已经在Kafka的Broker中集成，这就意味着我们不需要做额外的事情就可以运行它，无需编译任何Java源代码。\n[plain] view plain copy # Run the WordCount demo application. There won\u0026#39;t be any STDOUT output. # You can safely ignore any WARN log messages. $ ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo 注意，这里没有魔术式的部署，实际上，使用kafkaStreams库中的任何应用程序，就像启动任何普通的Java应用程序，该脚本kafka-run-class也只是一个简单的java -cp命令的包装。\n该WordCount示例程序将从输入topic中读取数据，然后计算wordCount，将计算结果不断进行输出。演示将运行几秒钟，然后和其他典型流处理应用程序不同的是，它将会自动终止。\n1.6. 检查输出结果 在本章节中，我们将使用内置的命令行工具从kafka中手工读取数据。在实际使用中，你可以通过其他方式，通过Kakfa客户端从Kafka中读取数据。比如，如果你可以在自己的应用程序中使用Kafka客户端将数据从Kakfa中迁移到其它数据系统。 现在，我们可以从kafka输出topic中读取数据并检查wordcount实例运行结果。\n[plain] view plain copy ./bin/kafka-console-consumer --zookeeper slave01:2181 \\ --topic streams-wordcount \\ --from-beginning \\ --formatter kafka.tools.DefaultMessageFormatter \\ --property print.key=true\\ --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \\ --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer kafka-console-consumer.sh --zookeeper slave01:2181 --topic streams-wordcount --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer Wordcount的数据将会被打印在如下的控制台中：\n[plain] view plain copy all 1 streams 1 lead 1 to 1 kafka 1 hello 1 kafka 2 streams 2 join 1 kafka 3 summit 1 这里，第一列是Kafka消息的key的字符串格式，第二列是消息的值，long类型。你可以通过Ctrl+c命令来终止控制台输出。 但是等一下，输出看起来是不是很奇怪？为什么会出现重复的条目？比如streams出现了两次：\n[plain] view plain copy # Why not this, you may ask? all 1 lead 1 to 1 hello 1 streams 2 join 1 kafka 3 summit 1 对于上面的输出的解释是，wordCount应用程序的输出实际上是持续更新的流，其中每行记录是一个单一的word(即Message Key，比如Kafka)的计数。对于同一个Key的多个记录，每个记录之后是前一个的更新。\n当第二个文本航的hello kafkastreams被处理的时候，我们观察到，相对第一次，已经存在的条目KTable被更新了(Kafak和Streams这两个单词). 修改后的记录被在此发送到了KStream。 这就解释了上述KStream第二列中显示的信息，为什么输出的topic上显示的内容，因为它是包含了变化的完整内容\n[plain] view plain copy all 1 streams 1 lead 1 to 1 kafka 1 hello 1 kafka 2 streams 2 join 1 kafka 3 summit 1 更多参看官网:http://kafka.apache.org/documentation/streams/\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafkastream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html","summary":"[toc] 1.1. 目标 本快速入门指南的目标是提供与KafkaStreams的第一个应用程序示例。我们将演示在你的第一个示例程序中，如果使用Kafka Str","title":"KafkaStream快速入门"},{"content":"[toc]\n1.前言: 对于整个选举算法的详情需要先了解Raft选举算法，kafka是基于该算法来实现leader选举的。有兴趣的读者可以参考之前的文章【分布式一致性协议：Raft算法详解 】。\nkafka 的选举有三类:\n控制器（Broker）选主 分区多副本选主 消费组选主 2.选举类型: 2.1 控制器（Broker）选举 所谓控制器就是一个Borker，在一个kafka集群中，有多个broker节点，但是它们之间需要选举出一个leader，其他的broker充当follower角色。集群中第一个启动的broker会通过在zookeeper中创建临时节点/controller来让自己成为控制器(其实大家竞争成为控制器,一般先启动的,先注册成功嘛)，其他broker启动时也会在zookeeper中创建临时节点，但是发现节点已经存在，所以它们会收到一个异常，意识到控制器已经存在，那么就会在zookeeper中创建watch对象，便于它们收到控制器变更的通知。\n那么如果控制器由于网络原因与zookeeper断开连接或者异常退出(此时其他borker也是竞争成为控制器)，那么其他broker通过watch收到控制器变更的通知，就会去尝试创建临时节点/controller，如果有一个broker创建成功，那么其他broker就会收到创建异常通知，也就意味着集群中已经有了控制器，其他broker只需创建watch对象即可。\n如果集群中有一个broker发生异常退出了，那么控制器就会检查这个broker是否有分区的副本leader，如果有那么这个分区就需要一个新的leader，此时控制器就会去遍历其他副本，决定哪一个成为新的leader，同时更新分区的ISR集合。\n如果有一个broker加入集群中，那么控制器就会通过Broker ID去判断新加入的broker中是否含有现有分区的副本，如果有，就会从分区副本中去同步数据。\n集群中每选举一次控制器，就会通过zookeeper创建一个controller epoch，每一个选举都会创建一个更大，包含最新信息的epoch，如果有broker收到比这个epoch旧的数据，就会忽略它们，kafka也通过这个epoch来防止集群产生“脑裂”。\n原文链接：https://blog.csdn.net/qq_37142346/article/details/91349100\ncontroller作用: 维护ISR集合,选举leader,增加分区时的重新分配工作\n2.2 分区副本选举机制 在kafka的集群中，会存在着多个主题topic，在每一个topic中，又被划分为多个partition，为了防止数据不丢失，每一个partition又有多个副本，在整个集群中，总共有三种副本角色：\n首领副本（leader）：也就是leader主副本，每个分区都有一个首领副本，所有的生产者与消费者的请求都会经过该副本来处理。 跟随者副本（follower）：除了首领副本外的其他所有副本都是跟随者副本，跟随者副本不处理来自客户端的任何请求，只负责从首领副本同步数据，保证与首领保持一致。如果首领副本发生崩溃，就会从这其中选举出一个leader。 首选首领副本：创建分区时指定的首选首领。如果不指定，则为分区的第一个副本。 我们希望每个分区的leader可以分布到不同的broker中，尽可能的达到负载均衡，所以会有一个首选首领，如果我们设置参数auto.leader.rebalance.enable为true，那么它会检查首选首领是否是真正的首领，如果不是，则会触发选举，让首选首领成为首领(如果设置了首选首领,则一定会让它成为leader)。\n原文链接：https://blog.csdn.net/qq_37142346/article/details/91349100\nKafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都复制了leader，只有ISR里的成员才有被选为Leader的可能。 默认的，如果follower与leader之间超过10s内没有发送请求，或者说两者数据差太多(指的是条数,估计是用offset来判断,差值可配置,默认为4000)，此时该follower就会被认为“不同步副本”(Out-Sync Relipcas)。而持续请求的副本就是“同步副本”，当leader发生故障时，会从“同步副本”(In-Sync Replicas)中选举为leader。其中的请求超时时间可以通过参数replica.lag.time.max.ms参数来配置。\n由一个控制器认定谁是leader,\n在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，如果由于服务宕机,导致某个分区的所有副本都失效,就无法保证数据不丢失了。这种情况下有两种可行的方案：\n等待ISR中的任一个Replica“活”过来，并且选它作为Leader 选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader 如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。\n选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。(此时称为脏leader选举)\nKafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 unclean.leader.election.enable 参数决定使用哪种方案，默认是true，采用第二种方案\n链接：https://www.jianshu.com/p/1f02328a4f2e\n链接: https://www.cnblogs.com/qingyunzong/p/9004703.html 这个leader的作用是接收读写操作,follwer只是个副本,如果leader挂了,则其中之一成为leader,继续接收读写操作\n2.3 消费组选主 在kafka的消费端，会有一个消费者协调器以及消费组，组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，那么如何选举的呢？\n如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader，如果某一个时刻leader消费者由于某些原因退出了消费组，那么就会重新选举leader，如何选举？\n原文链接：https://blog.csdn.net/qq_37142346/article/details/91349100\n消费组里有Rebalance 过程,做的是consumer如何达成一致来分配订阅topic的每个分区,其中就得先选leader,具体看 原理介绍\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E7%9A%84%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.html","summary":"[toc] 1.前言: 对于整个选举算法的详情需要先了解Raft选举算法，kafka是基于该算法来实现leader选举的。有兴趣的读者可以参考之前的文章","title":"kafka的选举机制"},{"content":"[toc]\n1、Kafka使用背景 在我们大量使用分布式数据库、分布式计算集群的时候，是否会遇到这样的一些问题：\na.我们想分析下用户行为（pageviews），以便我们设计出更好的广告位\nb.我想对用户的搜索关键词进行统计，分析出当前的流行趋势\nc.有些数据，存储数据库浪费，直接存储硬盘效率又低\n这些场景都有一个共同点：\n数据是由上游模块产生，上游模块，使用上游模块的数据计算、统计、分析，这个时候就可以使用消息系统，尤其是分布式消息系统！\n2、Kafka的定义 What is Kafka：它是一个分布式消息系统，由linkedin使用scala编写，用作LinkedIn的活动流（Activity Stream）和运营数据处理管道（Pipeline）的基础。具有高水平扩展和高吞吐量。\n3.kafka的特点 Zookeeper是一种在分布式系统中被广泛用来作为：分布式状态管理、分布式协调管理、分布式配置管理、和分布式锁服务的集群。\nkafka增加和减少服务器都会在Zookeeper节点上触发相应的事件kafka系统会捕获这些事件，进行新一轮的负载均衡，客户端也会捕获这些事件来进行新一轮的处理。\nKafka在底层摒弃了Java堆缓存机制，采用了操作系统级别的页缓存，同时将随机写操作改为顺序写，再结合Zero-Copy的特性极大地改善了IO性能。但是，这只是一个方面，毕竟单机优化的能力是有上限的。如何通过水平扩展甚至是线性扩展来进一步提升吞吐量呢？ Kafka就是使用了分区(partition)，通过将topic的消息打散到多个分区并分布保存在不同的broker上实现了消息处理(不管是producer还是consumer)的高吞吐量。\n4.Kafka相关概念 1.AMQP协议 Advanced Message Queuing Protocol （高级消息队列协议）\nThe Advanced Message Queuing Protocol (AMQP)：是一个标准开放的应用层的消息中间件（Message Oriented Middleware）协议。AMQP定义了通过网络发送的字节流的数据格式。因此兼容性非常好，任何实现AMQP协议的程序都可以和与AMQP协议兼容的其他程序交互，可以很容易做到跨语言，跨平台。\n上面说的3种比较流行的消息队列协议，要么支持AMQP协议，要么借鉴了AMQP协议的思想进行了开发、实现、设计。\n2.什么是消息系统？ ​ 消息系统负责将数据从一个应用程序传输到另一个应用程序，因此应用程序可以专注于数据，但不必担心如何共享数据。 分布式消息传递基于可靠消息队列的概念。 消息在客户端应用程序和消息传递系统之间异步排队。 有两种类型的消息传递模式可用 - 一种是点对点的，另一种是发布 - 订阅(pub-sub)消息传递系统。 大多数消息传递模式遵循pub-sub。\n点对点消息系统 在点对点系统中，消息被保存在一个队列中。 一个或多个消费者可以消费队列中的消息，但是特定的消息只能由最多一个消费者消费。 一旦消费者在队列中读取消息，消息就从该队列中消失。 这个系统的典型例子是一个订单处理系统，其中每个订单将由一个订单处理器处理，但是多订单处理器也可以同时工作。 下图描述了结构。\n发布-订阅消息系统 在发布-订阅系统中，消息被保存在一个主题中。 与点对点系统不同，消费者可以订阅一个或多个主题并使用该主题中的所有消息。 在发布-订阅系统中，消息生产者称为发布者，消息消费者称为订阅者。 一个真实的例子是Dish TV，它发布体育，电影，音乐等不同的频道，任何人都可以订阅他们自己的一套频道，并在他们的订阅频道可用时获得内容。\n来自* \u0026lt;https://www.yiibai.com/kafka/apache_kafka_introduction.html#article-start \u0026gt;\n3. 一些基本的概念 1、消费者（Consumer）：从消息队列中请求消息的客户端应用程序\n2、生产者（Producer） ：向broker发布消息的应用程序\n3、AMQP服务端（broker）：用来接收生产者发送的消息并将这些消息路由给服务器中的队列，便于fafka将生产者发送的消息，动态的添加到磁盘并给每一条消息一个偏移量，所以对于kafka一个broker就是一个应用程序的实例\n4、复制因子: 数据的备份数\n5、偏移量(offset) : 来表示哪些消息是否被消费,偏移量唯一,消费后只是消费者的偏移量改变了,直到过期消息才会被清除(偏移量有很多,如下图) 来自 http://orchome.com/5 6 、key : Kafka根据传递消息的key来进行分区的分配，即hash(key) % numPartitions ,这就保证了相同key的消息一定会被路由到相同的分区。如果你没有指定key,那Kafka几乎就是随机找一个分区发送无key的消息，然后把这个分区号加入到缓存中以备后面直接使用\n7 、group : 指消费群组,如果groupId一样(每个消费者都要指定groupId),说明是同一个群组,就变成了队列模式,如果每个groupId都不同,就成了发布订阅模式\n8 、主题（Topic）：一个主题类似新闻中的体育、娱乐、教育等分类概念，在实际工程中通常一个业务一个主题。\n9 、分区（Partition）：一个Topic中的消息数据按照多个分区组织，分区是kafka消息队列组织的最小单位，一个分区可以看作是一个,每个partition只能同一个group中的同一个consumer消费,一般情况下partition的数量大于等于broker的数量\n10、段(segment): Partition包含多个segment，每个segment对应一个文件，segment可以手动指定大小，当segment达到阈值时，将不再写数据，每个segment都是大小相同的。segment由多个不可变的记录组成。记录只会被append到segment中，不会被单独删除或修改,当某个segment上的消息条数达到配置值或消息发布时间超过阈值时，segment上的消息会被flush到磁盘,segment达到一定的大小后将不会再往该segment写数据，broker会创建新的segment。segment中的数据，默认保留7天数据。\n11、 Message：\n消息是Kafka通讯的基本单位，有一个固定长度的消息头和一个可变长度的消息体（payload）构成。在Java客户端中又称之为记录(Record)。 消息结构各部分说明如下:\n* CRC32: CRC32校验和，4个字节。\n* magic: Kafka服务程序协议版本号，用于做兼容。1个字节。\n* attributes: 该字段占1字节，其中低两位用来表示压缩方式，第三位表示时间戳类型（0表示LogCreateTime，1表示LogAppendTime），高四位为预留位置，暂无实际意义。\n* timestamp: 消息时间戳，当magic\u0026gt;0 时消息头必须包含该字段。8个字节。\n* key-length: 消息key长度，4个字节。\n* key: 消息key实际数据。\n* payload-length: 消息实际数据长度，4个字节。\n* payload: 消息实际数据\n在实际存储一条消息还包括12字节的额外开销（LogOverhead）:\n* 消息的偏移量: 8字节，类似于消息的Id。\n* 消息的总长度: 4字节\nhttps://mp.weixin.qq.com/s/17b-uA4vxnU_39xXdM9ihQ 如上图Consumer Group A中的consumer-C2挂掉，consumer-C1会接收P1,P2，即一个consumer Group中有其他consumer挂掉后能够重新平衡,反正数据不会丢,这是因为消费组会有选举机制,由组协调器负责(GroupCoordinator),每次有消费者加入或退出消费组,都会触发rebalance,重新分配分区(具体怎么分可看分区分配策略文章)\n一般消息系统，consumer存在两种消费模型：\npush：优势在于消息实时性高。劣势在于没有考虑consumer消费能力和饱和情况，容易导致producer压垮consumer。 pull：优势在可以控制消费速度和消费数量，保证consumer不会出现饱和。劣势在于当没有数据，会出现空轮询，消耗cpu。 kafka采用pull，并采用可配置化参数保证当存在数据并且数据量达到一定量的时候，consumer端才进行pull操作，否则一直处于block状态\n原文链接：https://blog.csdn.net/qq_29186199/article/details/80827085\nkafka支持的客户端语言：Kafka客户端支持当前大部分主流语言，包括：C、C++、Erlang、Java、.net、perl、PHP、Python、Ruby、Go、Javascript\n可以使用以上任何一种语言和kafka服务器进行通信（即辨析自己的consumer从kafka集群订阅消息也可以自己写producer程序）\n4. Kafka架构 生产者生产消息、kafka集群、消费者获取消息这样一种架构,\nkafka集群中的消息，是通过Topic（主题）来进行组织的\n主题（Topic）：一个主题类似新闻中的体育、娱乐、教育等分类概念，在实际工程中通常一个业务一个主题。\n分区（Partition）：一个Topic中的消息数据按照多个分区组织，分区是kafka消息队列组织的最小单位，一个分区可以看作是一个FIFO（ First Input First Output的缩写，先入先出队列）的队列。\nkafka分区是提高kafka性能的关键所在，当你发现你的集群性能不高时，常用手段就是增加Topic的分区，分区里面的消息是按照从新到老的顺序进行组织，消费者从队列头订阅消息，生产者从队列尾添加消息。\n备份（Replication）：为了保证分布式可靠性，kafka0.8开始对每个分区的数据进行备份（不同的Broker上），防止其中一个Broker宕机造成分区上的数据不可用。\nkafka0.7是一个很大的改变：1、增加了备份2、增加了控制借点概念，增加了集群领导者选举 。\n5.Kafka原理 5.1 消费 5.1.1 消费组 什么是consumer group? 一言以蔽之，consumer group是kafka提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费。个人认为，理解consumer group记住下面这三个特性就好了：\nconsumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程 group.id是一个字符串，唯一标识一个consumer group consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer(当然该分区还可以被分配给其他group) 5.1.2 coordinator Kafka提供了一个角色：coordinator来执行对于consumer group的管理。坦率说kafka对于coordinator的设计与修改是一个很长的故事。最新版本的coordinator也与最初的设计有了很大的不同。这里我只想提及两次比较大的改变。\n首先是0.8版本的coordinator，那时候的coordinator是依赖zookeeper来实现对于consumer group的管理的。Coordinator监听zookeeper的/consumers//ids的子节点变化以及/brokers/topics/数据变化来判断是否需要进行rebalance。group下的每个consumer都自己决定要消费哪些分区，并把自己的决定抢先在zookeeper中的/consumers//owners//下注册。很明显，这种方案要依赖于zookeeper的帮助，而且每个consumer是单独做决定的，没有那种“大家属于一个组，要协商做事情”的精神。\n基于这些潜在的弊端，0.9版本的kafka改进了coordinator的设计，提出了group coordinator——每个consumer group都会被分配一个这样的coordinator用于组管理和位移管理。这个group coordinator比原来承担了更多的责任，比如组成员管理、位移提交保护机制等。当新版本consumer group的第一个consumer启动的时候，它会去和kafka server确定谁是它们组的coordinator。之后该group内的所有成员都会和该coordinator进行协调通信。显而易见，这种coordinator设计不再需要zookeeper了，性能上可以得到很大的提升。后面的所有部分我们都将讨论最新版本的coordinator设计。\n上面简单讨论了新版coordinator的设计，那么consumer group如何确定自己的coordinator是谁呢？ 简单来说分为两步：\n确定consumer group位移信息写入__consumers_offsets的哪个分区。具体计算公式：\n__consumers_offsets partition# = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount) 注意：groupMetadataTopicPartitionCount由offsets.topic.num.partitions指定，默认是50个分区。 该分区leader所在的broker就是被选定的coordinator\n5.1.3 Rebalance rebalance本质上是一种协议，规定了一个consumer group下的所有consumer如何达成一致来分配订阅topic的每个分区。比如某个group下有20个consumer，它订阅了一个具有100个分区的topic。正常情况下，Kafka平均会为每个consumer分配5个分区。这个分配的过程就叫rebalance。\n这也是经常被提及的一个问题。rebalance的触发条件有三种：\n组成员发生变更(新consumer加入组、已有consumer主动离开组或已有consumer崩溃了——离开组是主动地发起rebalance；而崩溃则是被动地发起rebalance) 订阅主题数发生变更——这当然是可能的，如果你使用了正则表达式的方式进行订阅，那么新建匹配正则表达式的topic就会触发rebalance 订阅主题的分区数发生变更 Kafka新版本consumer默认提供了两种分配策略：range 和 round-robin。当然Kafka采用了可插拔式的分配策略，你可以创建自己的分配器以实现不同的分配策略。\nrebalance分为2步：Join和Sync\nJoin， 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求入组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。 Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。 consumer group的分区分配方案是在客户端执行的！Kafka将这个权利下放给客户端主要是因为这样做可以有更好的灵活性。比如这种机制下我可以实现类似于Hadoop那样的机架感知(rack-aware)分配方案，即为consumer挑选同一个机架下的分区数据，减少网络传输的开销\n来自: https://www.cnblogs.com/songanwei/p/9202803.html 5.2 Data Replication（副本策略） 5.2.1　消息传递同步策略 Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少，Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。\n为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。\nConsumer读消息也是从Leader读取，只有被commit过(所有同步副本 (ISR 中所有副本) 都保存了)的消息才会暴露给Consumer。\n5.2.2　ACK前需要保证有多少个备份 对于Kafka而言，定义一个Broker是否“活着”包含两个条件：\n一是它必须维护与ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。 二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。 Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica,由zookeeper维护）。如果一个Follower超过一定时间未向leader发送fetch请求，或者落后太多，Leader将把它从ISR中移除。\n这里所描述的“落后太多”指:\nFollower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/server.properties中通过replica.lag.max.messages配置，其默认值是4000,offest值） 或者 Follower超过一定时间（该值可在$KAFKA_HOME/config/server.properties中通过replica.lag.time.max.ms来配置，其默认值是10000）未向Leader发送fetch请求。\n一个副本可以不同步Leader有如下几个原因:\n慢副本：在一定周期时间内follower不能追赶上leader。最常见的原因之一是I / O瓶颈导致follower追加复制消息速度慢于从leader拉取速度。 卡住副本：在一定周期时间内follower停止从leader拉取请求。follower replica卡住了是由于GC暂停或follower失效或死亡。 新启动副本：当用户给主题增加副本因子时，新的follower不在同步副本列表中，直到他们完全赶上了leader日志。 一个partition的follower落后于leader足够多时，被认为不在同步副本列表或处于滞后状态。在Kafka-0.8.2.x中,副本滞后判断依据是副本落后于leader最大消息数量(replica.lag.max.messages)或replicas响应partition leader的最长等待时间(replica.lag.time.max.ms)。前者是用来检测缓慢的副本,而后者是用来检测失效或死亡的副本\n原文链接：https://blog.csdn.net/lizhitao/article/details/51718185 (含图解)\nKafka的复制机制既不是完全的同步复制，也不是单纯的异步复制(一半一半,follower像leader pull消息,只要有一个follower复制成功了就行,所以才出现leader从ISR中移除不同步的副本)。\n完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。 异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。 而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。\n需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的Follower，一条被commit的消息就不会丢失。\n5.2 生产 5.2.1 写入方式 producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率,kafka快的原因之一）。\n5.2.2 消息路由 producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：\n指定了 patition，则直接使用； 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition patition 和 key 都未指定，使用轮询选出一个 patition。 5.3 broker 5.3.1 存储方式 物理上把 topic 分成一个或多个 patition（对应 server.properties 中的 num.partitions=3 配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下：\n5.3.2 存储策略 无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：\n基于时间：log.retention.hours=168 基于大小：log.retention.bytes=1073741824 5.4 高效速度 Kafka会把收到的消息都写入到硬盘中，它绝对不会丢失数据。为了优化写入速度Kafka主要采用了两个技术， 顺序写入 和 MMFile 。\n5.4.1 写入数据 5.4.1.1 顺序写入磁盘 磁盘读写的快慢取决于你怎么使用它，也就是顺序读写或者随机读写。在顺序读写的情况下，某些优化场景磁盘的读写速度可以和内存持平（注：此处有疑问， 不推敲细节，参考 http://searene.me/2017/07/09/Why-is-Kafka-so-fast/ ）。\n因为硬盘是机械结构，每次读写都会寻址-\u0026gt;写入，其中寻址是一个“机械动作”，它是最耗时的。所以硬盘最讨厌随机I/O，最喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O。\n而且Linux对于磁盘的读写优化也比较多，包括read-ahead和write-behind，磁盘缓存等。如果在内存做这些操作的时候，一个是JAVA对象的内存开销很大，另一个是随着堆内存数据的增多，JAVA的GC时间会变得很长，使用磁盘操作有以下几个好处：\n磁盘顺序读写速度超过内存随机读写 JVM的GC效率低，内存占用大。使用磁盘可以避免这一问题 系统冷启动后，磁盘缓存依然可用 因此kafka写数据时在后面追加的(参考offset图解)\n5.4.1.2 Memory Mapped Files(内存映射文件) 即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以Kafka的数据并 不是实时的写入硬盘 ，它充分利用了现代操作系统 分页存储 来利用内存提高I/O效率。\nMemory Mapped Files(后面简称mmap)也被翻译成 内存映射文件 ，在64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）。\n通过mmap，进程像读写硬盘一样读写内存（当然是虚拟机内存），也不必关心内存的大小有虚拟内存为我们兜底。\n使用这种方式可以获取很大的I/O提升， 省去了用户空间到内核空间 复制的开销（调用文件的read会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中。）也有一个很明显的缺陷——不可靠， 写到mmap中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用flush的时候才把数据真正的写到硬盘。 Kafka提供了一个参数——producer.type来控制是不是主动flush，如果Kafka写入到mmap之后就立即flush然后再返回Producer叫 同步 (sync)；写入mmap之后立即返回Producer不调用flush叫 异步 (async)。\n5.4.2 读取数据 5.4.2.1 Zero Copy(零拷贝) 基于sendfile实现Zero Copy\n零拷贝不是没有拷贝, 只是减少了一个拷贝(对用户缓存区来说是无拷贝了)\n传统模式下，当需要对一个文件进行传输的时候，其具体流程细节如下：\n调用read函数，文件数据被copy到内核缓冲区 read函数返回，文件数据从内核缓冲区copy到用户缓冲区 write函数调用，将文件数据从用户缓冲区copy到内核与socket相关的缓冲区。 数据从socket缓冲区copy到相关协议引擎。 以上细节是传统read/write方式进行网络文件传输的方式，我们可以看到，在这个过程当中，文件数据实际上是经过了四次copy操作：\n硬盘—\u0026gt;内核buf—\u0026gt;用户buf—\u0026gt;socket相关缓冲区—\u0026gt;协议引擎\n而sendfile系统调用则提供了一种减少以上多次copy，提升文件传输性能的方法。\n在内核版本2.1中，引入了sendfile系统调用，以简化网络上和两个本地文件之间的数据传输。 sendfile的引入不仅减少了数据复制，还减少了上下文切换。\nsendfile(socket, file, len);\n运行流程如下：\nsendfile系统调用，文件数据被copy至内核缓冲区 再从内核缓冲区copy至内核中socket相关的缓冲区 最后再socket相关的缓冲区copy到协议引擎 相较传统read/write方式，2.1版本内核引进的sendfile已经减少了内核缓冲区到user缓冲区，再由user缓冲区到socket相关缓冲区的文件copy，而在内核版本2.4之后，文件描述符结果被改变，sendfile实现了更简单的方式，再次减少了一次copy操作。\n在apache，nginx，lighttpd等web服务器当中，都有一项sendfile相关的配置，使用sendfile可以大幅提升文件传输性能。\nKafka把所有的消息都存放在一个一个的文件中，当消费者需要数据的时候Kafka直接把文件发送给消费者，配合mmap作为文件读写方式，直接把它传给sendfile。\nKafka 的索引文件使用的是 mmap + write 方式，数据文件使用的是 sendfile 方式。\nJava NIO - 零拷贝实现 | Java 全栈知识体系 (pdai.tech) 5.4.2.2 批量压缩 在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，对于需要在广域网上的数据中心之间发送消息的数据流水线尤其如此。进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑。\n如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩 Kafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩 Kafka支持多种压缩协议，包括Gzip和Snappy压缩协议 来自 :https://www.cnblogs.com/binyue/p/10308754.html\n6.Kafka集群搭建 1、软件环境 1、linux一台或多台，大于等于2\n2、已经搭建好的zookeeper集群\n3、软件版本kafka_2.11-0.10.1.0.tgz\n2、创建目录并下载安装软件 tar -zxvf kafka_2.11-0.10.1.0.tgz\n3、修改配置文件 进入到config目录主要关注：server.properties 这个文件即可，\n修改配置文件：\n#每台服务器的broker.id都不能相同 broker.id=0 #hostname host.name=192.168.7.100 #在log.retention.hours=168 下面新增下面三项 message.max.byte=5242880 default.replication.factor=2 replica.fetch.max.bytes=5242880 #设置zookeeper的连接端口 zookeeper.connect=192.168.7.100:2181,192.168.7.101:2181,192.168.7.107:2181 # 消息存放的目录 ,记得创建文件夹 log.dirs=/usr/software/kafka_2.11-0.10.2.1/logs 下面是参数的解释 broker.id=0 #当前机器在集群中的唯一标识，和zookeeper的myid性质一样 port=19092 #当前kafka对外提供服务的端口默认是9092 host.name=192.168.7.100 #这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。 num.network.threads=3 #这个是borker进行网络处理的线程数 num.io.threads=8 #这个是borker进行I/O处理的线程数 log.dirs=/opt/kafka/kafkalogs/ #消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个 socket.send.buffer.bytes=102400 #发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能 socket.receive.buffer.bytes=102400 #kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘 socket.request.max.bytes=104857600 #这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小 num.partitions=1 #默认的分区数，一个topic默认1个分区数 log.retention.hours=168 #默认消息的最大持久化时间，168小时，7天 message.max.byte=5242880 #消息保存的最大值5M default.replication.factor=2 #kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务 replica.fetch.max.bytes=5242880 #取消息的最大直接数 log.segment.bytes=1073741824 #这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件 log.retention.check.interval.ms=300000 #每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除 log.cleaner.enable=false #是否启用log压缩，一般不用启用，启用的话可以提高性能 zookeeper.connect=192.168.7.100:2181,192.168.7.101:2181,192.168.7.107:1218 #设置zookeeper的连接端口 zookeeper.properties,修改/增加日志文件路径(记得创建文件夹)\ndataDir=/usr/software/kafka_2.11-0.10.2.1/zookeeper/dataDir dataLogDir=/usr/software/kafka_2.11-0.10.2.1/zookeeper/dataLogDir 4、启动Kafka集群并测试\n# 每台都要启动,需先启动zookeeper\nnohup sh kafka-server-start.sh -daemon ../config/server.properties \u0026gt;/dev/null \u0026amp;\nkafka-server-stop.sh 脚本好像不能正确关闭kafka,需要改动命令,如下\nPIDS=$(jps -lm | grep -i \u0026lsquo;kafka.Kafka\u0026rsquo;| awk \u0026lsquo;{print $1}\u0026rsquo;)\n关闭kafka后,用jps查看好像会延时,等待一会查看\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86.html","summary":"[toc] 1、Kafka使用背景 在我们大量使用分布式数据库、分布式计算集群的时候，是否会遇到这样的一些问题： a.我们想分析下用户行为（pagevie","title":"kafka介绍及原理"},{"content":"[toc]\n注:kafka依赖zookeeper,所以启动kafka前需开启zookeeper,kafka依赖zookeeper来分发消息,并会在zookeeper中存储分区和broker的信息\n1、启动服务 #从后台启动Kafka集群（3台都需要启动）\ncd /mysoftware/kafka_2.11-0.10.1.0//bin\n#进入到kafka的bin目录\n./kafka-server-start.sh -daemon ../config/server.properties\n(每台都)启动:\nkafka-server-start.sh config/server.properties \u0026gt;/dev/null \u0026amp; 停止: kafka-server-stop.sh \u0026gt;/dev/null \u0026amp; 如果启动报错,必须先停止,才能启动\n启动后会有kafka进程\n2. 创建Topic ./kafka-topics.sh --create --zookeeper slave02:2181 --replication-factor 2 --partitions 1 --topic mystream\n解释\n\u0026ndash;replication-factor 2 #复制两份 \u0026ndash;partitions 1 #创建1个分区 \u0026ndash;topic #主题为shuaige 3. 在一台服务器上创建一个发布者(相当于生产者) #创建一个broker，发布者\n./kafka-console-producer.sh --broker-list master:9092 --topic shuaige\n4 在一台服务器上创建一个订阅者(相当于消费者) \u0026gt;--bootstrap-server 新的kafka,可以用这个参数取代--zookeeper 端口号要变成9092 old: kafka-console-consumer.sh --zookeeper localhost:2181 --topic shuaige --from-beginning new: kafka-console-consumer.sh --bootstrap-server master:9092 --topic shuaige --from-beginning 5 、查看topic ./kafka-topics.sh --list --zookeeper localhost:2181\n6 、查看topic状态 ./kafka-topics.sh --describe --zookeeper localhost:2181 --topic shuaige\n#下面是显示信息\nTopic:ssports PartitionCount:1 ReplicationFactor:2 Configs:\nTopic: shuaige Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1\n#分区为为1 复制因子为2 他的 shuaige的分区为0\n#Replicas: 0,1 复制的为0，1\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E5%91%BD%E4%BB%A4.html","summary":"[toc] 注:kafka依赖zookeeper,所以启动kafka前需开启zookeeper,kafka依赖zookeeper来分发消息,并会在zo","title":"kafka命令"},{"content":"API 网关，即API Gateway，是大型分布式系统中，为了保护内部服务而设计的一道屏障，可以提供高性能、高可用的 API托管服务，从而帮助服务的开发者便捷地对外提供服务，而不用考虑安全控制、流量控制、审计日志等问题，统一在网关层将安全认证，流量控制，审计日志，黑白名单等实现。网关的下一层，是内部服务，内部服务只需开发和关注具体业务相关的实现。网关可以提供API发布、管理、维护等主要功能。开发者只需要简单的配置操作即可把自己开发的服务发布出去，同时置于网关的保护之下。\n对于 API Gateway，常见的选型有:\n基于 Openresty 的 Kong\n基于 Go 的 Tyk\n基于 Java 的 Zuul\n这样就很清楚了,只是技术选型不一样,功能都是一样的,都是用来做网关,各有优缺点\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/kong.html","summary":"API 网关，即API Gateway，是大型分布式系统中，为了保护内部服务而设计的一道屏障，可以提供高性能、高可用的 API托管服务，从而帮助服务的","title":"Kong"},{"content":"[toc]\n介绍 LFU全称是最不经常使用算法（Least Frequently Used），LFU算法的基本思想和所有的缓存算法一样，都是基于locality假设（局部性原理）：\n如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。\nLFU是基于这种思想进行设计：一定时期内被访问次数最少的页，在将来被访问到的几率也是最小的。\n相比于LRU（Least Recently Use）算法，LFU更加注重于使用的频率。\n原理 LFU将数据和数据的访问频次保存在一个容量有限的容器中，当访问一个数据时：\n该数据在容器中，则将该数据的访问频次加1。\n该数据不在容器中，则将该数据加入到容器中，且访问频次为1。\n当数据量达到容器的限制后，会剔除掉访问频次最低的数据。下图是一个简易的LFU算法示意图。\n上图中的LRU容器是一个链表，会动态地根据访问频次调整数据在链表中的位置，方便进行数据的淘汰，可以看到，在第四步时，因为需要插入数据F，而淘汰了数据E。\nLFU实现 和LRU类似, 都可以用数组或者链表来实现, 只不过要遍历全链表, 性能都不太好, 因为数据和频率在一起,\n基于双哈希表实现 所以换为用 map 键值对来维护，用频次作为键，用当前频次对应的一条具有先后访问顺序的链表来作为值; 再一个map存 key和数据的映射\nclass LFUCache { private int capacity; // 容量限制 private int size; // 当前数据个数 private int minFreq; // 当前最小频率 private Map\u0026lt;Integer, Node\u0026gt; map; // key和数据的映射 private Map\u0026lt;Integer, LinkedHashSet\u0026lt;Node\u0026gt;\u0026gt; freqMap; // 数据频率和对应数据组成的链表 public LFUCache(int capacity) { this.capacity = capacity; this.size = 0; this.minFreq = 1; this.map = new HashMap\u0026lt;\u0026gt;(); this.freqMap = new HashMap\u0026lt;\u0026gt;(); } public int get(int key) { Node node = map.get(key); if (node == null) { return -1; } // 增加数据的访问频率 freqPlus(node); return node.value; } public void put(int key, int value) { if (capacity \u0026lt;= 0) { return; } Node node = map.get(key); if (node != null) { // 如果存在则增加该数据的访问频次 node.value = value; freqPlus(node); } else { // 淘汰数据 eliminate(); // 新增数据并放到数据频率为1的数据链表中 Node newNode = new Node(key, value); map.put(key, newNode); LinkedHashSet\u0026lt;Node\u0026gt; set = freqMap.get(1); if (set == null) { set = new LinkedHashSet\u0026lt;\u0026gt;(); freqMap.put(1, set); } set.add(newNode); minFreq = 1; size++; } } private void eliminate() { if (size \u0026lt; capacity) { return; } LinkedHashSet\u0026lt;Node\u0026gt; set = freqMap.get(minFreq); Node node = set.iterator().next(); set.remove(node); map.remove(node.key); size--; } void freqPlus(Node node) { int frequency = node.frequency; LinkedHashSet\u0026lt;Node\u0026gt; oldSet = freqMap.get(frequency); oldSet.remove(node); // 更新最小数据频率 if (minFreq == frequency \u0026amp;\u0026amp; oldSet.isEmpty()) { minFreq++; } frequency++; node.frequency++; LinkedHashSet\u0026lt;Node\u0026gt; set = freqMap.get(frequency); if (set == null) { set = new LinkedHashSet\u0026lt;\u0026gt;(); freqMap.put(frequency, set); } set.add(node); } } class Node { int key; int value; int frequency = 1; Node(int key, int value) { this.key = key; this.value = value; } } 基于两个双向链表嵌套 用双hash的方式, 结构有些复杂, 所以采用两个链表的方式, 分别叫做外层链表，内层链表 ; 此方案全是链表的增删操作，因此时间复杂度可到 O(1)。\n我们把整体看成一个由 DoubleLinkedList组成的双向链表，然后，每一个 DoubleLinkedList 对象中又是一个由 Node 组成的双向链表。像极了 HashMap 数组加链表的形式。\n外层链表是按频次大小排序(当前频次变化时,移动的内层链表的节点, 不是外层链表的节点), 并且 内层节点使用头插法, 这样可以解决缓存末端\u0026quot;抖动\u0026quot;问题,还是很快找到要移除的节点, 即:\n//频次最小，最久未访问的元素，cache满时需要删除 lastLinkedList.pre.tail.pre 完整代码\npublic class LFUCache3 { Map\u0026lt;Integer,Node\u0026gt; cache; /** * 这两个代表的是以 DoubleLinkedList 连接成的双向链表的头尾节点， * 且为哨兵节点。每个list中，又包含一个由 node 组成的一个双向链表。 * 最外层双向链表中，freq 频次较大的 list 在前面，较小的 list 在后面 */ DoubleLinkedList firstLinkedList, lastLinkedList; int capacity; int size; public LFUCache3(int capacity){ this.capacity = capacity; cache = new HashMap\u0026lt;\u0026gt;(); //初始化外层链表的头尾节点，作为哨兵节点 firstLinkedList = new DoubleLinkedList(); lastLinkedList = new DoubleLinkedList(); firstLinkedList.next = lastLinkedList; lastLinkedList.pre = firstLinkedList; } //存储具体键值对信息的node private class Node { int key; int value; int freq = 1; Node pre; Node next; DoubleLinkedList doubleLinkedList; public Node(){ } public Node(int key, int value){ this.key = key; this.value = value; } } public int get(int key){ Node node = cache.get(key); if(node == null) return -1; freqInc(node); return node.value; } public void put(int key, int value){ if(capacity == 0) return; Node node = cache.get(key); if(node != null){ node.value = value; freqInc(node); }else{ if(size == capacity){ /** * 如果满了，则需要把频次最小的，且最久未访问的节点删除 * 由于list组成的链表频次从前往后依次减小，故最小的频次list是 lastLinkedList.pre * list中的双向node链表采用的是头插法，因此最久未访问的元素是 lastLinkedList.pre.tail.pre */ //最小频次list DoubleLinkedList list = lastLinkedList.pre; //最久未访问的元素，需要删除 Node deadNode = list.tail.pre; cache.remove(deadNode.key); list.removeNode(deadNode); size--; //如果删除deadNode之后，此list中的双向链表空了，则删除此list if(list.isEmpty()){ removeDoubleLinkedList(list); } } //没有满，则新建一个node Node newNode = new Node(key, value); cache.put(key,newNode); //判断频次为1的list是否存在，不存在则新建 DoubleLinkedList list = lastLinkedList.pre; if(list.freq != 1){ DoubleLinkedList newList = new DoubleLinkedList(1); addDoubleLinkedList(newList,list); newList.addNode(newNode); }else{ list.addNode(newNode); } size++; } } //修改频次 private void freqInc(Node node){ //从当前频次的list中移除当前 node DoubleLinkedList list = node.doubleLinkedList; if(list != null){ list.removeNode(node); } //如果当前list中的双向node链表空，则删除此list if(list.isEmpty()){ removeDoubleLinkedList(list); } //当前node频次加1 node.freq++; //找到当前list前面的list，并把当前node加入进去 DoubleLinkedList preList = list.pre; //如果前面的list不存在，则新建一个，并插入到由list组成的双向链表中 //前list的频次不等于当前node频次，则说明不存在 if(preList.freq != node.freq){ DoubleLinkedList newList = new DoubleLinkedList(node.freq); addDoubleLinkedList(newList,preList); newList.addNode(node); }else{ preList.addNode(node); } } //从外层双向链表中删除当前list节点 public void removeDoubleLinkedList(DoubleLinkedList list){ list.pre.next = list.next; list.next.pre = list.pre; } //知道了它的前节点，即可把新的list节点插入到其后面 public void addDoubleLinkedList(DoubleLinkedList newList, DoubleLinkedList preList){ newList.pre = preList; newList.next = preList.next; preList.next.pre = newList; preList.next = newList; } //维护一个双向DoubleLinkedList链表 + 双向Node链表的结构 private class DoubleLinkedList { //当前list中的双向Node链表所有频次都相同 int freq; //当前list中的双向Node链表的头结点 Node head; //当前list中的双向Node链表的尾结点 Node tail; //当前list的前一个list DoubleLinkedList pre; //当前list的后一个list DoubleLinkedList next; public DoubleLinkedList(){ //初始化内部链表的头尾节点，并作为哨兵节点 head = new Node(); tail = new Node(); head.next = tail; tail.pre = head; } public DoubleLinkedList(int freq){ head = new Node(); tail = new Node(); head.next = tail; tail.pre = head; this.freq = freq; } //删除当前list中的某个node节点 public void removeNode(Node node){ node.pre.next = node.next; node.next.pre = node.pre; } //头插法将新的node插入到当前list，并在新node中记录当前list的引用 public void addNode(Node node){ node.pre = head; node.next = head.next; head.next.pre = node; head.next = node; node.doubleLinkedList = this; } //当前list中的双向node链表是否存在有效节点 public boolean isEmpty(){ //只有头尾哨兵节点，则说明为空 return head.next == tail; } } } redis 中的LFU 它和LRU规则一样, 利用在key中时间钟字段, 不过把内部时钟的24位分成两部分，前16位还代表时钟，后8位代表一个计数器。8位只能代表255，但是redis并没有采用线性上升的方式，而是通过一个复杂的公式，通过配置两个参数来调整数据的递增速度。也会和LRU一样, 存在一个淘汰池, 从淘汰池中redis会对内部时钟最小的key进行淘汰（最小表示最不频繁使用），注意这个过程也是根据策略随机选择键\n更多详见: Redis的淘汰策略LRU与LFU.md Redis的缓存淘汰策略LRU与LFU - 简书 (jianshu.com) LFU五种实现方式，从简单到复杂 - 腾讯云开发者社区-腾讯云 (tencent.com) LFU算法及其优化策略——算法篇 - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/lfu.html","summary":"[toc] 介绍 LFU全称是最不经常使用算法（Least Frequently Used），LFU算法的基本思想和所有的缓存算法一样，都是基于locality假设（局部性原","title":"LFU-最不经常使用算法"},{"content":"==写得很乱,以后知识体系完善了,再整理==\n","permalink":"https://xiaokunji.com/zh/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html","summary":"==写得很乱,以后知识体系完善了,再整理==","title":"Linux常用命令"},{"content":"[toc]\n1. 定义 在 Lock 接口出现之前，Java 程序是靠 synchronized 关键字实现锁功能的，而 Java SE 5之后，并发包中新增了 Lock 接口（以及相关实现类）用来实现锁功能，它提供了与synchronized 关键字类似的同步功能,只是在使用时需要显式地获取和释放锁。\n虽然它缺少了（通过 synchronized 块或者方法所提供的）隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性、可中断的获取锁以及超时获取锁等多种 synchronized 关键字所不具备的同步特性。\nLock也是维护了一个锁（state），和一个等待队列(AQS)，这也是Lock在底层实现的两个核心元素。AQS队列解决了线程同步的问题，volatile定义的锁状态解决保证了线程对于临界区代码访问的互斥，并且解决了各个线程对于锁状态的可见性问题。\nLock 是一个接口，它定义了锁获取和释放的基本操作\n2. 使用 一般使用Lock会使用它的实现类, java中常用的就是ReentrantLock 类\nLock lock = new ReentrantLock(); lock.lock(); try { // do some thing } finally { lock.unlock(); } 不要将获取锁的过程写在 try 块中，因为如果在获取锁（自定义锁的实现） 时发生了异常，异常抛出的同时，也会导致锁无故释放。\n关键字synchronized与wait()和notify()/notifyAll()方法相结合可以实现等待/通知模式，类ReentrantLock也可以实现同样的功能，但需要借助于Condition对象\ncondition需要获得锁后使用\nprivate Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); 一把锁可以生成多个condition, (变量各自控制各自线程的等待和唤起)\n当调用 await()方法后，当前线程会释放锁并在此等待，而其他线程调用 Condition 对象的 signal()方法，通知当前线程后，当前线程才从 await()方法返回，并且在返回前已经获取了锁\n3. AQS 抽象队列同步器 AbstractQueuedSynchronizer（以下简称同步器），是用来构建锁或者其他同步组件的基础框架，它使用了一个 int 成员变量表示同步状态，通过内置的 FIFO 队列来完成资源获取线程的排队工作.\nAQS的主要使用方式是继承它作为一个内部辅助类实现同步原语，它可以简化你的并发工具的内部实现，屏蔽同步状态管理、线程的排队、等待与唤醒等底层操作。AQS设计基于模板方法模式，开发者需要继承同步器并且重写指定的方法，将其组合在并发组件的实现中，调用同步器的模板方法，模板方法会调用使用者重写的方法。\n使用AQS能简单且高效地构造出应用广泛的同步器，比如我们提到的ReentrantLock，Semaphore，ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。\nReentrantLock: 使用了AQS的独占获取和释放,用state变量记录某个线程获取独占锁的次数,获取锁时+1，释放锁时-1，在获取时会校验线程是否可以获取锁。 Semaphore: 使用了AQS的共享获取和释放，用state变量作为计数器，只有在大于0时允许线程进入。获取锁时-1，释放锁时+1。 CountDownLatch: 使用了AQS的共享获取和释放，用state变量作为计数器，在初始化时指定。只要state还大于0，获取共享锁会因为失败而阻塞，直到计数器的值为0时，共享锁才允许获取，所有等待线程会被逐一唤醒。 当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器，只要子类实现它的几个protected方法就可以了，\nAQS内部使用了一个volatile的变量state来作为资源的标识。同时定义了几个获取和改变state的protected方法，子类可以覆盖这些方法来实现自己的逻辑：\ngetState() setState() compareAndSetState() 这三种叫做均是原子操作，其中compareAndSetState的实现依赖于Unsafe的compareAndSwapInt()方法。\n而AQS类本身实现的是一些排队和阻塞的机制，比如具体线程等待队列的维护（如获取资源失败入队/唤醒出队等）。它内部使用了一个先进先出（FIFO）的双端队列，并使用了两个指针head和tail用于标识队列的头部和尾部。其数据结构如图：\nAQS如何防止内存泄露\nAQS维护了一个FIFO队列，AQS在无竞争条件下，甚至都不会new出head和tail节点。\n线程成功获取锁时设置head节点的方法为setHead，由于头节点的thread并不重要，此时会置node的thread和prev为null，\n完了之后还会置原先head也就是线程对应node的前驱的next为null，从而实现队首元素的安全移出。\n而在取消节点时，也会令node.thread = null，在node不为tail的情况下，会使node.next = node（之所以这样也是为了isOnSyncQueue实现更加简洁）\n虽然不是很懂\u0026hellip;.\nAbstractQueuedSynchronizer源码解读 - 活在夢裡 - 博客园 (cnblogs.com) 大致思路:\nAQS内部维护一个CLH队列来管理锁。\n线程会首先尝试获取锁，如果失败，则将当前线程以及等待状态等信息包成一个Node节点加到同步队列里。 接着会不断循环尝试获取锁（条件是当前节点为head的直接后继才会尝试）,如果失败则会阻塞自己，直至被唤醒； 而当持有锁的线程释放锁时，会唤醒队列中的后继线程。 总结: 通过一个队列来让线程们排队执行,就做到了线程安全, 然后通过一个状态来做标识(例如ReentrantLock用这个状态来表示加锁次数),\n在添加队列时需要获得(自己的)锁才能添加队列, 加到队列后让当前线程阻塞(park()函数),每次释放锁时,就把队列中的下一个线程启动(unpark()函数)\n**线程中断 : **\n在一个线程正常结束之前，如果被强制终止，那么就有可能造成一些比较严重的后果，设想一下如果现在有一个线程持有同步锁，然后在没有释放锁资源的情况下被强制休眠，那么这就造成了其他线程无法访问同步代码块。因此我们可以看到在 Java 中类似 Thread#stop() 方法被标为 @Deprecated。针对上述情况，我们不能直接将线程给终止掉，但有时又必须将让线程停止运行某些代码，那么此时我们必须有一种机制让线程知道它该停止了。此时可以通过 Thread#interrupt() 给线程该线程一个标志位，让该线程自己决定该怎么办。\nJava线程中断(Interrupt)与阻塞(park)的区别 - 周二鸭 - 博客园 (cnblogs.com) Thread.sleep、synchronized、LockSupport.park的线程阻塞区别 - 知乎 (zhihu.com) 3.1 资源共享模式 资源有两种共享模式，或者说两种同步方式：\n独占模式（Exclusive）：资源是独占的，一次只能一个线程获取。如ReentrantLock。\n共享模式（Share）：同时可以被多个线程获取，具体的资源个数可以通过参数指定。如Semaphore/CountDownLatch。\n就是共享锁和独占锁\n一般情况下，子类只需要根据需求实现其中一种模式，当然也有同时实现两种模式的同步类，如ReadWriteLock。\n前面说到,AQS通过队列来控制获取线程工作, 这个队列是先进先出的双向队列, 两种模式下都用了这个队列, 通过Node类来控制, 它下面有 前驱节点,后驱节点, 下一个等待节点等等属性.\n注意：通过Node我们可以实现两个队列，一是通过prev和next实现CLH队列(线程同步队列,双向队列)，二是nextWaiter实现Condition条件上的等待线程队列(单向队列)，这个Condition主要用在ReentrantLock类中。\n3.2 AQS的主要方法源码解析 AQS的设计是基于模板方法模式的，它有一些方法必须要子类去实现的，它们主要有：\nisHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 这些方法虽然都是protected方法，但是它们并没有在AQS具体实现，而是直接抛出异常（这里不使用抽象方法的目的是：避免强迫子类中把所有的抽象方法都实现一遍，减少无用功，这样子类只需要实现自己关心的抽象方法即可\n比如 Semaphore 只需要实现 tryAcquire 方法而不用实现其余不需要用到的模版方法）：\nprotected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } 首先看一下AQS中的嵌套类Node的定义。\nstatic final class Node { /** * 用于标记一个节点在共享模式下等待 */ static final Node SHARED = new Node(); /** * 用于标记一个节点在独占模式下等待 */ static final Node EXCLUSIVE = null; /** * 等待状态：取消 */ static final int CANCELLED = 1; /** * 等待状态：通知 */ static final int SIGNAL = -1; /** * 等待状态：条件等待 */ static final int CONDITION = -2; /** * 等待状态：传播 */ static final int PROPAGATE = -3; /** * 等待状态 */ volatile int waitStatus; /** * 前驱节点 */ volatile Node prev; /** * 后继节点 */ volatile Node next; /** * 节点对应的线程 */ volatile Thread thread; /** * 等待队列中的后继节点 */ Node nextWaiter; /** * 当前节点是否处于共享模式等待 */ final boolean isShared() { return nextWaiter == SHARED; } /** * 获取前驱节点，如果为空的话抛出空指针异常 */ final Node predecessor() throws NullPointerException { Node p = prev; if (p == null) { throw new NullPointerException(); } else { return p; } } Node() { } /** * addWaiter会调用此构造函数 */ Node(Thread thread, Node mode) { this.nextWaiter = mode; this.thread = thread; } /** * Condition会用到此构造函数 */ Node(Thread thread, int waitStatus) { this.waitStatus = waitStatus; this.thread = thread; } } 这里有必要专门梳理一下节点等待状态的定义，因为AQS源码中有大量的状态判断与跃迁。\n值 描述 CANCELLED (1) 当前线程因为超时或者中断被取消。这是一个终结态，也就是状态到此为止。 SIGNAL (-1) 当前线程的后继线程被阻塞或者即将被阻塞，当前线程释放锁或者取消后需要唤醒后继线程。这个状态一般都是后继线程来设置前驱节点的。 CONDITION (-2) 当前线程在condition队列中。 PROPAGATE (-3) 用于将唤醒后继线程传递下去，这个状态的引入是为了完善和增强共享锁的唤醒机制。在一个节点成为头节点之前，是不会跃迁为此状态的 0 表示无状态。 对于分析AQS中不涉及ConditionObject部分的代码，可以认为队列中的节点状态只会是CANCELLED, SIGNAL, PROPAGATE, 0这几种情况。\n以下仅讲述独占锁的方式\n3.2.1 获取资源 获取资源的入口是acquire(int arg)方法。arg是要获取的资源的个数，在独占模式下始终为1。我们先来看看这个方法的逻辑：\npublic final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 首先调用tryAcquire(arg)尝试去获取资源。前面提到了这个方法是在子类具体实现的。\n如果获取资源失败，就通过addWaiter(Node.EXCLUSIVE)方法把这个线程插入到等待队列中。其中传入的参数代表要插入的Node是独占式的。\nprivate Node addWaiter(Node mode) { // 生成该线程对应的Node节点 Node node = new Node(Thread.currentThread(), mode); // 将Node插入队列中 Node pred = tail; if (pred != null) { node.prev = pred; // 使用CAS尝试，如果成功就返回 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 如果等待队列为空或者上述CAS失败，再自旋CAS插入 enq(node); return node; } /** * 通过循环+CAS在队列中成功插入一个节点后返回。 */ private Node enq(final Node node) { for (;;) { Node t = tail; // 初始化head和tail if (t == null) { // 不知道为什么要单独new一个节点,而不是用入参, 而且它们表示的资源共享模式也不一样???????? if (compareAndSetHead(new Node())) tail = head; } else { /* * AQS的精妙就是体现在很多细节的代码，比如需要用CAS往队尾里增加一个元素 * 此处的else分支是先在CAS的if前设置node.prev = t，而不是在CAS成功之后再设置。 * 一方面是基于CAS的双向链表插入目前没有完美的解决方案，另一方面这样子做的好处是： * 保证每时每刻tail.prev都不会是一个null值，否则如果node.prev = t * 放在下面if的里面，会导致一个瞬间tail.prev = null，这样会使得队列不完整。 * * 这里的双向队列添加尾节点其实有三步, 1. 把node的前驱节点挂在尾节点上; 2. 把tail标识改到node上; 3. 把尾节点的后驱节点连上node * 如果把node.prev = t 放到cas中,相当于先执行了第二步,就会出现短暂的队列不完整(队列tail不见了) */ node.prev = t; // CAS设置tail为node，成功后把老的tail也就是t连接到node。 if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 上面的两个函数比较好理解，就是在队列的尾部插入新的Node节点，但是需要注意的是由于AQS中会存在多个线程同时争夺资源的情况，因此肯定会出现多个线程同时插入节点的操作，在这里是通过CAS自旋的方式保证了操作的线程安全性。\nOK，现在回到最开始的aquire(int arg)方法。现在通过addWaiter方法，已经把一个Node放到等待队列尾部了。而处于等待队列的结点是从头结点一个一个去获取资源的。具体的实现我们来看看acquireQueued方法\nfinal boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; // 自旋 for (;;) { final Node p = node.predecessor(); // 如果node的前驱结点p是head，表示node是第二个结点，就可以尝试去获取资源了 if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { // 拿到资源后，将head指向该结点。 // 所以head所指的结点，就是当前获取到资源的那个结点或null。 setHead(node); p.next = null; // help GC failed = false; return interrupted; } // 如果自己可以休息了，就进入waiting状态，直到被unpark() if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) // 发生异常了,需要取消这个node获得锁 cancelAcquire(node); } } 获取锁的大概通用流程如下：\n线程会首先尝试获取锁，如果失败，则将当前线程以及等待状态等信息包成一个Node结点加到同步队列里。接着会不断循环尝试获取锁（获取锁的条件是当前结点为head的直接后继才会尝试），如果失败则会尝试阻塞自己（阻塞的条件是当前节结点的前驱结点是SIGNAL状态,所以大部分节点都处于自旋状态），阻塞后将不会执行后续代码，直至被唤醒；当持有锁的线程释放锁时，会唤醒队列中的后继线程，或者阻塞的线程被中断或者时间到了，那么阻塞的线程也会被唤醒。\n这里parkAndCheckInterrupt方法内部使用到了LockSupport.park(this)，顺便简单介绍一下park。\nLockSupport类是Java 6 引入的一个类，提供了基本的线程同步原语。LockSupport实际上是调用了Unsafe类里的函数，归结到Unsafe里，只有两个函数：\npark(boolean isAbsolute, long time)：阻塞当前线程 unpark(Thread jthread)：使给定的线程停止阻塞 所以结点进入等待队列后，是调用park使它进入阻塞状态的。只有头结点的线程是处于活跃状态的。\n当然，获取资源的方法除了acquire外，还有以下三个：\nacquireInterruptibly：申请可中断的资源（独占模式） acquireShared：申请共享模式的资源 acquireSharedInterruptibly：申请可中断的资源（共享模式） 可中断的意思是，在线程中断时可能会抛出InterruptedException\n3.2.2 释放资源 释放资源相比于获取资源来说，会简单许多。在AQS中只有一小段实现。先释放锁,成功再唤醒后继线程\n源码：\npublic final boolean release(int arg) { // 先调用具体实现类的释放规则 if (tryRelease(arg)) { Node h = head; if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } private void unparkSuccessor(Node node) { // 如果状态是负数，尝试把它设置为0 int ws = node.waitStatus; if (ws \u0026lt; 0) compareAndSetWaitStatus(node, ws, 0); // 得到头结点的后继结点head.next Node s = node.next; // 如果这个后继结点为空或者状态大于0 // 通过前面的定义我们知道，大于0只有一种可能，就是这个结点已被取消 if (s == null || s.waitStatus \u0026gt; 0) { s = null; // 等待队列中所有还有用的结点，都向前移动 for (Node t = tail; t != null \u0026amp;\u0026amp; t != node; t = t.prev) if (t.waitStatus \u0026lt;= 0) s = t; } // 如果后继结点不为空， if (s != null) LockSupport.unpark(s.thread); } 3.2.3 超时释放资源 通过调用同步器的 doAcquireNanos(int arg,long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回 true，否则，返回 false。\n超时具体实现: 先拿到截止时间, 然后在自旋里看是否到了截止时间, 如果到了就算超时\n// java.util.concurrent.locks.AbstractQueuedSynchronizer#doAcquireNanos /** * The number of nanoseconds for which it is faster to spin * rather than to use timed park. A rough estimate suffices * to improve responsiveness with very short timeouts. */ static final long spinForTimeoutThreshold = 1000L; private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (nanosTimeout \u0026lt;= 0L) return false; final long deadline = System.nanoTime() + nanosTimeout; final Node node = addWaiter(Node.EXCLUSIVE); try { for (;;) { final Node p = node.predecessor(); if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC return true; } nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout \u0026lt;= 0L) { cancelAcquire(node); return false; } if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; nanosTimeout \u0026gt; SPIN_FOR_TIMEOUT_THRESHOLD) LockSupport.parkNanos(this, nanosTimeout); if (Thread.interrupted()) throw new InterruptedException(); } } catch (Throwable t) { cancelAcquire(node); throw t; } } 如果 nanosTimeout 小于等于 spinForTimeoutThreshold（1000 纳秒）时，将不会使该线程进行超时等待，而是进入快速的自旋过程。原因在于，非常短的超时等待无法做到十分精确，如果这时再进行超时等待，相反会让 nanosTimeout 的超时从整体上表现得反而不精确。因此，在超时非常短的场景下，同步器会进入无条件的快速自旋\n3.3 ReentrantLock (重入锁) 它有几个重要的功能,\n公平锁和非公平锁 可重入 默认是非公平锁, 毕竟性能高嘛\n可重入性是用了AQS中状态标识字段,其值表示重入次数\n获取锁\nCAS操作抢占锁，抢占成功则修改锁的状态为1，将线程信息记录到锁当中，返回state=1 抢占不成功，tryAcquire获取锁资源，获取成功直接返回，获取不成功，新建一个检点插入到当前AQS队列的尾部，acquireQueued（node）表示唤醒AQS队列中的节点再次去获取锁 释放锁\n获取锁的状态值，释放锁将状态值-1 判断当前释放锁的线程和锁中保存的线程信息是否一致，不一致会抛出异常 状态只-1直到为0，锁状态值为0表示不再占用，为空闲状态 ReentrantLock详解_SunStaday的博客-CSDN博客_reentrantlock 其实它自己没什么功能, 只是实现了AQS\n3.4 ReentrantReadWriteLock (读写锁) ReentrantReadWriteLock是Lock的另一种实现方式，我们已经知道了ReentrantLock是一个排他锁，同一时间只允许一个线程访问，而ReentrantReadWriteLock允许多个读线程同时访问，但不允许写线程和读线程、写线程和写线程同时访问(称为写饥饿)。相对于排他锁，提高了并发性。在实际应用中，大部分情况下对共享数据（如缓存）的访问都是读操作远多于写操作，这时ReentrantReadWriteLock能够提供比排他锁更好的并发性和吞吐量。\n读写锁内部维护了两个锁，一个用于读操作，一个用于写操作。所有 ReadWriteLock实现都必须保证 writeLock操作的内存同步效果也要保持与相关 readLock的联系。也就是说，成功获取读锁的线程会看到写入锁之前版本所做的所有更新。\nReentrantReadWriteLock支持以下功能：\n1）支持公平和非公平的获取锁的方式；\n2）支持可重入。读线程在获取了读锁后还可以获取读锁；写线程在获取了写锁之后既可以再次获取写锁又可以获取读锁；\n3）还允许从写入锁降级为读取锁，其实现方式是：先获取写入锁，然后获取读取锁，最后释放写入锁。但是，从读取锁升级到写入锁是不允许的；\n4）读取锁和写入锁都支持锁获取期间的中断；\n5）Condition支持。仅写入锁提供了一个 Conditon 实现；读取锁不支持 Conditon ，readLock().newCondition() 会抛出 UnsupportedOperationException。\nReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock(); ReentrantReadWriteLock.WriteLock writeLock = readWriteLock.writeLock(); writeLock.lock(); // todo do some thing writeLock.unlock(); ReentrantReadWriteLock.ReadLock readLock = readWriteLock.readLock(); readLock.lock(); // todo do some thing readLock.unlock(); ReentrantReadWriteLock实现了读写锁，但它有一个小弊端，就是在“写”操作的时候，其它线程不能写也不能读。我们称这种现象为“写饥饿”\n**导致写线程饥饿的情况：**当线程 A 持有读锁读取数据时，线程 B 要获取写锁修改数据就只能到队列里排队。此时又来了线程 C 读取数据，那么线程 C 就可以获取到读锁，而要执行写操作线程 B 就要等线程 C 释放读锁。由于该场景下读操作远远大于写的操作，此时可能会有很多线程来读取数据而获取到读锁，那么要获取写锁的线程 B 就只能一直等待下去，最终导致饥饿。\n读写锁饥饿问题解决方案之StampedLock | 码农家园 (codenong.com) 14 锁接口和类 · 深入浅出Java多线程 (redspider.group) ReentrantReadWriteLock用法 - 简书 (jianshu.com) 3.5 StampedLock StampedLock类是在Java 8 才发布的,它没有实现Lock接口和ReadWriteLock接口，但它其实是实现了“读写锁”的功能，并且性能比ReentrantReadWriteLock更高。StampedLock还把读锁分为了“乐观读锁”和“悲观读锁”两种。StampedLock的底层并不是基于AQS的。\n1 优点\n没有饥饿发生 支持锁升级、 锁降级、 超时和中断 一次唤醒所有读节点 2 缺点\n无condition\n非公平\n写不可重入\n前面提到了ReentrantReadWriteLock会发生“写饥饿”的现象，但StampedLock不会。它是怎么做到的呢？它的核心思想在于，在读的时候如果发生了写，应该通过重试的方式来获取新的值，而不应该阻塞写操作。这种模式也就是典型的无锁编程思想，和CAS自旋的思想一样\n个人感觉: 因为它引入了乐观锁机制, 当加了乐观读锁时还是可以拥有写锁的, 这样就避免的写饥饿,能否避免, 还是得看使用者,如果直接使用悲观读锁应该也会有写饥饿\n官方使用案例:\npublic class Point { private final StampedLock stampedLock = new StampedLock(); private double x; private double y; public void move(double deltaX, double deltaY) { long stamp = stampedLock.writeLock(); // 获取写锁 try { x += deltaX; y += deltaY; } finally { stampedLock.unlockWrite(stamp); // 释放写锁 } } public double distanceFromOrigin() { long stamp = stampedLock.tryOptimisticRead(); // 获得一个乐观读锁 // 注意下面两行代码不是原子操作 // 假设x,y = (100,200) double currentX = x; // 此处已读取到x=100，但x,y可能被写线程修改为(300,400) double currentY = y; // 此处已读取到y，如果没有写入，读取是正确的(100,200) // 如果有写入，读取是错误的(100,400) if (!stampedLock.validate(stamp)) { // 检查乐观读锁后是否有其他写锁发生 stamp = stampedLock.readLock(); // 获取一个悲观读锁 try { currentX = x; currentY = y; } finally { stampedLock.unlockRead(stamp); // 释放悲观读锁 } } return Math.sqrt(currentX * currentX + currentY * currentY); } //悲观读锁以及读锁升级写锁的使用 void moveIfAtOrigin(double newX,double newY) { long stamp = stampedLock.readLock(); //悲观读锁 try { while (x == 0.0 \u0026amp;\u0026amp; y == 0.0) { long ws = stampedLock.tryConvertToWriteLock(stamp); //读锁转换为写锁 if (ws != 0L) { //转换成功 stamp = ws; //票据更新 x = newX; y = newY; break; } else { stampedLock.unlockRead(stamp); //转换失败释放读锁 stamp = stampedLock.writeLock(); //强制获取写锁 } } } finally { stampedLock.unlock(stamp); //释放所有锁 } } } Java并发（8）- 读写锁中的性能之王：StampedLock - knock_小新 - 博客园 (cnblogs.com) StampedLock实现原理_曲终人散-CSDN博客 StampedLock详解_张孟浩_jay的博客-CSDN博客_stampedlock 读写锁饥饿问题解决方案之StampedLock_breakout_alex的博客-CSDN博客_读写锁写饥饿 3.6 Condition 接口 Condition的作用用一句话概括就是为了实现线程的等待（await）和唤醒（signal），多线程情况下为什么需要等待唤醒机制？原因是有些线程执行到某个阶段需要等待符合某个条件才可以继续执行，\n有一个经典的场景就是在容量有限的缓冲区实现生产者消费者模型，如果缓冲区满了，这个时候生产者就不能再生产了，就要阻塞等待消费者消费，当缓冲区为空了，消费者就要阻塞等待生产者生产.\n案例如下:\npublic class MyTest { Lock lock = new ReentrantLock(); Condition condition2 = lock.newCondition(); Condition condition1 = lock.newCondition(); public static void main(String[] args) { MyTest test = new MyTest(); new Thread(()-\u0026gt;{ try { test.awaitTest(); } catch (Exception e) { e.printStackTrace(); } }).start(); new Thread(test::signalTest).start(); } public void awaitTest() throws Exception{ try { lock.lock(); System.out.println(\u0026#34;等待中\u0026#34;); condition1.await(); System.out.println(\u0026#34;解除等待\u0026#34;); } finally { lock.unlock(); } } public void signalTest(){ try { lock.lock(); condition1.signal(); System.out.println(\u0026#34;继续执行\u0026#34;); } finally { lock.unlock(); } } } 结果如下:\n等待中\n继续执行\n解除等待\n等待队列 等待队列是一个 FIFO 的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在 Condition 对象上等待的线程，Condition condition = new ReentrantLock().newCondition(); ,每次调用newCondition()就会生成一个等待队列.\n如果一个线程调用了 Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态。\n这些节点复用了同步器中节点的定义，所以同步队列和等待队列中节点类型都是同步器的静态内部类 AbstractQueuedSynchronizer.Node。\n一个 Condition 包含一个等待队列， Condition 拥有首节点（firstWaiter）和尾节点（lastWaiter）。 当前线程调用 Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列 , 如图\nCondition 拥有首尾节点的引用，而新增节点只需要将原有的尾节点 nextWaiter 指向它，并且更新尾节点即可。(这和AQS的同步队列是一样的)\n上述节点引用更新的过程并没有使用 CAS 保证，原因在于调用 await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的\n在 Object 的监视器模型上，一个对象拥有一个同步队列和等待队列，而并发包中的Lock（更确切地说是同步器）拥有一个同步队列和多个等待队列 , 如图\n等待 调用 Condition 的 await()方法（或者以 await 开头的方法），会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。当从 await()方法返回时，当前线程一定获取了 Condition 相关联的锁 , 源码如下\n// java.util.concurrent.locks.AbstractQueuedSynchronizer.ConditionObject#await() public final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); // 当前线程加入等待队列 Node node = addConditionWaiter(); // 释放同步状态，也就是释放锁 (因为这是await嘛, 释放资源的暂停,这样同步队列中后面的节点就能执行了 ) int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) { LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } if (acquireQueued(node, savedState) \u0026amp;\u0026amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } 调用该方法的线程成功获取了锁的线程，也就是同步队列中的首节点，该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态 .\n如果从队列的角度去看，当前线程加入 Condition 的等待队列，同步队列的首节点并不会直接加入等待队列，而是通过addConditionWaiter()方法把当前线程构造成一个新的节点并将其加入等待队列中。 源码如下:\nprivate Node addConditionWaiter() { Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null \u0026amp;\u0026amp; t.waitStatus != Node.CONDITION) { unlinkCancelledWaiters(); t = lastWaiter; } Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; } 总结: 要使用condition, 则必须获得锁, (当前线程获得锁了,表示在同步队列的首位了), 此时使用await方法就会把这个节点放到等待队列中(还会释放锁)\n然后等待队列中的节点排队等待被唤醒\n通知\n调用 Condition 的 signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。 Condition 的 signal()方法， 源码如下:\npublic final void signal() { if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); } 调用该方法的前置条件是当前线程必须获取了锁，可以看到 signal()方法进行了isHeldExclusively()检查，也就是当前线程必须是获取了锁的线程。\n接着获取等待队列的首节点，将其移动到同步队列并使用 LockSupport 唤醒节点中的线程。节点从等待队列移动到同步队列的过程如图\n通过调用同步器的 enq(Node node)方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，当前线程再使用 LockSupport 唤醒该节点的线程。\n被唤醒后的线程，将从 await()方法中的 while 循环中退出（isOnSyncQueue(Nodenode)方法返回 true，节点已经在同步队列中），进而调用同步器的 acquireQueued()方法加入到获取同步状态的竞争中。\n成功获取同步状态（或者说锁）之后，被唤醒的线程将从先前调用的 await()方法返回，此时该线程已经成功地获取了锁。\n总结: 要想唤醒线程, 先得获取锁(也就是说,只有等待队列中的首位才有操作权限), 唤醒后,就会把节点加入到同步队列中,\n在同步队列中等待获得锁\nAQS-Condition介绍 - 猿起缘灭 - 博客园 (cnblogs.com) AbstractQueuedSynchronizer深入理解 - 博客园 (cnblogs.com) 图解AQS的设计与实现 - Java填坑笔记 - 博客园 (cnblogs.com) AQS(AbstractQueuedSynchronizer)源码深度解析(3)—同步队列以及独占式获取锁、释放锁的原理【一万字】_刘Java-CSDN博客 《Java并发编程艺术》\n11 AQS · 深入浅出Java多线程 (redspider.group) 并发工具（锁）：深入Lock+Condition - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/lock.html","summary":"[toc] 1. 定义 在 Lock 接口出现之前，Java 程序是靠 synchronized 关键字实现锁功能的，而 Java SE 5之后，并发包中新增了 Lock 接口（以及相关实现类）用来实现锁功能，它提供了","title":"Lock"},{"content":"[toc]\n介绍 log4j2最大的特点就是异步日志，其性能的提升主要也是从异步日志中受益，我们来看看如何使用log4j2的异步日志。\nLog4j2提供了两种实现日志的方式，一个是通过AsyncAppender，一个是通过AsyncLogger，分别对应前面我们说的Appender组件和Logger组件。注意这是两种不同的实现方式，在设计和源码上都是不同的体现。\nAsyncAppender方式 AsyncAppender是通过引用别的Appender来实现的，当有日志事件到达时，会开启另外一个线程来处理它们。需要注意的是，如果在Appender的时候出现异常，对应用来说是无法感知的。 AsyncAppender应该在它引用的Appender之后配置，默认使用 java.util.concurrent.ArrayBlockingQueue实现而不需要其它外部的类库。 当使用此Appender的时候，在多线程的环境下需要注意，阻塞队列容易受到锁争用的影响，这可能会对性能产生影响。这时候，我们应该考虑使用无所的异步记录器（AsyncLogger）。\n例子:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;Configuration status=\u0026#34;warn\u0026#34; name=\u0026#34;MyApp\u0026#34; packages=\u0026#34;\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;File name=\u0026#34;MyFile\u0026#34; fileName=\u0026#34;logs/app.log\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] %m%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/File\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;MyFile\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;error\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; AsyncLogger方式 AsyncLogger才是log4j2 的重头戏，也是官方推荐的异步方式。它可以使得调用Logger.log返回的更快。你可以有两种选择：全局异步和混合异步。\n全局异步就是，所有的日志都异步的记录，在配置文件上不用做任何改动，只需要在jvm启动的时候增加一个参数；\n混合异步就是，你可以在应用中同时使用同步日志和异步日志，这使得日志的配置方式更加灵活。因为Log4j文档中也说了，虽然Log4j2提供以一套异常处理机制，可以覆盖大部分的状态，但是还是会有一小部分的特殊情况是无法完全处理的，比如我们如果是记录审计日志，那么官方就推荐使用同步日志的方式，而对于其他的一些仅仅是记录一个程序日志的地方，使用异步日志将大幅提升性能，减少对应用本身的影响。混合异步的方式需要通过修改配置文件来实现，使用AsyncLogger标记配置。\n例子:\n引入配置\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.lmax\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;disruptor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 全局异步\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!-- Don\u0026#39;t forget to set system property -Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector to make all loggers asynchronous. --\u0026gt; \u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; 加载JVM启动参数里\njava -Dog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector\n混合异步\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!-- No need to set system property \u0026#34;log4j2.contextSelector\u0026#34; to any value when using \u0026lt;asyncLogger\u0026gt; or \u0026lt;asyncRoot\u0026gt;. --\u0026gt; \u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; root logger就是同步的，但是com.foo.Bar的logger就是异步的。\nAsyncAppender 和AsyncLogger 同时使用不会效果增倍 ; 全局异步和混合异步 同时使用也不会效果增倍\n所以不建议同时使用\n总结 1、Log4j 2的异步记录日志在一定程度上提供更好的吞吐量，但是一旦队列已满，appender线程需要等待，这个时候就需要设置等待策略，AsyncAppender是依赖于消费者最序列最后的消费者，会持续等待。\n2、因为AsyncLogger是采用Disruptor，通过环形队列无阻塞队列作为缓冲，多生产者多线程的竞争是通过CAS实现，无锁化实现，可以降低极端大的日志量时候的延迟尖峰，Disruptor 可是号称一个线程里每秒处理600万订单的高性能队列\nLog4j2简介和异步日志梳理 - 简书 (jianshu.com) log4j2异步那些事(2)\u0026ndash;AsyncLogger | BryantChang的博客 log4j2异步日志解读（二）AsyncLogger - lewis09 - 博客园 (cnblogs.com) Log4j2最佳实践 - 简书 (jianshu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/log4j2%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97.html","summary":"[toc] 介绍 log4j2最大的特点就是异步日志，其性能的提升主要也是从异步日志中受益，我们来看看如何使用log4j2的异步日志。 Log4j2提供了","title":"log4j2异步日志"},{"content":"1. format: 指定输出的格式和内容\n%(levelno)s: 打印日志级别的数值\n%(levelname)s: 打印日志级别名称\n%(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]\n%(filename)s: 打印当前执行程序名\n%(funcName)s: 打印日志的当前函数\n%(lineno)d: 打印日志的当前行号\n%(asctime)s: 打印日志的时间\n%(thread)d: 打印线程ID\n%(threadName)s: 打印线程名称\n%(process)d: 打印进程ID\n%(message)s: 打印日志信息\ndatefmt: 指定时间格式，同time.strftime()\nlevel: 设置日志级别，默认为logging.WARNING\nstream: 指定将日志的输出流，可以指定输出到sys.stderr,sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略\n例如:format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s datefmt=%a, %d %b %Y %H:%M:%S\n2. logging的几种handle方式\nlogging.StreamHandler: 日志输出到流，可以是sys.stderr、sys.stdout或者文件\nlogging.FileHandler: 日志输出到文件\n日志回滚方式，实际使用时用RotatingFileHandler和TimedRotatingFileHandler\nlogging.handlers.BaseRotatingHandler\nlogging.handlers.RotatingFileHandler\nlogging.handlers.TimedRotatingFileHandler\nlogging.handlers.SocketHandler: 远程输出日志到TCP/IP sockets\nlogging.handlers.DatagramHandler: 远程输出日志到UDP sockets\nlogging.handlers.SMTPHandler: 远程输出日志到邮件地址\nlogging.handlers.SysLogHandler: 日志输出到syslog\nlogging.handlers.NTEventLogHandler: 远程输出日志到Windows NT/2000/XP的事件日志\nlogging.handlers.MemoryHandler: 日志输出到内存中的制定buffer\nlogging.handlers.HTTPHandler: 通过\u0026quot;GET\u0026quot;或\u0026quot;POST\u0026quot;远程输出到HTTP服务器\nTimedRotatingFileHandler用法:\n#日志打印格式\nlog_fmt = \u0026#39;%(asctime)s\tFile \\\u0026#34;%(filename)s\\\u0026#34;,line %(lineno)s\t%(levelname)s: %(message)s\u0026#39; formatter = logging.Formatter(log_fmt) \\#创建TimedRotatingFileHandler对象 log_file_handler = TimedRotatingFileHandler(filename=\u0026#34;ds_update\u0026#34;, when=\u0026#34;M\u0026#34;, interval=2, backupCount=2) \\#log_file_handler.suffix = \u0026#34;%Y-%m-%d_%H-%M.log\u0026#34; #log_file_handler.extMatch = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}.log$\u0026#34;) log_file_handler.setFormatter(formatter) logging.basicConfig(level=logging.INFO) log = logging.getLogger() log.addHandler(log_file_handler) Log.info(\u0026#39;test!!\u0026#39;) filename：日志文件名的prefix； when：是一个字符串，用于描述滚动周期的基本单位，字符串的值及意义如下： “S”: Seconds\n“M”: Minutes\n“H”: Hours\n“D”: Days\n“W”: Week day (0=Monday)\n“midnight”: Roll over at midnight\ninterval: 滚动周期，单位有when指定，比如：when=’D’,interval=1，表示每天产生一个日志文件； backupCount: 表示日志文件的保留个数； 来自* \u0026lt;https://blog.csdn.net/ashi198866/article/details/46725813 \u0026gt;\n关于日志level.\n共有8个级别，按照从低到高为：All \u0026lt; Trace \u0026lt; Debug \u0026lt; Info \u0026lt; Warn \u0026lt; Error \u0026lt; Fatal \u0026lt; OFF.\nAll:最低等级的，用于打开所有日志记录.\nTrace:是追踪，就是程序推进以下，你就可以写个trace输出，所以trace应该会特别多，不过没关系，我们可以设置最低日志级别不让他输出.\nDebug:指出细粒度信息事件对调试应用程序是非常有帮助的.\nInfo:消息在粗粒度级别上突出强调应用程序的运行过程.\nWarn:输出警告及warn以下级别的日志.\nError:输出错误信息日志.\nFatal:输出每个严重的错误事件将会导致应用程序的退出的日志.\nOFF:最高等级的，用于关闭所有日志记录.\n来自* \u0026lt;https://blog.csdn.net/Q176782/article/details/78288734 \u0026gt;\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/logging.html","summary":"1. format: 指定输出的格式和内容 %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s:","title":"Logging"},{"content":"[toc]\nlogstash全是插件,从input到output,幸运的是官方文档都有些,https://www.elastic.co/guide/en/logstash/6.5/index.html ,\n总感觉logstash和flume很相似,他们都有三个级别,前,中,后,但是他们还是区别,logstash更倾向于日志收集,比较轻量级,flume更倾向于链路路由,比较重量级,https://blog.csdn.net/jek123456/article/details/65658790\n1.input 能从许多地方读取信息,从beats(专门读文件(log)的)工具,elasticsearch,shell,文件,jdbc,kafka,redis,rabbitmp,tcp/udp等等\n1.1 file插件 input{ file{ path =\u0026gt; [\u0026#34;/var/log/nginx/access.log\u0026#34;, \u0026#34;/var/log/nginx/error.log\u0026#34;] #处理的文件的路径, 可以定义多个路径 exclude =\u0026gt; \u0026#34;*.zip\u0026#34; #匹配排除 sincedb_path =\u0026gt; \u0026#34;/data/\u0026#34; #sincedb数据文件的路径, 默认\u0026lt;path.data\u0026gt;/plugins/inputs/file codec =\u0026gt; \u0026#34;plain\u0026#34; #默认是plain,可通过这个参数设置编码方式 # codec =\u0026gt; multiline { # 管理多线事件,将java的异常归纳到一条数据中 # pattern =\u0026gt;“^\\s” # what =\u0026gt;“previous” # } tags =\u0026gt; [\u0026#34;nginx\u0026#34;] #添加标记 type =\u0026gt; \u0026#34;nginx\u0026#34; #添加类型 discover_interval =\u0026gt; 2 #每隔多久去查一次文件, 默认15s stat_interval =\u0026gt; 1 #每隔多久去查一次文件是否被修改过, 默认1s start_position =\u0026gt; \u0026#34;beginning\u0026#34; #从什么位置开始读取文件数据, beginning和end, 默认是结束位置end } } 原文：https://blog.csdn.net/gekkoou/article/details/809 1.2 TCP/UDP插件 input{ tcp{ port =\u0026gt; 8888 #端口 mode =\u0026gt; \u0026#34;server\u0026#34; #操作模式, server:监听客户端连接, client:连接到服务器 host =\u0026gt; \u0026#34;0.0.0.0\u0026#34; #当mode为server, 指定监听地址, 当mode为client, 指定连接地址, 默认0.0.0.0 ssl_enable =\u0026gt; false #是否启用SSL, 默认false ssl_cert =\u0026gt; \u0026#34;\u0026#34; #SSL证书路径 ssl_extra_chain_certs =\u0026gt; [] #将额外的X509证书添加到证书链中 ssl_key =\u0026gt; \u0026#34;\u0026#34; #SSL密钥路径 ssl_key_passphrase =\u0026gt; \u0026#34;nil\u0026#34; #SSL密钥密码, 默认nil ssl_verify =\u0026gt; true #核实与CA的SSL连接的另一端的身份 tcp_keep_alive =\u0026gt; false #TCP是否保持alives } } input{ udp{ buffer_size =\u0026gt; 65536 #从网络读取的最大数据包大小, 默认65536 host =\u0026gt; 0.0.0.0 #监听地址 port =\u0026gt; 8888 #端口 queue_size =\u0026gt; 2000 #在内存中保存未处理的UDP数据包的数量, 默认2000 workers =\u0026gt; 2 #处理信息包的数量, 默认2 } } 原文：https://blog.csdn.net/gekkoou/article/details/809 2.filter Filter是Logstash功能强大的主要原因，它可以对Logstash Event进行丰富的处理，比如说解析数据、删除字段、类型转换等等，常见的有如下几个：\ndate: 日志解析 grok：正则匹配解析 dissect：分割符解析 mutate：对字段做处理，比如重命名、删除、替换等 json：按照json解析字段内容到指定字段中 geoip：增加地理位置数据 ruby： 利用ruby代码来动态修改Logstash Event 2.1 date插件 从字段解析日期以用作事件的Logstash时间戳，以下配置解析名为logdate的字段以设置Logstash时间戳：\nfilter { date { match =\u0026gt; [ \u0026#34;logdate\u0026#34;, \u0026#34;MMM dd yyyy HH:mm:ss\u0026#34; ] } } 返回结果:\n{\u0026#34;logdate\u0026#34;:\u0026#34;Jan 01 2018 12:02:03\u0026#34;} { \u0026#34;@version\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;Node2\u0026#34;, \u0026#34;@timestamp\u0026#34; =\u0026gt; 2018-01-01T04:02:03.000Z, \u0026#34;logdate\u0026#34; =\u0026gt; \u0026#34;Jan 01 2018 12:02:03\u0026#34; } 说明：\nmatch：类型为数组，用于指定日期匹配的格式，可以一次指定多种日志格式\nmatch =\u0026gt; [ \u0026#34;logdate\u0026#34;, \u0026#34;MMM dd yyyy HH:mm:ss\u0026#34; ,\u0026#34;MMM d yyyy HH:mm:ss\u0026#34;,\u0026#34;ISO8601\u0026#34;] target:类型为字符串，用于指定赋值的字段名，默认是@timestamp timezone：类型为字符串，用于指定时区 关于logstash时区的问题可以参考：logstash 时间戳时区问题 2.2 grok插件 将非结构化事件数据分析到字段中。 这个工具非常适用于系统日志，Apache和其他网络服务器日志，MySQL日志，以及通常为人类而不是计算机消耗的任何日志格式。但是消耗的资源也十分巨大\nfilter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\u0026#34; } } } 测试:\n55.3.244.1 GET /index.html 15824 0.043 以下配置将消息解析为字段：\nclient: 55.3.244.1 method: GET request: /index.html bytes: 15824 duration: 0.043 Grok语法：\n%{SYNTAX:SEMANTIC} # SYNTAX为grok pattern的名称，SEMANTIC为赋值字段名称 %{NUMBER:duration}可以匹配数值类型，但是grok匹配出的内容都是字符串类型，可以通过在最后指定为int或者float来强制转换类型。 %{NUMBER:duration:float} 常见pattern可以查看：GitHub 或者logstash家目录下的：\nvendor/bundle/jruby/2.3.0/gems/logstash-patterns-core-4.1.2/patterns 自定义匹配规则：\n格式：(?\u0026lt;field_name\u0026gt;the pattern here)\npattern_definitions参数，以键值对的方式定义pattern名称和内容 pattern_dir参数，以文件的形式被读取 filter{ grok { match =\u0026gt; {\u0026#34;message\u0026#34;=\u0026gt;\u0026#34;%{SERVICE:service}\u0026#34;} pattern_definitions =\u0026gt; {\u0026#34;SERVICE\u0026#34; =\u0026gt; \u0026#34;[a-z0-9]{10,11}\u0026#34;} #patterns_dir =\u0026gt; [\u0026#34;/opt/logstash/patterns\u0026#34;, \u0026#34;/opt/logstash/extra_patterns\u0026#34;] } } tag_on_failure: 默认是_grokparsefailure,可以基于此做判断 调试： 正则表达式： https://regexr.com/ grok： http://grokdebug.herokuapp.com/ http://grok.elasticsearch.cn/ x-pack\n2.3 dissect插件 基于分隔符原理解析数据，解决grok解析时消耗过多cpu资源的问题\n使用分隔符将非结构化事件数据提取到字段中。 解剖过滤器不使用正则表达式，速度非常快。 但是，如果数据的结构因行而异，grok过滤器更合适。\ndissect的应用有一定的局限性：主要适用于每行格式相似且分隔符明确简单的场景 dissect语法比较简单，有一系列字段(field)和分隔符(delimiter)组成\n%{}字段 %{}之间是分隔符 例如，假设日志中包含以下消息：\nApr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool... 以下配置解析消息：\nfilter { dissect { mapping =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{ts} %{+ts} %{+ts} %{src} %{prog}[%{pid}]: %{msg}\u0026#34; } } } 解剖过滤器应用后，事件将被解剖到以下领域：\n{ \u0026#34;msg\u0026#34; =\u0026gt; \u0026#34;Starting system activity accounting tool...\u0026#34;, \u0026#34;@timestamp\u0026#34; =\u0026gt; 2017-04-26T19:33:39.257Z, \u0026#34;src\u0026#34; =\u0026gt; \u0026#34;localhost\u0026#34;, \u0026#34;@version\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;localhost.localdomain\u0026#34;, \u0026#34;pid\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...\u0026#34;, \u0026#34;type\u0026#34; =\u0026gt; \u0026#34;stdin\u0026#34;, \u0026#34;prog\u0026#34; =\u0026gt; \u0026#34;systemd\u0026#34;, \u0026#34;ts\u0026#34; =\u0026gt; \u0026#34;Apr 26 12:20:02\u0026#34; } 说明\nApr 26 12:20:02 %{ts} %{+ts} %{+ts} #+代表该匹配值追加到ts字段下 { \u0026#34;ts\u0026#34;:\u0026#34;Apr 26 12:20:02\u0026#34; } two three one go %{+order/2} %{+order/3} %{+order/1} %{+order/4} #/后面的数字代表拼接的次序 { \u0026#34;order\u0026#34;: \u0026#34;one two three go\u0026#34; } a=1\u0026amp;b=2 %{?key1}=%{\u0026amp;key1}\u0026amp;%{?key2}=%{\u0026amp;key2} #%{?}代表忽略匹配值，但是富裕字段名，用于后续匹配用；%{\u0026amp;}代表将匹配值赋予key1的匹配值 { \u0026#34;a\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;b\u0026#34;:\u0026#34;2\u0026#34; } #dissect可以自动处理空的匹配值 John Smith,Big Oaks,Wood Lane,Hambledown,Canterbury,CB34RY %{name},%{addr1},%{addr2},%{addr3},%{city},%{zip} Jane Doe,4321 Fifth Avenue,,,New York,87432 { \u0026#34;name\u0026#34;:\u0026#34;Jane Doe\u0026#34;, \u0026#34;addr1\u0026#34;:\u0026#34;4321 Fifth Avenue\u0026#34;, \u0026#34;addr2\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;addr3\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;city\u0026#34;:\u0026#34;New York\u0026#34;, \u0026#34;zip\u0026#34;:\u0026#34;87432\u0026#34; } #dissect分割后的字段值都是字符串，可以使用convert_datatype属性进行类型转换 filter{ dissect{ convert_datatype =\u0026gt; {age =\u0026gt; \u0026#34;int\u0026#34;} } } 2.4 mutate插件 使用最频繁的操作，可以对字段进行各种操作，比如重命名、删除、替换、更新等，主要操作如下：\nconvert #类型转换 gsub #字符串替换 split/join/merge #字符串切割、数组合并为字符串、数组合并为数组 rename #字段重命名 update/replace #字段内容更新或替换 remove_field #删除字段 convert：实现字段类型的转换，类型为hash,仅支持转换为integer、float、string和Boolean filter{ mutate{ convert =\u0026gt; {\u0026#34;age\u0026#34; =\u0026gt; \u0026#34;integer\u0026#34;} } } gsub：对字段内容进行替换，类型为数组，每3项为一个替换配置 filter { mutate { gsub =\u0026gt; [ # replace all forward slashes with underscore \u0026#34;fieldname\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;_\u0026#34;, # replace backslashes, question marks, hashes, and minuses # with a dot \u0026#34;.\u0026#34; \u0026#34;fieldname2\u0026#34;, \u0026#34;[\\?#-]\u0026#34;, \u0026#34;.\u0026#34; ] } } split: 将字符串切割为数组 filter { mutate { split =\u0026gt; { \u0026#34;fieldname\u0026#34; =\u0026gt; \u0026#34;,\u0026#34; } } } join：将数组拼接为字符串 merge：将两个数组合并为1个数组，字符串会被转为1个元素的数组进行操作 rename：字段重命名 update/replace：更新字段内容，区别在于update只在字段存在时生效，而replace在字段不存在时会执行新增字段的操作 filter { mutate { update =\u0026gt; { \u0026#34;sample\u0026#34; =\u0026gt; \u0026#34;My new message\u0026#34; } update =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;source from c:%{source_host}\u0026#34; } #%{source_host}可以引用logstash Event中的字段值 } } input { stdin{type=\u0026gt;stdin} } filter{ dissect{ mapping =\u0026gt; {\u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{a}-%{b}-%{c}\u0026#34;} } mutate{ replace =\u0026gt; {\u0026#34;d\u0026#34; =\u0026gt;\u0026#34;source from c:%{c}\u0026#34;} } } output{ stdout{codec=\u0026gt;rubydebug} } hi-hello-123 { \u0026#34;a\u0026#34; =\u0026gt; \u0026#34;hi\u0026#34;, \u0026#34;b\u0026#34; =\u0026gt; \u0026#34;hello\u0026#34;, \u0026#34;@timestamp\u0026#34; =\u0026gt; 2018-06-29T02:01:24.473Z, \u0026#34;c\u0026#34; =\u0026gt; \u0026#34;123\u0026#34;, \u0026#34;d\u0026#34; =\u0026gt; \u0026#34;source from c:123\u0026#34;, \u0026#34;@version\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;Node2\u0026#34;, \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;hi-hello-123\u0026#34;, \u0026#34;type\u0026#34; =\u0026gt; \u0026#34;stdin\u0026#34; } 2.5 json插件 将字段内容为json格式的数据进行解析\nfilter { json { source =\u0026gt; \u0026#34;message\u0026#34; #要解析的字段名 target =\u0026gt; \u0026#34;msg_json\u0026#34; #解析后的存储字段，默认和message同级别 } } 2.6 geoip插件 常用的插件，根据ip地址提供对应的地域信息，比如经纬度、城市名等，方便进行地理数据分析\nfilter { geoip { source =\u0026gt; \u0026#34;clientip\u0026#34; } } 2.7 ruby插件 最灵活的插件，可以 以ruby语言来随心所欲的修改Logstash Event对象,ruby能实现逻辑,理论上来说可以完成你想要的任何操作,反正很吊就对了(前提是你得会ruby语言)\nfilter{ ruby{ code =\u0026gt; \u0026#39;size = event.get(\u0026#34;message\u0026#34;).size; event.set(\u0026#34;message_size\u0026#34;,size)\u0026#39; } } ruby { code =\u0026gt; \u0026#34;event.set(\u0026#39;@read_timestamp\u0026#39;,event.get(\u0026#39;@timestamp\u0026#39;))\u0026#34; } 原文：https://blog.csdn.net/wfs1994/article/details/80862952\n3.output 能输出到很多地方,比如,csv,elasticsearch,shell,file,kafka,mongDB,rabbitMq,solr,更多的参见官网\n3.1 elasticsearch插件 elasticsearch{ hosts=\u0026gt;[\u0026#34;172.132.12.3:9200\u0026#34;] action=\u0026gt;\u0026#34;index\u0026#34; index=\u0026gt;\u0026#34;indextemplate-logstash\u0026#34; #document_type=\u0026gt;\u0026#34;%{@type}\u0026#34; document_id=\u0026gt;\u0026#34;ignore\u0026#34; template=\u0026gt;\u0026#34;/opt/logstash-conf/es-template.json\u0026#34; template_name=\u0026gt;\u0026#34;es-template.json\u0026#34; template_overwrite=\u0026gt;true } 来自: https://yq.aliyun.com/articles/197785 下载工具:https://motrix.app/zh-CN/\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/logstash.html","summary":"[toc] logstash全是插件,从input到output,幸运的是官方文档都有些,https://www.elastic.co/guide/e","title":"Logstash"},{"content":"[toc]\n介绍 能用注解的方式来写代码.\n比如:在编译的时候回自动生成get和set方法(不是生成代码,是生成字节码,反编译后可以看到)\n安装 windows环境\n下载lombok.jar包https://projectlombok.org/download.html\n运行Lombok.jar:\nJava -jar D:\\software\\lombok.jar D:\\software\\lombok.jar 这是windows下lombok.jar所在的位置 数秒后将弹出一框，以确认eclipse的安装路径 (无法运行就跳过2,3步骤)\n确认完eclipse的安装路径后，点击install/update按钮，即可安装完成\n安装完成之后，请确认eclipse安装路径下(与eclipse.ini 同级目录下)是否多了一个lombok.jar包，并且其 配置文件eclipse.ini中是否 添加了如下内容: -javaagent:lombok.jar -Xbootclasspath/a:lombok.jar 如果上面的答案均为true，那么恭喜你已经安装成功，否则将缺少的部分添加到相应的位置即可\n重启eclipse或myeclipse,然后clean工程\n来自 https://www.cnblogs.com/justuntil/p/7120534.html 使用 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16.20\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; @Getter @Setter @ToString public class SysUserEntity{ private String name; private String age; } 然后就可以直接使用get和set方法,tostring方法,还有其他注解,就不写出来了\n来自 https://www.cnblogs.com/qnight/p/8997493.html 注解 @NoNull\n对方法上的参数进行判null处理,如果为空抛NullPointerException异常\n@CleanUp\n自动资源管理器,能自动关闭流,这个注解会吞噬原有的异常(就是自己的代码有流异常),小心使用\n使用: @Cleanup InputStream in = new FileInputStream(args[0]);\n@Getter/@Setter\n生成get/set方法\n@ToString\n生成toString方法\n@EqualsAndHashCode\n生成equal和hash方法\n@NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor\n生成构造函数,不带参数的; 构造参数含@nonNull属性的 ; 带全部参数的\nspring 有构造方式注入,可以结合@RequiredArgsConstructor 实现\n@Date\n包含@ToString, @EqualsAndHashCode, @Getter,@RequiredArgsConstructor,@setter但不包括final属性的参数\n@Value\n用在类或者方法上,将整个类变成final\n@Build\n构造器,能挨个给属性值注参,用了注解后,类里的参数默认值为null(即使你有自己的默认值),构造器不是私有的,大家都能调用,还有其他坑,建议先不使用 使用:Officer.build().id(\u0026quot;00001\u0026quot;).name(\u0026quot;simon qi\u0026quot;)\n@Synchronized\n给某个类加 Synchronized 锁\n@Log, @Slf4j, @CommonsLog\n日志注解, 分别属于java.util.logging.Logger,org.slf4j.Logger,org.apache.commons.logging.Log\n@SneakyThrows\n翻译就是暗中抛出异常 , 当我们需要抛出异常，在当前方法上调用，不用显示的在方法名后面写 throw\n@SneakyThrows(Exception.class)\n来自: https://projectlombok.org/features/all 学习Spring Boot：（十五）使用Lombok来优雅的编码 - KronChan - 博客园 (cnblogs.com) Lombok之@XXXArgsConstructor系列注解使用_cauchy6317的博客-CSDN博客_allargsconstructor注解 扩展 使用@Data用在派生类(有继承关系的)上会警告,如下:\nGenerating equals/hashCode implementation but without a call to superclass, even though this class does not extend java.lang.Object. If this is intentional, add '@EqualsAndHashCode(callSuper=false)' to your type.\n大致意思是默认子类的equals和hashCode方法，不会包含或者考虑基类的属性。我们可以通过反编译工具查看项目target/classes目录下的User.class的hashCode方法，默认情况下属性都是使用的他自身的属性。\n解决方式一：直接在子类上声明 @EqualsAndHashCode(callSuper = true) 解决方式二[推荐]：在项目的src/main/java根目录下创建lombok.config配置文件 内容如下:\n# 该配置声明这个配置文件是一个根配置文件，他会从该配置文件所在的目录开始扫描 config.stopBubbling=true #全局配置 equalsAndHashCode 的 callSuper 属性为true，这样就不用每个类都要去写了 lombok.equalsAndHashCode.callSuper=call 来源: https://blog.csdn.net/qq_15071263/article/details/91660519#3srcmainjavalombok_15 https://blog.csdn.net/feinifi/article/details/85275280 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/lombok.html","summary":"[toc] 介绍 能用注解的方式来写代码. 比如:在编译的时候回自动生成get和set方法(不是生成代码,是生成字节码,反编译后可以看到) 安装 window","title":"lombok"},{"content":"[toc]\n前言 LRU算法全称是最少最近使用算法（Least Recently Use），广泛的应用于缓存机制中。当缓存使用的空间达到上限后，就需要从已有的数据中淘汰一部分以维持缓存的可用性，而淘汰数据的选择就是通过LRU算法完成的。\nLRU算法的基本思想是基于局部性原理的时间局部性：\n如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。\n所以顾名思义，LRU算法会选出最近最少使用的数据进行淘汰。\n原理 一般来讲，LRU将访问数据的顺序或时间和数据本身维护在一个容器当中。当访问一个数据时：\n该数据不在容器当中，则设置该数据的优先级为最高并放入容器中。该数据在容器当中，则更新该数据的优先级至最高。当数据的总量达到上限后，则移除容器中优先级最低的数据。下图是一个简单的LRU原理示意图：\n如果我们按照7 0 1 2 0 3 0 4的顺序来访问数据，且数据的总量上限为3，则如上图所示，LRU算法会依次淘汰7 1 2这三个数据。\n由图感觉, 没有体现出最少的概念, 更贴切的描述应该是: 最近使用算法,\n朴素的LRU算法 那么我们现在就按照上面的原理，实现一个朴素的LRU算法。下面有三种方案：\n基于数组 方案：为每一个数据附加一个额外的属性——时间戳，当每一次访问数据时，更新该数据的时间戳至当前时间。当数据空间已满后，则扫描整个数组，淘汰时间戳最小的数据。\n不足：维护时间戳需要耗费额外的空间，淘汰数据时需要扫描整个数组。\n基于长度有限的双向链表 方案：访问一个数据时，当数据不在链表中，则将数据插入至链表头部，如果在链表中，则将该数据移至链表头部。当数据空间已满后，则淘汰链表最末尾的数据。\n不足：插入数据或取数据时，需要扫描整个链表。\n基于双向链表和哈希表 方案：为了改进上面需要扫描链表的缺陷，配合哈希表，将数据和链表中的节点形成映射，HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1) , 而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点，如图所示。\n重点是hash表, 以hash表为维度操作\n基于LinkedHashMap实现的LRU JDK给我们提供的LinkedHashMap直接实现LRU。因为LinkedHashMap的底层即为双向链表和哈希表的组合，所以可以直接拿来使用。\npublic class LRUCache extends LinkedHashMap { private int capacity; public LRUCache(int capacity) { // true表示 访问的顺序 ， false表示插入的顺序 // 注意这里将LinkedHashMap的accessOrder设为true super(16, 0.75f, true); this.capacity = capacity; } @Override protected boolean removeEldestEntry(Map.Entry eldest) { return super.size() \u0026gt; capacity; } } 默认LinkedHashMap并不会淘汰数据，所以我们重写了它的removeEldestEntry()方法，当数据数量达到预设上限后，淘汰数据，accessOrder设为true意为按照访问的顺序排序。整个实现的代码量并不大，主要都是应用LinkedHashMap的特性。\n正因为LinkedHashMap这么好用，所以我们可以看到Dubbo的LRU缓存LRUCache也是基于它实现的。\nLRU算法优化 朴素的LRU算法已经能够满足缓存的要求了，但是还是有一些不足。当热点数据较多时，有较高的命中率，但是如果有偶发性的批量操作，会使得热点数据被非热点数据挤出容器，使得缓存受到了“污染”。所以为了消除这种影响，又衍生出了下面这些优化方法。\nLRU-K LRU-K算法是对LRU算法的改进，将原先进入缓存队列的评判标准从访问一次改为访问K次，可以说朴素的LRU算法为LRU-1。\nLRU-K算法有两个队列，一个是缓存队列，一个是数据访问历史队列。当访问一个数据时，首先先在访问历史队列中累加访问次数，当历史访问记录超过K次后，才将数据缓存至缓存队列，从而避免缓存队列被污染。同时访问历史队列中的数据可以按照LRU的规则进行淘汰。具体如下图所示：\n实现一个LRU-K缓存\n// 直接继承我们前面写好的LRUCache public class LRUKCache extends LRUCache { private int k; // 进入缓存队列的评判标准 private LRUCache historyList; // 访问数据历史记录 public LRUKCache(int cacheSize, int historyCapacity, int k) { super(cacheSize); this.k = k; this.historyList = new LRUCache(historyCapacity); } @Override public Integer get(Integer key) { // 记录数据访问次数 Integer historyCount = historyList.get(key); historyCount = historyCount == null ? 0 : historyCount; historyList.put(key, ++historyCount); return super.get(key); } @Override public Integer put(Integer key, Integer value) { if (value == null) { return null; } // 如果已经在缓存里则直接返回缓存中的数据 if (super.get(key) != null) { return super.put(key, value);; } // 如果数据历史访问次数达到上限，则加入缓存 Integer historyCount = historyList.get(key); historyCount = historyCount == null ? 0 : historyCount; if (historyCount \u0026gt;= k) { // 移除历史访问记录 historyList.remove(key); return super.put(key, value); } } } 上面只是个简单的模型，并没有加上必要的并发控制。\n一般来讲，当K的值越大，则缓存的命中率越高，但是也会使得缓存难以被淘汰。综合来说，使用LRU-2的性能最优。\nredis中的LRU算法 redis没有使用标准的LRU算法, 只是近似的LRU算法, 因为嫌LinkedList占用的空间太大了\nredis通过计算每个key的闲置时间来决定是否要选它淘汰(全局时钟 减去 当前key的访问时钟), redis会随机选几个key, 它们的闲置时间都要大于一个阈值(其实会存入一个pool, 这个阈值就是pool中最小的闲置时间), 当内存不够时, 就从这几个key中淘汰闲置时间最大的值\nRedis的缓存淘汰策略LRU与LFU - 简书 (jianshu.com) 更多可见: Redis的淘汰策略LRU与LFU.md LRU算法及其优化策略——算法篇 - 掘金 (juejin.cn) 全面讲解LRU算法_Apple_Web的博客-CSDN博客_lru 算法 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/lru.html","summary":"[toc] 前言 LRU算法全称是最少最近使用算法（Least Recently Use），广泛的应用于缓存机制中。当缓存使用的空间达到上限后，就需要从已有的数据中淘汰一","title":"LRU-最少最近使用算法"},{"content":"[toc]\n1. maven介绍 我们都知道Maven本质上是一个插件框架，具有打包和jar管理的功能.\n对于打包来说,它的核心并不执行任何具体的构建任务，所有这些任务都交给插件来完成，例如编译源代码是由maven- compiler-plugin完成的。进一步说，每个任务对应了一个插件目标（goal），每个插件会有一个或者多个目标，例如maven- compiler-plugin的compile目标用来编译位于src/main/java/目录下的主源码，testCompile目标用来编译位于src/test/java/目录下的测试源码。\n用户可以通过两种方式调用Maven插件目标:\n第一种方式是将插件目标与生命周期阶段（lifecycle phase）绑定，这样用户在命令行只是输入生命周期阶段而已，例如Maven默认将maven-compiler-plugin的compile目标与 compile生命周期阶段绑定，因此命令mvn compile实际上是先定位到compile这一生命周期阶段，然后再根据绑定关系调用maven-compiler-plugin的compile目标。\n第二种方式是直接在命令行指定要执行的插件目标，例如mvn archetype:generate 就表示调用maven-archetype-plugin的generate目标，这种带冒号的调用方式与生命周期无关。\n2. 生命周期及插件 以下测试用的是maven默认的打包插件(打common包这种), 执行 mvn clean install 对应的输出\n2.1 总览生命周期 生命周期（lifecycle）由各个阶段组成，每个阶段由maven的插件plugin来执行完成。生命周期（lifecycle）主要包括clean、resources、complie、install、package、testResources、testCompile等，其中带test开头的都是用业编译测试代码或运行单元测试用例的。\n由上图可知，各个插件的执行顺序一般是：1：clean、２：resources、３：compile、４：testResources、５：testCompile、６：test、７：jar、８：install。\n在图中标记的地方每一行都是由冒号分隔的，前半部分是对应的插件，后半部分是插件的执行目标也就是插件执行产生的结果。\n现在我们来看下上面的pom文件，我们如配置了maven-compiler-plugin这个插件，其它的插件没有配置，但最后项目构建成功，说明maven内置的各种插件，如果pom中没有配置就调用默认的内置插件，如果pom中配置了就调用配置的插件。\n到此我们理解maven的构建过程或者有更多的人称是打包，就是由各种插件按照一定的顺序执行来完成项目的编译，单元测试、打包、布署的完成。各种插件的执行过程也就构成的maven的生命周期（lifecycle）。生命周期（lifecycle）各个阶段并不是独立的，可以单独执行如mvn clean，也可以一起执行如mvn clean install。而且有的mvn命令其是包括多个阶段的，如mvn compile其是包括了resources和compile两个阶段。下面分别来分析各个阶段需要的插件和输出的结果\n也就是说,每个步骤的插件包都能指定特定版本和配置其下参数\n2.2 打包插件 这个插件是把class文件、配置文件打成一个jar(war或其它格式)包。依赖包是不在jar里面的，需要建立lib目录，且jar和lib目录在同级目录。常用的打包插件有maven-jar-plugin、maven-assembly-plugin、maven-shade-plugin三种，下面分别介绍下各自己pom配置和使用特点。\n2.2.1 maven-jar-plugin 可执行jar与依赖包是分开，需建立lib目录里来存放需要的j依赖包，且需要jar和lib目录在同级目录, 这也是默认打包方式,所以打common包不写插件也能打包\n2.2.2 maven-assembly-plugin 这个插件可以把所有的依赖包打入到可执行jar包。需要在pom文件的plugin元素中引入才可以使用，功能非常强大，是maven中针对打包任务而提供的标准插件。它是Maven最强大的打包插件，它支持各种打包文件格式，包括zip、tar.gz、tar.bz2等等，通过一个打包描述文件设置（src/main/assembly.xml），它能够帮助用户选择具体打包哪些资源文件集合、依赖、模块，甚至本地仓库文件，每个项的具体打包路径用户也能自由控制。\n但是该插件有个bug会缺失spring的xds文件，导致无法运行jar，同时如果同级目录还有其它可执行jar文件依赖可能会产生冲突。\n2.2.3 maven-shade-plugin 需要在pom文件的plugin元素中引入才可以使用，它可以让用户配置Main-Class的值，然后在打包的时候将值填入/META-INF/MANIFEST.MF文件。关于项目的依赖，它很聪明地将依赖的JAR文件全部解压后，再将得到的.class文件连同当前项目的.class文件一起合并到最终的CLI包(可以直接运行的jar包)中，这样，在执行CLI JAR文件的时候，所有需要的类就都在Classpath中了。(springboot打包插件用的就是这个)\n如何选用这几个插件 如果在开发一个库，直接使用默认的maven-jar-plugin插件即可；\n如果是开发一个应用程序，可以考虑使用maven-shade-plugin进行打包生成Über jar(Über jar是将应用程序打包到单独的jar包中，该jar包包含了应用程序依赖的所有库和二进制包)\n如果打包生成了Über jar都不能满足你的需求的话，那么推荐使用maven-assembly-plugin插件来自定义打包内容。\n2.2.4 maven-war-plugin war项目默认的打包工具，默认情况下会打包项目编译生成的.class文件、资源文件以及项目依赖的所有jar包。\n2.2.4.1 jar和war 1、war是一个web模块，其中需要包括WEB-INF，是可以直接运行的WEB模块；jar一般只是包括一些class文件，在声明了Main_class之后是可以用java命令运行的。\n2、war包是做好一个web应用后，通常是网站，打成包部署到容器中；jar包通常是开发时要引用通用类，打成包便于存放管理。\n3、war是Sun提出的一种Web应用程序格式，也是许多文件的一个压缩包。这个包中的文件按一定目录结构来组织；classes目录下则包含编译好的Servlet类和Jsp或Servlet所依赖的其它类（如JavaBean）可以打包成jar放到WEB-INF下的lib目录下。\n1.我的一个springboot项目，用mvn install打包成jar，换一台有jdk的机器就直接可以用java -jar 项目名.jar的方式运行，没任何问题，为什么这里不需要tomcat也可以运行了？\n答: 通过jar运行实际上是启动了内置的tomcat,所以用的是应用的配置文件中的端口\n2.然后我打包成war放进tomcat运行，发现端口号变成tomcat默认的8080（我在server.port中设置端口8090）项目名称也必须加上了。 也就是说我在原来的机器的IDEA中运行，项目接口地址为 ip:8090/listall,打包放进另一台机器的tomcat就变成了ip:8080/项目名/listall。这又是为什么呢？\n答: 直接部署到tomcat之后，内置的tomcat就不会启用，所以相关配置就以安装的tomcat为准，与应用的配置文件就没有关系了\n2.2.5 maven-source-plugin 用来打源代码包的, 就像spring写的包,我们要下源代码来看注释,我们自己写的包有时也需要打出源码包给别人用,打完只有就会有一个-sources后缀的包\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-source-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;attach\u0026gt;true\u0026lt;/attach\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;compile\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 3. springboot打包插件 spring-boot-maven-plugin插件在Maven中提供了对Spring Boot的支持，可以帮助我们打包出可执行的jar包或者war包。其实spring-boot-maven-plugin所做的工作是在默认的maven-jar-plugin插件打包结束后，将项目依赖的jar包中的.class文件重新进行打包。\n[INFO] [INFO] --- maven-jar-plugin:2.6:jar (default-jar) @ helloworld --- [INFO] Building jar: /Users/gaozengrong/IdeaProjects/helloworld/target/helloworld-1.0-SNAPSHOT.jar [INFO] [INFO] --- spring-boot-maven-plugin:1.5.2.RELEASE:repackage (default) @ helloworld --- [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 26.357 s [INFO] Finished at: 2017-03-19T17:51:33+08:00 [INFO] Final Memory: 33M/289M [INFO] ------------------------------------------------------------------------ 可以看出，在调用maven-jar-plugin的goal:jar任务打包之后，又调用了spring-boot-maven-plugin的goal:repackage任务，这样会产生两个jar包。在helloworld这个工程里分别对应helloworld-1.0-SNAPSHOT.jar.original(maven-jar-plugin打包生成的jar包)，helloworld-1.0-SNAPSHOT.jar(spring-boot-maven-plugin重新打包生成的可执行jar包)。\n它使用的打包插件是 maven-shade-plugin\n![image-20210402184511005](C:\\Users\\gree\\AppData\\Roaming\\Typora\typora-user-images\\image-20210402184511005.png)\n4.实际操作 4.1 一个可用的打包插件配置, 插件外置且打出可发布的包 \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!--springboot自带的--\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 打包 --\u0026gt; \u0026lt;!-- 使用这个插件打包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 在1.8的环境下--\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;encoding\u0026gt;UTF-8\u0026lt;/encoding\u0026gt; \u0026lt;!--排除这些文件--\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;*.properties\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;*.xml\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 再使用这个插件打包 --\u0026gt; \u0026lt;!-- 允许打多个包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;make-zip\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;!-- 确定包名,输出路径,用assembly.xml文件 --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;finalName\u0026gt;${project.artifactId}\u0026lt;/finalName\u0026gt; \u0026lt;outputDirectory\u0026gt;target\u0026lt;/outputDirectory\u0026gt; \u0026lt;descriptors\u0026gt; \u0026lt;descriptor\u0026gt;src/assembly/assembly.xml\u0026lt;/descriptor\u0026gt; \u0026lt;/descriptors\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 再使用这个插件打包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-xmls\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;process-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;outputDirectory\u0026gt;${basedir}/target/classes\u0026lt;/outputDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;${basedir}/src/main/java\u0026lt;/directory\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*.xml\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 4.2 引用(SpringBoot的)可执行jar的方式 使用springboot插件打出来的包无法被其他应用程序引用,应该有多个可执行jar了(maven默认的可以被引用),\n比如: online-shop-activity-consumer 服务要引用 online-shop-activity-service (两个都是可运行的服务), 如果直接使用pom坐标是引不进来的\n要想被使用 , online-shop-activity-service 打包可采用如下方式\n\u0026lt;plugins\u0026gt; # 这是 springboot的打包插件,就是常说的服务包,它带了各种依赖和启动类,用这个插件打出来的包不能为其他服务包pom引用 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;true\u0026lt;/executable\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; # 如果要使用,就再打一个普通包,用于引用,改写成如下: \u0026lt;!-- 给jar包取个别名--\u0026gt; \u0026lt;finalName\u0026gt;online-shop-activity-service-1.0-SNAPSHOT-OfCommon\u0026lt;/finalName\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!--指定可运行的jar名称(也就是服务名)--\u0026gt; \u0026lt;classifier\u0026gt;1.0-SNAPSHOT\u0026lt;/classifier\u0026gt; \u0026lt;finalName\u0026gt;online-shop-activity-service\u0026lt;/finalName\u0026gt; \u0026lt;executable\u0026gt;true\u0026lt;/executable\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; 在online-shop-activity-consumer 中加入对应的坐标就可以使用了, 例如:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.gree.ecommerce\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;online-shop-activity-service\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 最后打出来的包:\n在项目目录下:\n在本地仓库下:\n使用的时候正常用,引入这个pom坐标就行了,\n其实这是用了\u0026quot;漏洞\u0026quot;, 打出来的包会发布到仓库中,平常部署运行时用的是target下的,,而且引用的是仓库的jar, 那额外生成一个给其他服务用, https://www.cnblogs.com/kingsonfu/p/11805455.html 5. maven和Nexus的关系 我们知道maven 具有打包和 jar管理功能 , 而jar包管理功能, 就是通过在Pom中指定坐标的形式将jar引入到项目中, 那这些jar包在哪呢?\n这引出了仓库的概念，maven通过仓库来统一管理各种构件。Maven的仓库分为本地仓库和远程仓库。\n当Maven根据坐标寻找构件时，它首先会查看本地仓库，如果本地仓库存在此构件，则直接使用；如果本地仓库不存在此构件，或者需要查看是否有更新的构件版本，Maven会再去远程仓库查找，发现需要的构件之后，下载到本地仓库再使用。\n而Nexus是一种远程仓库。在远程仓库中，默认的是中央仓库，中央仓库是Maven核心自带的远程仓库\n为了下载jar包的速度和包管理, 公司一般都会自己搭建Nexus系统, 所以在maven的setting.xml文件中会写自己公司的私服地址或者一些开源的私服地址, 例如阿里的私服 参考链接\n插件介绍 打包生命周期 maven-source-plugin使用 maven体系 springboot打包插件 Maven与nexus_超有韧性的猿媛的博客-CSDN博客_nexus和maven的区别 解决springboot的项目打成jar包，其他项目无法引用 springboot 打包插件spring-boot-maven-plugin打包机制及内部结构分析 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/maven%E6%89%93%E5%8C%85%E6%8F%92%E4%BB%B6.html","summary":"[toc] 1. maven介绍 我们都知道Maven本质上是一个插件框架，具有打包和jar管理的功能. 对于打包来说,它的核心并不执行任何具体的构建任务，所","title":"maven打包插件"},{"content":"配置文件,application.yml\nspring: data: mongodb: database: JHT_CPR uri: mongodb://10.10.203.16:27017/JHT_CPR 工具类:\npublic interface MongodbMapper\u0026lt;T\u0026gt; { public void save(String tableName, T entity); public void batchSave(String tableName, List\u0026lt;T\u0026gt; lists); // ... 这是接口,后面可以写很多 } 实现mongo工具类\n@Configuration @Component @Slf4j public abstract class MongodbMapperImpl\u0026lt;T\u0026gt; implements MongodbMapper\u0026lt;T\u0026gt; { @Autowired private MongoTemplate mongoTemplate; protected abstract Class\u0026lt;T\u0026gt; getEntityClass(); protected abstract String getCollectionName(); @Override public void save(String tableName, T entity) { // 获取对象中属性对应的字段及其值 Map\u0026lt;String, Object\u0026gt; map = ClassUtils.getColumnValue(entity); log.info(\u0026#34;新增数据：{}\u0026#34;, JSON.toJSONString(map)); // 数据 Document datas = new Document(); for (Map.Entry\u0026lt;String, Object\u0026gt; entry : map.entrySet()) { datas.append(entry.getKey(), entry.getValue()); } // TODO 带补充 //\tMongoCollection\u0026lt;Document\u0026gt; collection = mongoDatabase.getCollection(tableName); // //\tcollection.insertOne(datas); } /** * 分页查询数据 * @author xkj * @param pageIndex * @param pageSize * @param params 查询条件\u0026lt;字段名,字段值\u0026gt;, 字段值中 限定开头(^),限定结尾($),不写则为全匹配 * @param order 排序,如[id]、[id asc]、[id asc,name desc] * @return */ @Override public PageInfos\u0026lt;T\u0026gt; pageByProps(int pageIndex, int pageSize, Map\u0026lt;String, Object\u0026gt; params, String order) { // 创建分页模型对象 PageInfos\u0026lt;T\u0026gt; page = new PageInfos\u0026lt;T\u0026gt;(new ArrayList\u0026lt;T\u0026gt;(), 0L, 1, 10); // 查询总记录数 int count = countByCondition(createQuery(params, null)); // 封装结果数据 if (count \u0026gt; 0) { Query query = createQuery(params, order); // 设置分页信息 query.skip((pageIndex - 1) * pageSize); query.limit(pageSize); page = new PageInfos\u0026lt;T\u0026gt;(mongoTemplate.find(query, getEntityClass()), count, pageIndex, pageSize); } return page; } @Override public int countByCondition(Query query) { Long count = mongoTemplate.count(query, getEntityClass()); return count.intValue(); } /** * 创建带有where条件（只支持等值）和排序的Query对象 * * @param order 排序,如[id]、[id asc]、[id asc,name desc] * @param list\t需要模糊查询的字段 colName-R,colName-L 分别表示限定开头(^),限定结尾($),不写则为全匹配 * @param list1\t时间字段做比较(类似排序) [updateTime \u0026gt;=],[updateTime \u0026gt;], [updateTime \u0026lt;],[updateTime \u0026lt;=] * @return Query对象 */ protected Query createQuery(Map\u0026lt;String,Object\u0026gt; params, String order) { Query query = new Query(); // TODO 指定字段查 // where 条件 for (Entry\u0026lt;String, Object\u0026gt; param : params.entrySet()) { String key = param.getKey(); Object value = param.getValue(); if (StringUtils.isNoneBlank(key, value+\u0026#34;\u0026#34;)) { if (StringUtils.containsAny(value+\u0026#34;\u0026#34;, \u0026#34;^\u0026#34;, \u0026#34;$\u0026#34;)) { query.addCriteria(Criteria.where(key).regex(value+\u0026#34;\u0026#34;)); } else { query.addCriteria(Criteria.where(key).is(value)); } } } // 排序 List\u0026lt;Order\u0026gt; orderList = parseOrder(order); if (CollectionUtils.isNotEmpty(orderList)) { query.with(Sort.by(orderList)); } return query; } /** * propValue,包含匹配字符做模糊查询, 分别表示限定开头(^),限定结尾($),不写则为全匹配 * @author xkj * @param propName 字段名 * @param propValue 字段值 * @param order 排序值 * @return */ @Override public List\u0026lt;T\u0026gt; findByProp(String propName, Object propValue, String order) { Query query = new Query(); // 参数 if (StringUtils.containsAny(propValue + \u0026#34;\u0026#34;, \u0026#34;^\u0026#34;, \u0026#34;$\u0026#34;)) { query.addCriteria(Criteria.where(propName).regex(propValue + \u0026#34;\u0026#34;)); } else { query.addCriteria(Criteria.where(propName).is(propValue)); } // 排序 List\u0026lt;Order\u0026gt; orderList = parseOrder(order); if (CollectionUtils.isNotEmpty(orderList)) { // query.with(new Sort(orderList)); query.with(Sort.by(orderList)); } return mongoTemplate.find(query, getEntityClass()); } @Override public List\u0026lt;T\u0026gt; findByProps(Map\u0026lt;String, Object\u0026gt; map,String order) { Query query = createQuery(map, order); return mongoTemplate.find(query, getEntityClass()); } @Override public List\u0026lt;T\u0026gt; aggregate(Class\u0026lt;T\u0026gt; clazz,AggregationOperation... operations) { Aggregation aggregation = Aggregation.newAggregation(operations); AggregationResults\u0026lt;T\u0026gt; results = mongoTemplate.aggregate(aggregation,getCollectionName(), clazz); return results.getMappedResults(); } /** * 使用聚合分页查询 * @author xkj * @param clazz 返参类 * @param pageSize * @param pageIndex * @param order 排序, * @param operations 聚合类数组 * @return */ @Override public \u0026lt;I\u0026gt; PageInfos\u0026lt;I\u0026gt; pageByAggregate(Class\u0026lt;I\u0026gt; clazz, Integer pageSize, Integer pageIndex, String order,AggregationOperation... operations) { List\u0026lt;AggregationOperation\u0026gt; countOp = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(operations)); // count countOp.add(Aggregation.count().as(\u0026#34;id\u0026#34;)); Aggregation aggregationCount = Aggregation.newAggregation(countOp); AggregationResults\u0026lt;I\u0026gt; resultCount = mongoTemplate.aggregate(aggregationCount, getCollectionName(), clazz); long count = 0; Pattern pattern = Pattern.compile(\u0026#34;(id=)\\d\u0026#34;); Matcher matcher = pattern.matcher(resultCount.getRawResults().toString()); if (matcher.find()) { count = NumberUtils.toLong(matcher.group().substring(matcher.group().length() - 1)); } PageInfos\u0026lt;I\u0026gt; result = new PageInfos\u0026lt;\u0026gt;(null, count, pageIndex, pageSize); if (count \u0026gt; 0) { // 排序 List\u0026lt;AggregationOperation\u0026gt; queryOp = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(operations)); List\u0026lt;Order\u0026gt; orders = parseOrder(order); if (CollectionUtils.isNotEmpty(orders)) { SortOperation sortOperation = Aggregation.sort(Sort.by(orders)); queryOp.add(sortOperation); } // 分页 SkipOperation skipOperation = Aggregation.skip((long) (pageIndex - 1) * pageSize); LimitOperation limitOperation = Aggregation.limit(pageSize); queryOp.add(limitOperation); queryOp.add(skipOperation); Aggregation aggregation = Aggregation.newAggregation(queryOp); // 求值 AggregationResults\u0026lt;I\u0026gt; results = mongoTemplate.aggregate(aggregation, getCollectionName(), clazz); result = new PageInfos\u0026lt;\u0026gt;(results.getMappedResults(), count, pageIndex, pageSize); } return result; } @Override public \u0026lt;I\u0026gt; List\u0026lt;I\u0026gt; executeCommand(Class\u0026lt;I\u0026gt; clazz,String jsonCommand) { Document document = mongoTemplate.executeCommand(jsonCommand); List\u0026lt;I\u0026gt; results = ManagerUtil.copyBeans(document.get(\u0026#34;results\u0026#34;), clazz) ; return results; } } 业务类\n@Autowired private ResultRecordMapper resultMapper; /** * 分页查询服务调用记录 * @author xkj * @param input * @return */ @Override public BaseResponse queryServiceUsedRecord(SerUsedRecordInput input) { BaseResponse response = new BaseResponse(); input.calculateOffset(); Map\u0026lt;String, Object\u0026gt; params = buildSerUsedQuery(input); PageInfos\u0026lt;CallRecordDetail\u0026gt; record = resultMapper.pageByProps(input.getPageIndex(), input.getPageSize(),params, \u0026#34;createTime desc\u0026#34;); PageInfos\u0026lt;SerUsedRecordDTO\u0026gt; result = new PageInfos\u0026lt;\u0026gt;(ManagerUtil.copyBeans(record.getData(), SerUsedRecordDTO.class), record.getTotalCount(),record.getPageIndex(), record.getPageSize()); response.setRespData(result); resultCode.setSuccessResponse(response); return response; } /** * 构建 查询服务调用记录 查询条件 * @author xkj * @param input * @return */ private Map\u0026lt;String, Object\u0026gt; buildSerUsedQuery(SerUsedRecordInput input) { Map\u0026lt;String, Object\u0026gt; params = new HashMap\u0026lt;\u0026gt;(); if (StringUtils.isNotBlank(input.getName())) { params.put(ManagerConstants.MONGO_CALLER_INFO_CALLER_NAME, \u0026#34;^.*\u0026#34; + input.getName() + \u0026#34;.*$\u0026#34;);// 全模糊 } if (StringUtils.isNotBlank(input.getSerialNumber())) { params.put(ManagerConstants.MONGO_CALLER_INFO_SERIAL_NUMBER, input.getSerialNumber()); } if (input.getCallType() != null) { params.put(ManagerConstants.MONGO_CALLER_INFO_CALL_TYPE, input.getCallType()); } if (input.getIdentEffect() != null) { params.put(ManagerConstants.MONGO_CALLER_INFO_IDENT_EFFECT, input.getIdentEffect()); } if (StringUtils.isNotBlank(input.getCarNumber())) { params.put(ManagerConstants.MONGO_CALLER_INFO_FRONT_IDENT_RESULT, \u0026#34;^.*\u0026#34; + input.getCarNumber() + \u0026#34;.*$\u0026#34;);// 全模糊 params.put(ManagerConstants.MONGO_CALLER_INFO_CLOUD_IDENT_RESULT, \u0026#34;^.*\u0026#34; + input.getCarNumber() + \u0026#34;.*$\u0026#34;);// 全模糊 params.put(ManagerConstants.MONGO_CALLER_INFO_HUMAN_IDENT_RESULT, \u0026#34;^.*\u0026#34; + input.getCarNumber() + \u0026#34;.*$\u0026#34;);// 全模糊 } return params; } /** * 分页查询算法识别记录 * @author xkj * @param input * @return */ @Override public BaseResponse queryAgloIdentRecord(SerUsedRecordInput input) { BaseResponse response = new BaseResponse(); input.calculateOffset(); //\tStringBuilder commSB= new StringBuilder(); //\tcommSB.append(\u0026#34;{aggregate : \u0026#39;CPR_CALLER_INFO\u0026#39;, pipeline : \u0026#34;); //\tcommSB.append(\u0026#34;[\u0026#34;); //\tcommSB.append(\u0026#34;{\u0026#39;$unwind\u0026#39;:{\u0026#39;path\u0026#39;:\u0026#39;$algos\u0026#39;,\u0026#39;preserveNullAndEmptyArrays\u0026#39;:true}},\u0026#34;); //\tcommSB.append(\u0026#34;{\u0026#39;$match\u0026#39;:{\u0026#39;algos.algo_id\u0026#39;:1}},\u0026#34;); //\tcommSB.append(\u0026#34;{\u0026#39;$group\u0026#39;:{\u0026#39;_id\u0026#39;:{\u0026#39;algo_id\u0026#39;:\u0026#39;$algos.algo_id\u0026#39;,\u0026#39;serial_number\u0026#39;:\u0026#39;$serial_number\u0026#39;},\u0026#39;algoName\u0026#39;:{\u0026#39;$last\u0026#39;:\u0026#39;$algos.algo_name\u0026#39;},\u0026#39;identResult\u0026#39;:{\u0026#39;$last\u0026#39;:\u0026#39;$algos.ident_result\u0026#39;},\u0026#39;serialNumber\u0026#39;:{\u0026#39;$last\u0026#39;:\u0026#39;$serial_number\u0026#39;}}}\u0026#34;); //\tcommSB.append(\u0026#34;{$project:{\u0026#39;algoName\u0026#39;:\u0026#39;$algoName\u0026#39;,\u0026#39;serNumber\u0026#39;:\u0026#39;$serNumber\u0026#39;,\u0026#39;identResult\u0026#39;:\u0026#39;$identResult\u0026#39;,\u0026#39;identEffect\u0026#39;:\u0026#39;$identEffect\u0026#39;,\u0026#39;humanIdentResult\u0026#39;:\u0026#39;$humanIdentResult\u0026#39;,\u0026#39;createTime\u0026#39;:\u0026#39;$create_time\u0026#39;,\u0026#39;_id\u0026#39;:0}},\u0026#34;); //\tcommSB.append(\u0026#34;]}\u0026#34;); //\tList\u0026lt;AlgoIdentPageDTO\u0026gt; list = resultMapper.executeCommand(commSB.toString()); PageInfos\u0026lt;AlgoIdentPageDTO\u0026gt; list = resultMapper.pageByAggregate(AlgoIdentPageDTO.class, input.getPageSize(), input.getPageIndex(), \u0026#34;create_time desc\u0026#34;, buildAlgoIdentRecordOperation(input)); response.setRespData(list); resultCode.setSuccessResponse(response); return response; } private AggregationOperation[] buildAlgoIdentRecordOperation(SerUsedRecordInput input) { // 平铺数组 UnwindOperation unwindOperation = Aggregation.unwind(ManagerConstants.MONGO_CALLER_INFO_ALGOS, true); // 匹配条件 Criteria criteria = new Criteria(); if(input.getAlgoId() != null) {// 算法id criteria.and(ManagerConstants.MONGO_CALLER_INFO_ALGOS_ALGOID).is(input.getAlgoId()); } if(StringUtils.isNotBlank(input.getSerialNumber())) {// 业务流水号 criteria.and(ManagerConstants.MONGO_CALLER_INFO_SERIAL_NUMBER).is(input.getSerialNumber()); } if(StringUtils.isNotBlank(input.getCarNumber())) {// 车牌号 // 人工识别车牌 or 算法识别车牌 criteria.orOperator( Criteria.where(ManagerConstants.MONGO_CALLER_INFO_ALGOS_IDENT_RESULT) .regex(\u0026#34;^.*\u0026#34; + input.getCarNumber() + \u0026#34;.*$\u0026#34;), Criteria.where(ManagerConstants.MONGO_CALLER_INFO_HUMAN_IDENT_RESULT) .regex(\u0026#34;^.*\u0026#34; + input.getCarNumber() + \u0026#34;.*$\u0026#34;)) ;// 全模糊 } MatchOperation matchOperation = Aggregation.match(criteria); // 分组 GroupOperation groupOperation = Aggregation.group(ManagerConstants.MONGO_CALLER_INFO_ALGOS_ALGOID,ManagerConstants.MONGO_CALLER_INFO_SERIAL_NUMBER) .last(ManagerConstants.MONGO_CALLER_INFO_ALGOS_NAME).as(\u0026#34;algo_name\u0026#34;) .last(ManagerConstants.MONGO_CALLER_INFO_ALGOS_IDENT_RESULT).as(\u0026#34;ident_result\u0026#34;) .last(ManagerConstants.MONGO_CALLER_INFO_HUMAN_IDENT_RESULT).as(ManagerConstants.MONGO_CALLER_INFO_HUMAN_IDENT_RESULT) .last(ManagerConstants.MONGO_CALLER_INFO_IDENT_EFFECT).as(ManagerConstants.MONGO_CALLER_INFO_IDENT_EFFECT) .last(ManagerConstants.MONGO_CALLER_INFO_SERIAL_NUMBER).as(ManagerConstants.MONGO_CALLER_INFO_SERIAL_NUMBER) .last(\u0026#34;create_time\u0026#34;).as(\u0026#34;create_time\u0026#34;); // 过滤字段 //\tProjectionOperation projectionOperation = Aggregation.project(\u0026#34;serialNumber\u0026#34;,\u0026#34;identResult\u0026#34;).andExclude(\u0026#34;_id\u0026#34;); return ArrayUtils.toArray(unwindOperation,matchOperation,groupOperation); } mongodbMapper类\npublic interface ResultRecordMapper extends MongodbMapper\u0026lt;CallRecordDetail\u0026gt; { } mongodbMapper实现类\n@Component public class ResultRecordMapperImpl extends MongodbMapperImpl\u0026lt;CallRecordDetail\u0026gt; implements ResultRecordMapper { @Override protected Class\u0026lt;CallRecordDetail\u0026gt; getEntityClass() { return CallRecordDetail.class; } @Override protected String getCollectionName() { return ManagerConstants.MONGO_CALLER_INFO; } } 以后可能会遇到更好的工具类吧,这是借鉴主数据中心的代码\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%88%86%E6%94%AF/mongodb-springboot.html","summary":"配置文件,application.yml spring: data: mongodb: database: JHT_CPR uri: mongodb://10.10.203.16:27017/JHT_CPR 工具类: public interface MongodbMapper\u0026lt;T\u0026gt; { public void save(String tableName, T entity); public void batchSave(String tableName, List\u0026lt;T\u0026gt; lists); // ... 这是接口,后面可以写很多 } 实现mongo工","title":"MongoDB-SpringBoot"},{"content":"[toc]\n1. 什么是Morris遍历(莫里斯遍历) 普通的二叉树遍历大多需要栈在存储节点,最差的情况下需要存储整棵树,Morris遍历则是将空间复杂度降到了O(1)级别。Morris遍历用到了“线索二叉树”的概念，其实就是利用了叶子节点的左右空指针来存储某种遍历前驱节点或者后继节点。因此没有使用额外的空间。\nmorris遍历的特点: 无栈;空间复杂度为O(1);无递归(用while代替了)\n来源: https://blog.csdn.net/danmo_wuhen/article/details/104339630 https://blog.csdn.net/woshinannan741/article/details/52839946 2. Morris遍历的算法思想 假设当前节点为cur，并且开始时赋值为根节点root。\n判断cur节点是否为空\n如果不为空\n1）如果cur没有左孩子，cur向右更新，即（cur = cur.right）\n2）如果cur有左孩子，则从左子树找到最右侧节点pre\n如果pre的右孩子为空，则将右孩子指向cur。pre.right = cur 如果pre的右孩子为cur，则将其指向为空。pre.right = null。（还原树结构） cur为空时，停止遍历\n算法核心是 : 把右叶子节点指向上级(或者上面节点),这样就不用回归(达到了栈的效果),此时树就变成了一个无向环形图,把右叶子节点取消后继节点,树就恢复了,此时也到了输出条件,如此往复直到遍历整棵树\n来源: https://blog.csdn.net/danmo_wuhen/article/details/104339630 https://www.cnblogs.com/AnnieKim/archive/2013/06/15/morristraversal.html https://blog.csdn.net/woshinannan741/article/details/52839946 以后遇到了再详细看吧\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/morris%E9%81%8D%E5%8E%86%E8%8E%AB%E9%87%8C%E6%96%AF%E9%81%8D%E5%8E%86.html","summary":"[toc] 1. 什么是Morris遍历(莫里斯遍历) 普通的二叉树遍历大多需要栈在存储节点,最差的情况下需要存储整棵树,Morris遍历则是将空间复杂度降","title":"morris遍历(莫里斯遍历)"},{"content":"在application.properties 中添加配置文件\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;allowMultiQueries=true spring.datasource.username=xkj spring.datasource.password=xiaokunji spring.datasource.initialSize=50 spring.datasource.minIdle=10 spring.datasource.maxActive=100 spring.datasource.maxWait=60000 mybatis.mapper-locations=classpath*:mapper/*Mapper.xml mybatis.type-aliases-package=com.xkj.demo.entity # 打印sql查询结果值 mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl #默认插入空值 mybatis.configuration.jdbc-type-for-null=null # 支持驼峰 mybatis.configuration.map-underscore-to-camel-case=true 注:扫描实体类和Mapper文件用配置文件了,在spring中是用xml方式\n导入包:\n\u0026lt;!-- MyBatis --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis.spring.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- MySQL --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.41\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 实例化接口:\n@Autowired private UserService userService;//service接口不贴了,普通接口 方法调用:\n@RequestMapping(\u0026#34;/getUser\u0026#34;) public User getUser() { User user = userService.getUser(); logger.info(\u0026#34;用户数据:{}\u0026#34;,user); return user; } 实现类:\n@Service public class UserServiceImpl implements UserService { @Autowired public UserDao userDao; @Override public User getUser() { User user = userDao.getUser(); return user; } } 映射mapper的接口类:\n@Mapper public interface UserDao { public User getUser() ; } mapper文件:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.xkj.demo.dao.UserDao\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;getUser\u0026#34; resultType=\u0026#34;com.xkj.demo.entity.User\u0026#34;\u0026gt; select id,name,password from person \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%88%86%E6%94%AF/mybatis-springboot.html","summary":"在application.properties 中添加配置文件 spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;allowMultiQueries=true spring.datasource.username=xkj spring.datasource.password=xiaokunji spring.datasource.initialSize=50 spring.datasource.minIdle=10 spring.datasource.maxActive=100 spring.datasource.maxWait=60000 mybatis.mapper-locations=classpath*:mapper/*Mapper.xml mybatis.type-aliases-package=com.xkj.demo.entity # 打印sql查询结果值 mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl #默认插入空值 mybatis.configuration.jdbc-type-for-null=null # 支持驼峰 mybatis.configuration.map-underscore-to-camel-case=true 注:扫描","title":"MyBatis-SpringBoot"},{"content":"[toc]\n1.前言 1.1 概念介绍: 1.1.1、分区 对业务透明，分区只不过把存放数据的文件分成了许多小块，例如mysql中的一张表对应三个文件.MYD,MYI,frm。\n根据一定的规则把数据文件(MYD)和索引文件（MYI）进行了分割，分区后的表呢，还是一张表。分区可以把表分到不同的硬盘上，但不能分配到不同服务器上。\n优点：数据不存在多个副本，不必进行数据复制，性能更高。 缺点：分区策略必须经过充分考虑，避免多个分区之间的数据存在关联关系，每个分区都是单点，如果某个分区宕机，就会影响到系统的使用。 1.1.2、分片 对业务透明，在物理实现上分成多个服务器，不同的分片在不同服务器上。如HDFS。\n1.1.3、分表 同库分表：所有的分表都在一个数据库中，由于数据库中表名不能重复，因此需要把数据表名起成不同的名字。\n优点：由于都在一个数据库中，公共表，不必进行复制，处理更简单。 缺点：由于还在一个数据库中，CPU、内存、文件IO、网络IO等瓶颈还是无法解决，只能降低单表中的数据记录数。表名不一致，会导后续的处理复杂（参照mysql meage存储引擎来处理） 不同库分表：由于分表在不同的数据库中，这个时候就可以使用同样的表名。\n优点：CPU、内存、文件IO、网络IO等瓶颈可以得到有效解决，表名相同，处理起来相对简单。 缺点：公共表由于在所有的分表都要使用，因此要进行复制、同步。一些聚合的操作，join,group by,order等难以顺利进行。 1.1.4、分库 分表和分区都是基于同一个数据库里的数据分离技巧，对数据库性能有一定提升，但是随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。\n当业务系统的数据容量接近或超过单台服务器的容量、QPS/TPS接近或超过单个数据库实例的处理极限等。此时，往往是采用垂直和水平结合的数据拆分方法，把数据服务和数据存储分布到多台数据库服务器上。\n分库只是一个通俗说法，更标准名称是数据分片，采用类似分布式数据库理论指导的方法实现，对应用程序达到数据服务的全透明和数据存储的全透明\n来自: https://www.cnblogs.com/ijavanese/p/9512369.html 2.介绍 2.1 什么是MyCat？ 简单的说，MyCAT就是：\n一个新颖的数据库中间件产品； 一个彻底开源的、面向企业应用开发的“大数据库集群”； 支持事务、ACID、可以替代MySQL的加强版数据库； 一个可以视为“MySQL”集群的企业级数据库，用来替代昂贵的Oracle集群； 一个融合内存缓存技术、Nosql技术、HDFS大数据的新型SQL Server； 结合传统数据库和新型分布式数据仓库的新一代企业级数据库产品。 2.2 MyCat的目标 MyCAT的目标是：低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。\n2.3 MyCat的关键特性 ​ 支持 SQL 92标准\n支持Mysql集群，可以作为Proxy使用\n支持JDBC连接ORACLE、DB2、SQL Server，将其模拟为MySQL Server使用\n支持NoSQL数据库\n支持galera for mysql集群，percona-cluster或者mariadb cluster，提供高可用性数据分片集群\n自动故障切换，高可用性\n支持读写分离，支持Mysql双主多从，以及一主多从的模式\n支持全局表，数据自动分片到多个节点，用于高效表关联查询\n支持独有的基于E-R 关系的分片策略，实现了高效的表关联查询\n支持一致性Hash分片，有效解决分片扩容难题\n多平台支持，部署和实施简单\n支持Catelet开发，类似数据库存储过程，用于跨分片复杂SQL的人工智能编码实现，143行Demo完成跨分片的两个表的JION查询。\n支持NIO与AIO两种网络通信机制，Windows下建议AIO，Linux下目前建议NIO\n支持Mysql存储过程调用\n以插件方式支持SQL拦截和改写\n支持自增长主键、支持Oracle的Sequence机制\n2.4 总体架构 MyCAT的架构如下图所示：\nMyCAT使用MySQL的通讯协议模拟成一个MySQL服务器，并建立了完整的Schema（数据库）、Table （数据表）、User（用户）的逻辑模型，并将这套逻辑模型映射到后端的存储节点DataNode（MySQL Instance）上的真实物理库中，这样一来，所有能使用MySQL的客户端以及编程语言都能将MyCAT当成是MySQLServer来使用，不必开发新的客户端协议。\n当MyCAT收到一个客户端发送的SQL请求时，会先对SQL进行语法分析和检查，分析的结果用于SQL路由，SQL路由策略支持传统的基于表格的分片字段方式进行分片，也支持独有的基于数据库E-R关系的分片策略，对于路由到多个数据节点（DataNode）的SQL，则会对收到的数据集进行“归并”然后输出到客户端。\nSQL执行的过程，简单的说，就是把SQL通过网络协议发送给后端的真正的数据库上进行执行，对于MySQL Server来说，是通过MySQL网络协议发送报文，并解析返回的结果，若SQL不涉及到多个分片节点，则直接返回结果，写入客户端的SOCKET流中，这个过程是非阻塞模式（NIO）。\nDataNode是MyCAT的逻辑数据节点，映射到后端的某一个物理数据库的一个Database，为了做到系统高可用，每个DataNode可以配置多个引用地址（DataSource），当主DataSource被检测为不可用时，系统会自动切换到下一个可用的DataSource上，这里的DataSource即可认为是Mysql的主从服务器的地址。\n2.5 术语 与任何一个传统的关系型数据库一样，MyCAT也提供了“数据库”的定义，并有用户授权的功能，下面是MyCAT逻辑库相关的一些概念：\nschema:逻辑库，与MySQL中的Database（数据库）对应，一个逻辑库中定义了所包括的Table。 table：表，即物理数据库中存储的某一张表，与传统数据库不同，这里的表格需要声明其所存储的逻辑数据节点DataNode，这是通过表格的分片规则定义来实现的，table可以定义其所属的“子表(childTable)”，子表的分片依赖于与“父表”的具体分片地址，简单的说，就是属于父表里某一条记录A的子表的所有记录都与A存储在同一个分片上。 分片规则：是一个字段与函数的捆绑定义，根据这个字段的取值来返回所在存储的分片（DataNode）的序号，每个表格可以定义一个分片规则，分片规则可以灵活扩展，默认提供了基于数字的分片规则，字符串的分片规则等。 dataNode: MyCAT的逻辑数据节点，是存放table的具体物理节点，也称之为分片节点，通过DataSource来关联到后端某个具体数据库上，一般来说，为了高可用性，每个DataNode都设置两个DataSource，一主一从，当主节点宕机，系统自动切换到从节点。 dataHost：定义某个物理库的访问地址，用于捆绑到dataNode上。 MyCAT目前通过配置文件的方式来定义逻辑库和相关配置：\nMYCAT_HOME/conf/schema.xml中定义逻辑库，表、分片节点等内容； MYCAT_HOME/conf/rule.xml中定义分片规则； MYCAT_HOME/conf/server.xml中定义用户以及系统相关变量，如端口等。下图给出了MyCAT 一个可能的逻辑库到物理库（MySQL的完整映射关系），可以看出其强大的分片能力以及灵活的Mysql集群整合能力。 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mycat.html","summary":"[toc] 1.前言 1.1 概念介绍: 1.1.1、分区 对业务透明，分区只不过把存放数据的文件分成了许多小块，例如mysql中的一张表对应三个文件.MYD,M","title":"MyCat"},{"content":"在使用上的差别:\nmysql中 where 条件允许 1,0,true,false 条件,而Oracle只允许true/false,\n例如 : select * from person where 1 ,相当于where 1=1 ,是个恒等式\n事项 MySQL Oracle 注释 例子 where 支持1,0,true,false 支持true/false mysql : select * from person where 1Oracle: select * from person where 1=1 as 表和字段都能用,也可以不用 仅字段能用 都支持不写as,直接写别名就行 当前时间 SYSDATE() SYSDATE Oracle:SELECT SYSDATE FROM JBP_ACCOUNT 判空 IFNULL nvl oracle: nvl( LOCK_FLAG,\u0026rsquo;\u0026rsquo;) as lockFlag, 连接字符串 concat(str1.str2,str3,\u0026hellip;..) 不定参数 concat(str1,str2) 仅两个参数, 可以用|| 任意连接字符串 eg. \u0026lsquo;3\u0026rsquo;|| id || \u0026lsquo;4\u0026rsquo; 时间格式化 DATE_FORMAT(#{time} , %Y-%m-%d %H:%i:%s) TO_CHAR(yyyy-MM-dd hh24:mi:ss) 查找字符串 find_in_set(a,b) 可用替代 instr find_in_set查找a在b中的位置,b是又逗号隔开的字符串,用到了比特运算,速度很快 分组并合并 GROUP_CONCAT wm_concat ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%B7%AE%E5%BC%82/mysql%E4%B8%8Eoracle.html","summary":"在使用上的差别: mysql中 where 条件允许 1,0,true,false 条件,而Oracle只允许true/false, 例如 : select * from person where 1 ,相当于where 1=1 ,是个恒等式 事","title":"MySQL与Oracle"},{"content":"[toc]\n前言 阿里开源的注册中心和配置中心\nNacos 官方文档 Nacos 源码 1. 概念 Name Space\n用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置。Namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。\nConfiguration\n配置文件\nData ID\nNacos 中的某个配置集的 ID。配置集 ID 是组织划分配置的维度之一。Data ID 通常用于组织划分系统的配置集。一个系统或者应用可以包含多个配置集，每个配置集都可以被一个有意义的名称标识。Data ID 通常采用类 Java 包（如 com.taobao.tc.refund.log.level）的命名规则保证全局唯一性。此命名规则非强制。\nGroup\nNacos 中的一组配置集，是组织配置的维度之一。通过一个有意义的字符串（如 Buy 或 Trade ）对配置集进行分组，从而区分 Data ID 相同的配置集。当您在 Nacos 上创建一个配置时，如果未填写配置分组的名称，则配置分组的名称默认采用 DEFAULT_GROUP 。配置分组的常见场景：不同的应用或组件使用了相同的配置类型，如 database_url 配置和 MQ_topic 配置。\nVirtual Cluster\n同一个服务下的所有服务实例组成一个默认集群, 集群可以被进一步按需求划分，划分的单位可以是虚拟集群。\n如图: Nacos 概念 2. 架构 3. 安装 Nacos Server下载地址：\nhttps://github.com/alibaba/nacos/releases 下载解压后打开bin目录\n/work/nacos/bin/startup.sh -m standalone\nwindows: startup.cmd -m standalone\n启动完后访问后台\nNacos Server的后台访问地址：\nhttp://192.168.10.13:8848/nacos/index.html\n默认账号和密码为：nacos/nacos\n3.1 Nacos Server 有两种运行模式： standalone cluster (默认模式) 3.1.1 standalone 模式 此模式一般用于 demo 和测试，不用改任何配置，直接敲以下命令执行\n3.1.2 cluster 模式 cluster 模式需要依赖 MySQL，然后改两个配置文件：\nconf/cluster.conf conf/application.properties conf/cluster.conf\n# 集群节点(此处是在同一台机器上,所以用ip区分) 192.168.10.13:8841 192.168.10.13:8845 192.168.10.13:8848 conf/application.properties\nspring.datasource.platform=mysql db.num=1 db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8\u0026amp;connectTimeout=1000\u0026amp;socketTimeout=3000\u0026amp;autoReconnect=true\u0026amp;useUnicode=true\u0026amp;useSSL=false\u0026amp;serverTimezone=UTC\u0026amp;AllowPublicKeyRetrieval=True db.user.0=test db.password.0=xiaokunji 每个节点上需要改这两个文件\n注意在application.properties文件中修改端口\n启动后 任意节点都能进入管理界面\n阿里巴巴NACOS（3）- 部署Nacos的生产集群环境-阿里云开发者社区 (aliyun.com) 一般集群部署需要搭配NGINX, 这样就可以统一管理对外ip, 且需要利用NGINX的故障转移策略, nacos本身不具备故障转移\nNacos 集群部署 - CanntBelieve - 博客园 (cnblogs.com) 4. 注册中心 参数解释\n参数 描述 com.alibaba.nacos.naming.log.level Naming客户端的日志级别，改属性通过客户端启动时通过命令行加参数指定 注：默认为info spring.cloud.nacos.discovery.heart-beat-interval nacos客户端向服务端发送心跳的时间间隔，默认5s\n注：客户端向服务端每隔5s向服务端发送心跳请求，进行服务续租，告诉服务端该实例IP健康。若在3次心跳的间隔时间(默认15s)内服务端没有接受到该实例的心跳请求，则认为该实例不健康，该实例将无法被消费。如果再次经历3次心跳的间隔时间，服务端接受到该实例的请求，那么会立刻将其设置外健康，并可以被消费，若未接受到，则删除该实例的注册信息。推荐配置为5s，如果有的业务线希望服务下线或者出故障时希望尽快被发现，可以适当减少该值。 spring.cloud.nacos.discovery.heart-beat-timeout: 服务端没有接受到客户端心跳请求就将其设为不健康的时间间隔，默认为15s. 注：推荐值该值为15s即可，如果有的业务线希望服务下线或者出故障时希望尽快被发现，可以适当减少该值。 spring.cloud.nacos.discovery.log-name: nacos客户端会在启动时打印一部分发送注册请求信息和异常日志，可以通过日志查看注册的nacos集群地址、服务名、nameSpace、IP、元数据等内容，文件名默认为naming.log . 注:推荐将该日志的位置设置为和其他日志在一个文件夹下 spring.cloud.nacos.discovery.metadata: 给服务添加一些标签，例如属于什么业务线，该元数据会持久化存储在服务端，但是客户端消费时不会获取到此值，默认为空. 注:推荐为空，我们可以通过已经注册的服务名来找到具体的业务线，无需添加metadata spring.cloud.nacos.discovery.namespace: 命名空间ID，Nacos通过不同的命名空间来区分不同的环境，进行数据隔离，服务消费时只能消费到对应命名空间下的服务。 spring.cloud.nacos.discovery.naming-load-cache-at-start: 默认为false。客户端在启动时是否读取本地配置项(一个文件)来获取服务列表. 注：推荐该值为false，若改成true。则客户端会在本地的一个文件中保存服务信息，当下次宕机启动时，会优先读取本地的配置对外提供服务。 spring.cloud.nacos.discovery.port: 向nacos注册服务时，服务对应的端口号 . 注:无需修改，默认为应用对外提供服务的端口号,server.port spring.cloud.nacos.discovery.register-enabled: 该项目是否向注册中心注册服务，默认为true . 注：如果服务从注册中心只消费服务，没有对外提供服务，那么该值可设置为false，可减少客户端线程池的创建，无需向服务端发送心跳请求，提高性能。 spring.cloud.nacos.discovery.server-addr nacos集群地址。注：多个IP可以通过“，”号隔离，例如192.168.80.1:8848,192.168.80.1:8848 填写域名时前缀不要加上http:// spring.cloud.nacos.discovery.service: 项目向注册中心注册服务时的服务名，默认为spring.application.name 变量 . 注:该服务名必须使用小写，因为nacos服务名区分大小写，如果服务名不完全匹配，那么无法调用服务 spring.cloud.nacos.discovery.watch-delay: 默认为30s。客户端在启动时会创建一个线程池，该线程定期去查询服务端的信息列表，该请求不会立刻返回，默认等待30s，若在30s内，服务端信息列表发生变化，则该请求立刻返回，通知客户端拉取服务端的服务信息列表，若30s内，没有变化，则30s时该请求返回响应，客户端服务列表不变，再次发生该请求。 注：推荐该值为30s即可，无需修改 spring.cloud.nacos.discovery.watch.enabled: 默认为true,客户端在启动时会创建一个线程池，该线程定期去查询服务端的信息列表，该请求不会立刻返回，默认等待30s，若在30s内，服务端信息列表发生变化，则该请求立刻返回，通知客户端拉取服务端的服务信息列表，若30s内，没有变化，则30s时该请求返回响应，客户端服务列表不变，再次发生该请求。 注:推荐该功能为true，这是nacos类似长连接推送服务变化的功能，不要关闭 spring.cloud.nacos.discovery.weight: nacos支持服务端基于权重的负载均衡，该值默认为1 . 注:建议该值保持默认即可，因为代码可能会部署到不同的服务器上，无法确保某台服务器的配置一定较好，如果有需要修改该值的需求，可以上控制台修改，这样可以保证对应IP服务器的权重值较高 watch-delay 有什么作用?\n4.1 基本使用 pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 使用配置文件如下:\nspring: cloud: nacos: -- 配置中心 config: namespace: xkjNamespace server-addr: 192.168.2.101:8841,192.168.2.101:8845,192.168.2.101:8848 group: xkjGroup -- 注册中心 discovery: server-addr: ${spring.cloud.nacos.config.server-addr} cluster-name: xkjCluster -- 此处是命名空间的id namespace: xkjNamespace service: javaDemoDiscovery group: xkjGroup application: name: javaDemo 4.2 在不写本机地址时,nacos如何发现ip的 与 Eureka使用了同一个工具类, 有spring-cloud-common提供的 InetUtils 类, 因此两者获取ip是同样的原理,详见 [Eureka文章](https://github.com/xiaokunji/myNotes/blob/main/java 及其框架/Spring Cloud/Eureka.md)\n简单来说就是遍历所有网卡, 去除一些回旋地址,忽略地址等规则取索引最小的, 默认返回本地地址\n5. 配置中心 在 Nacos Spring Cloud 中，dataId 的完整格式如下：\n${prefix}-${spring.profile.active}.${file-extension}\nprefix 默认为所属工程配置spring.application.name 的值（即：nacos-provider），也可以通过配置项 spring.cloud.nacos.config.prefix来配置； spring.profile.active 即为当前环境对应的 profile，详情可以参考 Spring Boot文档。 注意：当spring.profile.active 为空时，对应的连接符 - 也将不存在，dataId 的拼接格式变成 ${prefix}.${file-extension} file-exetension 为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension 来配置。目前只支持 properties 和 yaml 类型；默认为 properties ； Nacos 入门教程_cristianoxm的博客-CSDN博客_nacos 教程 pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-config\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置文件:\n新增bootstrap.yml文件，配置信息写在该文件里。（问题：如放在application.yml会导致项目启动报找不到配置属性错误，原因：application.yml与bootstrap.yml加载顺序优先级问题。）\nspring: application: name: javaDemo cloud: nacos: server-addr: ${spring.cloud.client.ip-address}:8848 config: namespace: xkjNamespace file-extension: yml shared-configs: - data-id: mysqlTest.yml refresh: true group: xkjGroup extension-configs: - data-id: MyTest.yml refresh: true group: xkjGroup group: xkjGroup # 该配置如果放在share和extentsion配置的前面,将覆盖其下的组 java使用类:\n@Value(\u0026#34;${my.name:本地值name}\u0026#34;) private String name; @Value(\u0026#34;${person.name:本地值person.name}\u0026#34;) private String person_name; @Value(\u0026#34;${personAge:本地值personAge}\u0026#34;) private String personAge; @Value(\u0026#34;${personName:本地值personName}\u0026#34;) private String personName; 5.1 动态配置 5.1.1 使用 通常获取配置文件的方式\n@Value\n@ConfigurationProperties(Prefix)\n如果是在运行时要动态更新的话，\n第一种方式要在bean上加@RefreshScope 第二种方式是自动支持的。\n5.1.2 原理详解 5.1.2.1 采用延迟线程池定时执行\u0026quot;监听\u0026quot;文件是否有修改 服务启动后就回轮询的打印图上信息, 当有配置被改动时, 这个[] 就会包含数据了, 可想而知这是监听日志了, 那就找到这段代码\nClientWorker.java\nclass LongPollingRunnable implements Runnable { private int taskId; public LongPollingRunnable(int taskId) { this.taskId = taskId; } @Override public void run() { List\u0026lt;CacheData\u0026gt; cacheDatas = new ArrayList\u0026lt;CacheData\u0026gt;(); List\u0026lt;String\u0026gt; inInitializingCacheList = new ArrayList\u0026lt;String\u0026gt;(); try { // check failover config for (CacheData cacheData : cacheMap.get().values()) { if (cacheData.getTaskId() == taskId) { cacheDatas.add(cacheData); try { checkLocalConfig(cacheData); if (cacheData.isUseLocalConfigInfo()) { cacheData.checkListenerMd5(); } } catch (Exception e) { LOGGER.error(\u0026#34;get local config info error\u0026#34;, e); } } } // check server config List\u0026lt;String\u0026gt; changedGroupKeys = checkUpdateDataIds(cacheDatas, inInitializingCacheList); LOGGER.info(\u0026#34;get changedGroupKeys:\u0026#34; + changedGroupKeys); // 省略剩下代码 ..... } } } 从这里就能看出, 先是对配置做了一些检查, 然后就打印结果, 而且这个是在run方法里, 说明这里肯定是开了线程在跑的, 找到调用LongPollingRunnable这个类的地方\n还是在同一个类中, 发现是在execute中执行的, 那就是弄了一个线程池, 而且这里是在for循环里, 看一下任务, 就会联想到多个配置文件的情况, 同时监听的\npublic void checkConfigInfo() { // 分任务 int listenerSize = cacheMap.get().size(); // 向上取整为批数 int longingTaskCount = (int) Math.ceil(listenerSize / ParamUtil.getPerTaskConfigSize()); if (longingTaskCount \u0026gt; currentLongingTaskCount) { for (int i = (int) currentLongingTaskCount; i \u0026lt; longingTaskCount; i++) { // 要判断任务是否在执行 这块需要好好想想。 任务列表现在是无序的。变化过程可能有问题 executorService.execute(new LongPollingRunnable(i)); } currentLongingTaskCount = longingTaskCount; } } 看一下这个线程池的参数.\n@SuppressWarnings(\u0026#34;PMD.ThreadPoolCreationRule\u0026#34;) public ClientWorker(final HttpAgent agent, final ConfigFilterChainManager configFilterChainManager, final Properties properties) { this.agent = agent; this.configFilterChainManager = configFilterChainManager; // Initialize the timeout parameter init(properties); executor = Executors.newScheduledThreadPool(1, new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setName(\u0026#34;com.alibaba.nacos.client.Worker.\u0026#34; + agent.getName()); t.setDaemon(true); return t; } }); // 执行的线程池 executorService = Executors.newScheduledThreadPool(Runtime.getRuntime().availableProcessors(), new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setName(\u0026#34;com.alibaba.nacos.client.Worker.longPolling.\u0026#34; + agent.getName()); t.setDaemon(true); return t; } }); executor.scheduleWithFixedDelay(new Runnable() { @Override public void run() { try { checkConfigInfo(); } catch (Throwable e) { LOGGER.error(\u0026#34;[\u0026#34; + agent.getName() + \u0026#34;] [sub-check] rotate check error\u0026#34;, e); } } }, 1L, 10L, TimeUnit.MILLISECONDS); } 还会发现这里还有个延迟线程池,而且只有一个线程数, 发现里面执行了checkConfigInfo(), 这刚好LongPollingRunnable 类执行的所在方法.\n至此, 它的定时执行就清楚了, 它开了一个单线程的延时线程池,每隔10ms执行一次, 线程里再用线程池去\u0026quot;监听\u0026quot;文件是否有修改\n但是我们发现日志间隔并不是10ms,而且这个间隔也太小了, 肯定不合理\n5.1.2.2 通过长轮询的方式获得修改过的文件及其内容 去看一下它是如何\u0026quot;监听\u0026quot;, 跟进com.alibaba.nacos.client.config.impl.ClientWorker#checkUpdateDataIds\n/** * 从Server获取值变化了的DataID列表。返回的对象里只有dataId和group是有效的。 保证不返回NULL。 */ List\u0026lt;String\u0026gt; checkUpdateDataIds(List\u0026lt;CacheData\u0026gt; cacheDatas, List\u0026lt;String\u0026gt; inInitializingCacheList) throws IOException { // 构造参数- 通过配置dataId/group/tenant等数据来指定文件 StringBuilder sb = new StringBuilder(); for (CacheData cacheData : cacheDatas) { if (!cacheData.isUseLocalConfigInfo()) { sb.append(cacheData.dataId).append(WORD_SEPARATOR); sb.append(cacheData.group).append(WORD_SEPARATOR); if (StringUtils.isBlank(cacheData.tenant)) { sb.append(cacheData.getMd5()).append(LINE_SEPARATOR); } else { sb.append(cacheData.getMd5()).append(WORD_SEPARATOR); sb.append(cacheData.getTenant()).append(LINE_SEPARATOR); } if (cacheData.isInitializing()) { // cacheData 首次出现在cacheMap中\u0026amp;首次check更新 inInitializingCacheList .add(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant)); } } } boolean isInitializingCacheList = !inInitializingCacheList.isEmpty(); // 核心方法- 检查更新文件 return checkUpdateConfigStr(sb.toString(), isInitializingCacheList); } com.alibaba.nacos.client.config.impl.ClientWorker#checkUpdateConfigStr\n/** * 从Server获取值变化了的DataID列表。返回的对象里只有dataId和group是有效的。 保证不返回NULL。 */ List\u0026lt;String\u0026gt; checkUpdateConfigStr(String probeUpdateString, boolean isInitializingCacheList) throws IOException { List\u0026lt;String\u0026gt; params = new ArrayList\u0026lt;String\u0026gt;(2); params.add(Constants.PROBE_MODIFY_REQUEST); params.add(probeUpdateString); List\u0026lt;String\u0026gt; headers = new ArrayList\u0026lt;String\u0026gt;(2); headers.add(\u0026#34;Long-Pulling-Timeout\u0026#34;); // 设置长轮询的过期时间 headers.add(\u0026#34;\u0026#34; + timeout); // told server do not hang me up if new initializing cacheData added in if (isInitializingCacheList) { headers.add(\u0026#34;Long-Pulling-Timeout-No-Hangup\u0026#34;); headers.add(\u0026#34;true\u0026#34;); } if (StringUtils.isBlank(probeUpdateString)) { return Collections.emptyList(); } try { // In order to prevent the server from handling the delay of the client\u0026#39;s long task, // increase the client\u0026#39;s read timeout to avoid this problem. long readTimeoutMs = timeout + (long) Math.round(timeout \u0026gt;\u0026gt; 1); // 长轮询请求 HttpResult result = agent.httpPost(Constants.CONFIG_CONTROLLER_PATH + \u0026#34;/listener\u0026#34;, headers, params, agent.getEncode(), readTimeoutMs); if (HttpURLConnection.HTTP_OK == result.code) { setHealthServer(true); // 解析返参 return parseUpdateDataIdResponse(result.content); } else { setHealthServer(false); LOGGER.error(\u0026#34;[{}] [check-update] get changed dataId error, code: {}\u0026#34;, agent.getName(), result.code); } } catch (IOException e) { setHealthServer(false); LOGGER.error(\u0026#34;[\u0026#34; + agent.getName() + \u0026#34;] [check-update] get changed dataId exception\u0026#34;, e); throw e; } return Collections.emptyList(); } 就会发现它是发了一个请求过去, 然后通过parseUpdateDataIdResponse(result.content) 方法解析出返参里面的 dataId/group/tenant等数据\n这个请求中设置了一些长轮询的参数,表示这是一个长轮询的请求\n长轮询: 客户端发起Long Polling，此时如果服务端没有相关数据，会hold住请求，直到服务端有相关数据，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次Long Polling。\n这里只是拿到了一个dataId和group等数据,那什么时候拿到具体的配置信息呢? 继续往下看 LongPollingRunnable#run\n// 检查更新的dataId List\u0026lt;String\u0026gt; changedGroupKeys = checkUpdateDataIds(cacheDatas, inInitializingCacheList); LOGGER.info(\u0026#34;get changedGroupKeys:\u0026#34; + changedGroupKeys); // 遍历这些文件 for (String groupKey : changedGroupKeys) { String[] key = GroupKey.parseKey(groupKey); String dataId = key[0]; String group = key[1]; String tenant = null; if (key.length == 3) { tenant = key[2]; } try { // 获得具体配置 String[] ct = getServerConfig(dataId, group, tenant, 3000L); CacheData cache = cacheMap.get().get(GroupKey.getKeyTenant(dataId, group, tenant)); // 把内容直接写到cacheMap中 cache.setContent(ct[0]); if (null != ct[1]) { cache.setType(ct[1]); } LOGGER.info(\u0026#34;[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}, type={}\u0026#34;, agent.getName(), dataId, group, tenant, cache.getMd5(), ContentUtils.truncateContent(ct[0]), ct[1]); } catch (NacosException ioe) { String message = String.format( \u0026#34;[%s] [get-update] get changed config exception. dataId=%s, group=%s, tenant=%s\u0026#34;, agent.getName(), dataId, group, tenant); LOGGER.error(message, ioe); } } for (CacheData cacheData : cacheDatas) { if (!cacheData.isInitializing() || inInitializingCacheList .contains(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant))) { // 检查md5 cacheData.checkListenerMd5(); cacheData.setInitializing(false); } } // ....省略剩下代码 } com.alibaba.nacos.client.config.impl.ClientWorker#getServerConfig 获得具体配置的方法\npublic String[] getServerConfig(String dataId, String group, String tenant, long readTimeout) throws NacosException { String[] ct = new String[2]; if (StringUtils.isBlank(group)) { group = Constants.DEFAULT_GROUP; } HttpResult result = null; try { List\u0026lt;String\u0026gt; params = null; if (StringUtils.isBlank(tenant)) { params = new ArrayList\u0026lt;String\u0026gt;(Arrays.asList(\u0026#34;dataId\u0026#34;, dataId, \u0026#34;group\u0026#34;, group)); } else { params = new ArrayList\u0026lt;String\u0026gt;(Arrays.asList(\u0026#34;dataId\u0026#34;, dataId, \u0026#34;group\u0026#34;, group, \u0026#34;tenant\u0026#34;, tenant)); } // 通过get请求,获得具体配置 result = agent.httpGet(Constants.CONFIG_CONTROLLER_PATH, null, params, agent.getEncode(), readTimeout); } catch (IOException e) { String message = String.format( \u0026#34;[%s] [sub-server] get server config exception, dataId=%s, group=%s, tenant=%s\u0026#34;, agent.getName(), dataId, group, tenant); LOGGER.error(message, e); throw new NacosException(NacosException.SERVER_ERROR, e); } switch (result.code) { case HttpURLConnection.HTTP_OK: // 先放到本地文件中 LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, result.content); // 将请求返参放入ct数组中 ct[0] = result.content; if (result.headers.containsKey(CONFIG_TYPE)) { ct[1] = result.headers.get(CONFIG_TYPE).get(0); } else { ct[1] = ConfigType.TEXT.getType(); } return ct; case HttpURLConnection.HTTP_NOT_FOUND: // 省略剩下代码...... } 至此, 清楚了它是如何拿到具体配置的了, 它通过(一次post请求)长轮询的方式和服务端建立连接, 获得dataId/group等数据, 再通过这些参数发起get请求获得具体的配置文件内容,并写到本地缓存中使用\n5.1.2.3 拿到配置后通过applicationContext更新到项目内存中 它取到这些配置后,是如何写到项目的内存中并使其生效的呢?\ntry { String[] ct = getServerConfig(dataId, group, tenant, 3000L); CacheData cache = cacheMap.get().get(GroupKey.getKeyTenant(dataId, group, tenant)); cache.setContent(ct[0]); if (null != ct[1]) { cache.setType(ct[1]); } LOGGER.info(\u0026#34;[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}, type={}\u0026#34;, agent.getName(), dataId, group, tenant, cache.getMd5(), ContentUtils.truncateContent(ct[0]), ct[1]); } catch (NacosException ioe) { String message = String.format( \u0026#34;[%s] [get-update] get changed config exception. dataId=%s, group=%s, tenant=%s\u0026#34;, agent.getName(), dataId, group, tenant); LOGGER.error(message, ioe); } } for (CacheData cacheData : cacheDatas) { if (!cacheData.isInitializing() || inInitializingCacheList .contains(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant))) { // 检查md5 cacheData.checkListenerMd5(); cacheData.setInitializing(false); } } // 省略剩下代码..... } 在取到具体配置后,遍历cacheDatas数据,并检查md5, 跟进去看一下, 它开始出现监听器了\nvoid checkListenerMd5() { for (ManagerListenerWrap wrap : listeners) { if (!md5.equals(wrap.lastCallMd5)) { safeNotifyListener(dataId, group, content, type, md5, wrap); } } } private void safeNotifyListener(final String dataId, final String group, final String content, final String type, final String md5, final ManagerListenerWrap listenerWrap) { final Listener listener = listenerWrap.listener; Runnable job = new Runnable() { @Override public void run() { ClassLoader myClassLoader = Thread.currentThread().getContextClassLoader(); ClassLoader appClassLoader = listener.getClass().getClassLoader(); try { if (listener instanceof AbstractSharedListener) { AbstractSharedListener adapter = (AbstractSharedListener) listener; adapter.fillContext(dataId, group); LOGGER.info(\u0026#34;[{}] [notify-context] dataId={}, group={}, md5={}\u0026#34;, name, dataId, group, md5); } // 执行回调之前先将线程classloader设置为具体webapp的classloader，以免回调方法中调用spi接口是出现异常或错用（多应用部署才会有该问题）。 Thread.currentThread().setContextClassLoader(appClassLoader); ConfigResponse cr = new ConfigResponse(); cr.setDataId(dataId); cr.setGroup(group); cr.setContent(content); configFilterChainManager.doFilter(null, cr); String contentTmp = cr.getContent(); // 处理配置信息 listener.receiveConfigInfo(contentTmp); // compare lastContent and content if (listener instanceof AbstractConfigChangeListener) { Map data = ConfigChangeHandler.getInstance().parseChangeData(listenerWrap.lastContent, content, type); ConfigChangeEvent event = new ConfigChangeEvent(data); ((AbstractConfigChangeListener)listener).receiveConfigChange(event); listenerWrap.lastContent = content; } listenerWrap.lastCallMd5 = md5; LOGGER.info(\u0026#34;[{}] [notify-ok] dataId={}, group={}, md5={}, listener={} \u0026#34;, name, dataId, group, md5, listener); } catch (NacosException de) { LOGGER.error(\u0026#34;[{}] [notify-error] dataId={}, group={}, md5={}, listener={} errCode={} errMsg={}\u0026#34;, name, dataId, group, md5, listener, de.getErrCode(), de.getErrMsg()); } catch (Throwable t) { LOGGER.error(\u0026#34;[{}] [notify-error] dataId={}, group={}, md5={}, listener={} tx={}\u0026#34;, name, dataId, group, md5, listener, t.getCause()); } finally { Thread.currentThread().setContextClassLoader(myClassLoader); } } }; final long startNotify = System.currentTimeMillis(); try { if (null != listener.getExecutor()) { listener.getExecutor().execute(job); } else { job.run(); } } catch (Throwable t) { LOGGER.error(\u0026#34;[{}] [notify-error] dataId={}, group={}, md5={}, listener={} throwable={}\u0026#34;, name, dataId, group, md5, listener, t.getCause()); } final long finishNotify = System.currentTimeMillis(); LOGGER.info(\u0026#34;[{}] [notify-listener] time cost={}ms in ClientWorker, dataId={}, group={}, md5={}, listener={} \u0026#34;, name, (finishNotify - startNotify), dataId, group, md5, listener); } 这么长的代码,核心就是处理了那个runable, 其中调用了listener.receiveConfigInfo(contentTmp) 方法处理的监听器,它是一个抽象类, 找到它的实现类\ncom.alibaba.cloud.nacos.refresh.NacosContextRefresher\nprivate void registerNacosListener(final String groupKey, final String dataKey) { String key = NacosPropertySourceRepository.getMapKey(dataKey, groupKey); Listener listener = listenerMap.computeIfAbsent(key, lst -\u0026gt; new AbstractSharedListener() { @Override public void innerReceive(String dataId, String group, String configInfo) { refreshCountIncrement(); nacosRefreshHistory.addRefreshRecord(dataId, group, configInfo); // todo feature: support single refresh for listening // 通过applicationContext的事件去更新配置 applicationContext.publishEvent( new RefreshEvent(this, null, \u0026#34;Refresh Nacos config\u0026#34;)); if (log.isDebugEnabled()) { log.debug(String.format( \u0026#34;Refresh Nacos config group=%s,dataId=%s,configInfo=%s\u0026#34;, group, dataId, configInfo)); } } }); try { configService.addListener(dataKey, groupKey, listener); } catch (NacosException e) { log.warn(String.format( \u0026#34;register fail for nacos listener ,dataId=[%s],group=[%s]\u0026#34;, dataKey, groupKey), e); } } 至此, 清楚了获得的配置是如何生效的, 它将获得发生修改过的文件, 如果md5不一样了, 则执行监听器,通过applicationContext 更新配置到项目内存中\n明明已经知道了哪些文件被修改了,为啥还有比对md5, 因为可能是没有修改具体内容,只是点了编辑并保存\nmd5用的是java的digest和位移,md5可能存在冲突, 那怎么解决冲突问题的?\n5.1.3 总结 1.Nacos 客户端会循环请求服务端变更的数据，并且超时时间设置为30s，当配置发生变化时，请求的响应会立即返回，否则会一直等到 29.5s+ 之后再返回响应 2.Nacos 客户端能够实时感知到服务端配置发生了变化。 3.实时感知是建立在客户端拉和服务端“推”的基础上，但是这里的服务端“推”需要打上引号，因为服务端和客户端直接本质上还是通过 http 进行数据通讯的，之所以有“推”的感觉，是因为服务端主动将变更后的数据通过 http 的 response 对象提前写入了。 Long Polling长轮询详解 - 简书 (jianshu.com) NACOS动态配置 - barryzhou - 博客园 (cnblogs.com) spring boot 配置文件动态更新原理 以Nacos为例 - 二奎 - 博客园 (cnblogs.com) Nacos 配置中心原理分析 -第一篇- 简书 (jianshu.com) Nacos 配置中心原理分析 -第二篇 - 简书 (jianshu.com) A\u0026amp;Q nacos集群架构?\n答: 和Eureka集群架构一样,他的数据是点对点的, 本身是主从结构,也就是说会产生过半选举,脑裂等知识点, 默认支持cap理论中的ap模式. 生产部署nacos集群时,推荐通过NGINX给nacos集群做负载均衡\n浅谈Nacos中的CAP - 包子卖完了嘛 - 博客园 (cnblogs.com) nacos如何实现动态配置?\n答: 使用长轮询机制,由客户端定时发起请求询问服务端是否有修改, 如果存在修改则通过applicationContext的publishEvent更新内存中的配置\n一次请求后将阻塞29.5s+, 才会发起下一次请求, 如果遇到服务端存在修改文件, 则会立即返回.\n定时机制是开了一个延时线程池,其中只有一个线程,每隔10ms发起一次请求, 服务启动时该任务就执行了\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/nacos.html","summary":"[toc] 前言 阿里开源的注册中心和配置中心 Nacos 官方文档 Nacos 源码 1. 概念 Name Space 用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置。Na","title":"Nacos"},{"content":"Netty是由JBOSS提供的一个java开源框架，现为 Github上的独立项目。Netty提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。 也就是说，Netty 是一个基于NIO的客户、服务器端的编程框架，使用Netty 可以确保你快速和简单的开发出一个网络应用，例如实现了某种协议的客户、服务端应用。Netty相当于简化和流线化了网络应用的编程开发过程，例如：基于TCP和UDP的socket服务开发。\n简单的说,可以用来做网络编程,取代websocket, 其底层使用NIO(new IO:同步非阻塞IO, 这玩意听说贼难)\nnetty能够受到青睐的原因有三：\n并发高 传输快 封装好 并发高是因为非阻塞接收,使用多线程接收\n传输快是因为 使用了零拷贝\n封装好是因为 封装了NIO,让网络编程变得简单(但还是太难\u0026hellip;)\n来源: https://www.jianshu.com/p/b9f3f6a16911 NIO的零拷贝和kafka的零拷贝应该是一样的,零拷贝其实是分类型的,总的来说是减少了数据拷贝的次数\n零拷贝分类:https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/netty.html","summary":"Netty是由JBOSS提供的一个java开源框架，现为 Github上的独立项目。Netty提供异步的、事件驱动的网络应用程序框架和工具，用","title":"Netty"},{"content":" 配置: ~ 为区分大小写匹配 ~* 为不区分大小写匹配 !~和!~*分别为区分大小写不匹配及不区分大小写不匹配 来自* \u0026lt;https://www.cnblogs.com/xuey/p/7631690.html \u0026gt;\nlocation ~ .*\\.(sh|bash)?$ { # 拦截sh和bash结尾的访问,并返回403状态码 return 403; } # 如果localhost 后的地址有冲突 ,比如还有以下,则匹配最长的(也就是最精准的),所以上面这个location会起作用 location / { # 拦截所有的访问,并返回200状态码 return 200; } # 用等号是最精准的 location = /index.html { # 拦截index的访问,并返回200状态码 return 200; } 主流软件负载均衡器对比(LVS、Nginx、HAproxy) - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E6%9C%8D%E5%8A%A1%E5%99%A8/nginx.html","summary":"配置: ~ 为区分大小写匹配 ~* 为不区分大小写匹配 !~和!~*分别为区分大小写不匹配及不区分大小写不匹配 来自* \u0026lt;https://www.cnblogs.com/xuey/p/7631690.html \u0026gt; location ~ .*\\.(sh|bash)?$ { # 拦截sh和bash结尾","title":"Nginx"},{"content":"以下是基于ucap(Django,contos7)项目:\n安装NIGNX:\n安装uwsgi: pip install uwsgi\n#配置NGINX\n配置文件: vim /etc/nginx/conf.d/nginx_mas_ucap.conf\n\\# nginx_mas_ucap.conf \\# the upstream component nginx needs to connect to upstream mas_ucap { server unix:///opt/mas_ucap/unix.sock; # for a file socket \\#server 192.168.153.128:8080; } \\# configuration of the server server { \\# the port your site will be served on listen 8000; \\# the domain name it will serve for, substitute your machine\u0026#39;s IP address or FQDN server_name 192.168.153.128; charset utf-8; \\# max upload size client_max_body_size 75M; \\# adjust to taste client_header_buffer_size 32k; large_client_header_buffers 4 32k; access_log /var/log/nginx/mas_ucap_test_access.log; error_log /var/log/nginx/mas_ucap_test_error.log; \\#proxy_connect_timeout 600; \\#proxy_read_timeout 600; \\#proxy_send_timeout 600; \\# your Django project\u0026#39;s media files - amend as required \\#location /media { \\# alias /usr/share/nginx/html/mas_ucap/media; \\#} \\# your Django project\u0026#39;s static files - amend as required location /static/ { ​ alias /usr/share/nginx/html/mas_ucap/static/; } \\# Finally, send all non-media requests to the Django server. location / { ​ \\#uwsgi_pass 192.168.153.128:8080; ​ uwsgi_pass mas_ucap; ​ \\# the uwsgi_params file you installed ​ include /etc/nginx/uwsgi_params; ​ uwsgi_read_timeout 600s; } } #配置uwsgi [uwsgi] #uwsgi 的配置文件,运行这个文件(位置无要求)( 命令: uwsgi [--ini ] uwsgi.ini ) # Django-related settings # the base directory (full path) chdir = /home/xkj/MAS/project/mas-ucap-2017_test/web # Django\u0026#39;s wsgi file module = sitecore.wsgi # the virtualenv (full path) home = /home/xkj/Envs/ucap # process-related settings # master master = true # maximum number of worker processes processes = 2 # the socket (use the full path to be safe socket = /opt/mas_ucap/unix2.sock # ... with appropriate permissions - may be needed # chmod-socket = 664 # clear environment on exit vacuum = true # set an environment variable env = DJANGO_SETTINGS_MODULE=sitecore.settings # create a pidfile pidfile = mas_ucap2.pid # respawn processes taking more than 20 seconds harakiri = 600 # limit the project to 128 MB # limit-as = 128 # respawn processes after serving 5000 requests max-requests = 5000 # background the process \u0026amp; log daemonize = /opt/mas_ucap/log/uwsgi.log ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E6%9C%8D%E5%8A%A1%E5%99%A8/nginx%E5%92%8Cuwsgi.html","summary":"以下是基于ucap(Django,contos7)项目: 安装NIGNX: 安装uwsgi: pip install uwsgi #配置NGINX 配置文件: vim /etc/nginx/conf.d/nginx_mas_ucap.conf \\# nginx_mas_ucap.conf \\# the upstream component nginx needs","title":"Nginx和uwsgi"},{"content":"[toc]\n(OAuth1和OAuth2差别较大,这里讲解的是OAuth2)\n1.介绍 简单说，OAuth 就是一种授权机制。数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（token），用来代替密码，供第三方应用使用\n令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。\n（1）令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。\n（2）令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。\n（3）令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。\n上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。\n注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。 这也是为什么令牌的有效期，一般都设置得很短的原因。\nOAuth 2.0 对于如何颁发令牌的细节，规定得非常详细。具体来说，一共分成四种授权类型（authorization grant），即四种颁发令牌的方式，适用于不同的互联网场景。\n授权码（authorization-code） 隐藏式（implicit） 密码式（password）： 客户端凭证（client credentials） 注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。\n1.1 授权码方式 授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。\n这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。\n第一步，A 网站提供一个链接，用户点击后就会跳转到 B 网站，授权用户数据给 A 网站使用。下面就是 A 网站跳转 B 网站的一个示意链接。\nhttps://b.com/oauth/authorize? response_type=code\u0026amp; client_id=CLIENT_ID\u0026amp; redirect_uri=CALLBACK_URL\u0026amp; scope=read 上面 URL 中，response_type 参数表示要求返回授权码（code），\nclient_id 参数让 B 知道是谁在请求，\nredirect_uri 参数是 B 接受或拒绝请求后的跳转网址，\nscope 参数表示要求的授权范围（这里是只读）。\n第二步，用户跳转后，B 网站会要求用户登录，然后询问是否同意给予 A 网站授权。用户表示同意，这时 B 网站就会跳回redirect_uri参数指定的网址。跳转时，会传回一个授权码，就像下面这样。\nhttps://a.com/callback?code=AUTHORIZATION_CODE 上面 URL 中，code参数就是授权码。\n第三步，A 网站拿到授权码以后，就可以在后端，向 B 网站请求令牌。\nhttps://b.com/oauth/token? client_id=CLIENT_ID\u0026amp; client_secret=CLIENT_SECRET\u0026amp; grant_type=authorization_code\u0026amp; code=AUTHORIZATION_CODE\u0026amp; redirect_uri=CALLBACK_URL 上面 URL 中，client_id 参数和client_secret 参数用来让 B 确认 A 的身份（client_secret参数是保密的，因此只能在后端发请求），\ngrant_type 参数的值是AUTHORIZATION_CODE，表示采用的授权方式是授权码，\ncode 参数是上一步拿到的授权码，\nredirect_uri 参数是令牌颁发后的回调网址。\n第四步，B 网站收到请求以后，就会颁发令牌。具体做法是向redirect_uri指定的网址，发送一段 JSON 数据。\n{ \u0026#34;access_token\u0026#34;:\u0026#34;ACCESS_TOKEN\u0026#34;, \u0026#34;token_type\u0026#34;:\u0026#34;bearer\u0026#34;, \u0026#34;expires_in\u0026#34;:2592000, \u0026#34;refresh_token\u0026#34;:\u0026#34;REFRESH_TOKEN\u0026#34;, \u0026#34;scope\u0026#34;:\u0026#34;read\u0026#34;, \u0026#34;uid\u0026#34;:100101, \u0026#34;info\u0026#34;:{...} } 上面 JSON 数据中，access_token字段就是令牌，A 网站在后端拿到了.\n1.2.密码方式 如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为\u0026quot;密码式\u0026quot;（password）。\n第一步，A 网站要求用户提供 B 网站的用户名和密码。拿到以后，A 就直接向 B 请求令牌。\nhttps://oauth.b.com/token? grant_type=password\u0026amp; username=USERNAME\u0026amp; password=PASSWORD\u0026amp; client_id=CLIENT_ID 上面 URL 中，grant_type参数是授权方式，这里的password表示\u0026quot;密码式\u0026quot;，\nusername和password是 B 的用户名和密码。\n第二步，B 网站验证身份通过后，直接给出令牌。注意，这时不需要跳转，而是把令牌放在 JSON 数据里面，作为 HTTP 回应，A 因此拿到令牌。\n这种方式需要用户给出自己的用户名/密码，显然风险很大，因此只适用于其他授权方式都无法采用的情况，而且必须是用户高度信任的应用。\n来自:http://www.ruanyifeng.com/blog/2019/04/oauth-grant-types.html\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/oauth2.html","summary":"[toc] (OAuth1和OAuth2差别较大,这里讲解的是OAuth2) 1.介绍 简单说，OAuth 就是一种授权机制。数据的所有者告诉系统，同意授权","title":"OAuth2"},{"content":"[toc]\n前言 OpenFeign 全称 Spring Cloud OpenFeign，它是 Spring 官方推出的一种声明式服务调用与负载均衡组件，它的出现就是为了替代进入停更维护状态的 Feign。OpenFeign 是 Spring Cloud 对 Feign 的二次封装，它具有 Feign 的所有功能，并在 Feign 的基础上增加了对 Spring MVC 注解的支持，例如 @RequestMapping、@GetMapping 和 @PostMapping 等。\nOpenFeign：Spring Cloud声明式服务调用组件（非常详细） (biancheng.net) 本文章基于openFeign 2.X\nFeign 的启动原理 注入@Import 我们在使用OpenFegin时, 需要加@EnableFeignClients, 我们先看一下这个注解\n@Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Import(FeignClientsRegistrar.class) public @interface EnableFeignClients {...} 重点在第四个 @Import 上，一般使用此注解都是想要动态注册 Spring Bean 的\nclass FeignClientsRegistrar implements ImportBeanDefinitionRegistrar, ResourceLoaderAware, EnvironmentAware { // ..... // 资源加载器，可以加载 classpath 下的所有文件 private ResourceLoader resourceLoader; // 上下文，可通过该环境获取当前应用配置属性等 private Environment environment; @Override public void setEnvironment(Environment environment) { this.environment = environment; } @Override public void setResourceLoader(ResourceLoader resourceLoader) { this.resourceLoader = resourceLoader; } @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { // 注册 ＠EnableFeignClients 提供的自定义配置类中的相关 Bean 实例 registerDefaultConfiguration(metadata,registry); // 扫描 packge，注册被 @FeignClient 修饰的接口类为 IOC Bean registerFeignClients(metadata, registry); } // ..... } 其中 ImportBeanDefinitionRegistrar 负责动态注入 IOC Bean，它有一个registerBeanDefinitions(), 用来做bean的注入具体逻辑,\nFeignClientsRegistrar分别注入了 Feign 配置类、FeignClient Bean,\n添加全局配置 registerDefaultConfiguration 方法流程如下\n获取 @EnableFeignClients 注解上的属性以及对应 Value 生成 FeignClientSpecification（存储 Feign 中的配置类） 对应的构造器 BeanDefinitionBuilder FeignClientSpecification Bean 名称为 default. + @EnableFeignClients 修饰类全限定名称 + FeignClientSpecification @EnableFeignClients defaultConfiguration 默认为 {}，如果没有相关配置，默认使用 FeignClientsConfiguration 并结合 name 填充到 FeignClientSpecification，最终注册为 IOC Bean 注册 FeignClient 接口 将重点放在 registerFeignClients 上，该方法主要就是将修饰了 @FeignClient 的接口注册为 IOC Bean\n扫描 @EnableFeignClients 注解，如果有 clients，则加载指定接口，为空则根据 scanner 规则扫描出修饰了 @FeignClient 的接口\n获取 @FeignClient 上对应的属性，根据 configuration 属性去创建接口级的 FeignClientSpecification 配置类 IOC Bean\n将 @FeignClient 的属性设置到 FeignClientFactoryBean 对象上，并注册 IOC Bean\n@FengnClient 修饰的接口实际上使用了 Spring 的代理工厂生成代理类，所以这里会把修饰了 @FeignClient 接口的 BeanDefinition 设置为 FeignClientFactoryBean 类型，而 FeignClientFactoryBean 继承自 FactoryBean\n也就是说，当我们定义 @FeignClient 修饰接口时，注册到 IOC 容器中 Bean 类型变成了 FeignClientFactoryBean\n在 Spring 中，FactoryBean 是一个工厂 Bean，用来创建代理 Bean。工厂 Bean 是一种特殊的 Bean，对于需要获取 Bean 的消费者而言，它是不知道 Bean 是普通 Bean 或是工厂 Bean 的。工厂 Bean 返回的实例不是工厂 Bean 本身，而是会返回执行了工厂 Bean 中 FactoryBean#getObject 逻辑的实例\n以后可以写一篇FactoryBean的文章\nFeignClientFactoryBean 获得具体bean的方法\n构造 feign.Builder对象 \u0026lt;T\u0026gt; T getTarget() { FeignContext context = applicationContext.getBean(FeignContext.class); // 拿到 Feign.Builder Feign.Builder builder = feign(context); if (!StringUtils.hasText(url)) { if (!name.startsWith(\u0026#34;http\u0026#34;)) { url = \u0026#34;http://\u0026#34; + name; } else { url = name; } url += cleanPath(); // 通过负载拿到bean return (T) loadBalance(builder, context, new HardCodedTarget\u0026lt;\u0026gt;(type, name, url)); } ....// 省略代码 } org.springframework.cloud.openfeign.FeignClientFactoryBean#feign\nprotected Feign.Builder feign(FeignContext context) { FeignLoggerFactory loggerFactory = get(context, FeignLoggerFactory.class); Logger logger = loggerFactory.create(type); // 从context拿出 encoder , Decoder , Contract 对象 Feign.Builder builder = get(context, Feign.Builder.class) // required values .logger(logger) .encoder(get(context, Encoder.class)) .decoder(get(context, Decoder.class)) .contract(get(context, Contract.class)); // 配置feign, 例如 超时、重试、404 配置, 错误Encode, 拦截器等等 configureFeign(context, builder); return builder; } 动态代理生成 org.springframework.cloud.openfeign.FeignClientFactoryBean#loadBalance\nClient： Feign 发送请求以及接收响应等都是由 Client 完成，该类默认 Client.Default，另外支持 HttpClient、OkHttp 等客户端\n然后进入org.springframework.cloud.openfeign.HystrixTargeter#target\n然后构建了fegin对象 feign.Feign.Builder#target(feign.Target\u0026lt;T\u0026gt;)\npublic \u0026lt;T\u0026gt; T target(Target\u0026lt;T\u0026gt; target) { return build().newInstance(target); } public Feign build() { Client client = Capability.enrich(this.client, capabilities); Retryer retryer = Capability.enrich(this.retryer, capabilities); List\u0026lt;RequestInterceptor\u0026gt; requestInterceptors = this.requestInterceptors.stream() .map(ri -\u0026gt; Capability.enrich(ri, capabilities)) .collect(Collectors.toList()); Logger logger = Capability.enrich(this.logger, capabilities); Contract contract = Capability.enrich(this.contract, capabilities); Options options = Capability.enrich(this.options, capabilities); Encoder encoder = Capability.enrich(this.encoder, capabilities); Decoder decoder = Capability.enrich(this.decoder, capabilities); InvocationHandlerFactory invocationHandlerFactory = Capability.enrich(this.invocationHandlerFactory, capabilities); QueryMapEncoder queryMapEncoder = Capability.enrich(this.queryMapEncoder, capabilities); SynchronousMethodHandler.Factory synchronousMethodHandlerFactory = new SynchronousMethodHandler.Factory(client, retryer, requestInterceptors, logger, logLevel, decode404, closeAfterDecode, propagationPolicy, forceDecoding); ParseHandlersByName handlersByName = new ParseHandlersByName(contract, options, encoder, decoder, queryMapEncoder, errorDecoder, synchronousMethodHandlerFactory); return new ReflectiveFeign(handlersByName, invocationHandlerFactory, queryMapEncoder); } } 生成代理类 feign.ReflectiveFeign#newInstance\nnewInstance 方法对 @FeignClient 修饰的接口中 SpringMvc 等配置进行解析转换，对接口类中的方法进行归类，生成动态代理类\n根据 newInstance 方法按照行为大致划分，共做了四件事\n处理 @FeignCLient 注解（SpringMvc 注解等）封装为 MethodHandler 包装类 遍历接口中所有方法，过滤 Object 方法，并将默认方法以及 FeignClient 方法分类, 并创建收集起来 创建动态代理对应的 InvocationHandler 并创建 Proxy 实例 接口内 default 方法 绑定动态代理类 MethodHandler 将方法参数、方法返回值、参数集合、请求类型、请求路径进行解析存储\n在创建InvocationHandler时, 创建的其实现类是 feign.ReflectiveFeign.FeignInvocationHandler\nInvocationHandler handler = factory.create(target, methodToHandler); public interface InvocationHandlerFactory { InvocationHandler create(Target target, Map\u0026lt;Method, MethodHandler\u0026gt; dispatch); /** * Like {@link InvocationHandler#invoke(Object, java.lang.reflect.Method, Object[])}, except for a * single method. */ interface MethodHandler { Object invoke(Object[] argv) throws Throwable; } static final class Default implements InvocationHandlerFactory { @Override public InvocationHandler create(Target target, Map\u0026lt;Method, MethodHandler\u0026gt; dispatch) { return new ReflectiveFeign.FeignInvocationHandler(target, dispatch); } } } 以上, 在项目启动时处理\n以后补一篇jdk动态代理\n在我们调用 @FeignClient 接口时，会被 FeignInvocationHandler#invoke 拦截，并在动态代理方法中执行下述逻辑\n接口注解信息封装为 HTTP Request 通过 Ribbon 获取服务列表，并对服务列表进行负载均衡调用（服务名转换为 ip+port） 请求调用后，将返回的数据封装为 HTTP Response，继而转换为接口中的返回类型 feign.ReflectiveFeign.FeignInvocationHandler#invoke\n@Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (\u0026#34;equals\u0026#34;.equals(method.getName())) { try { Object otherHandler = args.length \u0026gt; 0 \u0026amp;\u0026amp; args[0] != null ? Proxy.getInvocationHandler(args[0]) : null; return equals(otherHandler); } catch (IllegalArgumentException e) { return false; } } else if (\u0026#34;hashCode\u0026#34;.equals(method.getName())) { return hashCode(); } else if (\u0026#34;toString\u0026#34;.equals(method.getName())) { return toString(); } // 这个dispatch 就是 methodToHandler 集合 // 其结构 : private final Map\u0026lt;Method, MethodHandler\u0026gt; dispatch; return dispatch.get(method).invoke(args); // 它反射的时候用的是Method类, 所以feign接口是允许重载的, mybatis的反射xml的接口不允许重载的 } 执行具体方法的invoke\nfeign.SynchronousMethodHandler#invoke\n@Override public Object invoke(Object[] argv) throws Throwable { // 构建 Request 模版类 RequestTemplate template = buildTemplateFromArgs.create(argv); // 存放连接、超时时间等配置类 Options options = findOptions(argv); 失败重试策略类 Retryer retryer = this.retryer.clone(); while (true) { try { // 执行 return executeAndDecode(template, options); } catch (RetryableException e) { try { // 重试操作 retryer.continueOrPropagate(e); } catch (RetryableException th) { Throwable cause = th.getCause(); if (propagationPolicy == UNWRAP \u0026amp;\u0026amp; cause != null) { throw cause; } else { throw th; } } if (logLevel != Logger.Level.NONE) { logger.logRetry(metadata.configKey(), logLevel); } continue; } } } Object executeAndDecode(RequestTemplate template, Options options) throws Throwable { Request request = targetRequest(template); if (logLevel != Logger.Level.NONE) { logger.logRequest(metadata.configKey(), logLevel, request); } Response response; long start = System.nanoTime(); try { // 执行 response = client.execute(request, options); // ensure the request is set. TODO: remove in Feign 12 response = response.toBuilder() .request(request) .requestTemplate(template) .build(); } catch (IOException e) { if (logLevel != Logger.Level.NONE) { logger.logIOException(metadata.configKey(), logLevel, e, elapsedTime(start)); } throw errorExecuting(request, e); } long elapsedTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start); if (decoder != null) // decode 责任链 return decoder.decode(response, metadata.returnType()); ......// 省略代码 } 然后 org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#execute\n然后 com.netflix.client.AbstractLoadBalancerAwareClient#executeWithLoadBalancer(S, com.netflix.client.config.IClientConfig)\n执行远端调用逻辑中使用到了 Rxjava （响应式编程），可以看到通过底层获取 server 后将服务名称转变为 ip+port 的方式\npublic T executeWithLoadBalancer(final S request, final IClientConfig requestConfig) throws ClientException { LoadBalancerCommand\u0026lt;T\u0026gt; command = buildLoadBalancerCommand(request, requestConfig); try { return command.submit( // 这特么是个参数-----参数开始 new ServerOperation\u0026lt;T\u0026gt;() { @Override public Observable\u0026lt;T\u0026gt; call(Server server) { // 拿到了具体的ip+port URI finalUri = reconstructURIWithServer(server, request.getUri()); S requestForServer = (S) request.replaceUri(finalUri); try { return Observable.just(AbstractLoadBalancerAwareClient.this.execute(requestForServer, requestConfig)); } catch (Exception e) { return Observable.error(e); } } })// 参数结束 .toBlocking() .single(); } catch (Exception e) { Throwable t = e.getCause(); if (t instanceof ClientException) { throw (ClientException) t; } else { throw new ClientException(e); } } } public Observable\u0026lt;T\u0026gt; submit(final ServerOperation\u0026lt;T\u0026gt; operation) {} submit方法有个入参, 就是那一大坨\n网络调用默认使用 JDK 的 HttpURLConnection，可以配置使用 HttpClient 或者 OkHttp\n具体怎么拿到ip和port的, 就是接下来的负责均衡知识了\nFeign 如何负载均衡 一般而言，我们生产者注册多个服务，消费者调用时需要使用负载均衡从中 轮询机制选取一个健康并且可用的生产者服务\n默认是轮询机制, 可以修改\nopenFegin 2.X 才有负载均衡 , 3.0 之后就移除了Ribbon, 一般用spring-cloud-starter-loadbalancer代替\ncom.netflix.loadbalancer.reactive.LoadBalancerCommand#selectServer\n因为 Feign 内部集成 Ribbon，所以也支持此特性，一起看下它是怎么做的\ngetServerFromLoadBalancer()\npublic Server getServerFromLoadBalancer(@Nullable URI original, @Nullable Object loadBalancerKey) throws ClientException { String host = null; int port = -1; if (original != null) { host = original.getHost(); } if (original != null) { Pair\u0026lt;String, Integer\u0026gt; schemeAndPort = deriveSchemeAndPortFromPartialUri(original); port = schemeAndPort.second(); } // Various Supported Cases // The loadbalancer to use and the instances it has is based on how it was registered // In each of these cases, the client might come in using Full Url or Partial URL ILoadBalancer lb = getLoadBalancer(); if (host == null) { // Partial URI or no URI Case // well we have to just get the right instances from lb - or we fall back if (lb != null){ // 选择出合适的服务 Server svc = lb.chooseServer(loadBalancerKey); if (svc == null){ throw new ClientException(ClientException.ErrorType.GENERAL, \u0026#34;Load balancer does not have available server for client: \u0026#34; + clientName); } host = svc.getHost(); if (host == null){ throw new ClientException(ClientException.ErrorType.GENERAL, \u0026#34;Invalid Server for :\u0026#34; + svc); } logger.debug(\u0026#34;{} using LB returned Server: {} for request {}\u0026#34;, new Object[]{clientName, svc, original}); return svc; } ......// 省略代码 进入按空间位置获取服务\ncom.netflix.loadbalancer.ZoneAwareLoadBalancer#chooseServer\n/** * 最终都是使用父级的 chooseServer() * 如果有多个区域, 就随机选一个 */ public Server chooseServer(Object key) { // 如果只有一个区域直接使用父级的 if (!ENABLED.get() || getLoadBalancerStats().getAvailableZones().size() \u0026lt;= 1) { logger.debug(\u0026#34;Zone aware logic disabled or there is only one zone\u0026#34;); return super.chooseServer(key); } Server server = null; try { LoadBalancerStats lbStats = getLoadBalancerStats(); Map\u0026lt;String, ZoneSnapshot\u0026gt; zoneSnapshot = ZoneAvoidanceRule.createSnapshot(lbStats); logger.debug(\u0026#34;Zone snapshots: {}\u0026#34;, zoneSnapshot); if (triggeringLoad == null) { triggeringLoad = DynamicPropertyFactory.getInstance().getDoubleProperty( \u0026#34;ZoneAwareNIWSDiscoveryLoadBalancer.\u0026#34; + this.getName() + \u0026#34;.triggeringLoadPerServerThreshold\u0026#34;, 0.2d); } if (triggeringBlackoutPercentage == null) { triggeringBlackoutPercentage = DynamicPropertyFactory.getInstance().getDoubleProperty( \u0026#34;ZoneAwareNIWSDiscoveryLoadBalancer.\u0026#34; + this.getName() + \u0026#34;.avoidZoneWithBlackoutPercetage\u0026#34;, 0.99999d); } // 拿到可用的区域列表 Set\u0026lt;String\u0026gt; availableZones = ZoneAvoidanceRule.getAvailableZones(zoneSnapshot, triggeringLoad.get(), triggeringBlackoutPercentage.get()); logger.debug(\u0026#34;Available zones: {}\u0026#34;, availableZones); if (availableZones != null \u0026amp;\u0026amp; availableZones.size() \u0026lt; zoneSnapshot.keySet().size()) { // 随机算一个区域 String zone = ZoneAvoidanceRule.randomChooseZone(zoneSnapshot, availableZones); logger.debug(\u0026#34;Zone chosen: {}\u0026#34;, zone); if (zone != null) { // 拿到具体的负载均衡规则 BaseLoadBalancer zoneLoadBalancer = getLoadBalancer(zone); // 根据规则获取服务信息 server = zoneLoadBalancer.chooseServer(key); } } } catch (Exception e) { logger.error(\u0026#34;Error choosing server using zone aware logic for load balancer={}\u0026#34;, name, e); } if (server != null) { return server; } else { logger.debug(\u0026#34;Zone avoidance logic is not invoked.\u0026#34;); return super.chooseServer(key); } 可用的判断包括 是否有存活节点; 熔断次数不能太多等等\n拿到负载均衡对象\ncom.netflix.loadbalancer.ZoneAwareLoadBalancer#getLoadBalancer\nBaseLoadBalancer getLoadBalancer(String zone) { zone = zone.toLowerCase(); BaseLoadBalancer loadBalancer = balancers.get(zone); if (loadBalancer == null) { // We need to create rule object for load balancer for each zone // 拿到具体的规则 , 默认是轮询 IRule rule = cloneRule(this.getRule()); loadBalancer = new BaseLoadBalancer(this.getName() + \u0026#34;_\u0026#34; + zone, rule, this.getLoadBalancerStats()); BaseLoadBalancer prev = balancers.putIfAbsent(zone, loadBalancer); if (prev != null) { loadBalancer = prev; } } return loadBalancer; } com.netflix.loadbalancer.BaseLoadBalancer#getRule\nprivate final static IRule DEFAULT_RULE = new RoundRobinRule(); protected IRule rule = DEFAULT_RULE; public IRule getRule() { return rule; } 拿到规则后, 调用chooseServer()\ncom.netflix.loadbalancer.BaseLoadBalancer#chooseServer\n调用了其下实现类\ncom.netflix.loadbalancer.RoundRobinRule#choose(com.netflix.loadbalancer.ILoadBalancer, java.lang.Object)\npublic Server choose(ILoadBalancer lb, Object key) { if (lb == null) { log.warn(\u0026#34;no load balancer\u0026#34;); return null; } Server server = null; int count = 0; while (server == null \u0026amp;\u0026amp; count++ \u0026lt; 10) { List\u0026lt;Server\u0026gt; reachableServers = lb.getReachableServers(); // 所有服务的ip + port List\u0026lt;Server\u0026gt; allServers = lb.getAllServers(); int upCount = reachableServers.size(); int serverCount = allServers.size(); if ((upCount == 0) || (serverCount == 0)) { log.warn(\u0026#34;No up servers available from load balancer: \u0026#34; + lb); return null; } // 下一个服务的下标 int nextServerIndex = incrementAndGetModulo(serverCount); server = allServers.get(nextServerIndex); if (server == null) { /* Transient. */ Thread.yield(); continue; } if (server.isAlive() \u0026amp;\u0026amp; (server.isReadyToServe())) { return (server); } // Next. server = null; } if (count \u0026gt;= 10) { log.warn(\u0026#34;No available alive servers after 10 tries from load balancer: \u0026#34; + lb); } return server; } com.netflix.loadbalancer.RoundRobinRule#incrementAndGetModulo\nprivate int incrementAndGetModulo(int modulo) { for (;;) { // AtomicInteger nextServerCyclicCounter = nextServerCyclicCounter = new AtomicInteger(0); 类初始化时 初始化了变量 int current = nextServerCyclicCounter.get(); int next = (current + 1) % modulo; if (nextServerCyclicCounter.compareAndSet(current, next)) return next; } } 日志配置 3、openFeign日志配置_搞钱自律的博客-CSDN博客_openfeign日志 注意: feign调试日志是debug级别输出,springboot默认的日志级别是info，所以要调节springboot的日志级别(可以指定目录的调节)\n使用okHttp Feign、httpclient、OkHttp3 结合使用 - 疯狂创客圈 - 博客园 (cnblogs.com) 修改负载均衡策略 OpenFeign修改负载均衡策略_一觉睡过头的菜鸡的博客-CSDN博客_openfeign负载均衡 掌握 SpringCloud OpenFeign 核心原理 - 知乎 (zhihu.com) 推荐文章:\n万字长文 | 深入理解 OpenFeign 的架构原理 (qq.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/openfeign.html","summary":"[toc] 前言 OpenFeign 全称 Spring Cloud OpenFeign，它是 Spring 官方推出的一种声明式服务调用与负载均衡组件，它的出现就是为了替代进入停更维护状态的 Feign。Op","title":"openFeign"},{"content":"OpenResty(又称：ngx_openresty) 是一个基于 NGINX 的可伸缩的 Web 平台，由中国人章亦春发起，提供了很多高质量的第三方模块。\nOpenResty 是一个强大的 Web 应用服务器，Web 开发人员可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块,更主要的是在性能方面，OpenResty可以 快速构造出足以胜任 10K 以上并发连接响应的超高性能 Web 应用系统。\n360，UPYUN，阿里云，新浪，腾讯网，去哪儿网，酷狗音乐等都是 OpenResty 的深度用户。\n(我目前还是nginx都还不会用,哪懂写模块\u0026hellip;.)\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8/openresty.html","summary":"OpenResty(又称：ngx_openresty) 是一个基于 NGINX 的可伸缩的 Web 平台，由中国人章亦春发起，提供了很多高质量的第三方模块。 OpenResty 是一","title":"OpenResty"},{"content":"[toc]\n1. 定义 ElasticStack在升级到5.0版本之后，带来了一个新的脚本语言，painless。这里说“新的“是相对与已经存在groove而言的。Groove脚本开启之后，如果被人误用可能带来各种漏洞，由于这些外部的脚本引擎太过于强大，用不好或者设置不当就会引起安全风险，基于安全和性能方面，所以elastic.co开发了一个新的脚本引擎，名字就叫Painless，和Groove的沙盒机制不一样，Painless使用白名单来限制函数与字段的访问，针对es的场景来进行优化，只做es数据的操作，更加轻量级，速度要快好几倍，并且支持Java静态类型，语法保持Groove类似，还支持Java的lambda表达式。\n可以做很多事情,改字段值 / 加字段 / 查询时处理结果值等等\n2. 语法 脚本变量\nctx._source.field：add, contains, remove, indexOf, length\nctx.op：应该对文档应用的操作：索引或删除\nctx._index：访问文档元数据字段\n_score 只在script_score中有效\ndoc[‘field’], doc[‘field’].value: add, contains, remove, indexOf, length\n1、常用数据类型： int、double、String、List、Map、bool (和java一致）\n2、变量定义：\n​ 定义变量有两种方式，动态类型和静态类型，建议用静态类型，静态类型的计算速度是动态类型的10倍\n动态类型定义方式： def a = \u0026ldquo;abc\u0026rdquo; 静态类型定义方式： int a = 1; Sting b = \u0026ldquo;asdsd\u0026rdquo; 3、获取文档中某个字段的值的用法：\n获取doc下字段： doc[\u0026rsquo;name\u0026rsquo;].value\nnested字段取出来后就是数组，可用下标获取, 后面不用加value： doc['expect_jobs'][1]\ndoc以外的字段可直接使用，如下：\nPOST twitter/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.age = params.value\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;value\u0026#34;: 34 } } } 如果params是其他类型,基本遵循java的语法\nlambda语法:\nlist.sort((x, y) -\u0026gt; x - y); list.sort(Integer::compare); 3. 脚本使用 脚本格式\n\u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;source\u0026#34; | \u0026#34;id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;params\u0026#34;: { ... } } 三个参数分别为:\n脚本编写的语言，默认为 painless。 脚本本身可以指定为内联脚本的 source 或存储脚本的 id。 应传递给脚本的任何命名参数。 访问source里的字段\nPainless中用于访问字段值的语法取决于上下文。在Elasticsearch中，有许多不同的Plainless上下文。就像那个链接显示的那样，Plainless上下文包括：ingest processor, update, update by query, sort，filter等等。 Context 访问字段 Ingest node: 访问字段使用ctx ctx.field_name Updates: 使用_source 字段 ctx._source.field_name\n这里的updates包括_update，_reindex以及update_by_query。这里，我们对于context（上下文的理解）非常重要。它的意思是针对不同的API，在使用中ctx所包含的字段是不一样的\n3.1 inline 脚本 PUT twitter/_doc/1 { \u0026#34;user\u0026#34; : \u0026#34;双榆树-张三\u0026#34;, \u0026#34;message\u0026#34; : \u0026#34;今儿天气不错啊，出去转转去\u0026#34;, \u0026#34;uid\u0026#34; : 2, \u0026#34;age\u0026#34; : 20, \u0026#34;city\u0026#34; : \u0026#34;北京\u0026#34;, \u0026#34;province\u0026#34; : \u0026#34;北京\u0026#34;, \u0026#34;country\u0026#34; : \u0026#34;中国\u0026#34;, \u0026#34;address\u0026#34; : \u0026#34;中国北京市海淀区\u0026#34;, \u0026#34;location\u0026#34; : { \u0026#34;lat\u0026#34; : \u0026#34;39.970718\u0026#34;, \u0026#34;lon\u0026#34; : \u0026#34;116.325747\u0026#34; } } 在这个文档里，我们现在想把age修改为30，那么一种办法就是把所有的文档内容都读出来，让修改其中的age想为30，再重新用同样的方法写进去。首先这里需要有几个动作：先读出数据，然后修改，再次写入数据。显然这样比较麻烦。在这里我们可以直接使用Painless语言直接进行修改：\nPOST twitter/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.age = 30\u0026#34; } } 这里的source表明是我们的Painless代码。这里我们只写了很少的代码在DSL之中。这种代码称之为inline。在这里我们直接通过ctx._source.age来访问 _souce里的age。这样我们通过编程的办法直接对年龄进行了修改\n每次更新值不一样,es会认为是不同的脚本,都会进行编译,所以需要用参数的方式,这样es就会当成一个脚本\nElasticsearch第一次看到一个新脚本，它会编译它并将编译后的版本存储在缓存中。无论是inline或是stored脚本都存储在缓存中。新脚本可以驱逐缓存的脚本。默认的情况下是可以存储100个脚本。我们可以通过设置script.cache.max_size来改变其大小，或者通过script.cache.expire来设置过期的时间。这些设置需要在config/elasticsearch.yml里设置。\nPOST twitter/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.age = params.value\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;value\u0026#34;: 34 } } } GET hockey/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script_fields\u0026#34;: { \u0026#34;total_goals\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; int total = 0; for (int i = 0; i \u0026lt; doc[\u0026#39;goals\u0026#39;].length; ++i) { total += doc[\u0026#39;goals\u0026#39;][i]; } return total; \u0026#34;\u0026#34;\u0026#34; } } } } source里面可以用python的多行字符串方式写语法\nsource也能具有查询的功能\n3.2 stored 脚本 source字段不光能写脚本,还能写id,这个id就是指的是一个写好的脚本,这样,脚本就能更好的管理,不用内嵌到查询中\n在这种情况下，scripts可以被存放于一个集群的状态中。它之后可以通过ID进行调用：\nPUT _scripts/add_age { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;ctx._source.age += params.value\u0026#34; } } 在这里，我们定义了一个叫做add_age的script。它的作用就是帮我们把source里的age加上一个数值。我们可以在之后调用它：\nPOST twitter/_update/1 { \u0026#34;script\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;add_age\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;value\u0026#34;: 2 } } } 使用Painless访问Doc里的值 文档里的值可以通过一个叫做doc的Map值来访问。例如，以下脚本计算玩家的总进球数。 此示例使用类型int和for循环。\nGET hockey/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; int total = 0; for (int i = 0; i \u0026lt; doc[\u0026#39;goals\u0026#39;].length; ++i) { total += doc[\u0026#39;goals\u0026#39;][i]; } return total; \u0026#34;\u0026#34;\u0026#34; } } } } } 或者，您可以使用script_fields而不是function_score执行相同的操作：\nGET hockey/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;script_fields\u0026#34;: { \u0026#34;total_goals\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; int total = 0; for (int i = 0; i \u0026lt; doc[\u0026#39;goals\u0026#39;].length; ++i) { total += doc[\u0026#39;goals\u0026#39;][i]; } return total; \u0026#34;\u0026#34;\u0026#34; } } } } 脚本优化 使用脚本缓存, 预先缓存可以节省第一次的查询时间\n使用ingest pipeline进行预先计算\n相比于_source.field_name使用doc[‘field_name’]语法速度更快, doc语法使用doc value , 列存储\n参考链接:\nhttps://blog.csdn.net/qq_24499615/article/details/116161674 https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-lambdas.html https://blog.csdn.net/u013613428/article/details/78134170 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/painless_%E8%84%9A%E6%9C%AC.html","summary":"[toc] 1. 定义 ElasticStack在升级到5.0版本之后，带来了一个新的脚本语言，painless。这里说“新的“是相对与已经存在groove","title":"painless_脚本"},{"content":"经典使用:\n在需要调试的地方加入代码:\nimport pdb ; pdb.set_trace()\n使用参数:\nh（elp）[ 命令 ] 没有参数:打印可用命令的列表。\n有参数:打印关于该命令的帮助。例如:h n\nb（reak）[[ filename：] lineno | 函数 [，条件 ]] lineno:在当前文件中lineon行设置一个中断。\n没有参数:列出所有中断，包括每个断点，断点被击中的次数，当前的忽略计数，以及相关的条件（如果有的话）。\ntbreak [[ filename：] lineno | 函数 [，条件 ]] 临时断点，当它被首次击中时被自动删除。参数与break一样。\ncl（ear）[ filename：lineno | bpnumber [ bpnumber … ]] 文件名：lineno : 清除此行中的所有断点。\n无参:清除所有断点\ns(tep) 执行当前行，有函数就进入函数。\nn(ext) 继续执行，直到当前功能中的下一行达到或返回。\nr(eturn) 继续执行，直到当前函数返回。\nc(ont(inue)) 继续执行，仅在遇到断点时停止。\nj（ump）lineno 跳到指定行\nl（ist）[ first [，last ]] 列出当前文件的源代码。默认显示11行\na（rgs） 打印当前函数的参数列表。\np 变量名(表达式) 打印变量名的值\nprint也可以使用，但不是调试器命令 - 这将执行Python print语句。\npp 表达式 像p命令一样，除了该表达式的值使用该pprint模块漂亮打印。\nq(uit)/exit 退出调试,quit比较暴力\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/pdb.html","summary":"经典使用: 在需要调试的地方加入代码: import pdb ; pdb.set_trace() 使用参数: h（elp）[ 命令 ] 没有参数:打印可用命令的列表。 有参数:打印关于该命令的帮助。例如:","title":"Pdb"},{"content":"pip 使用:\n导出各种包: pip freeze \u0026gt; requirments.txt\n从文件中安装各种包 pip install -r requirments.txt\nvirtualenv`\n(1).安装:\npip install virtualenv (2).创建一个隔离环境\nvirtualenv myproject (3)使用这个隔离环境\nsource venv/bin/activate (4)退出\ndeactivate\nvirtualenvwrapper\n(1).安装:\nlinux: pip install virtualenvwrapper win: pip install virtualenvwrapper-win\n(2).配置:\n在~/.bashrc写入以下内容(在其他配置文件也是可以的)\nexport WORKON_HOME=~/Envs\nsource /usr/local/bin/virtualenvwrapper.sh\n第一行：virtualenvwrapper存放虚拟环境目录\n第二行：virtrualenvwrapper会安装到python的bin目录下，所以该路径是python安装目录下bin/virtualenvwrapper.sh\n读入配置文件，立即生效\nsource ~/.bashrc (3).使用\n①.创建虚拟环境:\nmkvirtualenv venv ②.查看当前虚拟环境目录\nworkon ③.切换虚拟环境:\nworkon venv ④.退出:\ndeactivate ⑤.删除虚拟环境\nrmvirtualenv venv\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/pip.html","summary":"pip 使用: 导出各种包: pip freeze \u0026gt; requirments.txt 从文件中安装各种包 pip install -r requirments.txt virtualenv` (1).安装: pip install virtualenv (2).创建一个隔离环境 virtualenv myproject (3)使用这个隔离环境 source venv/bin/activate (4)退出","title":"pip"},{"content":"[toc]\nparent 现在有这样一个场景，有两个web项目A、B，一个java项目C，它们都需要用到同一个jar包：common.jar。如果分别在三个项目的pom文件中定义各自对common.jar的依赖，那么当common.jar的版本发生变化时，三个项目的pom文件都要改，项目越多要改的地方就越多，很麻烦。这时候就需要用到parent标签, 我们创建一个parent项目，打包类型为pom，parent项目中不存放任何代码，只是管理多个项目之间公共的依赖。在parent项目的pom文件中定义对common.jar的依赖，ABC三个子项目中只需要定义\u0026lt;parent\u0026gt;\u0026lt;/parent\u0026gt;，++parent标签中写上parent项目的pom坐标就可以引用到common.jar了++。\n上面的问题解决了，我们在切换一个场景，有一个springmvc.jar，只有AB两个web项目需要，C项目是java项目不需要，那么又要怎么去依赖。如果AB中分别定义对springmvc.jar的依赖，当springmvc.jar版本变化时修改起来又会很麻烦。解决办法是在parent项目的pom文件中使用\u0026lt;dependencyManagement\u0026gt;\u0026lt;/dependencyManagement\u0026gt;将springmvc.jar管理起来，++如果有哪个子项目要用，那么子项目在自己的POM文件中使用++\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 标签中写上springmvc.jar的坐标，不需要写版本号，可以依赖到这个jar包了。这样springmvc.jar的版本发生变化时只需要修改parent中的版本就可以了。\n原文：https://blog.csdn.net/qq_41254677/article/details/\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/pom%E6%A0%87%E7%AD%BE.html","summary":"[toc] parent 现在有这样一个场景，有两个web项目A、B，一个java项目C，它们都需要用到同一个jar包：common.jar。如果分别在三个项目的","title":"pom标签"},{"content":"目录结构\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!--springboot自带的--\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 打包 --\u0026gt; \u0026lt;!-- 使用这个插件打包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 在1.8的环境下--\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;encoding\u0026gt;UTF-8\u0026lt;/encoding\u0026gt; \u0026lt;!--排除这些文件--\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;*.properties\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;*.xml\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 再使用这个插件打包 --\u0026gt; \u0026lt;!-- 允许打多个包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;make-zip\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;!-- 确定包名,输出路径,用assembly.xml文件 --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;finalName\u0026gt;${project.artifactId}\u0026lt;/finalName\u0026gt; \u0026lt;outputDirectory\u0026gt;target\u0026lt;/outputDirectory\u0026gt; \u0026lt;descriptors\u0026gt; \u0026lt;descriptor\u0026gt;src/assembly/assembly.xml\u0026lt;/descriptor\u0026gt; \u0026lt;/descriptors\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 再使用这个插件打包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-xmls\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;process-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;outputDirectory\u0026gt;${basedir}/target/classes\u0026lt;/outputDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;${basedir}/src/main/java\u0026lt;/directory\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*.xml\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 运行: 直接install,就行了,\n结果:\n文件: assembly.xml # 这是打普通包的插件, 常用于common包,skenton包等公共包,,没有启动类,且结构与服务包不一样,可以被其他服务包引用 \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-source-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;plugins\u0026gt; # 这是 springboot的打包插件,就是常说的服务包,它带了各种依赖和启动类,用这个插件打出来的包不能为其他服务包pom引用 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;executable\u0026gt;true\u0026lt;/executable\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; # 如果要使用,就再打一个普通包,用于引用,改写成如下: 关键是\u0026lt;classifier\u0026gt;标签,这是新jar的后缀,新包是可执行的,原包名是可引用的普通jar \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 额外打包普通jar用于 activity-consumer服务--\u0026gt; \u0026lt;mainClass\u0026gt;com.gree.ecommerce.ActivityServiceApplication\u0026lt;/mainClass\u0026gt; \u0026lt;classifier\u0026gt;OfConsumer\u0026lt;/classifier\u0026gt; \u0026lt;executable\u0026gt;true\u0026lt;/executable\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; 参见 : https://www.cnblogs.com/kingsonfu/p/11805455.html ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/pom%E6%89%93%E5%8C%85.html","summary":"目录结构 \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!--springboot自带的--\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 打包 --\u0026gt; \u0026lt;!-- 使用这个插件打包 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- 在1.8的环境下--\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt;","title":"pom打包"},{"content":"PostgreSQL与MySQL 的区别 特性 MySQL PostgreSQL 实例 通过执行 MySQL 命令（mysqld）启动实例。一个实例可以管理一个或多个数据库。一台服务器可以运行多个 mysqld 实例。一个实例管理器可以监视 mysqld 的各个实例。 通过执行 Postmaster 进程（pg_ctl）启动实例。一个实例可以管理一个或多个数据库，这些数据库组成一个集群。集群是磁盘上的一个区域，这个区域在安装时初始化并由一个目录组成，所有数据都存储在这个目录中。使用 initdb 创建第一个数据库。一台机器上可以启动多个实例。 数据库 数据库是命名的对象集合，是与实例中的其他数据库分离的实体。一个 MySQL 实例中的所有数据库共享同一个系统编目。 数据库是命名的对象集合，每个数据库是与其他数据库分离的实体。每个数据库有自己的系统编目，但是所有数据库共享 pg_databases。 数据缓冲区 通过 innodb_buffer_pool_size 配置参数设置数据缓冲区。这个参数是内存缓冲区的字节数，InnoDB 使用这个缓冲区来缓存表的数据和索引。在专用的数据库服务器上，这个参数最高可以设置为机器物理内存量的 80%。 Shared_buffers 缓存。在默认情况下分配 64 个缓冲区。默认的块大小是 8K。可以通过设置 postgresql.conf 文件中的 shared_buffers 参数来更新缓冲区缓存。 数据库连接 客户机使用 CONNECT 或 USE 语句连接数据库，这时要指定数据库名，还可以指定用户 id 和密码。使用角色管理数据库中的用户和用户组。 客户机使用 connect 语句连接数据库，这时要指定数据库名，还可以指定用户 id 和密码。使用角色管理数据库中的用户和用户组。 身份验证 MySQL 在数据库级管理身份验证。 基本只支持密码认证。 PostgreSQL 支持丰富的认证方法：信任认证、口令认证、Kerberos 认证、基于 Ident 的认证、LDAP 认证、PAM 认证 加密 可以在表级指定密码来对数据进行加密。还可以使用 AES_ENCRYPT 和 AES_DECRYPT 函数对列数据进行加密和解密。可以通过 SSL 连接实现网络加密。 可以使用 pgcrypto 库中的函数对列进行加密/解密。可以通过 SSL 连接实现网络加密。 审计 可以对 querylog 执行 grep。 可以在表上使用 PL/pgSQL 触发器来进行审计。 查询解释 使用 EXPLAIN 命令查看查询的解释计划。 使用 EXPLAIN 命令查看查询的解释计划。 备份、恢复和日志 InnoDB 使用写前（write-ahead）日志记录。支持在线和离线完全备份以及崩溃和事务恢复。需要第三方软件才能支持热备份。 在数据目录的一个子目录中维护写前日志。支持在线和离线完全备份以及崩溃、时间点和事务恢复。 可以支持热备份。 JDBC 驱动程序 可以从 参考资料 下载 JDBC 驱动程序。 可以从 参考资料 下载 JDBC 驱动程序。 表类型 取决于存储引擎。例如，NDB 存储引擎支持分区表，内存引擎支持内存表。 支持临时表、常规表以及范围和列表类型的分区表。不支持哈希分区表。 由于PostgreSQL的表分区是通过表继承和规则系统完成了，所以可以实现更复杂的分区方式。 索引类型 取决于存储引擎。MyISAM：BTREE，InnoDB：BTREE。 支持 B-树、哈希、R-树和 Gist 索引。 约束 支持主键、外键、惟一和非空约束。对检查约束进行解析，但是不强制实施。 支持主键、外键、惟一、非空和检查约束。 存储过程和用户定义函数 支持 CREATE PROCEDURE 和 CREATE FUNCTION 语句。存储过程可以用 SQL 和 C++ 编写。用户定义函数可以用 SQL、C 和 C++ 编写。 没有单独的存储过程，都是通过函数实现的。用户定义函数可以用 PL/pgSQL（专用的过程语言）、PL/Tcl、PL/Perl、PL/Python 、SQL 和 C 编写。 触发器 支持行前触发器、行后触发器和语句触发器，触发器语句用过程语言复合语句编写。 支持行前触发器、行后触发器和语句触发器，触发器过程用 C 编写。 系统配置文件 my.conf Postgresql.conf 数据库配置 my.conf Postgresql.conf 客户机连接文件 my.conf pg_hba.conf XML 支持 有限的 XML 支持。 有限的 XML 支持。 数据访问和管理服务器 OPTIMIZE TABLE —— 回收未使用的空间并消除数据文件的碎片 myisamchk -analyze —— 更新查询优化器所使用的统计数据（MyISAM 存储引擎） mysql —— 命令行工具 MySQL Administrator —— 客户机 GUI 工具 Vacuum —— 回收未使用的空间 Analyze —— 更新查询优化器所使用的统计数据 psql —— 命令行工具 pgAdmin —— 客户机 GUI 工具 并发控制 支持表级和行级锁。InnoDB 存储引擎支持 READ_COMMITTED、READ_UNCOMMITTED、REPEATABLE_READ 和 SERIALIZABLE。使用 SET TRANSACTION ISOLATION LEVEL 语句在事务级设置隔离级别。 支持表级和行级锁。支持的 ANSI 隔离级别是 Read Committed（默认 —— 能看到查询启动时数据库的快照）和 Serialization（与 Repeatable Read 相似 —— 只能看到在事务启动之前提交的结果）。使用 SET TRANSACTION 语句在事务级设置隔离级别。使用 SET SESSION 在会话级进行设置。 **MySQL相对于PostgreSQL的劣势： **\nMySQL PostgreSQL 最重要的引擎InnoDB很早就由Oracle公司控制。目前整个MySQL数据库都由Oracle控制。 BSD协议，没有被大公司垄断。 对复杂查询的处理较弱，查询优化器不够成熟 很强大的查询优化器，支持很复杂的查询处理。 只有一种表连接类型:嵌套循环连接(nested-loop),不支持排序-合并连接(sort-merge join)与散列连接(hash join)。 都支持 性能优化工具与度量信息不足 提供了一些性能视图，可以方便的看到发生在一个表和索引上的select、delete、update、insert统计信息，也可以看到cache命中率。网上有一个开源的pgstatspack工具。 InnoDB的表和索引都是按相同的方式存储。也就是说表都是索引组织表。这一般要求主键不能太长而且插入时的主键最好是按顺序递增，否则对性能有很大影响。 不存在这个问题。 大部分查询只能使用表上的单一索引;在某些情况下，会存在使用多个索引的查询,但是查询优化器通常会低估其成本,它们常常比表扫描还要慢。 不存在这个问题 表增加列，基本上是重建表和索引，会花很长时间。 表增加列，只是在数据字典中增加表定义，不会重建表 存储过程与触发器的功能有限。可用来编写存储过程、触发器、计划事件以及存储函数的语言功能较弱 除支持pl/pgsql写存储过程，还支持perl、python、Tcl类型的存储过程：pl/perl，pl/python，pl/tcl。 也支持用C语言写存储过程。 不支持Sequence。 支持 不支持函数索引，只能在创建基于具体列的索引。 不支持物化视图。 支持函数索引，同时还支持部分数据索引，通过规则系统可以实现物化视图的功能。 执行计划并不是全局共享的, 仅仅在连接内部是共享的。 执行计划共享 MySQL支持的SQL语法(ANSI SQL标准)的很小一部分。不支持递归查询、通用表表达式（Oracle的with 语句）或者窗口函数（分析函数）。 都 支持 不支持用户自定义类型或域(domain) 支持。 对于时间、日期、间隔等时间类型没有秒以下级别的存储类型 可以精确到秒以下。 身份验证功能是完全内置的，不支持操作系统认证、PAM认证，不支持LDAP以及其它类似的外部身份验证功能。 支持OS认证、Kerberos 认证 、Ident 的认证、LDAP 认证、PAM 认证 不支持database link。有一种叫做Federated的存储引擎可以作为一个中转将查询语句传递到远程服务器的一个表上,不过,它功能很粗糙并且漏洞很多 有dblink，同时还有一个dbi-link的东西，可以连接到oracle和mysql上。 Mysql Cluster可能与你的想象有较大差异。开源的cluster软件较少。 复制(Replication)功能是异步的,并且有很大的局限性.例如,它是单线程的(single-threaded),因此一个处理能力更强的Slave的恢复速度也很难跟上处理能力相对较慢的Master. 有丰富的开源cluster软件支持。 explain看执行计划的结果简单。 explain返回丰富的信息。 类似于ALTER TABLE或CREATE TABLE一类的操作都是非事务性的.它们会提交未提交的事务，并且不能回滚也不能做灾难恢复 DDL也是有事务的。 PostgreSQL与MySQL比较 - 江湖一浪子 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%B7%AE%E5%BC%82/postgresql%E4%B8%8Emysql.html","summary":"PostgreSQL与MySQL 的区别 特性 MySQL PostgreSQL 实例 通过执行 MySQL 命令（mysqld）启动实例。一个实例可以管理一个或多个数据库。一台服务器可以运","title":"PostgreSQL与MySQL"},{"content":"1：sublime下载安装\n​ https://www.sublimetext.com/3 2:安装插件Package Control(注意版本问题 2/3)\n​ crtl+` 调出shell，粘贴下列代码：\nimport urllib.request,os,hashlib; h = \u0026#39;6f4c264a24d933ce70df5dedcf1dcaee\u0026#39; + \u0026#39;ebe013ee18cced0ef93d5f746d80ef60\u0026#39;; pf = \u0026#39;Package Control.sublime-package\u0026#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( \u0026#39;http://packagecontrol.io/\u0026#39; + pf.replace(\u0026#39; \u0026#39;, \u0026#39;%20\u0026#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(\u0026#39;Error validating download (got %s instead of %s), please try manual install\u0026#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), \u0026#39;wb\u0026#39; ).write(by) 3:退出重启sublime\n在菜单-\u0026gt;preferences-\u0026gt;Package Settings和package control选项，就说明安装package control成功了\n4: crtl+shift+p： 输入install ，分别安装以下插件(存在无法下载错误)\nAutoPep8、BracketHighligter、Trailing Spaces、\nConvertToUTF8、Anaconda、SFTP、AutoFileName、FileDiffs 、IMESupport 、SublimeCodeIntel\n**5:**修改Preferences\u0026ndash;\u0026gt;插件\u0026ndash;\u0026gt;Settings-User，添加对应的设置，可以直接使用我的设置或进行修改（需要对其python路径做修改，详情见附件sublimePluginsSettings）\n**6:**在windows下进行文件共享,使用SFTP插件\nhttp://blog.csdn.net/qwe1992314/article/details/52232426 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83.html","summary":"1：sublime下载安装 ​ https://www.sublimetext.com/3 2:安装插件Package Control(注意版本问题 2/3) ​ crtl+` 调出shell，粘贴下列代码： import urllib.request,os,hashlib; h = \u0026#39;6f4c264a24d933ce70df5dedcf1dcaee\u0026#39; + \u0026#39;ebe013ee18cced0ef93d5f746d80ef60\u0026#39;; pf = \u0026#39;Package","title":"python搭建idle开发环境"},{"content":"[toc]\n释放内存其实在每次处理命令时都会执行, 只是满足判断条件才执行 , 例如内存满了, 需要淘汰key等等条件, 若发现已用内存超出maxmemory，会计算需释放的内存量。这个 释放内存大小=已使用内存量-maxmemory。\nredis中的LRU实现 redis没有使用标准的LRU算法, 只是近似的LRU算法, 因为嫌LinkedList占用的空间太大了(因为起码要记录头尾指针)\n简述: redis通过计算每个key的闲置时间来决定是否要选它淘汰(全局时钟 减去 当前key的访问时钟), redis会随机选几个key, 它们的闲置时间都要大于一个阈值(其实会存入一个pool, 这个阈值就是pool中最小的闲置时间), 当内存不够时, 就从这几个key中淘汰闲置时间最大的值\n首先看一下全局时钟定义\n#define LRU_BITS 24 struct redisServer { pid_t pid; /* Main process pid. */ char *configfile; /* Absolute config file path, or NULL */ ….. unsigned lruclock:LRU_BITS; /* Clock for LRU eviction */ ... }; redisServer 中包含了redis服务器启动之后的基本信息(PID,配置文件路径,serverCron运行频率hz等),外部可调用模块信息，网络信息，RDB/AOF信息，日志信息，复制信息等等。\n上述结构体中lruclock:LRU_BITS,其中存储了服务器自启动之后的lru时钟，该时钟是全局的lru时钟。该时钟100ms更新一次。\n可以通过hz来调整,默认情况hz=10,因此每1000ms/10=100ms执行一次定时任务\n因此lrulock最大能到(2**24-1)/3600/24 = 194天,如果超过了这个时间，lrulock重新开始。对于redis server来说，server.lrulock表示的是一个全局的lrulock，\n如果全局时钟 小于 对象时钟, 则会再加上时钟最大值 REDIS_LRU_CLOCK_MAX , 也就是 194天 的秒数, 相当于是第二轮了\n/* Given an object returns the min number of seconds the object was never * requested, using an approximated LRU algorithm. */ #define REDIS_LRU_CLOCK_MAX ((1\u0026lt;\u0026lt;REDIS_LRU_BITS)-1) /* Max value of obj-\u0026gt;lru */ #define REDIS_LRU_CLOCK_RESOLUTION 1 /* LRU clock resolution in seconds */ unsigned long estimateObjectIdleTime(robj *o) { if (server.lruclock \u0026gt;= o-\u0026gt;lru) { return (server.lruclock - o-\u0026gt;lru) * REDIS_LRU_CLOCK_RESOLUTION; } else { return ((REDIS_LRU_CLOCK_MAX - o-\u0026gt;lru) + server.lruclock) * REDIS_LRU_CLOCK_RESOLUTION; } } 那么对于每个redisObject都有一个自己的lrulock。这样每redisObject就可以根据自己的lrulock和全局的server.lrulock比较，来确定是否能够被淘汰掉。\n再看一下 redisObject的实现\n#define REDIS_LRU_BITS 24 typedef struct redisObject { unsigned type:4; unsigned encoding:4; // 从这里看出, 这个字段在不同淘汰策略下, 存的内容是不一样的 unsigned lru:LRU_BITS; /* LRU time (relative to server.lruclock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits decreas time). */ int refcount; void *ptr; } robj 在Redis的dict中每次按key获取一个值的时候，都会调用lookupKey函数,如果配置使用了LRU模式,该函数会更新value中的lru字段为当前秒级别的时间戳\nrobj *lookupKey(redisDb *db, robj *key, int flags) { ... if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_LFU) { //如果配置的是lfu方式，则更新lfu updateLFU(val); } else { val-\u0026gt;lru = LRU_CLOCK();//否则按lru方式更新 } ... } 第一次随机选取的key都会放入一个pool中(pool的大小为16),pool中的key是按lru大小顺序排列的。接下来每次随机选取的key lru值必须大于pool中最小的lru才会继续放入，直到将pool放满。放满之后，每次如果有新的key需要放入，需要将pool中lru最大的一个key取出。淘汰的时候，直接从pool中选取一个lru最大的值然后将其淘汰。这样一来，每次移除的Key并不仅仅是随机选择的N个Key里面最大的，而且还是pool里面idle time最大的\n随机选的key的数量默认为5, 由 server.maxmemory_samples 控制, maxmemory_samples 就是样本大小, 值越大, 约接近LRU, 淘汰的正确率越高, 性能消耗也越大\n采用\u0026quot;pool\u0026quot;，把一个全局排序问题 转化成为了 局部的比较问题。(尽管排序本质上也是比较，囧)。要想知道idle time 最大的key，精确的LRU需要对全局的key的idle time排序，然后就能找出idle time最大的key了。但是可以采用一种近似的思想，即随机采样(samping)若干个key，这若干个key就代表着全局的key，把samping得到的key放到pool里面，每次采样之后更新pool，使得pool里面总是保存着随机选择过的key的idle time最大的那些key。需要evict key时，直接从pool里面取出idle time最大的key，将之evict掉。这种思想是很值得借鉴的。\nredis中的LFU实现 简述: 它和LRU规则一样, 利用在key中时间钟字段, 不过把内部时钟的24位分成两部分，前16位还代表时钟，后8位代表一个计数器。8位只能代表255，但是redis并没有采用线性上升的方式，而是结合增长因子来计数, 而且还有衰退因子来减少计数。也会和LRU一样, 存在一个淘汰池, 从淘汰池中redis会对内部时钟最小的key进行淘汰（最小表示最不频繁使用），注意这个过程也是根据策略随机选择键\n在LFU模式下，Redis对象头的24bit lru字段被分成两段来存储，高16bit存储ldt(Last Decrement Time)，低8bit存储logc(Logistic Counter)。\nldt(Last Decrement Time) 高16bit用来记录最近一次计数器降低的时间，由于只有8bit，存储的是Unix分钟时间戳取模2^16，16bit能表示的最大值为65535（65535/24/60≈45.5），大概45.5天会折返（折返指的是取模后的值重新从0开始）。\n源码如下:\n/* Return the current time in minutes, just taking the least significant * 16 bits. The returned time is suitable to be stored as LDT (last decrement * time) for the LFU implementation. */ // server.unixtime是Redis缓存的Unix时间戳 // 可以看出使用的Unix的分钟时间戳，取模2^16 unsigned long LFUGetTimeInMinutes(void) { return (server.unixtime/60) \u0026amp; 65535; } /* Given an object last access time, compute the minimum number of minutes * that elapsed since the last access. Handle overflow (ldt greater than * the current 16 bits minutes time) considering the time as wrapping * exactly once. */ unsigned long LFUTimeElapsed(unsigned long ldt) { // 获取系统当前的LFU time unsigned long now = LFUGetTimeInMinutes(); // 如果now \u0026gt;= ldt 直接取差值 if (now \u0026gt;= ldt) return now-ldt; // 如果now \u0026lt; ldt 增加上65535 // 注意Redis 认为折返就只有一次折返，多次折返也是一次，我思考了很久感觉这个应该是可以接受的，本身Redis的淘汰算法就带有随机性 return 65535-ldt+now; } logc(Logistic Counter) 低8位用来记录访问频次，8bit能表示的最大值为255，logc肯定无法记录真实的Rediskey的访问次数，其实从名字可以看出存储的是访问次数的对数值，每个新加入的key的logc初始值为5（LFU_INITI_VAL），这样可以保证新加入的值不会被首先选中淘汰；logc每次key被访问时都会更新；此外，logc会随着时间衰减。\nlogc 算法调整 redis.conf 提供了两个配置项，用于调整LFU的算法从而控制Logistic Counter的增长和衰减。\n-- 用于调整Logistic Counter的增长速度，lfu-log-factor值越大，Logistic Counter增长越慢。 lfu-log-factor 10 -- 用于调整Logistic Counter的衰减速度，它是一个以分钟为单位的数值，默认值为1,；lfu-decay-time值越大，衰减越慢。 lfu-decay-time 1 增长的源码:\n/* Logarithmically increment a counter. The greater is the current counter value * the less likely is that it gets really implemented. Saturate it at 255. */ uint8_t LFULogIncr(uint8_t counter) { // Logistic Counter最大值为255 if (counter == 255) return 255; // 取一个0~1的随机数r double r = (double)rand()/RAND_MAX; // counter减去LFU_INIT_VAL （LFU_INIT_VAL为每个key的Logistic Counter初始值，默认为5） double baseval = counter - LFU_INIT_VAL; // 如果衰减之后已经小于5了，那么baseval \u0026lt; 0取0 if (baseval \u0026lt; 0) baseval = 0; // lfu-log-factor在这里被使用 // 可以看出如果lfu_log_factor的值越大，p越小 // r \u0026lt; p的概率就越小，Logistic Counter增加的概率就越小（因此lfu_log_factor越大增长越缓慢） double p = 1.0/(baseval*server.lfu_log_factor+1); if (r \u0026lt; p) counter++; return counter; } 衰减的源代码：\n/* If the object decrement time is reached decrement the LFU counter but * do not update LFU fields of the object, we update the access time * and counter in an explicit way when the object is really accessed. * And we will times halve the counter according to the times of * elapsed time than server.lfu_decay_time. * Return the object frequency counter. * * This function is used in order to scan the dataset for the best object * to fit: as we check for the candidate, we incrementally decrement the * counter of the scanned objects if needed. */ unsigned long LFUDecrAndReturn(robj *o) { // 获取lru的高16位，也就是ldt unsigned long ldt = o-\u0026gt;lru \u0026gt;\u0026gt; 8; // 获取lru的低8位，也就是logc unsigned long counter = o-\u0026gt;lru \u0026amp; 255; // 根据配置的lfu-decay-time计算Logistic Counter需要衰减的值 unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0; if (num_periods) counter = (num_periods \u0026gt; counter) ? 0 : counter - num_periods; return counter; } LFU 与 LRU 有一个共同点，当内存达到max_memory时，选择key是随机抓取的，因此Redis为了使这种随机性更加准确，设计了一个淘汰池，这个淘汰池对于LFU和LRU算的都适应，只是淘汰池的排序算法有区别而已。\n扩展 淘汰池的更新策略 这个淘汰池就是使用LRU和LFU时, 需要淘汰key时, 是从淘汰池按时间钟来淘汰, 源码如下:\nvoid evictionPoolPopulate(int dbid, dict *sampledict, dict *keydict, struct evictionPoolEntry *pool) { int j, k, count; dictEntry *samples[server.maxmemory_samples]; count = dictGetSomeKeys(sampledict,samples,server.maxmemory_samples); for (j = 0; j \u0026lt; count; j++) { unsigned long long idle; sds key; robj *o; dictEntry *de; de = samples[j]; key = dictGetKey(de); /* If the dictionary we are sampling from is not the main * dictionary (but the expires one) we need to lookup the key * again in the key dictionary to obtain the value object. */ if (server.maxmemory_policy != MAXMEMORY_VOLATILE_TTL) { if (sampledict != keydict) de = dictFind(keydict, key); o = dictGetVal(de); } /* Calculate the idle time according to the policy. This is called * idle just because the code initially handled LRU, but is in fact * just a score where an higher score means better candidate. */ // 空闲时间的计算方式 LRU 、 LFU 、最快过期的可以 三种过期策略下 都是不同的 if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_LRU) { idle = estimateObjectIdleTime(o); } else if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_LFU) { /* When we use an LRU policy, we sort the keys by idle time * so that we expire keys starting from greater idle time. * However when the policy is an LFU one, we have a frequency * estimation, and we want to evict keys with lower frequency * first. So inside the pool we put objects using the inverted * frequency subtracting the actual frequency to the maximum * frequency of 255. */ idle = 255-LFUDecrAndReturn(o); } else if (server.maxmemory_policy == MAXMEMORY_VOLATILE_TTL) { /* In this case the sooner the expire the better. */ idle = ULLONG_MAX - (long)dictGetVal(de); } else { serverPanic(\u0026#34;Unknown eviction policy in evictionPoolPopulate()\u0026#34;); } /* Insert the element inside the pool. * First, find the first empty bucket or the first populated * bucket that has an idle time smaller than our idle time. */ k = 0; // 在这里判断 池中某一个key的空闲时间是否小于 当前key的空闲时间 while (k \u0026lt; EVPOOL_SIZE \u0026amp;\u0026amp; pool[k].key \u0026amp;\u0026amp; pool[k].idle \u0026lt; idle) k++; if (k == 0 \u0026amp;\u0026amp; pool[EVPOOL_SIZE-1].key != NULL) { /* Can\u0026#39;t insert if the element is \u0026lt; the worst element we have * and there are no empty buckets. */ continue; } else if (k \u0026lt; EVPOOL_SIZE \u0026amp;\u0026amp; pool[k].key == NULL) { /* Inserting into empty position. No setup needed before insert. */ } else { /* Inserting in the middle. Now k points to the first element * greater than the element to insert. */ if (pool[EVPOOL_SIZE-1].key == NULL) { /* Free space on the right? Insert at k shifting * all the elements from k to end to the right. */ /* Save SDS before overwriting. */ sds cached = pool[EVPOOL_SIZE-1].cached; memmove(pool+k+1,pool+k, sizeof(pool[0])*(EVPOOL_SIZE-k-1)); pool[k].cached = cached; } else { /* No free space on right? Insert at k-1 */ k--; /* Shift all elements on the left of k (included) to the * left, so we discard the element with smaller idle time. */ sds cached = pool[0].cached; /* Save SDS before overwriting. */ if (pool[0].key != pool[0].cached) sdsfree(pool[0].key); memmove(pool,pool+1,sizeof(pool[0])*k); pool[k].cached = cached; } } /* Try to reuse the cached SDS string allocated in the pool entry, * because allocating and deallocating this object is costly * (according to the profiler, not my fantasy. Remember: * premature optimizbla bla bla bla. */ int klen = sdslen(key); if (klen \u0026gt; EVPOOL_CACHED_SDS_SIZE) { pool[k].key = sdsdup(key); } else { memcpy(pool[k].cached,key,klen+1); sdssetlen(pool[k].cached,klen); pool[k].key = pool[k].cached; } pool[k].idle = idle; pool[k].dbid = dbid; } } Redis的LRU缓存淘汰算法实现 - 腾讯云开发者社区-腾讯云 (tencent.com) Redis中的LFU算法 - 再见紫罗兰 - 博客园 (cnblogs.com) Redis精通系列——LFU算法详述(Least Frequently Used - 最不经常使用)_李子捌的博客-CSDN博客_lfu算法 Redis的缓存淘汰策略LRU与LFU - 简书 (jianshu.com) Redis的LRU算法 - 演变历程详解 (cnblogs.com) 全面讲解LRU算法_Apple_Web的博客-CSDN博客_lru 算法 由浅入深介绍 Redis LRU 策略的具体实现 \u0026ndash; Redis中国用户组（CRUG） ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/redis%E7%9A%84%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5lru%E4%B8%8Elfu.html","summary":"[toc] 释放内存其实在每次处理命令时都会执行, 只是满足判断条件才执行 , 例如内存满了, 需要淘汰key等等条件, 若发现已用内存超出maxmemory，","title":"Redis的淘汰策略LRU与LFU"},{"content":"[toc]\n前言 Redis目前基本的数据类型有String、List、Set、ZSet、Hash五种，首先Redis是C语言开发的，所以底层就是用C语言封装数据结构或者C语言本身提供的数据结构来存储。\nredis内部的主要数据结构主要有简单字符串（SDS）、双端链表、字典、压缩列表、跳跃表、整数集合。\nRedis内部并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统-redisObject，这个对象系统包含了我们所熟知的五种基本类型数据，也就是字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象。redisObject 是 Redis 类型系统的核心, 数据库中的每个键、值, 以及 Redis 本身处理的参数, 都表示为这种数据类型。\n而它们每一种对象都使用到了至少一种前面所介绍的数据结构。下面介绍一下redis内部的主要几个数据结构简单字符串（SDS）、双端链表、压缩列表、跳跃表的定义。然后再介绍一下redis基本的五种数据类型，也就是五种类型的对象用到了上面的哪些数据结构。\n![image-20220727113350889](.\redis底层数据结构.assets\\image-20220727113350889.png)\nRedis相关知识\u0026mdash;-对象机制_小舟~的博客-CSDN博客 1. redis的底层数据结构 1.1 SDS（Simple Dynamic String）简单字符串 1、redis定义：\n//定义了一个char 指针 typedef char *sds; /* Note: sdshdr5 is never used, we just access the flags byte directly. *\tsdshdr5 已经不用了, 在处理时会自动转为 sdshdr8 * However is here to document the layout of type 5 SDS strings. */ struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 前 3 位保存类型，后 5 位保存字符串长度 */ char buf[]; }; //lsb 代表有效位的意思， //__attribute__ ((__packed__)) 代表structure 采用手动对齐的方式。 struct __attribute__ ((__packed__)) sdshdr8 { //buf 已经使用的长度 uint8_t len; /* used */ //buf 分配的长度，等于buf[]的总长度-1，因为buf有包括一个/0的结束符 uint8_t alloc; /* excluding the header and null terminator */ //只有3位有效位，因为类型的表示就是0到4，所有这个8位的flags 有5位没有被用到 unsigned char flags; /* 3 lsb of type, 5 unused bits */ //实际的字符串存在这里 char buf[]; }; // 还定义了其他长度的类型, 与上面的变化只有len和alloc， 就是长度不同而已 struct __attribute__ ((__packed__)) sdshdr16 { }; struct __attribute__ ((__packed__)) sdshdr32 { }; struct __attribute__ ((__packed__)) sdshdr64 { }; 这么多类型, 是为了针对不同的数据大小, 因为统一用int的话, 就占太多空间了(int 是 4字节)\nflags字段用来表示 是sdshdr8类型还是其他类型, 因为总共定义了5种类型, 所以需要8bit , ( 2^2 \u0026lt; 5 \u0026lt; 2^3 )\n2、使用范围：在redis里面，C本身的字符串只会作为字符串字面量（String literal）只用在一些不必对字符串值修改的地方，比如打印日志。 而redis需要使用字符串存储并且会修改的地方，都使用了SDS来存储。例如Key/vaule值。\n**3、优点：**使用SDS来存储字符串的优点：\nSDS的len属性直接记录了长度，获取字符串长度的复杂度为O(1)。\nC字符串本身不记录长度容易产生缓存区溢出，而SDS杜绝了缓冲区的溢出。\nC字符串本身不记录长度，每次修改都要重新分配内存，SDS减少了重新分配内存次数。\n因为有alloc字段, 可以预先分配内存\n优化了字符串缩短操作。并且可以保存任意格式的二进制数据，而C字符串必须含有编码。\nc语言中, 字符串是用 表示文本结尾的, 而二进制压缩后是会出现的 , SDS是利用len字段来表示真实的数据有多长, 按长度取值\nredis sds - 知乎 (zhihu.com) 读书笔记：Redis设计之动态字符串SDS - Jefferywang的烂笔头 (wangjunfeng.com) redis 系列，要懂redis，首先得看懂sds c++基础之uint8_t_时光机ﾟ的博客-CSDN博客_uint8_t Redis 源码解析 3：字符串 SDS - 小新是也 - 博客园 (cnblogs.com) 1.2 链表 (list) **1、链表：**listNode结构来保存，多个listNode可以形成双向链表，redis定义了list表示头节点来持有链表，下图分别是节点listNode和链表list的定义。\n2、redis定义：\n节点listNode 链表list **3、使用范围：**链表在redis中用作了列表键、发布与订阅、慢查询、监视器等\n1.3 跳跃表（skiplist） **1、跳跃表：**是一种有序得数据结构，通过在每个节点上维持多个指向其他节点的指针，从而达到快速访问的目的，可以理解为改进版的双端链表，改进的手段是通过空间换取了时间。\n在 Redis 中跳表是有序集合（zset）的底层实现之一。\n功能一：zset支持快速插入和删除\n对应的解决思路：针对快速插入和删除，有没有想到什么？首选肯定是链表，所以，底层基础得有一个value和score组成的node连接起来的链表。\n功能二：zset有序且支持范围查询\n对应的解决思路：有序这个条件，我们可以先让链表按照顺序排列，但查找来说，链表的查询时间复杂度为O(n)，并不高效，还要满足范围查找，如何解决这个问题？那么这时候就想到，能不能给链表做一个索引，提高它的查找效率的同时，让它也能支持范围查找，构建索引的话，是为了提高效率，如果只构建一层索引，数据量小的时候无所谓，但数据量大的时候呢？好像无法起到根本上提升效率的作用，所以应该给链表添加多级索引，简单示意图如下所示（引用自极客时间王争）：\n跳表结构:\n数据结构之Redis-跳表 - 简书 (jianshu.com) Redis 学习笔记（篇三）：跳表 - 风中抚雪 - 博客园 (cnblogs.com) **2、复杂度：**跳跃表支持平均O(logN)、最坏O(N)的查找复杂度，大部分条件下，跳跃表的效率可以和平衡树媲美，并且实现比平衡树简单。\n跳跃表节点zskiplistNode 跳跃表zskiplist 3、跳跃表结构图： 仔细观察上图跳跃表的结构后，发现如果节点的层数越高，那么这个节点访问其他节点的速度就越快。换言之，level越高，代表了这个跳跃表的查找效率可能会比较高。当然并不是绝对的，因为redis每次创建跳跃表节点时，程序是根据幂次定律(越大的数出现概率越小)， 生成层数高度。同时，节点的顺序是根据每个节点的分值排序的，如果分值相同，那么根据对象的大小排序。\n1.4 压缩列表（ziplist） **1、压缩列表：**是redis为了节省内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构，一个压缩列表的可以包含多个节点，每个节点可以保存一个字节数组或者一个整数值。\n2、压缩列表结构图： 3、压缩列表特点：\n是一种为节省内存开发的顺序性数据结构 可以包含多个节点，每个节点保存一个字节数组或者整数值 添加新节点到压缩列表，或者从压缩列表删除节点，可能会引发连锁更新操作，但是机率不高 1.5 字典(dict) dict（字典） 是 Redis 中哈希键和有序集合键的底层实现之一。\n可以看到图中，当我给一个 哈希结构中放了两个短的值，此时 哈希的编码方式是 ziplist, 而当我插入一个比较长的值，哈希的编码方式成为了 hashtable.\n![image-20220727145427750](.\redis底层数据结构.assets\\image-20220727145427750.png)\ntypedef struct dict{ // 类型特定函数 dictType *type; // 私有数据 void *private; // 哈希表 dictht ht[2]; // rehash 索引，当当前的字典不在 rehash 时，值为-1 int trehashidx; } type 和 private\n这两个属性是为了实现字典多态而设置的，当字典中存放着不同类型的值，对应的一些复制，比较函数也不一样，这两个属性配合起来可以实现多态的方法调用。\nht[2] 这是一个长度为 2 的 dictht结构的数组，dictht就是哈希表。\ntrehashidx 这是一个辅助变量，用于记录 rehash 过程的进度，以及是否正在进行 rehash 等信息。\n看完字段介绍，我们发现，字典这个数据结构，本质上是对 hashtable的一个简单封装，因此字典的实现细节主要就来到了 哈希表上。\n哈希表 哈希表的定义如下：\ntypedef struct dictht{ // 哈希表的数组 dictEntry **table; // 哈希表的大小 unsigned long size; // 哈希表的大小的掩码，用于计算索引值，总是等于 size-1 unsigned long sizemasky; // 哈希表中已有的节点数量 unsigned long used; } 其中哈希表中的节点的定义如下：\ntypedef struct dictEntry{ // 键 void *key; // 值 union { void *val; uint64_tu64; int64_ts64; }v; // 指向下一个节点的指针 struct dictEntry *next; } dictEntry; ![image-20220727145608599](.\redis底层数据结构.assets\\image-20220727145608599.png)\n上图是一个没有处在 rehash 状态下的字典。可以看到，字典持有两张哈希表，其中一个的值为 null, 另外一个哈希表的 size=4, 其中两个位置上已经存放了具体的键值对，而且没有发生 hash 冲突。\n哈希冲突 Redis 的哈希表处理 Hash 冲突的方式和 Java 中的 HashMap 一样，选择了分桶的方式，也就是常说的链地址法。Hash 表有两维，第一维度是个数组，第二维度是个链表，当发生了 Hash 冲突的时候，将冲突的节点使用链表连接起来，放在同一个桶内。\n由于第二维度是链表，我们都知道链表的查找效率相比于数组的查找效率是比较差的。那么如果 hash 冲突比较严重，导致单个链表过长，那么此时 hash 表的查询效率就会急速下降。\n因为有冲突所以需要扩容\n扩容与缩容 当哈希表过于拥挤，查找效率就会下降，当 hash 表过于稀疏，对内存就有点太浪费了，此时就需要进行相应的扩容与缩容操作。\n想要进行扩容缩容，那么就需要描述当前 hasd 表的一个填充程度，总不能靠感觉。这就有了 负载因子 这个概念。\n负载因子是用来描述哈希表当前被填充的程度。计算公式是：负载因子=哈希表以保存节点数量 / 哈希表的大小.\n在 Redis 的实现里，扩容缩容有三条规则：\n当 Redis 没有进行 BGSAVE 相关操作，且 负载因子\u0026gt;1的时候进行扩容。\n负载因子\u0026gt;5的时候，强行进行扩容。\n当负载因子\u0026lt;0.1的时候，进行缩容。\n根据程序当前是否在进行 BGSAVE 相关操作，扩容需要的负载因子条件不相同。\n这是因为在进行 BGSAVE 操作时，存在子进程，操作系统会使用 写时复制 (Copy On Write) 来优化子进程的效率。Redis 尽量避免在存在子进程的时候进行扩容，尽量的节省内存。\n熟悉 hash 表的读者们应该知道，扩容期间涉及到到 rehash 的问题。\n因为需要将当前的所有节点挪到一个大小不一致的哈希表中，且需要尽量保持均匀，因此需要将当前哈希表中的所有节点，重新进行一次 hash. 也就是 rehash.\n渐进式 hash 原理 在 Java 的 HashMap 中，实现方式是 新建一个哈希表，一次性的将当前所有节点 rehash 完成，之后释放掉原有的 hash 表，而持有新的表。\n而 Redis 不是，Redis 使用了一种名为渐进式 hash 的方式来满足自己的性能需求。对于单线程的 Redis 来说，表示很难接受这样的延时，因此 Redis 选择使用 一点一点搬的策略。\nRedis 实现了渐进式 hash. 过程如下：\n假如当前数据在 ht[0] 中，那么首先为 ht[1] 分配足够的空间。 在字典中维护一个变量，rehashindex = 0. 用来指示当前 rehash 的进度。 在 rehash 期间，每次对 字典进行 增删改查操作，在完成实际操作之后，都会进行 一次 rehash 操作，将 ht[0] 在rehashindex 位置上的值 rehash 到 ht[1] 上。将 rehashindex 递增一位。 随着不断的执行，原来的 ht[0] 上的数值总会全部 rehash 完成，此时结束 rehash 过程。 将 rehashindex 置为-1. 在上面的过程中有两个问题没有提到：\n假如这个服务器很空余呢？中间几小时都没有请求进来，那么同时保持两个 table, 岂不是很浪费内存？ 解决办法是：在 redis 的定时函数里，也加入帮助 rehash 的操作，这样子如果服务器空闲，就会比较快的完成 rehash.\n迁移时不是一个一个的移动, 而是按hash桶的维度, 一次一百个的移动\n在保持两个 table 期间，该哈希表怎么对外提供服务呢？ 解决办法：对于添加操作，直接添加到 ht[1] 上，因此这样才能保证 ht[0] 的数量只会减少不会增加，才能保证 rehash 过程可以完结。而删除，修改，查询等操作会在 ht[0] 上进行，如果得不到结果，会去 ht[1] 再执行一遍。\n与此同时，渐进式 hash 也带来了一个问题，那就是 在 rehash 的时间内，需要保存两个 hash 表，对内存的占用稍大，而且如果在 redis 服务器本来内存满了的时候，突然进行 rehash 会造成大量的 key 被抛弃。\n小应用 假设有两张表，一张工作量表，一张积分表，积分=工作量*系数。 系数是有可能改变的，当系数发生变化之后，需要重新计算所有过往工作量的对应新系数的积分情况。 而工作量表的数据量比较大，如果在系数发生变化的一瞬间开始重新计算，可以会导致系统卡死，或者系统负载上升，影响到在线服务。 怎么解决这个问题？\n我们只需要额外记录一个标志着正在进行重新计算过程中的变量即可。之后的思路就完全和 Redis 一致了。\n首先我们可以在某个用户请求自己的积分的时候，再帮他计算新的积分。来分散系统压力。 如果系统压力并不大，可以在系统定时任务里重算一小部分（一个 batch), 具体多少可以由数据量决定。 2. Redis五种基本数据类型 上面提到过，redis并没有使用上面的数据结构直接用来实现键值对数据库，也就是常说的五种基本数据类型，而是创建了一个对象系统，这个系统包含了字符串对象，列表对象、哈希对象、集合对象、和有序集合对象这五种基本数据类型。这样做有一个好处是，我们可以针对不同的场景，对相同的数据类型对象使用不同的数据结构，来优化提高效率\n也就是说每种基本类型都是使用好几种对象来实现的\nredisObject对象 **1、对象：**redis的键值对都是一个redisObject结构，该结构中有三个属性，type类型属性、encoding编码属性、ptr指向底层数据结构属性。\nredisObject对象定义 数据库的key值都是一个string字符串对象 2、编码常量： 2.1 String类型 字符串对象的编码是int、raw、embstr。参考上面的编码常量表，也就是说字符串类型的数据底层的数据结构使用的是整数、SDS、embstr编码的SDS。\n1、编码转换\n即上述几种编码会在何时转换，也就是redis底层决定用什么存储字符串数据？。\n当int类型的编码通过操作存储的是字符串值，那么字符串对象的编码将从int变为raw。\n2.2 List类型 列表对象的编码可以是zipList压缩列表和linkedlist双端链表。\n1、编码转换\n即上述两种编码会在何时转换，也就是redis底层什么时候会用压缩列表存储列表数据？什么时候会使用双端链表存储列表数据。 当列表同时满足以下两个条件时，列表对象会使用zipList编码，也就是压缩列表\n列表对象保存的所有字符串元素的长度都小于64字节 列表保存的元素少于512个， 2、配置\n上述两个条件是支持配置的，也就是说我们可以redis直接读取我们的配置，来决定列表list类型底层使用什么样的数据结构来存储数据\nlist-max-ziplist-value list-max-ziplist-entries 2.3 Set类型 集合对象使用的是intset整数集合(intset底层使用的是整数集合数据结构)或者hashtable哈希表（hashtable底层使用的是字典数据结构）\n1、编码转换\n当集合对象同时满足下面两个条件，会使用intset编码\n集合对象保存的所有对象都是整数值 集合对象保存的元素数量小于512个； 2、配置\n上述第二个条件是支持配置的。\nset-max-intset-entries 2.4 ZSet类型 有序集合的编码使用的是ziplist压缩列表和skiplist跳跃表。\n**注意：**上面介绍skiplist的时候我们可以从结构图中明显看到存储集合元素的时候，score在每个节点中式如何存储的。那么如果ZSet使用的式ziplist压缩列表，redis怎么存储score和value值呢？其实很简单，每个集合的元素都使用两个节点来存储，第一个节点保存的是成员（member），第二个元素保存的是元素的分值（score）\n1、编码转换\n当有序集合对象可以同时满足以下两个条件时，使用ziplist编码\n有序集合的所有元素长度都小于64字节 有序集合的元素数量小于128个； 2、配置\n上述两个条件是支持配置的。\nzset-max-ziplist-value zset-max-ziplist-entries 2.5 Hash类型 哈希对象使用的是ziplist压缩列表或hashtable哈希表。（hashtable底层使用的是字典数据结构，我们并没有在本文做详细介绍，有需要可以自己了解）\n1、编码转换\n当哈希对象同时满足下面两个条件，使用ziplist压缩列表\n哈希对象保存的所有键值对的键和值的字符串长度都小于64字节 哈希对象保存的键值对的数量小于512个； 2、配置\n上述两个条件是支持配置的。\nhash-max-ziplist-value hash-max-ziplist-entries Redis基本数据类型底层数据结构 - coffeebabe - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html","summary":"[toc] 前言 Redis目前基本的数据类型有String、List、Set、ZSet、Hash五种，首先Redis是C语言开发的，所以底层就是用C语","title":"redis底层数据结构"},{"content":"[toc]\n1. RowKey的作用 1.1 介绍 Rowkey是每一行的主键,在每行的开头, 也是可以存储数据的,只是它具有索引的作用,一般不是存在业务数据,而是发挥索引作用,所以里面填充什么值很重要,此为Rowkey设计\n1.2 RowKey在查询中的作用 HBase中RowKey可以唯一标识一行记录，在HBase中检索数据有以下三种方式：\n通过 get 方式，指定 RowKey 获取唯一一条记录 通过 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 链接：https://www.jianshu.com/p/89bcd80890d6\n当大量请求访问HBase集群的一个或少数几个节点，造成少数RegionServer的读写请求过多、负载过大，而其他RegionServer负载却很小，这样就造成热点现象。大量访问会使热点Region所在的主机负载过大，引起性能下降，甚至导致Region不可用。所以我们在向HBase中插入数据的时候，应尽量均衡地把记录分散到不同的Region里去，平衡每个Region的压力。\n下面根据一个例子分别介绍下根据RowKey进行查询的时候支持的情况。 如果我们RowKey设计为uid+phone+name，那么这种设计可以很好的支持一下的场景:\nuid=873969725 AND phone=18900000000 AND name=zhangsan uid= 873969725 AND phone=18900000000 uid= 873969725 AND phone=189? uid= 873969725 难以支持的场景：\nphone=18900000000 AND name = zhangsan phone=18900000000 name=zhangsan 从上面的例子中可以看出，在进行查询的时候，根据RowKey从前向后匹配，所以我们在设计RowKey的时候选择好字段之后，还应该结合我们的实际的高频的查询场景来组合选择的字段，越高频的查询字段排列越靠左。\n1.3 RowKey在Region中的作用 在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有StartRowKey和StopRowKey，这是表示 Region 存储的 RowKey 的范围，HBase 表的数据时按照 RowKey 来分散到不同的 Region，要想将数据记录均衡的分散到不同的Region中去，因此需要 RowKey 满足这种散列的特点。此外，在数据读写过程中也是与RowKey 密切相关，RowKey在读写过程中的作用：\n读写数据时通过 RowKey 找到对应的 Region； MemStore 中的数据是按照 RowKey 的字典序排序； HFile 中的数据是按照 RowKey 的字典序排序。 2. RowKey设计原则 长度原则\nRowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节，原因如下：\n数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。 MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。 目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。\n唯一原则\n必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。\n排序原则\nHBase的RowKey是按照ASCII有序排序的，因此我们在设计RowKey的时候要充分利用这点。\n散列原则\n设计的RowKey应均匀的分布在各个HBase节点上。\n3. RowKey字段选择 RowKey字段的选择，遵循的最基本原则是唯一性，RowKey必须能够唯一的识别一行数据。无论应用的负载特点是什么样，RowKey字段都应该参考最高频的查询场景。数据库通常都是以如何高效的读取和消费数据为目的，而不是数据存储本身。然后，结合具体的负载特点，再对选取的RowKey字段值进行改造，组合字段场景下需要重点考虑字段的顺序。\n4. 避免数据热点的方法 在对HBase的读写过程中，如何避免热点现象呢？主要有以下几种方法：\nReversing\n如果经初步设计出的RowKey在数据分布上不均匀，但RowKey尾部的数据却呈现出了良好的随机性，此时，可以考虑将RowKey的信息翻转，或者直接将尾部的bytes提前到RowKey的开头。Reversing可以有效的使RowKey随机分布，但是牺牲了RowKey的有序性。\n缺点：\n利于Get操作，但不利于Scan操作，因为数据在原RowKey上的自然顺序已经被打乱。\nSalting\nSalting（加盐）的原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之间的RowKey的开头不同。随机数能保障数据在所有Regions间的负载均衡。\n缺点：\n因为添加的是随机数，基于原RowKey查询时无法知道随机数是什么，那样在查询的时候就需要去各个可能的Regions中查找，Salting对于读取是利空的。并且加盐这种方式增加了读写时的吞吐量。\nHashing 基于 RowKey 的完整或部分数据进行 Hash，而后将Hashing后的值完整替换或部分替换原RowKey的前缀部分。这里说的 hash 包含 MD5、sha1、sha256 或 sha512 等算法。\n缺点：\n与 Reversing 类似，Hashing 也不利于 Scan，因为打乱了原RowKey的自然顺序。\n时间戳反转\n一个常见的数据处理问题是快速获取数据的最新版本，使用反转的时间戳作为RowKey的一部分对这个问题十分有用，可以用Long.Max_Value - timestamp追加到key的末尾。 举例: 在推贴流表里，你使用倒序时间戳（Long.MAX_VALUE - 时间戳）然后附加上用户ID来构成行健。现在你基于用户ID扫描紧邻的n行就可以找到用户需要的n条最新推帖。这里行健的结构对于读性能很重要。把用户ID放在开头有助于你设置扫描，可以轻松定义起始键。\nhttps://www.cnblogs.com/swordfall/p/10597802.html 5. 总结 在HBase的使用过程，设计RowKey是一个很重要的一个环节。我们在进行RowKey设计的时候可参照如下步骤：\n结合业务场景特点，选择合适的字段来做为RowKey，并且按照查询频次来放置字段顺序 通过设计的RowKey能尽可能的将数据打散到整个集群中，均衡负载，避免热点问题 设计的RowKey应尽量简短\n链接：https://www.jianshu.com/p/89bcd80890d6 https://www.cnblogs.com/duanxz/p/4660784.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/rowkey.html","summary":"[toc] 1. RowKey的作用 1.1 介绍 Rowkey是每一行的主键,在每行的开头, 也是可以存储数据的,只是它具有索引的作用,一般不是存在业务数据,而是发","title":"Rowkey"},{"content":"[TOC]\n1. 每个租户提供独立的数据库系统 这种方案的实现方式是所有租户共享同一个应用，但应用后端会连接多个数据库系统，一个租户单独使用一个数据库系统。这种方案的用户数据隔离级别最高，安全性最好，租户间的数据能够实现物理隔离。但成本较高。\n2. 每个租户提供独立的表空间 这种方案的实现方式，就是所有租户共享同一个应用，应用后端只连接一个数据库系统，所有租户共享这个数据库系统，每个租户在数据库系统中拥有一个独立的表空间。\n3. 按租户id字段区分租户 这种方案是多租户方案中最简单的设计方式，即在每张表中都添加一个用于区分租户的字段（如租户id或租户代码）来标识每条数据属于哪个租户，其作用很像外键。当进行查询的时候每条语句都要添加该字段作为过滤条件，其特点是所有租户的数据全都存放在同一个表中，数据的隔离性是最低的，完全是通过字段来区分的。\n三种数据隔离方案的优劣势分析 隔离方案 成本 支持租户数量 优点 不足 独立数据库系统 高 少 隔离级别最高，安全性最好，能够满足不同租户的独特需求，出现故障时恢复数据比较容易 维护成本和购置成本高, 数据收集分析成本高 共享数据库，独立表空间 中 较多 提供了一定程度的逻辑数据隔离，一个数据库系统可支持多个租户 统一数据库支持的Schema有限, 夸租户统计数据复杂 按租户id字段区分 低 非常多 维护和购置成本最低，每个数据库能够支持的租户数量最多 隔离级别最低，安全性也最低，数据备份和恢复非常复杂，需要逐表逐条备份和还原 saas系统多租户数据隔离的实现（一）数据隔离方案 - 湖畔清茶杨柳飘 - 博客园 (cnblogs.com) SAAS服务多租户数据隔离架构 - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E6%9E%B6%E6%9E%84/saas%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9A%94%E7%A6%BB.html","summary":"[TOC] 1. 每个租户提供独立的数据库系统 这种方案的实现方式是所有租户共享同一个应用，但应用后端会连接多个数据库系统，一个租户单独使用一个数据库系统。","title":"Saas的数据隔离"},{"content":"云计算机经过这么多年的发展，逐渐进化到用户仅需关注业务和所需的资源。通过Swarm、K8S这些编排工具，容器服务让开发者的体验达到很完美的境界。我曾经觉得Docker可以替代虚机，用户只要关注自己的计算和需要的资源就行，不需要操心到机器这一层。但是因为Docker对资源的隔离不够好，各大云厂商的做法还是一个Docker对应一台虚机，不仅成本高，给用户暴露虚机也多余了。\n用户为什么需要关注业务运行所需要的CPU、内存、网络情况？还有没有更好的解决方案？Serverless架构应运而生，让人们不再操心运行所需的资源，只需关注自己的业务逻辑，并且为实际消耗的资源付费。可以说，随着Serverless架构的兴起，真正的云计算时代才算到来了。(其实就是把这部分管理托管出去,就像现在的买阿里服务器一样,不要维护服务器,只管用就行)\n容器在开发模式方面并没有提出新的想法，大家还是在用传统的那一套开发模式，需要写一个大而全的后端服务。与之对比，Serverless架构是事件驱动的，这样让后端的开发体验变得跟前端和移动端很类似了。针对不同客户的需求，先让其购买好相关的资源，然后一个个填坑，给不同的产品添加各种事件处理逻辑就行。这就跟iOS开发一样，界面写出来，然后处理一个个事件就好了，大家都很容易理解这种开发模式。\nService Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。\n来自:https://blog.csdn.net/weixin_33845477/article/details/87941863\nServerless的主要优点\n开发者更加专注于业务逻辑，开发效率更高。开发一个典型的服务器端项目，需要花很多时间处理依赖、线程、日志、发布和使用服务、部署及维护等相关的工作，基于Serverless架构则不需要操心这些工作。 用户为实际使用的资源付费。用户购买的ECS使用时间一般不到5成，但是为另外5成闲置时间付费了。Lambda按照运行的时间收费，成本会低很多。 NO Architecture，NO Ops。架构师的责任是设计一个高可用、高扩展的架构。运维负责整个系统稳定可靠地运行，适当缩减和增加资源。大型云厂商能保证产品的高可用，Serverless架构本身就是高扩展的。Serverless不再需要服务器端的工作人员，给客户节省了大量的资源。架构师和运维的同学应该好好思考一下未来的出路了。架构师可以转型去做销售，整理用户的需求，然后写写CloudFormation的template就好了。 还是成本。IT行业一些领先的公司基础设施非常完善，开发工程师写好代码，然后通过发布平台发布，感觉也是挺方便的。比起Serverless的架构，成本还是要高不少。 机器成本。日常、预发、线上，1+1+2=4台服务器少不了。 时刻要关注业务数据，盘点资源，看看是否需要扩容和缩减资源。扩容容易，缩减难，造成大量资源闲置。 全链路压测是不是很烦？ Serverless的主要缺点\n排查问题困难，因为逻辑散落在各处，一个操作可能触发成百上千个Lambda执行。AWS的X-Ray和CloudWatch等产品可以帮助用户排查问题。 准备runtime需要时间，流量瞬间爆发容易导致超时。 带状态的Lambda写起来很困难。 Lambda运行有诸多资源限制，比如运行时长、内存、磁盘、打开的文件数量等。 厂商锁定。云计算是赢者通吃的行业，大而全的云厂商优势巨大，Serverless加剧了这种趋势。以前用户还需要自己写很多服务器端的逻辑，迁移的时候，把服务器端代码重新部署一下。采用Serverless架构之后，代码都是各个平台的Lambda代码片段，没法迁移。从客户的角度来看，是不希望自己被某家云厂商所绑架的。所以云计算行业需要做很多标准化的工作，方便用户无缝在各种云之间迁移。 链接：https://www.jianshu.com/p/51a19ef5f8cf\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/serviceless.html","summary":"云计算机经过这么多年的发展，逐渐进化到用户仅需关注业务和所需的资源。通过Swarm、K8S这些编排工具，容器服务让开发者的体验达到很完美的境","title":"Serviceless"},{"content":"settings.xml 中的参数看文件中的描述即可\n仓库和镜像的区别\n其中有两个标签, ,, 它们写的东西差不多,前者是仓库,后者是仓库\n​\trepository是指在局域网内部搭建的repository，它跟central repository, jboss repository等的区别仅仅在于其URL是一个内部网址 . ​\tmirror则相当于一个代理，它会拦截去指定的远程repository下载构件的请求，然后从自己这里找出构件回送给客户端。配置mirror的目的一般是出于网速考虑\n不过，很多internal repository搭建工具往往也提供mirror服务，比如Nexus就可以让同一个URL,既用作internal repository，又使它成为所有repository的mirror\n\u0026lt;settings\u0026gt; ... \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;maven.net.cn\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;one of the central mirrors in china\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.net.cn/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; ... \u0026lt;/settings\u0026gt; 表示该镜像所指向的仓库id是哪个?\n该例中，的值为central，表示该配置为中央仓库的镜像，任何对于中央仓库的请求都会转至该镜像，用户也可以使用同样的方法配置其他仓库的镜像。另外三个元素id,name,url与一般仓库配置无异，表示该镜像仓库的唯一标识符、名称以及地址\nMaven还支持更高级的镜像配置：\n* 匹配所有远程仓库。 external:* 匹配所有远程仓库，使用localhost的除外，使用file://协议的除外。也就是说，匹配所有不在本机上的远程仓库。 repo1,repo2 匹配仓库repo1和repo2，使用逗号分隔多个远程仓库。 *,!repo1 匹配所有远程仓库，repo1除外，使用感叹号将仓库从匹配中排除。 这里的仓库配置,指的在pom.xml中配置的; 好像在也能配置\n\u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;jdk14\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Repository for JDK 1.4 builds\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://www.myhost.com/maven/jdk14\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/settings%E6%96%87%E4%BB%B6.html","summary":"settings.xml 中的参数看文件中的描述即可 仓库和镜像的区别 其中有两个标签, ,, 它们写的东西差不多,前者是仓库,后者是仓库 ​ repository是指在局域网内","title":"settings文件"},{"content":"[toc]\nif 一、条件测试的表达式：\n[ expression ] 括号两端必须要有空格\n[[ expression ]] 括号两端必须要有空格\ntest expression 组合测试条件：\n-a: and -o: or !: 非 二、整数比较：\n-eq 测试两个整数是否相等 -ne 测试两个整数是否不等 -gt 测试一个数是否大于另一个数 -lt 测试一个数是否小于另一个数 -ge 大于或等于 -le 小于或等于 命令间的逻辑关系\n逻辑与：\u0026amp;\u0026amp; 第一个条件为假 第二个条件不用在判断，最总结果已经有 第一个条件为真，第二个条件必须得判断 逻辑或：|| 三、字符串比较\n字符串比较： == 等于 两边要有空格 != 不等 \u0026gt; 大于 \u0026lt; 小于 四、文件测试\n-z string 测试指定字符是否为空，空着真，非空为假 -n string 测试指定字符串是否为不空，空为假 非空为真 -e FILE 测试文件是否存在 -f file 测试文件是否为普通文件 -d file 测试指定路径是否为目录 -r file 测试文件对当前用户是否可读 -w file 测试文件对当前用户是否可写 -x file 测试文件对当前用户是都可执行 -z 是否为空 为空则为真 -a 是否不空 五、if 语法\nif 判断条件 0为真 其他都为假\n单分支if语句\nif 判断条件；then statement1 statement2 ....... fi 双分支的if语句：\nif 判断条件；then statement1 statement2 ..... else statement3 statement4 fi Note:\nif 语句进行判断是否为空\n[ \u0026quot;$name” = \u0026quot;\u0026quot; ]\n等同于\n[ ! \u0026quot;$name\u0026quot; ]\n[ -z \u0026quot;$name\u0026quot; ]\nNote:\n使用if语句的时候进行判断如果是进行数值类的 ，建议使用 let(())进行判断\n对于字符串等使用test[ ] or [[ ]] 进行判断\n(())中变量是可以不使用$来引用的\nexample：表述数字范围的时候 可以使用if可以是使用case\nif [ $x -gt 90 -o $x -lt 100 ] case $x in 100) 9[0-9]) if [ \u0026quot;X$name\u0026quot; != \u0026quot;x\u0026quot; ]\n这个语句的意思是如果$name为空，那么X=X成立折执行下面的结果；\n写脚本的时候很多时候需要用到回传命令，$?如果上一个命令执行成功，回传值为0，否则为1~255之间的任何一个\n0为真 非0为假\n来自: https://blog.51cto.com/lovelace/1211353 引号/单引号/反引号 双引号（\u0026quot; \u0026ldquo;）：在双引号中，除了$, \u0026lsquo;\u0026rsquo;, `和\\以外所有的字符都解释成字符本身。也就说可以读取变量 单引号（\u0026rsquo; \u0026lsquo;）：在单引号中所有的字符包括特殊字符（$,\u0026rsquo;\u0026rsquo;,`和\\）都将解释成字符本身而成为普通字符。 反引号（` `）：在反引号中的字符串将解释成shell命令来执行。 来自:https://www.jb51.net/article/108330.htm\n变量 字符串变量 注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则：\n命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 使用一个定义过的变量，只要在变量名前面加美元符号即可，如：\nyour_name=\u0026#34;qinjx\u0026#34; echo $your_name echo ${your_name} your_name=\u0026#34;xkj\u0026#34; echo $your_name Shell 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。\n类似于 C 语言，数组元素的下标由 0 开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于 0。\n定义数组 在 Shell 中，用括号来表示数组，数组元素用\u0026quot;空格\u0026quot;符号分割开。定义数组的一般形式为：\n数组名=(值1 值2 ... 值n)\n例如：\narray_name=(value0 value1 value2 value3)\n可以不使用连续的下标，而且下标的范围没有限制。\narray_name[0]=value0 array_name[1]=value1 array_name[n]=valuen 读取数组元素值的一般格式是：\n${数组名[下标]} # 例如: valuen=${array_name[n]} echo ${array_name[@]} #输出全部元素 来自: https://www.runoob.com/linux/linux-shell-variable.html 参数取值 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2\n$? 上个命令的退出状态，或函数的返回值。成功返回0，失败返回1\n$# 传递给脚本或函数的参数个数\n$* 所有这些参数都被双引号引住。若一个脚本接收两个参数，$等于$1$2\n$0 正在被执行命令的名字。对于shell脚本而言，这是被激活命令的路径\n$@ 被双引号(” “)包含时，与 $* 稍有不同。若一个脚本接收到两个参数，$@等价于$1$2\n$$ 当前shell的进程号。对于shell脚本，这是其正在执行时的进程ID\n$! 前一个后台命令的进程号\nhttps://www.cnblogs.com/ipoke/p/11417479.html ","permalink":"https://xiaokunji.com/zh/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86/shell/shell%E5%91%BD%E4%BB%A4.html","summary":"[toc] if 一、条件测试的表达式： [ expression ] 括号两端必须要有空格 [[ expression ]] 括号两端必须要有空格 test expression 组合测试条件： -a: and -o: or !: 非 二、整数比较： -eq 测试两个整数是否","title":"shell命令"},{"content":"工作原理是通过过滤器拦截,做一些操作,来达到身份认证和权限管理的作用\n有拦截功能,跳转功能\nshiro自带拦截器,也有相应实现,具体实现不需要我们操作,只需做在什么情况下跳转什么页面(或者操作),比如:当密码错误时,要做什么(自己写操作) ; 没有登录时,做什么等等\n详情: https://blog.csdn.net/ityouknow/article/details/73836159 @Bean(name = \u0026#34;shiroFilter\u0026#34;) public ShiroFilterFactoryBean getShiroFilterFactoryBean(DefaultWebSecurityManager securityManager) { logger.info(\u0026#34;-------------getShiroFilterFactoryBean()+++++++++++\u0026#34;); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 必须设置 SecurityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 如果不设置默认会自动寻找Web工程根目录下的\u0026#34;/login.jsp\u0026#34;页面 shiroFilterFactoryBean.setLoginUrl(\u0026#34;/login\u0026#34;); // // 登录成功后要跳转的连接 // shiroFilterFactoryBean.setSuccessUrl(\u0026#34;/user\u0026#34;); // shiroFilterFactoryBean.setUnauthorizedUrl(\u0026#34;/403\u0026#34;); Map\u0026lt;String, String\u0026gt; filterChainDefinitionMap = new LinkedHashMap\u0026lt;String, String\u0026gt;(); // authc：该过滤器下的页面必须验证后才能访问，它是Shiro内置的一个拦截器org.apache.shiro.web.filter.authc.FormAuthenticationFilter // anon：它对应的过滤器里面是空的,什么都没做 filterChainDefinitionMap.put(\u0026#34;/getUser\u0026#34;, \u0026#34;anon\u0026#34;);// 这里为了测试，只限制/user，实际开发中请修改为具体拦截的请求规则 logger.info(\u0026#34;##################从数据库读取权限规则，加载到shiroFilter中##################\u0026#34;); filterChainDefinitionMap.put(\u0026#34;/user/edit/**\u0026#34;, \u0026#34;authc,perms[user:edit]\u0026#34;);// 这里为了测试，固定写死的值，也可以从数据库或其他配置中读取 filterChainDefinitionMap.put(\u0026#34;/hello\u0026#34;, \u0026#34;authc\u0026#34;); filterChainDefinitionMap.put(\u0026#34;/**\u0026#34;, \u0026#34;anon\u0026#34;);//anon 可以理解为不拦截 shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; } ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/shiro_%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86.html","summary":"工作原理是通过过滤器拦截,做一些操作,来达到身份认证和权限管理的作用 有拦截功能,跳转功能 shiro自带拦截器,也有相应实现,具体实现不需要我","title":"Shiro_权限管理"},{"content":"[toc]\nSpring-Cloud-Sleuth是Spring Cloud的组成部分之一，为SpringCloud应用实现了一种分布式追踪解决方案，其兼容了Zipkin, HTrace和log-based追踪,官网:https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_running_examples\n1.术语(Terminology) **Span：**基本工作单元，例如，在一个新建的span中发送一个RPC等同于发送一个回应请求给RPC，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、关键值注释(tags)、span的ID、以及进度ID(通常是IP地址)\nspan在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。\n**Trace：**一系列spans组成的一个树状结构，例如，如果你正在跑一个分布式大数据工程，你可能需要创建一个trace。\n**Annotation：**用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束\ncs - Client Sent -客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络延迟 ss - Server Sent -注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳便可得到服务端需要的处理请求时间 cr - Client Received -表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳便可得到客户端从服务端获取回复的所有所需时间 将Span和Trace在一个系统中使用Zipkin注解的过程图形化：\n每个颜色的注解表明一个span(总计7个spans，从A到G)，如果在注解中有这样的信息：\nTrace Id = X\nSpan Id = D\nClient Sent\n这就表明当前span将Trace-Id设置为X，将Span-Id设置为D，同时它还表明了ClientSent事件。\nspans 的parent/child关系图形化：\n来源: https://blog.csdn.net/u010257992/article/details/52474639 2.使用 单独使用sleuth十分简单\npom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-sleuth\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 得先有才能不写版本号\n将日志格式改为\n\u0026lt;layout class=\u0026#34;ch.qos.logback.classic.PatternLayout\u0026#34;\u0026gt; \u0026lt;pattern\u0026gt;%date [%thread] [%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}] %-5level %logger{80}:%line - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; 把traceId和SpanId打印出来\n然后就可以在日志中按照traceId和spanId找请求了,如果日志收集到了es中,也可以给traceId和spanId单独做个列,用es来找也是可以的\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/sleuth.html","summary":"[toc] Spring-Cloud-Sleuth是Spring Cloud的组成部分之一，为SpringCloud应用实现了一种分布式追踪解决方案，其","title":"Sleuth"},{"content":"1.SLF4J(Simple logging Facade for Java) 意思为简单日志门面，它是把不同的日志系统的实现进行了具体的抽象化，只提供了统一的日志使用接口，使用时只需要按照其提供的接口方法进行调用即可，由于它只是一个接口，并不是一个具体的可以直接单独使用的日志框架，所以最终日志的格式、记录级别、输出方式等都要通过接口绑定的具体的日志系统来实现，这些具体的日志系统就有log4j,logback,java.util.logging等，它们才实现了具体的日志系统的功能。\n如何使用SLF4J?\n既然SLF4J只是一个接口，那么实际使用时必须要结合具体的日志系统来使用，我们首先来看SLF4J和各个具体的日志系统进行绑定时的框架原理图：\n其实slf4j原理很简单，他只提供一个核心slf4j api(就是slf4j-api.jar包)，这个包只有日志的接口，并没有实现，所以如果要使用就得再给它提供一个实现了些接口的日志包，比 如：log4j,common logging,jdk log日志实现包等，但是这些日志实现又不能通过接口直接调用，实现上他们根本就和slf4j-api不一致，因此slf4j又增加了一层来转换各日志实现包的使 用，当然slf4j-simple除外。其结构如下：\nslf4j-api(接口层)\n|\n各日志实现包的连接层( slf4j-jdk14, slf4j-log4j)\n|\n各日志实现包\n所以，结合各日志实现包使用时提供的jar包情况为：\nSLF4J和logback结合使用时需要提供的jar:slf4j-api.jar,logback-classic.jar,logback-core.jar\nSLF4J和log4j结合使用时需要提供的jar:slf4j-api.jar,slf4j-log412.jar,log4j.jar\nSLF4J和JDK中java.util.logging结合使用时需要提供的jar:slf4j-api.jar,slf4j-jdk14.jar\nSLF4J和simple(SLF4J本身提供的一个接口的简单实现)结合使用时需要提供的jar:slf4j-api.jar,slf4j-simple.jar\n当然还有其他的日志实现包，以上是经常会使用到的一些。\n注意，以上slf4j和各日志实现包结合使用时最好只使用一种结合，不然的话会提示重复绑定日志，并且会导致日志无法输出。\n为什么要使用SLF4J?\nslf4j是一个日志接口，自己没有具体实现日志系统，只提供了一组标准的调用api,这样将调用和具体的日志实现分离，使用slf4j后有利于根据自己实际的需求更换具体的日志系统，比如，之前使用的具体的日志系统为log4j,想更换为logback时，只需要删除log4j相关的jar,然后加入logback相关的jar和日志配置文件即可，而不需要改动具体的日志输出方法，试想如果没有采用这种方式，当你的系统中日志输出有成千上万条时，你要更换日志系统将是多么庞大的一项工程。如果你开发的是一个面向公众使用的组件或公共服务模块，那么一定要使用slf4的这种形式，这有利于别人在调用你的模块时保持和他系统中使用统一的日志输出。 slf4j日志输出时可以使用{}占位符，如，logger.info(\u0026ldquo;testlog: {}\u0026rdquo;, \u0026ldquo;test\u0026rdquo;)，而如果只使用log4j做日志输出时，只能以logger.info(\u0026ldquo;testlog:\u0026quot;+\u0026ldquo;test\u0026rdquo;)这种形式，前者要比后者在性能上更好，后者采用+连接字符串时就是new 一个String 字符串，在性能上就不如前者。 2.log4j(log for java) Log4j是Apache的一个开源项目，通过使用Log4j，我们可以控制日志信息输送的目的地是控制台、文件、GUI组件，甚至是套接口服务器、NT的事件记录器、UNIX Syslog守护进程等；我们也可以控制每一条日志的输出格式；通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。最令人感兴趣的就是，这些可以通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。\n如何使用？\n引入jar,使用log4j时需要的jar为：log4j.jar。\n定义配置文件log4j.properties或log4j.xml\n在具体的类中进行使用：\n在需要日志输出的类中加入：private static final Logger logger = Logger.getLogger(Tester.class); //通过Logger获取Logger实例 在需要输出日志的地方调用相应方法即可：logger.debug(“System …..”); logger.log(Level.DEBUG,\u0026ldquo;这是debug\u0026rdquo;);// 可以指定打印级别,但logback不支持,需要自己实现 关于如何单独使用log4j，建议详细阅读以下文章：\nhttps://blog.csdn.net/u012422446/article/details/51199724 https://blog.csdn.net/azheng270/article/details/2173430/ http://shmilyaw-hotmail-com.iteye.com/blog/2410764 3.logback logback同样是由log4j的作者设计完成的，拥有更好的特性，用来取代log4j的一个日志框架,是slf4j的原生实现(即直接实现了slf4j的接口，而log4j并没有直接实现，所以就需要一个适配器slf4j-log4j12.jar),logback一共有以下几个模块：\nlogback-core：其它两个模块的基础模块 logback-classic：它是log4j的一个改良版本，同时它完整实现了slf4j API使你可以很方便地更换成其它日志系统如log4j或JDK14 Logging logback-access：访问模块与Servlet容器集成提供通过Http来访问日志的功能 同样，单独使用它时，需要引入以上jar,然后进行配置文件的配置，最后就是在相关类中进行使用，使用时加入以下语句:\nprivate final static Logger logger = LoggerFactory.getLogger(Test.class); logger.info(\u0026#34;打印日志\u0026#34;); 对于logback的使用，详细使用方法及配置推荐阅读以下文章：\nhttps://www.cnblogs.com/warking/p/5710303.html springboot 默认使用框架是logback\n4.总结如下： 1、slf4j是java的一个日志门面，实现了日志框架一些通用的api，log4j和logback是具体的日志框架。\n2、他们可以单独的使用，也可以绑定slf4j一起使用。\n单独使用，分别调用框架自己的方法来输出日志信息。绑定slf4j一起使用。调用slf4j的api来输入日志信息，具体使用与底层日志框架无关（需要底层框架的配置文件）。显然不推荐单独使用日志框架。假设项目中已经使用了log4j，而我们此时加载了一个类库，而这个类库依赖另一个日志框架。这个时候我们就需要维护两个日志框架，这是一个非常麻烦的事情。而使用了slf4j就不同了，由于应用调用的抽象层的api，与底层日志框架是无关的，因此可以任意更换日志框架。\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/slf4j%E5%92%8Clog4j%E5%92%8Clogback%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB.html","summary":"1.SLF4J(Simple logging Facade for Java) 意思为简单日志门面，它是把不同的日志系统的实现进行了具体的抽象化，只提供了统一的日志使用接口，使用时只需要按照其提供的接口方法进行","title":"slf4j和log4j和logback的联系和区别"},{"content":"[toc]\nSOA Service-Oriented Architecture 面向服务的架构\nSOA（面向服务的架构）定义了一种可通过服务接口复用软件组件的方法。 此类接口会使用通用的通信标准，这些标准能够快速合并到新应用程序中，而不必每次都执行深度集成。\nSOA架构和微服务架构的区别 首先SOA和微服务架构一个层面的东西，而对于ESB和微服务网关是一个层面的东西，一个谈到是架构风格和方法，一个谈的是实现工具或组件。\nSOA（Service Oriented Architecture）“面向服务的架构”:他是一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。一个服务 通常以独立的形式存在与操作系统进程中。各个服务之间 通过网络调用。\n微服务架构:其实和 SOA 架构类似,微服务是在 SOA 上做的升华，微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。\n微服务架构 = 80%的SOA服务架构思想 + 100%的组件化架构思想 + 80%的领域建模思想\n原文链接：https://blog.csdn.net/zpoison/java/article/details/80729052\n很多人说微服务是 SOA 的延续，都强调松耦合，只是 SOA 高度依赖服务总线（ESB），而微服务不需要。\n从理论上讲，您可以在不使用 ESB 的情况下实施 SOA，但每个应用程序所有者都必须通过自己独有的方式来公开服务接口，这就需要完成大量工作（即使接口最终可复用也不例外），并且在未来会带来巨大的维护挑战。 实际上，ESB 最终被认为是所有 SOA 实施都包含的事实元素，以致于有时将这两个术语作为同义词使用，从而造成了混乱。\nhttps://www.zhihu.com/question/37808426/answer/536875376 捷顺的天启系统更像是SOA系统,其划分不完全隔离,主要体现在数据库的不分离,存在公共业务,天启和其他业务系统一起更像SOA,天启平台像那个总线\n什么是 SOA（面向服务的架构）？ - 中国 | IBM ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E6%9E%B6%E6%9E%84/soa%E6%9E%B6%E6%9E%84%E5%92%8C%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E7%9A%84%E5%8C%BA%E5%88%AB.html","summary":"[toc] SOA Service-Oriented Architecture 面向服务的架构 SOA（面向服务的架构）定义了一种可通过服务接口复用软件组件的方法。 此类接口会使用通用的通信标准，这些标准能够快速合并","title":"SOA架构和微服务架构的区别"},{"content":"[toc]\n1. spark shuffle过程 spark中管理shuffle的过程有一个shuffleManage负责管理, 在spark 2.X 之后,主要负责的是sortshufflemanager, 其中主要的是sortshuffle,shuffle分为两个阶段,shufflewriter和shuffleread\na) map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是5M\nb) 在shuffle的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过5M时，比如现在内存结构中的数据为5.01M，那么他会申请5.01*2-5=5.02M内存给内存数据结构。\nc) 如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。\nd) 在溢写之前内存结构中的数据会进行排序分区\ne) 然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是1万条数据，\nf) map task执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。\ng) reduce task去map端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。\nhttps://blog.csdn.net/qq_21835703/article/details/79104733 2. spark streaming 读取kafka数据的两种方式 这两种方式分别是：\nReceiver-base 使用Kafka的高层次Consumer API来实现。receiver从Kafka中获取的数据都存储在Spark Executor的内存中，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。\nDirect Spark1.3中引入Direct方式，用来替代掉使用Receiver接收数据，这种方式会周期性地查询Kafka，获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。(使用比received更底层的api)\n3. Spark提供的两种共享变量 Spark 程序的大部分操作都是 RDD 操作，通过传入函数给 RDD 操作函数来计算，这些函数在不同的节点上并发执行，内部的变量有不同的作用域，不能相互访问，有些情况下不太方便。\n广播变量，是一个只读对象，在所有节点上都有一份缓存，创建方法是 SparkContext.broadcast()。创建之后再更新它的值是没有意义的，一般用 val 来修改定义。 计数器，只能增加，可以用计数或求和，支持自定义类型。创建方法是 SparkContext.accumulator(V, name)。只有 Driver 程序可以读这个计算器的变量，RDD 操作中读取计数器变量是无意义的。但节点可以对该计算器进行增加（？？？） 以上两种类型都是 Spark 的共享变量。\nhttps://zhuanlan.zhihu.com/p/49169166 4. 解释一下Spark Master的选举过程 与hadoop一样，spark也存在单点故障问题，为此，spark的standalone模式提供了master的HA，与hadoop一样，一个是active，一个是standby状态，当active挂了，standby会顶上。\nspark HA的主备切换主要基于两种机制：\n基于文件系统\n基于zk集群。\n前者在挂了后需要手动切换，而基于zk的HA可以自动实现切换。策略可以通过配置文件spark-env.sh配置spark.deploy.recoveryMode\n切换的过程如下：\n首先，standby的master去读取持久化（可能是磁盘或者zk）的storedAPP，storeddriver，storedworker的相关信息。读取后，如果storedAPP，storeddriver，storedworker有任何一个非空，那么就会启动master恢复机制，将持久化的APP，driver，worker信息重新注册到内存中，将application和worker的状态修改为UNknown，然后向该APP对应的driver，worer发送自己的地址信息，driver，worer如果没有挂，那么在接收到master发送的地址后，就会返回响应消息给新的master。此时，master在接收到driver，worker的消息后，会使用completeRecory对没有响应的组件进行清理，最后调用master的schedul方法，对正在等待的driver和app调度，如启动executor。\n原文链接：https://blog.csdn.net/englishsname/article/details/50631372\nhttps://blog.csdn.net/zhanglh046/article/details/78485745 5. Spark Streaming小文件问题 ​\t使用 Spark Streaming 时，如果实时计算结果要写入到 HDFS，那么不可避免的会遇到一个问题，那就是在默认情况下会产生非常多的小文件，这是由 Spark Streaming 的微批处理模式和 DStream(RDD) 的分布式(partition)特性导致的，Spark Streaming 为每个 Partition 启动一个独立的线程（一个 task/partition 一个线程）来处理数据，一旦文件输出到 HDFS，那么这个文件流就关闭了.\n处理 Spark Streaming 小文件的典型方法(大致如下):\n增加 batch 大小\nCoalesce大法好 , 小文件的基数是 batch_number * partition_number, 所以就是合并分区数\nSpark Streaming 外部来处理: 用 Hive 或者 Spark Sql 这样的“sql on hadoop”系统类进一步进行数据分析，而这些表一般都是按照半小时或者一小时、一天\n自己调用 foreach 去 append, 写入文件时,不要新建,而是追加已有的\nhttps://zhuanlan.zhihu.com/p/49169166 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/spark.html","summary":"[toc] 1. spark shuffle过程 spark中管理shuffle的过程有一个shuffleManage负责管理, 在spark 2.X 之后,主要负责的是sor","title":"spark"},{"content":"[toc]\n1. Mesos: 一个资源统一管理和调度平台,类似YARN,个人感觉不如YARN好\n出自:http://dongxicheng.org/mapreduce-nextgen/mesos_vs_yarn/\n2. DGA: 在spark里每一个操作生成一个RDD，RDD之间连一条边，最后这些RDD和他们之间的边组成一个有向无环图，这个就是DAG。(一般来说,生成新的RDD后,旧的不会使用,会等着被JVM回收)\n来自 \u0026lt;http://blog.csdn.net/sinat_31726559/article/details/51738155\u0026gt; 3. yarn和spark: spark可以运行在yarn上,做资源管理,但是spark有自己的资源管理平台(可以深究,TaskScheduler)\n4. Spark core: Spark Core实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集（resilient distributed dataset，简称RDD）的API定义。 Spark Core提供了创建和操作这些集合的多个API。\n来自 \u0026lt;https://www.douban.com/note/536766108/?from=tag\u0026gt; 5. RDD类型 6. PageRank算法 7. Spark Shuffle 以reduceByKey为例,涉及按key对于RDD成员进行重组。将具有相同key但分布在不同节点上的成员聚会在一个节点上,以便对他们的value进行操作,这个重组过程就是shuffle操作。因为shuffle操作会涉及数据的传输，所以成本特别高，而且过程复杂。\n简单的说就是需要操作所有数据时,整合数据的过程叫shuffle\n8. RDD不可变,那RDD会不会存在回收? RDD 使用的是java内存,每生成一个RDD,就生成了一个对象(即使不接收),当RDD不再使用时(可能是标记法吧),又JVM回收\n9. 为什么Flink比spark快? Flink所用事物类型更底层,在运行时就经过了优化 flink提供了基于每个事件的流式处理机制,而spark是用小批量来模拟流式，也就是多个事件的集合 以上只是简述,并非完全.来自: https://www.jianshu.com/p/905ca3a7edb9 10.checkpoint checkpoint在spark中主要有两块应用：\n一块是在spark core中对RDD做checkpoint，可以切断做checkpoint RDD的依赖关系，将RDD数据保存到可靠存储（如HDFS）以便数据恢复；\n另外一块是应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理（如之前waiting batch的job会在重启后继续处理）。\n注: checkpoint着重的是DAG图,会保存当前RDD,比如:RDD经过很多转换,最后才触发action;\ncache着重的是数据,比如:需要重复用到该数据 https://www.cnblogs.com/superhedantou/p/9004820.html https://blog.csdn.net/j904538808/article/details/80104525 11.SparkSQL 和 impala SparkSQL优势是能做业务处理,因为是RDD,能和其他RDD无缝连接,\nimpala优势是查询速度够快,(感觉能用web系统的操作)\n12.springStream中的 updatestateByKey函数和Mapwithstate函数 SparkStreaming 7*24 小时不间断的运行，有时需要管理一些状态，比如wordCount，每个batch的数据不是独立的而是需要累加的，这时就需要sparkStreaming来维护一些状态(这些状态可以是任意类型)，目前有两种方案updateStateByKey和mapWithState，(后者性能比前者好)\nupdateStateByKey\nssc.checkpoint(\u0026#34;.\u0026#34;) // 需打开checkpiont def updateFunction(currValues:Seq[Int],preValue:Option[Int]): Option[Int] = { val currValueSum = currValues.sum //上面的Int类型都可以用对象类型替换 Some(currValueSum + preValue.getOrElse(0)) //当前值的和加上历史值 } kafkaStream.map(r =\u0026gt; (r._2,1)).updateStateByKey(updateFunction _)// vaule值是state(任意类型),更加key对state做处理(stream来维护状态) mapWithState\n可以使用initialState(RDD)来初始化key的值。 另外，还可以指定timeout函数，该函数的作用是，如果一个key超过timeout设定的时间没有更新值，那么这个key将会失效。这个控制需要在func中实现，必须使用state.isTimingOut()来判断失效的key值。如果在失效时间之后，这个key又有新的值了，则会重新计算。如果没有使用isTimingOut，则会报错。 val initialRDD = ssc.sparkContext.parallelize(List[(String, Int)]()) words.map((_, 1)).mapWithState(StateSpec.function(func).timeout(Seconds(30)).initialState(initialRDD)).print() /** * 定义一个函数，该函数有三个类型word: String, option: Option[Int], state: State[Int] * 其中word代表统计的单词， * option代表的是历史数据（使用option是因为历史数据可能有，也可能没有，如第一次进来的数据就没有历史记录）， * state代表的是返回的状态 */ val func: (String, Option[Int], State[Int]) =\u0026gt; Any = (word: String, option: Option[Int], state: State[Int]) =\u0026gt; { if(state.isTimingOut()){ //如果key超过时间没有更新 println(word + \u0026#34;is timeout\u0026#34;) }else{ val sum = option.getOrElse(0) + state.getOption().getOrElse(0) // 单词和该单词出现的频率/ 获取历史数据，当前值加上上一个批次的该状态的值 // 更新状态 state.update(sum) (word, sum) } } 链接：https://www.jianshu.com/p/9f743301f589 https://my.oschina.net/u/3875806/blog/2986549 http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark.html","summary":"[toc] 1. Mesos: 一个资源统一管理和调度平台,类似YARN,个人感觉不如YARN好 出自:http://dongxicheng.org/mapreduce","title":"Spark"},{"content":"[toc]\n1. 介绍 Shuffle描述着数据从map task输出到reduce task输入的这段过程。shuffle是连接Map和Reduce之间的桥梁，Map的输出要用到Reduce中必须经过shuffle这个环节，shuffle的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task需要跨节点去拉取其它节点上的map task结果。这一过程将会产生网络资源消耗和内存，磁盘IO的消耗。通常shuffle分为两部分：Map阶段的数据准备和Reduce阶段的数据拷贝处理。一般将在map端的Shuffle称之为Shuffle Write，在Reduce端的Shuffle称之为Shuffle Read.\n在Spark的中，负责shuffle过程的执行、计算和处理的组件 主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种(前者在2.0版本后已遗弃)\nspark shuffle 演进的历史\nSpark 0.8及以前 Hash Based Shuffle Spark 0.8.1 为Hash Based Shuffle引入File Consolidation机制 Spark 0.9 引入ExternalAppendOnlyMap Spark 1.1 引入Sort Based Shuffle，但默认仍为Hash Based Shuffle Spark 1.2 默认的Shuffle方式改为Sort Based Shuffle Spark 1.4 引入Tungsten-Sort Based Shuffle Spark 1.6 Tungsten-sort并入Sort Based Shuffle Spark 2.0 Hash Based Shuffle退出历史舞台 所以现在就只有 sortshuffle\n2. HashShuffleManager ![img](F:\\学习资料\\个人笔记\\MDImages\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark_shuffle.html","summary":"[toc] 1. 介绍 Shuffle描述着数据从map task输出到reduce task输入的这段过程。shuffle是连接Map和Reduce之间的桥梁","title":"spark_shuffle"},{"content":"[toc]\n1.前言: SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，但是Shark对Hive有太多依赖(如采用Hive的语法解析器、查询优化器等等),所以SparkSQL抛弃原有Shark的代码，汲取了Shark的一些优点，如内存列存储（In-Memory Columnar Storage）、Hive兼容性等，重新开发了SparkSQL代码.\n不再受限于Hive，只是兼容Hive\n而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎\n来自* \u0026lt;http://www.cnblogs.com/shishanyuan/p/4723604.html \u0026gt;\n可以这么说, sparkSQL做到了大部分HIVE的功能,但是比它快(因为重写了底层,而且用的是RDD),将操作写到代码里了\n对SparkSQL来说,主要的工作就是写SQL,和写自定义函数了(类似于HIVE),\n大致流程: 从数据源(HIVE,HDFS,文本等)读取数据,然后写SQL查询,(有业务处理就处理),复杂的就写自定义函数(继承UserDefinedFunction或者 UserDefinedAggregateFunction)\n查sql前当然要有数据和表,在代码里只能写临时表,要想创建永久表,还是得用hive那一套(相关语句可以直接运行也可以写代码里)\n目前比较流行的是SparkSQL 连接Hive,这样可以操作Hive中的数据,(在spark1.2.1后,自带Hive,但是sparksql用到了hive的元数据,所以需要额外提供mysql(用来存储元数据,当然也有自带的叫derby),但局限性比较大,估计生产中还是会用外部Hive),这样就又得学Hive,暂时放弃\n1.1 Spark SQL基础 使用spark SQL有两种方式，一种是作为分布式SQL引擎,此时只需要写SQL就可以进行计算。另一种是吃饭spark程序中通过领域API的形式来操作数据(被抽象为DateFrame)。\n简单的说: 一种是和SQL引擎去查数据(已经有数据了,一般和HIVE一起用); 一种是通过API的方式加载数据并操作(一般是从本地,HDFS,JDBC等加载文本数据)\n2.为什么sparkSQL的性能得到提升 这个简单了解即可，\n内存列存储（In-Memory Columnar Storage） 字节码生成技术 scala代码优化 3.sparkSQL组成 sparkSQL1.1总体上由四个模块组成：core、catalyst、hive、hive-Thriftserver：\ncore 处理数据的输入输出，从不同的数据源获取数据（RDD、Parquet、json等），将查询结果输出成schemaRDD； catalyst 处理查询语句的整个处理过程，包括解析、绑定、优化、物理计划等，说其是优化器，还不如说是查询引擎； hive 对hive数据的处理 hive-ThriftServer 提供CLI和JDBC/ODBC接口 在这四个模块中，catalyst处于最核心的部分，其性能优劣将影响整体的性能。由于发展时间尚短，还有很多不足的地方，但其插件式的设计，为未来的发展留下了很大的空间。下面是catalyst的一个设计图：\n其中虚线部分是以后版本要实现的功能，实线部分是已经实现的功能。从上图看，catalyst主要的实现组件有：\nsqlParse，完成sql语句的语法解析功能，目前只提供了一个简单的sql解析器；\nAnalyzer，主要完成绑定工作，将不同来源的Unresolved LogicalPlan和数据元数据（如hive metastore、Schema catalog）进行绑定，生成resolved LogicalPlan；\noptimizer对resolved LogicalPlan进行优化，生成optimized LogicalPlan；\nPlanner将LogicalPlan转换成PhysicalPlan；\nCostModel，主要根据过去的性能统计数据，选择最佳的物理执行计划\n这些组件的基本实现方法：\n先将sql语句通过解析生成Tree，然后在不同阶段使用不同的Rule应用到Tree上，通过转换完成各个组件的功能。\nAnalyzer使用Analysis Rules，配合数据元数据（如hive metastore、Schema catalog），完善Unresolved LogicalPlan的属性而转换成resolved LogicalPlan；\noptimizer使用Optimization Rules，对resolved LogicalPlan进行合并、列裁剪、过滤器下推等优化作业而转换成optimized LogicalPlan；\nPlanner使用Planning Strategies，对optimized LogicalPlan进行转换，转换成可以执行的物理计划。\n4.sparkSQL组件之解析 太复杂,以后看\nhttps://www.aboutyun.com/forum.php?mod=viewthread\u0026tid=20910 5.sparkSQL 窗口函数 窗口函数是spark sql模块从1.4之后开始支持的，主要用于解决对一组数据进行操作，同时为每条数据返回单个结果，比如计算指定访问数据的均值、计算累进和或访问当前行之前行数据等，这些场景使用普通函数实现是比较困难的。数量不多,但功能强大.(其实窗口函数HIve就有了,语法基本一样)(数据源针对的是HIVE,需要打包到spark中测试)\nSpark SQL支持三类窗口函数：排名函数、分析函数和聚合函数。以下汇总了Spark SQL支持的排名函数和分析函数。对于聚合函数来说，普通的聚合函数(类似sum,max)都可以作为窗口聚合函数使用\n类别 SQL DataFrame 含义 排名函数 rank rank 为相同组的数据计算排名，如果相同组中排序字段相同，当前行的排名值和前一行相同；如果相同组中排序字段不同，则当前行的排名值为该行在当前组中的行号；因此排名序列会出现间隙 排名函数 dense_rank denseRank 为相同组内数据计算排名，如果相同组中排序字段相同，当前行的排名值和前一行相同；如果相同组中排序字段不同，则当前行的排名值为前一行排名值加1；排名序列不会出现间隙 排名函数 percent_rank percentRank 该值的计算公式(组内排名-1)/(组内行数-1)，如果组内只有1行，则结果为0(求的是排序百分比) 排名函数 ntile ntile 将组内数据排序然后按照指定的n切分成n个桶，该值为当前行的桶号(桶号从1开始) (就是把分组后的数据平均分给各个桶,) 排名函数 row_number rowNumber 将组内数据排序后，该值为当前行在当前组内的从1开始的递增的唯一序号值 分析函数 cume_dist cumeDist 该值的计算公式为：组内小于等于当前行值的行数/组内总行数 分析函数 lag lag 用法：lag(input, [offset, [default]]),计算组内当前行按照排序字段排序的之前offset行的input列的值，如果offset大于当前窗口(组内当前行之前行数)则返回default值，default值默认为null 分析函数 lead lead 用法：lead(input, [offset, [default]])，计算组内当前行按照排序字段排序的之后offset行的input列的值，如果offset大于当前窗口(组内当前行之后行数)则返回default值，default值默认为null 当一个函数被作为窗口函数使用时，需要为该窗口函数定义相关的窗口规范。窗口规范定义了哪些行会包括到给定输入行相关联的帧(frame)中。窗口规范包括三部分：\n分区规范：定义哪些行属于相同分区，这样在对帧中数据排序和计算之前相同分区的数据就可以被收集到同一台机器上。如果没有指定分区规范，那么所有数据都会被收集到单个机器上处理。 排序规范：定义同一个分区中所有数据的排序方式，从而确定了给定行在他所属分区中的位置 帧规范：指定哪些行会被当前输入行的帧包括，通过其他行对于当前行的相对位置实现。 如果使用sql语句的话，PARTITION BY关键字用来为分区规范定义分区表达式、 ORDER BY关键字用来为排序规范定义排序表达式。格式：OVER (PARTITION BY \u0026hellip; ORDER BY \u0026hellip; )。\n如果使用DataFrame API的话，API提供了函数来定义窗口规范。实例如下：\nimport org.apache.spark.sql.expressions.Window val windowSpec = Window.partitionBy(...).orderBy(...) 为了分区和排序操作，需要定义帧的开始边界、结束边界和帧的类型，这也是一个帧规范的三部分。一共有五种边界：UNBOUNDED PRECEDING(分区第一行)，UNBOUNDED FOLLOWING(分区最后一行)，CURRENT ROW， PRECEDING(当前行之前行)和 FOLLOWING(当前行之后行)。有两种帧类型：ROW帧和RANGE帧。\n例子:\n如果我们需要统计每个类别最畅销和次畅销的产品，首先需要基于产品的收入对相同类别的产品进行排名，然后基于排名取出最畅销和次畅销的产品。使用窗口函数实现的sql语句如下：\nSELECT product, category, revenue FROM ( SELECT produce, category, revenue, dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) as rank FROM productRevenue) tmp WHERE rank \u0026lt;= 2 如果需要统计相同类别中每种产品与该类别中最畅销产品收入差距又该如何呢？首先为了计算差距，需要先找到每个类别中收入最高的产品\nSELECT product, category, revenue, (max(revenue) OVER(PARTITION BY category ORDER BY revenue DESC) - revenue) as revenue_diff FROM produceRevenue https://blog.csdn.net/Shie_3/article/details/82890897 https://blog.csdn.net/coding_hello/article/details/90664447 https://www.cnblogs.com/abc8023/p/10910741.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparksql.html","summary":"[toc] 1.前言: SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，但是Shark对H","title":"SparkSQL"},{"content":"[toc]\n1.概述 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。\n来自* \u0026lt;http://www.cnblogs.com/shishanyuan/p/4747735.html \u0026gt;\n2. Streaming架构 2.1 计算流程： Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。\n图Spark Streaming构架\n2.2 容错性： 对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。\n对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。\nSpark Streaming中RDD的lineage关系图\n2.3 实时性： 对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右）\n2.4 扩展性与吞吐量： Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍。\n来自* \u0026lt;http://www.cnblogs.com/shishanyuan/p/4747735.html \u0026gt;\n3.使用 3.1 基本使用(接收套接字) import org.apache.spark._ import org.apache.spark.streaming._ import org.apache.spark.streaming.StreamingContext._ // Create a local StreamingContext with two working thread and batch interval of 1 second. // The master requires 2 cores to prevent from a starvation scenario. val conf = new SparkConf().setMaster(\u0026#34;local[2]\u0026#34;).setAppName(\u0026#34;NetworkWordCount\u0026#34;) val ssc = new StreamingContext(conf, Seconds(5)) // 后面时间的含义: //1. 每隔5秒切一次RDD(可能包含多个) //2. 每隔5秒提交一个job,或者说每隔5秒执行一次 //3. 这个job需5秒内算完,不然下次就要来了,造成处理积压 // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.socketTextStream(\u0026#34;localhost\u0026#34;, 9999) // Split each line into words val words = lines.flatMap(_.split(\u0026#34; \u0026#34;)) import org.apache.spark.streaming.StreamingContext._ // Count each word in each batch val pairs = words.map(word =\u0026gt; (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print() ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate 使用:\n1.在集群中执行命令: nc -lk 9999\n2.然后输入内容,用空格隔开, 3.然后在IDE控制台就能看见单词计数结果 解释: 1.创建StreamingContext对象 同Spark初始化需要创建SparkContext对象一样，使用Spark Streaming就需要创建StreamingContext对象。创建StreamingContext对象所需的参数与SparkContext基本一致，包括指明Master，设定名称(如NetworkWordCount)。需要注意的是参数Seconds(1)，Spark Streaming需要指定处理数据的时间间隔，如上例所示的1s，那么Spark Streaming会以1s为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置； 2.创建InputDStream ​ 如同Storm的Spout，Spark Streaming需要指明数据源。如上例所示的socketTextStream，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括Kafka、 Flume、HDFS/S3、Kinesis和Twitter等数据源； 3.操作Dstream ​ 对于从数据源得到的DStream，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的WordCount执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用Map和ReduceByKey方法进行计算，当然最后还有使用print()方法输出结果； 4.启动Spark Streaming ​ 之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当ssc.start()启动后程序才真正进行所有预期的操作。\n来自 \u0026lt;http://www.cnblogs.com/shishanyuan/p/4747735.html \u0026gt;http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html 3.2 HDFS接收 streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) eg. streamingContext.fileStream(\u0026#34;hdfs://192.168.2.101:8020/data/\u0026#34;) Spark Streaming将监视目录dataDirectory并处理在该目录中创建的任何文件（不支持在嵌套目录中写入的文件）。注意\n文件必须具有相同的数据格式。 必须dataDirectory通过原子地将文件移动或重命名到数据目录中来创建文件。 移动后，不得更改文件。因此，如果文件被连续追加，则不会读取新数据。 对于简单的文本文件，有一个更简单的方法streamingContext.textFileStream(dataDirectory)。文件流不需要运行接收器，因此不需要分配内核。\n**基于自定义接收器的流：**可以使用通过自定义接收器接收的数据流创建DStream。有关更多详细信息，请参见《定制接收器指南》 。 **RDD队列作为流：**为了使用测试数据测试Spark Streaming应用程序，还可以使用，基于RDD队列创建DStream streamingContext.queueStream(queueOfRDDs)。推送到队列中的每个RDD将被视为DStream中的一批数据，并像流一样进行处理。 3.3 windows窗口函数 （实现一阶段内的累加 ，而不是程序启动时）\n​ 假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。\n​ 窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错。\n​ 优化后的window操作要保存状态所以要设置checkpoint路径，没有优化的window操作可以不设置checkpoint路径。\nval sc = new SparkConf().setMaster(\u0026#34;local[3]\u0026#34;).setAppName(\u0026#34;WindowsFunc\u0026#34;) val ssc = new StreamingContext(sc,Seconds(2)) // 每隔5秒接收一次(以前的也保留) ssc.checkpoint(\u0026#34;./checkPoint\u0026#34;) val line = ssc.socketTextStream(\u0026#34;192.168.2.101\u0026#34;,9999)// Receiver机制会占用一个线程 val wc = line.flatMap(_.split(\u0026#34; \u0026#34;)).map((_,1)) wc.reduceByKeyAndWindow(_+_,Seconds(8),Seconds(2)).print()// 每隔四秒算前八秒的数据 reduceByKeyAndWindow 函数\n`reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])``\n前面的func作用和上一个reduceByKeyAndWindow相同，后面的invFunc是用于处理流出rdd的。 因为窗口滑动的原因,就会造成数据重复计算(如上图的time3),使用该函数可以减少计算(会计算出重复量.再计算出增量,再合并)\n在下面这个例子中，如果把3秒的时间窗口当成一个池塘，池塘每一秒都会有鱼游进或者游出，那么第一个函数表示每由进来一条鱼，就在该类鱼的数量上累加。而第二个函数是，每由出去一条鱼，就将该鱼的总数减去一。(简单这么理解,当数据出窗时,就会执行这个反向函数)\n`wc.reduceByKeyAndWindow(+,-,Seconds(8),Seconds(2)).print()// 每隔四秒算前八秒的数据(其结果与上方一直)``\n以后用到了再看吧\nhttps://www.cnblogs.com/yjd_hycf_space/p/7053722.html http://www.freesion.com/article/487428411/ https://www.iteye.com/blog/humingminghz-2308138 https://blog.csdn.net/legotime/article/details/51836040 3.4 连接kafka package xkj.sparkStream import com.alibaba.fastjson.JSON import entity.MyPerson import org.apache.kafka.common.serialization.StringDeserializer import org.apache.log4j.Level import org.apache.log4j.Logger._ import org.apache.spark.rdd.RDD import org.apache.spark.sql.{SaveMode, SparkSession} import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies} import org.apache.spark.streaming.{Seconds, StreamingContext} object KafkaTest { def main(args: Array[String]): Unit = { getLogger(\u0026#34;org.apache.hadoop\u0026#34;).setLevel(Level.ERROR) getLogger(\u0026#34;org.apache.zookeeper\u0026#34;).setLevel(Level.WARN) getLogger(\u0026#34;org.apache.hive\u0026#34;).setLevel(Level.WARN) getLogger(\u0026#34;org.apache\u0026#34;).setLevel(Level.WARN) val spark = SparkSession.builder().master(\u0026#34;local[3]\u0026#34;).appName(\u0026#34;KafkaTest\u0026#34;).getOrCreate() import spark.implicits._ val ssc = new StreamingContext(spark.sparkContext, Seconds(4)) ssc.checkpoint(\u0026#34;./checkPoint\u0026#34;) val kafkaParams = Map[String, Object]( \u0026#34;bootstrap.servers\u0026#34; -\u0026gt; \u0026#34;192.168.2.101:9092\u0026#34;, \u0026#34;key.deserializer\u0026#34; -\u0026gt; classOf[StringDeserializer], \u0026#34;value.deserializer\u0026#34; -\u0026gt; classOf[StringDeserializer], \u0026#34;group.id\u0026#34; -\u0026gt; \u0026#34;xkj\u0026#34; ) val topics: Set[String] = Set(\u0026#34;mystream\u0026#34;) // val stream = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)) // stream.foreachRDD(rdd =\u0026gt; { // rdd.foreach(line =\u0026gt; { // println(\u0026#34;key=\u0026#34; + line.key() + \u0026#34;, value=\u0026#34; + line.value() + \u0026#34;, offset=\u0026#34; + line.offset()) // }) // }) val streamPerson = KafkaUtils.createDirectStream(ssc,LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](topics,kafkaParams)) streamPerson.foreachRDD(rdd=\u0026gt;{ val v: RDD[String] = rdd.map(_.value()) val rddP: RDD[MyPerson] = v.map(JSON.parseObject(_, classOf[MyPerson])) val df = rddP.toDF() df.show() df.write.mode(SaveMode.Append).json(\u0026#34;hdfs://192.168.2.101:9000/user\u0026#34;) }) ssc.start() ssc.awaitTermination() } } 特别注意版本:\npom.xml\n\u0026lt;properties\u0026gt; \u0026lt;spark.version\u0026gt;2.3.0\u0026lt;/spark.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming-kafka-0-10_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; https://blog.csdn.net/zhaolq1024/article/details/85685189 https://blog.51cto.com/simplelife/2311296 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparkstreaming.html","summary":"[toc] 1.概述 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、","title":"SparkStreaming"},{"content":"[toc]\n1. 介绍 SPI 全称：Service Provider Interface，是Java提供的一套用来被第三方实现或者扩展的接口，它可以用来启用框架扩展和替换组件。比如java.sql.Driver接口，其他不同厂商可以针对同一接口做出不同的实现，MySQL和PostgreSQL都有不同的实现提供给用户，而Java的SPI机制可以为某个接口寻找服务实现。Java中SPI机制主要思想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是 解耦。\n简单来说, 框架只提供接口(规范), 具体实现由业务自行实现, 调用的时候自然就调用了具体的业务类,\n和spring 的ioc很类似, 定义接口, 由实现类完成业务, 只是 ioc使用 @service 等注解完成类注入, SPI 通过java.util.ServiceLoader 注入,\n如此, 即使没有spring框架也能做到定义接口, 由业务方来实现, 比如在common包中,没有spring环境, 但需要提供这种能力,就可以用SPI\n2. 案例介绍 定义一个接口：\nPhone.java\npackage com.light.sword; /** * @author: Jack * 2021/1/31 上午1:44 */ public interface Phone { String getSystemInfo(); } 两个实现\npackage com.light.sword; /** * 华为手机 * @author: Jack * 2021/1/31 上午1:48 */ public class Huawei implements Phone { @Override public String getSystemInfo() { return \u0026#34;Hong Meng\u0026#34;; } } package com.light.sword; /** * 苹果手机 * @author: Jack * 2021/1/31 上午1:48 */ public class IPhone implements Phone { @Override public String getSystemInfo() { return \u0026#34;iOS\u0026#34;; } } 然后需要在resources目录下新建 META-INF/services 目录，并且在这个目录下新建一个与上述接口的全限定名一致的文件:\ncom.light.sword.Phone (这是一个文件，是的，一切皆是文件。)\n在这个文件中写入接口的实现类的全限定名（文件 com.light.sword.Phone 中写死的内容）：\ncom.light.sword.Huawei com.light.sword.IPhone 如图\n注意：这个META-INF/services 目录是写死的约定，在 java.util.ServiceLoader 源码实现中, java.util.ServiceLoader#PREFIX 可以看到这个目录的硬编码。\n加载实现类并调用服务\npackage com.light.sword; import java.util.ServiceLoader; public class Main { public static void main(String[] args) { // 通过 ServiceLoader.load() 来加载实现类 ServiceLoader\u0026lt;Phone\u0026gt; phoneServiceLoader = ServiceLoader.load(Phone.class); phoneServiceLoader.forEach(provider -\u0026gt; { String systemInfo = provider.getSystemInfo(); System.out.println(systemInfo); }); } } 这样一个简单的 Java SPI 的demo就完成了。可以看到其中最为核心的就是通过一系列的约定（其实，就是按照人家 java.util.ServiceLoader 的规范标准来）， 然后，通过ServiceLoader 这个类来加载具体的实现类，进而调用实现类的服务。\n3. SPI 实现原理解析 首先，ServiceLoader实现了Iterable接口，所以它有迭代器的属性，这里主要都是实现了迭代器的hasNext和next方法。这里主要都是调用的lookupIterator的相应hasNext和next方法，lookupIterator是懒加载迭代器。\n其次，LazyIterator中的hasNext方法，静态变量PREFIX就是”META-INF/services/”目录，这也就是为什么需要在classpath下的META-INF/services/目录里创建一个以服务接口命名的文件。\n最后，通过反射方法Class.forName()加载类对象，并用newInstance方法将类实例化，并把实例化后的类缓存到providers对象中，(LinkedHashMap\u0026lt;String,S\u0026gt;类型） 然后返回实例对象。\n3.1. SPI 的不足 1.不能按需加载，需要遍历所有的实现，并实例化，然后在循环中才能找到我们需要的实现。如果不想用某些实现类，或者某些类实例化很耗时，它也被载入并实例化了，这就造成了浪费。\n2.获取某个实现类的方式不够灵活，只能通过 Iterator 形式获取，不能根据某个参数来获取对应的实现类。（Spring 的BeanFactory，ApplicationContext 就要高级一些了。）\n3.多个并发多线程使用 ServiceLoader 类的实例是不安全的。\n4. 扩展 要使用SPI, 重点是要在 META-INF/services 目录下, 写接口文件以及具体实现, 这样十分麻烦\n谷歌有一个工具, 可以自动帮我们完成这个操作, 它就是auto-service,\n4.1 使用 未实验\nmaven依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.auto.service\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;auto-service-annotations\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-rc6\u0026lt;/version\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.auto.service\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;auto-service\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-rc6\u0026lt;/version\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 实现类\n@AutoService(Phone.class) public class IPhone implements Phone { // ... } @AutoService(Phone.class) public class Huawei implements Phone { // ... } 这样就能正常使用了, 编译代码后, 在META-INF/services 会自动出现相应文件\nJava SPI (Service Provider Interface) 机制详解-腾讯云开发者社区-腾讯云 (tencent.com) google-auto之自动生成组件化文件_zcswl7961的博客-CSDN博客 java注解处理器之Google Auto Service - strongmore - 博客园 (cnblogs.com) Google Auto Service_原理 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/spi.html","summary":"[toc] 1. 介绍 SPI 全称：Service Provider Interface，是Java提供的一套用来被第三方实现或者扩展的接口，它可以用来启用框架扩展和替换组件。比","title":"SPI"},{"content":"[toc]\n1. 介绍 SpringCache提供基本的Cache抽象，并没有具体的缓存能力，需要配合具体的缓存实现来完成，目前SpringCache支持redis、ehcache、simple（基于内存,使用ConcurrentHashMap）等方式来实现缓存。\n因为spring的缓存机制会引入一些问题, 一些大型项目可能不适合, 比如并发场景下出现异常等等, 但大部分的项目还是可以适用的(毕竟中国没有多少具备高并发的公司)\n2. 常用注解 2.1 @EnableCaching 开启缓存功能，一般使用在springboot的启动类或配置类上\n2.2 @Cacheable 使用缓存。在方法执行前Spring先查看缓存中是否有数据，如果有数据，则直接返回缓存数据；没有则调用方法并将方法返回值放进缓存。\n**注解参数解释: **\n@Cacheable属性名 用途 备注 cacheNames/value 指定缓存空间的名称，不同缓存空间的数据是彼此隔离的 key 同一个cacheNames中通过key区别不同的缓存。如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合，如：@CachePut(value = “demo”, key = “‘user’+#user.id”)，字符串中spring表达式意外的字符串部分需要用单引号 SpringCache提供了与缓存相关的专用元数据，如target、methodMame、result、方法参数等，如：@CachePut(value = “demo”, key = “#result==null”)\n当方法的参数和值一样时, 会导致重复; 如果是基本参数: 参数名+值 如果是对象参数: 参数的类名+对象的成员变量+值 (类似于toString的结果) keyGenrator key的生成策略，SpringCache默认使用SimpleKeyGenerator，默认情况下将参数值作为键，但是可能会导致key重复出现，因此一般需要自定义key的生成策略 和key参数二选一 cacheManager 指定缓存管理器(例如ConcurrentHashMap、Redis等)。 cacheResolver 和cacheManager作用一样，使用时二选一。 condition (指定缓存的条件（对参数判断，满足什么条件时才缓存）)\ncondition是在调用方法之前判断条件，满足条件才缓存, 支持spel语法 @Cacheable(cacheNames=“book”, condition=\u0026quot;#name.length() \u0026lt; 32\u0026quot;) unless (否定缓存的条件（对结果判断，满足什么条件时不缓存）)\nunless是在调用方法之后判断条件，如果SpEL条件成立，则不缓存【条件满足不缓存】,支持spel语法 @Cacheable(cacheNames = “hello”,unless=\u0026quot;#result.id.contains(‘1’)\u0026quot; ) sync 缓存过期之后，如果多个线程同时请求对某个数据的访问，会同时去到数据库，导致数据库瞬间负荷增高。Spring4.3为@Cacheable注解提供了一个新的参数“sync”（默认为false）。当设置它为true时，只有一个线程的请求会去到数据库，其他线程都会等待直到缓存可用。这个设置可以减少对数据库的瞬间并发访问。 2.3 @CachePut 更新缓存，将方法的返回值放到缓存中\n2.4 @CacheEvict 清空缓存\n2.5 @Caching @Caching是一个组注解，可以为一个方法定义提供基于@Cacheable、@CacheEvict或者@CachePut注解的数组。\n例如下面这个例子, 在方法上加 @caching并标注需要的逻辑, 这样调用这个方法时就能同时触发多个操作\n@Caching( put = { @CachePut(value = \u0026#34;myCache\u0026#34;, key = \u0026#34;#result.id\u0026#34;) }, evict = { @CacheEvict(value = \u0026#34;otherCache\u0026#34;, key = \u0026#34;#id\u0026#34;) }, cacheable = { @Cacheable(value = \u0026#34;myFifthCache\u0026#34;, key = \u0026#34;#result.id\u0026#34;) } ) public MyEntity saveData(Long id, String name) { MyEntity entity = new MyEntity(id, name); return entity; } 2.6 @CacheConfig： 一个类中可能会有多个缓存操作，而这些缓存操作可能是重复的。这个时候可以使用 @CacheConfig是一个类级别的注解.\n它可以存放该类中所有缓存的公有属性，比如设置缓存空间的名字cacheNames、key生成策略keyGenerator、缓存管理器cacheManager\n3. 缓存具体实现 支持多种缓存提供程序，包括 Caffeine、Hazelcast 、 Redis和ConcurrentHashMap(默认值)。\n也就是说支持本地缓存和第三方缓存框架, 如果是第三方的缓存就需要额外的pom坐标支持\n推荐caffeine这个本地缓存框架, 性能好,功能全, 只是需要额外引入caffeine这个框架,就有点小成本了\n其次推荐使用redis, 功能全,性能好(没有caffeine这种本地缓存快), 项目中反正都要引入redis的, 刚好顺便能用\n这里以redis为例\n4. 使用 4.1 引入依赖 \u0026lt;dependencies\u0026gt; \u0026lt;!--引入缓存--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-cache\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--引入redis--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 4.2 配置类 @Configuration public class RedisConfiguration { // ${cache} 获取配置文件的配置信息 #{}是spring表达式，获取Bean对象的属性 @Value(\u0026#34;#{${cache}}\u0026#34;) private Map\u0026lt;String, Long\u0026gt; ttlParams; /** * @param redisConnectionFactory * @功能描述 redis作为缓存时配置缓存管理器CacheManager，主要配置序列化方式、自定义 * \u0026lt;p\u0026gt; * 注意：配置缓存管理器CacheManager有两种方式： * 方式1：通过RedisCacheConfiguration.defaultCacheConfig()获取到默认的RedisCacheConfiguration对象， * 修改RedisCacheConfiguration对象的序列化方式等参数【这里就采用的这种方式】 * 方式2：通过继承CachingConfigurerSupport类自定义缓存管理器，覆写各方法，参考： * https://blog.csdn.net/echizao1839/article/details/102660649 * \u0026lt;p\u0026gt; * 切记：在缓存配置类中配置以后，yaml配置文件中关于缓存的redis配置就不会生效，如果需要相关配置需要通过@value去读取 */ @Bean public CacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) { RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() // 设置key采用String的序列化方式 .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(StringRedisSerializer.UTF_8)) //设置value序列化方式采用jackson方式序列化 .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(serializer())) //当value为null时不进行缓存 .disableCachingNullValues() // 配置缓存空间名称的前缀 (这里会覆盖 @cacheable注解的vaule字段) .prefixCacheNameWith(\u0026#34;demo:\u0026#34;) //全局配置缓存过期时间【可以不配置】 .entryTtl(Duration.ofMinutes(30L)); //专门指定某些缓存空间的配置，如果过期时间【主要这里的key为缓存空间名称】 Map\u0026lt;String, RedisCacheConfiguration\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); Set\u0026lt;Map.Entry\u0026lt;String, Long\u0026gt;\u0026gt; entries = ttlParams.entrySet(); for (Map.Entry\u0026lt;String, Long\u0026gt; entry : entries) { //指定特定缓存空间对应的过期时间 map.put(\u0026#34;user\u0026#34;, redisCacheConfiguration.entryTtl(Duration.ofSeconds(40))); map.put(entry.getKey(), redisCacheConfiguration.entryTtl(Duration.ofSeconds(entry.getValue()))); } return RedisCacheManager .builder(redisConnectionFactory) .cacheDefaults(redisCacheConfiguration) //默认配置 .withInitialCacheConfigurations(map) //某些缓存空间的特定配置 .build(); } /** * 自定义缓存的redis的KeyGenerator【key生成策略】 * 注意: 该方法只是声明了key的生成策略,需在@Cacheable注解中通过keyGenerator属性指定具体的key生成策略 * 可以根据业务情况，配置多个生成策略 * 如: @Cacheable(value = \u0026#34;key\u0026#34;, keyGenerator = \u0026#34;cacheKeyGenerator\u0026#34;) */ @Bean public KeyGenerator keyGenerator() { /** * target: 类 * method: 方法 * params: 方法参数 */ return (target, method, params) -\u0026gt; { //获取代理对象的最终目标对象 StringBuilder sb = new StringBuilder(); sb.append(target.getClass().getSimpleName()).append(\u0026#34;:\u0026#34;); sb.append(method.getName()).append(\u0026#34;:\u0026#34;); //调用SimpleKey的key生成器 Object key = SimpleKeyGenerator.generateKey(params); return sb.append(key); }; } /** * @param redisConnectionFactory：配置不同的客户端，这里注入的redis连接工厂不同： JedisConnectionFactory、LettuceConnectionFactory * @功能描述 ：配置Redis序列化，原因如下： * （1） StringRedisTemplate的序列化方式为字符串序列化， * RedisTemplate的序列化方式默为jdk序列化（实现Serializable接口） * （2） RedisTemplate的jdk序列化方式在Redis的客户端中为乱码，不方便查看， * 因此一般修改RedisTemplate的序列化为方式为JSON方式【建议使用GenericJackson2JsonRedisSerializer】 */ @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { GenericJackson2JsonRedisSerializer genericJackson2JsonRedisSerializer = serializer(); RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate = new RedisTemplate\u0026lt;\u0026gt;(); // key采用String的序列化方式 redisTemplate.setKeySerializer(StringRedisSerializer.UTF_8); // value序列化方式采用jackson redisTemplate.setValueSerializer(genericJackson2JsonRedisSerializer); // hash的key也采用String的序列化方式 redisTemplate.setHashKeySerializer(StringRedisSerializer.UTF_8); //hash的value序列化方式采用jackson redisTemplate.setHashValueSerializer(genericJackson2JsonRedisSerializer); redisTemplate.setConnectionFactory(redisConnectionFactory); return redisTemplate; } /** * 此方法不能用@Ben注解，避免替换Spring容器中的同类型对象 */ public GenericJackson2JsonRedisSerializer serializer() { return new GenericJackson2JsonRedisSerializer(); } } 4.3 开启并使用 @EnableCaching //开启缓存的主键 @SpringBootApplication public class RedisDemoApplication { public static void main(String[] args) { SpringApplication.run(RedisDemoApplication.class, args); } } @Service @CacheConfig(cacheNames = \u0026#34;user\u0026#34;,keyGenerator = \u0026#34;keyGenerator\u0026#34;) public class RedisServiceImpl implements RedisService { @Cacheable(value = \u0026#34;user\u0026#34;, key = \u0026#34;\u0026#39;list\u0026#39;\u0026#34;) @Override public List\u0026lt;User\u0026gt; list() { System.out.println(\u0026#34;=========list\u0026#34;); List\u0026lt;User\u0026gt; users = new ArrayList\u0026lt;\u0026gt;(); return users; } @CacheEvict(value = \u0026#34;user\u0026#34;, key = \u0026#34;\u0026#39;list\u0026#39;\u0026#34;) @Override public void del(Integer id) { System.out.println(\u0026#34;************************************+id\u0026#34;); } @CachePut(value = \u0026#34;demo\u0026#34;, key = \u0026#34;#result==null\u0026#34;) @Override public void select(Integer id) { System.out.println(\u0026#34;===============dddd================\u0026#34;); } } 4.4 配置文件 spring: cache: type: redis # 指定使用的缓存类型,不传会自动识别 redis: 当自定义ChacheManager时，就这里的配置不需要配置，配置了也不起作用 use-key-prefix: true key-prefix: \u0026#34;demo:\u0026#34; time-to-live: 60000 #缓存超时时间 单位：ms cache-null-values: false #是否缓存空值 cache-names: user # 初始化一些缓存名 ttl: \u0026#39;{\u0026#34;user\u0026#34;:60,\u0026#34;dept\u0026#34;:30}\u0026#39; #自定义某些缓存空间的过期时间 网上有些文章会使用配置文件的方式, 其实这是 spring-boot-autoconfigure 自动配置的能力, 它会根据你引入的包(redis/caffeine), 自动注入对应的缓存机制 , 比如redis的 org.springframework.boot.autoconfigure.cache.RedisCacheConfiguration\n如果需要自定义一些配置, 就需要按4.1~4.3节那种配置\n5. 注意事项 @Cache注解的方法必须为 public\n默认情况下，@CacheEvict标注的方法执行期间抛出异常，则不会清空缓存。\n基于 proxy 的 spring aop 带来的内部调用问题(aop的通用问题)\n使用内存时一定要设置过期时间和允许使用内存大小 等等, 不然在极端情况下会把内存吃满\n默认的 ConcurrentHashMap不支持设置过期时间和允许使用的内存大小\nredis不支持允许使用的内存大小\ncaffeine都能支持(这也是推荐使用它的原因之一)\nSpring Cache 整合 Redis 做缓存使用~ 快速上手~ - 掘金 (juejin.cn) SpringBoot实现Redis缓存（SpringCache+Redis的整合）_springboot redis cache_user2025的博客-CSDN博客 Spring Cache，key重复问题 SpringBoot项目中使用缓存Cache的正确姿势！！! - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/springcache.html","summary":"[toc] 1. 介绍 SpringCache提供基本的Cache抽象，并没有具体的缓存能力，需要配合具体的缓存实现来完成，目前SpringCache支持r","title":"SpringCache"},{"content":"2017-11-10T10:28:34.939+0800 I COMMAND [conn57] command DJangoLearn.sale_mongo command: aggregate { aggregate: \u0026#34;sale_mongo\u0026#34;, pipeline: [{ $match: { id: 45 } }, { $group: { _id: \u0026#34;$id\u0026#34;, maxPrice: { $max: \u0026#34;$sellPrice\u0026#34; }, minPrice: { $min: \u0026#34;\\$sellPrice\u0026#34; }, avgPrice: {\\ $avg: \u0026#34;\\$sellPrice\u0026#34; } } }], cursor: {} } planSummary: COLLSCAN keysExamined: 0 docsExamined: 5000000 cursorExhausted: 1 numYields: 39194 nreturned: 1 reslen: 181 locks: { Global: { acquireCount: { r: 78396 } }, Database: { acquireCount: { r: 39198 } }, Collection: { acquireCount: { r: 39197 } } } protocol: op_query 3668 ms 参数:\nplanSummary: 查询的计划,集合扫描(CollectionScan)\nkeysExamined: key的扫描数量 #考虑创建或调整索引以提高查询性能 #IXSCAN 索引扫描(加索引)\ndocsExamined: 文档的扫描数量 (与结果行数量差太多,考虑加索引)\ncursorExhausted: 消耗的游标数\nnumYields:39194: 查询等待插入的次数 #计数器，报告操作已经完成的次数， nreturned: 返回的行数\nreslen: 操作结果文档的字节长度 #如果太长,考虑去除多余字段 locks:{\nGlobal: 全局下获取意向共享锁(表/页)的操作次数 (写)\nDatabase: 数据库获得的锁\nCollection: 集合获得的锁\n}\nprotocol: 协议\n2017-11-20T10:13:45.346+0800 I COMMAND [conn4] command DJangoLearn.sale_mongo command: aggregate { aggregate: \u0026#34;sale_mongo\u0026#34;, pipeline: [ { match: { id: 43 } }, {match: { id: 43 } }, {group: { _id: \u0026#34;id\u0026#34;, maxPrice: {id\u0026#34;, maxPrice: {max: \u0026#34;sellPrice\u0026#34; }, minPrice: {sellPrice\u0026#34; }, minPrice: {min: \u0026#34;sellPrice\u0026#34; }, avgPrice: {sellPrice\u0026#34; }, avgPrice: {avg: \u0026#34;$sellPrice\u0026#34; } } } ], cursor: {} } planSummary: IXSCAN { id: 1 } keysExamined:1000 docsExamined:1000 cursorExhausted:1 numYields:8 nreturned:1 reslen:181 locks:{ Global: { acquireCount: { r: 24 } }, Database: { acquireCount: { r: 12 } }, Collection: { acquireCount: { r: 11 } } } protocol:op_query 339ms ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/sql%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3.html","summary":"2017-11-10T10:28:34.939+0800 I COMMAND [conn57] command DJangoLearn.sale_mongo command: aggregate { aggregate: \u0026#34;sale_mongo\u0026#34;, pipeline: [{ $match: { id: 45 } }, { $group: { _id: \u0026#34;$id\u0026#34;, maxPrice: { $max: \u0026#34;$sellPrice\u0026#34; }, minPrice: { $min: \u0026#34;\\$sellPrice\u0026#34; }, avgPrice: {\\ $avg: \u0026#34;\\$sellPrice\u0026#34; } } }], cursor: {} } planSummary: COLLSCAN keysExamined: 0 docsExamined: 5000000 cursorExhausted: 1 numYields: 39194 nreturned: 1 reslen: 181 locks: { Global: { acquireCount: { r: 78396 } }, Database: {","title":"sql命令参数详解"},{"content":"[toc]\n1.少用count（distinct） count（distinct）是由一个reduce task来完成的，这一个reduce需要处理的数据量太大，就会导致整个job很难完成。 count（distinct）可以使用先group by再count的方式来替换 2. 合理使用MapJoin 3. 合理使用动态分区 动态分区可以优化诸如需要往指定分区插入数据的这种操作。\n配置参数\nhive.exec.dynamic.partition：是否开启动态分区，默认为false，设置成true hive.exec.dynamic.partition.mode：默认值表示必须指定至少一个静态分区，默认为strict，设置成nonstrict hive.exec.max.dynamic.partitions.pernode：在每个执行MR的节点上，最大可以创建多少个动态分区，默认100，按实际情况来定 hive.exec.max.created.files：整个MR Job中，最大可以创建多少个HDFS文件，默认值：100000，一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。 hive.error.on.empty.partition：当有空分区生成时，是否抛出异常，默认值：false，一般不需要设置。 4. 可以替代 in/exists 语句 hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：left semi join\nselect a.id, a.name from a left semi join b on a.id = b.id;\n5. 合理设置map/reduce数量 6. 避免数据倾斜 https://www.cnblogs.com/qingyunzong/p/8847775.html#_label4 https://www.cnblogs.com/duanxingxing/p/6874318.html https://blog.csdn.net/qq_32038679/article/details/80557286 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/sql%E4%BC%98%E5%8C%96.html","summary":"[toc] 1.少用count（distinct） count（distinct）是由一个reduce task来完成的，这一个reduce需要处理的数据","title":"sql优化"},{"content":"第一步：应用程序把查询SQL语句发给服务器端执行 我们在数据层执行SQL语句时，应用程序会连接到相应的数据库服务器，把SQL语句发送给服务器处理。\n第二步：服务器解析请求的SQL语句 SQL计划缓存，经常用查询分析器的朋友大概都知道这样一个事实，往往一个查询语句在第一次运行的时候需要执行特别长的时间，但是如果你马上或者在一定时间内运行同样的语句，会在很短的时间内返回查询结果。原因是：\n服务器在接收到查询请求后，并不会马上去数据库查询，而是在数据库中的计划缓存中找是否有相对应的执行计划。如果存在，就直接调用已经编译好的执行计划，节省了执行计划的编译时间。 如果所查询的行已经存在于数据缓冲存储区中，就不用查询物理文件了，而是从缓存中取数据，这样从内存中取数据就会比从硬盘上读取数据快很多，提高了查询效率。数据缓冲存储区会在后面提到。 如果在SQL计划缓存中没有对应的执行计划，服务器首先会对用户请求的SQL语句进行语法效验，如果有语法错误，服务器会结束查询操作，并用返回相应的错误信息给调用它的应用程序。\n注意：此时返回的错误信息中，只会包含基本的语法错误信息，例如select 写成selec等，错误信息中如果包含一列表中本没有的列，此时服务器是不会检查出来的，因为只是语法验证，语义是否正确放在下一步进行。\n语法符合后，就开始验证它的语义是否正确。例如，表名、列名、存储过程等等数据库对象是否真正存在，如果发现有不存在的，就会报错给应用程序，同时结束查询。\n接下来就是获得对象的解析锁，我们在查询一个表时，首先服务器会对这个对象加锁，这是为了保证数据的统一性，如果不加锁，此时有数据插入，但因为没有加锁的原因，查询已经将这条记录读入，而有的插入会因为事务的失败会回滚，就会形成脏读的现象。\n接下来就是对数据库用户权限的验证。SQL语句语法，语义都正确，此时并不一定能够得到查询结果，如果数据库用户没有相应的访问权限，服务器会报出权限不足的错误给应用程序，在稍大的项目中，往往一个项目里面会包含好几个数据库连接串，这些数据库用户具有不同的权限，有的是只读权限，有的是只写权限，有的是可读可写，根据不同的操作选取不同的用户来执行。稍微不注意，无论你的SQL语句写的多么完善，完美无缺都没用。\n解析的最后一步，就是确定最终的执行计划。当语法、语义、权限都验证后，服务器并不会马上给你返回结果，而是会针对你的SQL进行优化，选择不同的查询算法以最高效的形式返回给应用程序。例如在做表联合查询时，服务器会根据开销成本来最终决定采用hash join,merge join ，还是loop join，采用哪一个索引会更高效等等。不过它的自动化优化是有限的，要想写出高效的查询SQL还是要优化自己的SQL查询语句。\n当确定好执行计划后，就会把这个执行计划保存到SQL计划缓存中，下次在有相同的执行请求时，就直接从计划缓存中取，避免重新编译执行计划。\n第三步：语句执行 服务器对SQL语句解析完成后，服务器才会知道这条语句到底表态了什么意思，接下来才会真正的执行SQL语句。\n此时分两种情况：\n如果查询语句所包含的数据行已经读取到数据缓冲存储区的话，服务器会直接从数据缓冲存储区中读取数据返回给应用程序，避免了从物理文件中读取，提高查询速度。 如果数据行没有在数据缓冲存储区中，则会从物理文件中读取记录返回给应用程序，同时把数据行写入数据缓冲存储区中，供下次使用。 来源: https://www.cnblogs.com/wuyun-blog/p/4697144.html 示意图 https://www.cnblogs.com/ChangAn223/p/10686639.html 扩展 sql的解析顺序: https://www.bilibili.com/video/BV12b411K7Zu?p=189 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B.html","summary":"第一步：应用程序把查询SQL语句发给服务器端执行 我们在数据层执行SQL语句时，应用程序会连接到相应的数据库服务器，把SQL语句发送给服务器处","title":"sql执行过程"},{"content":" Spark Streaming与Storm的优劣分析\n事实上，Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。\nSpark Streaming仅仅在吞吐量上比Storm要优秀，而吞吐量这一点，也是历来挺Spark Streaming，贬Storm的人着重强调的。但是问题是，是不是在所有的实时计算场景下，都那么注重吞吐量？不尽然。因此，通过吞吐量说Spark Streaming强于Storm，不靠谱。\n事实上，Storm在实时延迟度上，比Spark Streaming就好多了，前者是纯实时，后者是准实时。而且，Storm的事务机制、健壮性 / 容错性、动态调整并行度等特性，都要比Spark Streaming更加优秀。\nSpark Streaming，有一点是Storm绝对比不上的，就是：它位于Spark生态技术栈中，因此Spark Streaming可以和Spark Core、Spark SQL无缝整合，也就意味着，我们可以对实时处理出来的中间数据，立即在程序中无缝进行延迟批处理、交互式查询等操作。这个特点大大增强了Spark Streaming的优势和功能。\n来自* \u0026lt;http://blog.csdn.net/kwu_ganymede/article/details/50296831 \u0026gt;\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/storm%E4%B8%8Esparkstreaming.html","summary":"Spark Streaming与Storm的优劣分析 事实上，Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优","title":"Storm与SparkStreaming"},{"content":"[toc]\n1. 介绍 Structured Streaming是Spark2.0版本提出的新的实时流框架（2.0和2.1是实验版本，从Spark2.2开始为稳定版本），相比于Spark Streaming，优点如下：\n1.同样能支持多种数据源的输入和输出，参考如上的数据流图\n2.以结构化的方式操作流式数据，能够像使用Spark SQL处理离线的批处理一样，处理流数据，代码更简洁，写法更简单\n3.基于Event-Time，相比于Spark Streaming的Processing-Time更精确，更符合业务场景\n4.解决了Spark Streaming存在的代码升级，DAG图变化引起的任务失败，无法断点续传的问题（Spark Streaming的硬伤！！！）\n原文链接：https://blog.csdn.net/lovechendongxing/article/details/81748237\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/structuredstreaming.html","summary":"[toc] 1. 介绍 Structured Streaming是Spark2.0版本提出的新的实时流框架（2.0和2.1是实验版本，从Spark2.2开始为稳定版本），相比于","title":"StructuredStreaming"},{"content":"[TOC]\n介绍 Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。Swagger 让部署管理和使用功能强大的API从未如此简单。\n使用 pom.xml \u0026lt;!-- swagger2 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springfox-swagger2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- swagger 原生ui --\u0026gt; \u0026lt;!-- \u0026lt;dependency\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;groupId\u0026gt;io.springfox\u0026lt;/groupId\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;artifactId\u0026gt;springfox-swagger-ui\u0026lt;/artifactId\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;version\u0026gt;2.9.2\u0026lt;/version\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;/dependency\u0026gt; --\u0026gt; \u0026lt;!-- bootstrap 写的ui--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.xiaoymin\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;swagger-bootstrap-ui\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.9.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- spring4All自己封装的,用的swagger自带的ui,这个一个包就足够了, 1.7版本用的是swagger2.9.0,但很多库没有这个包...所以跨过这个版本 --\u0026gt; \u0026lt;!-- \u0026lt;dependency\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;groupId\u0026gt;com.spring4all\u0026lt;/groupId\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;artifactId\u0026gt;swagger-spring-boot-starter\u0026lt;/artifactId\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;version\u0026gt;1.9.0.RELEASE\u0026lt;/version\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;/dependency\u0026gt; --\u0026gt; \u0026lt;!--高本版好像没有guava包,报错就加上--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;27.0.1-jre\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 重点是springfox-swagger2 包, UI可以随便,bootstrp的界面更友好\nbootstrp地址:http://localhost:8998/doc.html swagger-ui地址: http://localhost:8998/swagger-ui.html java 配置类 @Configuration // 表明这个类是个配置类 @EnableSwagger2 // 启用swagger public class Swagger2 { @Value(\u0026#34;${sop.swagger.enable:false}\u0026#34;) //默认值为false public boolean isEnable; @Bean public Docket createRestApi() { System.out.println(\u0026#34;环境取值: isEnable:\u0026#34;+isEnable); return new Docket(DocumentationType.SWAGGER_2) .enable(isEnable) .apiInfo(apiInfo()) .select() // 扫描的包路径 .apis(RequestHandlerSelectors.basePackage(\u0026#34;com.example.demo\u0026#34;)) // 监听所有方法 .paths(PathSelectors.any()) .build(); } // 构建 api文档的详细信息函数,注意这里的注解引用的是哪个 private ApiInfo apiInfo() { return new ApiInfoBuilder() // 页面标题 .title(\u0026#34;swagger的界面\u0026#34;) // 创建人 .contact(new Contact(\u0026#34;xkj\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;)) // 版本号 .version(\u0026#34;1.0\u0026#34;) // 描述 .description(\u0026#34;这是我的swagger\u0026#34;).build(); } 使用 @Api(value=\u0026#34;测试控制类-val\u0026#34;) @RestController @RequestMapping(\u0026#34;/test\u0026#34;) @Slf4j public class TestControll{ @GetMapping(\u0026#34;hello\u0026#34;) @ApiOperation(value=\u0026#34;嗨\u0026#34;,notes=\u0026#34;简单接口\u0026#34; ) public BaseResponse hello() { BaseResponse response = new BaseResponse(); setResponse(response, ResponseEnum.JS002) ; return response; } @PutMapping(\u0026#34;addHelp\u0026#34;) @ApiParam(required=true,name=\u0026#34;help\u0026#34;,value=\u0026#34;帮助类2\u0026#34;) public BaseResponse addHelp(@RequestBody HelpInfo help) { log.info(\u0026#34;入参:{}\u0026#34;,help); BaseResponse response= new BaseResponse(); setSuccessResponse(response); log.info(\u0026#34;返参:{}\u0026#34;,LocalDateTime.now()); return response; } } 注解 @Api()\n用于类； 表示标识这个类是swagger的资源\ntags–表示说明 value–也是说明，可以使用tags替代,1.5以后这个将废弃 @ApiOperation()\n用于方法； 表示一个http请求的操作\nvalue用于方法描述 notes用于提示内容 tags可以重新分组（视情况而用） @ApiParam()\n用于方法，参数，字段说明； 表示对参数的添加元数据（说明或 必填等）\nname–参数名 value–参数说明 required–是否必填 @ApiModel() 用于类 表示对类进行说明，用于参数用实体类接收\nvalue–表示对象名 description–描述 @ApiModelProperty()\n用于方法，字段,表示对model属性的说明或者数据操作更改\nvalue–字段说明 name–重写属性名字 dataType–重写属性类型 required–是否必填 example–举例说明 hidden–隐藏 @ApiIgnore()\n用于类或者方法上，可以不被swagger显示在页面上\n@ApiImplicitParam()\n用于方法 表示单独的请求参数\nname–参数ming value–参数说明 dataType–数据类型 paramType–参数类型,包括 query,header,path,body,form example–举例说明 defaultValue：参数的默认值 @ApiImplicitParams() 用于方法，包含多个 @ApiImplicitParam\n@ApiResponses：用于包含接口的一组响应。\n@ApiResponse：用在注解@ApiResponses中，表达一个错误的响应信息。\n来源: https://blog.csdn.net/wyb880501/article/details/79576784 异常 访问页面弹框提示baseUrl不对 如图:\n原因有二:\n没有加载到swagger配置类 swagger的资源被拦截器拦截了(暂未遇到) 解决:\n检查swagger配置类是否能被项目扫描到; 检查配置类上的@Configuration 和@EnableSwagger2; 在过滤器中排除swagger的资源 扩展 使用swagger是不是有以下问题:\n每个项目都要集成swagger-mg-ui包，可不可以 文档和项目分开 ？ document.html是个静态页面，怎么能做一些 权限控制 呢？登录后才能查看 其他编程语言 产生的swagger文档怎么可以使用这个文档工具查看呢？ 可不可以关闭线上的文档，用 本地的文档页面调试线上的接口 呢？ 项目非常的多，需要记许多的文档地址，能不能 统一管理 呢？ zyplayer-doc-swagger 可以解决这些烦恼. 这玩意把文档管理做成了一个系统,而不仅仅是个文档,\nhttps://gitee.com/zyplayer/zyplayer-doc ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%88%86%E6%94%AF/swagger-springboot.html","summary":"[TOC] 介绍 Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的","title":"Swagger-SpringBoot"},{"content":"[toc]\n滑动窗口 我们先来看发送方与接收方之间，两者之间的交互\n如图，这个就是我们把两个包一起发送，然后一起确认。我们改善了吞吐量的问题\n问题：我们每次需要发多少个包过去呢？发送多少包是最优解呢？\n我们能不能把第一个和第二个包发过去后，收到第一个确认包就把第三个包发过去呢？而不是去等到第二个包的确认包才去发第三个包。这样就很自然的产生了我们\u0026quot;滑动窗口\u0026quot;的实现。\n在图中，我们可看出灰色1号2号3号包已经发送完毕，并且已经收到Ack。这些包就已经是过去式。4、5、6、7号包是黄色的，表示已经发送了。但是并没有收到对方的Ack，所以也不知道接收方有没有收到。8、9、10号包是绿色的。是我们还没有发送的。这些绿色也就是我们接下来马上要发送的包。 可以看出我们的窗口正好是11格。后面的11-16还没有被读进内存。要等4号-10号包有接下来的动作后，我们的包才会继续往下发送。\n正常情况 可以看到4号包对方已经被接收到，所以被涂成了灰色。“窗口”就往右移一格，这里只要保证“窗口”是7格的。 我们就把11号包读进了我们的缓存。进入了“待发送”的状态。8、9号包已经变成了黄色，表示已经发送出去了。接下来的操作就是一样的了，确认包后，窗口往后移继续将未发送的包读进缓存，把“待发送“状态的包变为”已发送“。\n丢包情况 有可能我们包发过去，对方的Ack丢了。也有可能我们的包并没有发送过去。从发送方角度看就是我们没有收到Ack。\n发生的情况：一直在等Ack。如果一直等不到的话，我们也会把读进缓存的待发送的包也一起发过去。但是，这个时候我们的窗口已经发满了。所以并不能把12号包读进来，而是始终在等待5号包的Ack。\n超时重发 这里有一点要说明：这个Ack是要按顺序的。必须要等到5的Ack收到，才会把6-11的Ack发送过去。这样就保证了滑动窗口的一个顺序。\n这时候可以看出5号包已经接受到Ack，后面的6、7、8号包也已经发送过去已Ack。窗口便继续向后移动。\n这个与gateway的滑动窗口限流有点不一样 , gateway的限流是会丢弃流量的, tcp的滑动窗口不会丢弃, 都在后面排队\n一篇带你读懂TCP之“滑动窗口”协议 - Coder编程 - 博客园 (cnblogs.com) TCP滑动窗口 - alifpga - 博客园 (cnblogs.com) 粘包和拆包 TCP 协议是流式协议。所谓流式协议，即协议的内容是像流水一样的字节流，内容与内容之间没有明确的分界标志，需要我们人为地去给这些协议划分边界。\nA 与 B 进行 TCP 通信，A 先后给 B 发送了一个 100 字节和 200 字节的数据包，B 可能收到以一次或者多次任意形式的总数为 300 字节, 对于 B 来说，如果不人为规定多少字节作为一个数据包，B 每次是不知道应该把收到的数据中多少字节作为一个有效的数据包的，而规定每次把多少数据当成一个包就是协议格式定义的内容之一。\n粘包 有的面试官可能会这么问：网络通信时，如何解决粘包、丢包或者包乱序问题？\n如果是 TCP 协议，在大多数场景下，是不存在丢包和包乱序问题的，TCP 通信是可靠通信方式，TCP 协议栈通过序列号和包重传确认机制保证数据包的有序和一定被正确发到目的地；\n如果是 UDP 协议，如果不能接受少量丢包，那就要自己在 UDP 的基础上实现类似 TCP 这种有序和可靠传输机制了（例如 RTP 协议、RUDP 协议）。所以，问题拆解后，只剩下如何解决粘包的问题。\n如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。\n接收端收到粘包后, 如何正确的分包呢?\n分包方法 固定包长的数据包 即每个协议包的长度都是固定的。举个例子，例如我们可以规定每个协议包的大小是 64 个字节，每次收满 64 个字节，就取出来解析（如果不够，就先存起来）。\n这种通信协议的格式简单但灵活性差。如果包内容不足指定的字节数，剩余的空间需要填充特殊的信息，如 ；如果包内容超过指定字节数，又得分包分片，需要增加额外处理逻辑——在发送端进行分包分片，在接收端重新组装包片（分包和分片内容在接下来会详细介绍）。\n以指定字符（串）为包的结束标志 这种协议包比较常见，即字节流中遇到特殊的符号值时就认为到一个包的末尾了。例如，我们熟悉的 FTP协议，发邮件的 SMTP 协议，一个命令或者一段数据后面加上\u0026quot; \u0026ldquo;（即所谓的 CRLF）表示一个包的结束。对端收到后，每遇到一个” “就把之前的数据当做一个数据包。\n这种协议一般用于一些包含各种命令控制的应用中，其不足之处就是如果协议数据包内容部分需要使用包结束标志字符，就需要对这些字符做转码或者转义操作，以免被接收方错误地当成包结束标志而误解析。\n包头 + 包体格式 这种格式的包一般分为两部分，即包头和包体，包头是固定大小的，且包头中必须含有一个字段来说明接下来的包体有多大。例如:\nstruct msg_header { int32_t bodySize; int32_t cmd; }; 这就是一个典型的包头格式，bodySize 指定了这个包的包体是多大。由于包头大小是固定的（这里是 size(int32_t) + sizeof(int32_t) = 8 字节），对端先收取包头大小字节数目（当然，如果不够还是先缓存起来，直到收够为止），然后解析包头，根据包头中指定的包体大小来收取包体，等包体收够了，就组装成一个完整的包来处理。在有些实现中，包头中的 bodySize可能被另外一个叫 packageSize 的字段代替，这个字段的含义是整个包的大小，这个时候，我们只要用 packageSize 减去包头大小（这里是 sizeof(msg_header)）就能算出包体的大小，原理同上。\n解包与处理 这里我们以 包头 + 包体 这种格式的数据包来说明。处理流程如下：\n拆包 如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。\n拆包之后, 可能只发送了一部分包, 这种包也称为半包.\nNetty对粘包和拆包问题的处理 Netty对解决粘包和拆包的方案做了抽象，提供了一些解码器（Decoder）来解决粘包和拆包的问题。如：\nLineBasedFrameDecoder：以行为单位进行数据包的解码 (这种其实就是以特殊符号结尾作为标识)； DelimiterBasedFrameDecoder：以特殊的符号作为分隔来进行数据包的解码； FixedLengthFrameDecoder：以固定长度进行数据包的解码； LenghtFieldBasedFrameDecode：适用于消息头包含消息长度的协议（最常用）； ByteToMessageDecoder : 根据你的协议具体格式重写 decode() 方法来对数据包解包 面试题：聊聊TCP的粘包、拆包以及解决方案 - 知乎 (zhihu.com) 怎么解决TCP网络传输「粘包」问题？ - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/tcp.html","summary":"[toc] 滑动窗口 我们先来看发送方与接收方之间，两者之间的交互 如图，这个就是我们把两个包一起发送，然后一起确认。我们改善了吞吐量的问题 问题：我们每次","title":"TCP"},{"content":"[toc]\n1. ThreadLocal是什么？ 从名字我们就可以看到ThreadLocal 叫做本地线程变量，意思是说，ThreadLocal 中填充的的是当前线程的变量，该变量对其他线程而言是封闭且隔离的，ThreadLocal 为变量在每个线程中创建了一个副本，这样每个线程都可以访问自己内部的副本变量。\n简单实用\n@Test public void testThreadLocal() { ThreadLocal\u0026lt;String\u0026gt; local = new ThreadLocal\u0026lt;\u0026gt;(); IntStream.range(0, 10) .forEach(i -\u0026gt; new Thread(() -\u0026gt; { local.set(Thread.currentThread().getName() + \u0026#34;:\u0026#34; + i); System.out.println(\u0026#34;线程：\u0026#34; + Thread.currentThread().getName() + \u0026#34;,local:\u0026#34; + local.get()); }).start() ); } 输出结果： 线程：Thread-0,local:Thread-0:0 线程：Thread-1,local:Thread-1:1 线程：Thread-2,local:Thread-2:2 线程：Thread-3,local:Thread-3:3 线程：Thread-4,local:Thread-4:4 线程：Thread-5,local:Thread-5:5 线程：Thread-6,local:Thread-6:6 线程：Thread-7,local:Thread-7:7 线程：Thread-8,local:Thread-8:8 线程：Thread-9,local:Thread-9:9 2. ThreadLocal的具体实现 2.1 ThreadLocal结构 每一个线程都有一个 ThreadLocalMap ； 该 Map 底层由 Entry 数组构成，含有多个 Entry ； Entry 中 key 为 ThreadLocal 的弱引用， value 为我们保存的值 线程下的threadLocalMap\npublic class Thread implements Runnable { /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; } 2.2 具体实现 从结构可以看出, 每个线程在向ThreadLocal里塞值的时候，其实都是向自己所持有的ThreadLocalMap里塞入数据；读的时候同理，首先从自己线程中取出自己持有的ThreadLocalMap，然后再根据ThreadLocal引用作为key取出value，基于以上描述，ThreadLocal实现了变量的线程隔离（当然，毕竟变量其实都是从自己当前线程实例中取出来的）。\n所以在上面的案例中, 会生成10个map, 每个map中就只有一个元素, key都是local变量, value是线程名称\n由此看出 , threadLocal底层使用map结构存储信息, key为当前的线程下的threadLocal对象, value 是我们的业务值\n它与hashMap有很多相似之处, 比如 扩展因子, 初始化大小等等, 但是threadLocal解决hash冲突使用的线性探测\n解决哈希冲突的四种方法 1.开放地址方法(再散列法)\n可以通俗理解为所有的地址都对所有的数值开放，而不是链式地址法的封闭方式，一个数值固定在一个索引地址位置。p1=hash(key)如果冲突就在p1地址的基础上+1或者散列处理，p2=hash(p1)\u0026hellip;.\n(1）线性探测\n按顺序决定值时，如果某数据的值已经存在，则在原来值的基础上往后加一个单位，直至不发生哈希冲突。\n（2）再平方探测\n按顺序决定值时，如果某数据的值已经存在，则在原来值的基础上先加1的平方个单位，若仍然存在则加1的平方个单位。随之是2的平方，3的平方等等。直至不发生哈希冲突。\n和线性探测相比就是改变探测了步长。因为如果都是+1来探测在数据量比较大的情况下，效率会很差。\n2.链式地址法\n对于相同的值，使用链表进行连接。使用数组存储每一个链表。（HashMap的哈希冲突解决方法）\n3.建立公共溢出区\n建立公共溢出区存储所有哈希冲突的数据。\n4.再哈希法\n对于冲突的哈希值再次进行哈希处理，直至没有哈希冲突。\n一文理解哈希冲突四种解决方法 - 简书 (jianshu.com) 2.3 引用关系 2.3.1 threadLocal引用关系 引用关系如上图所示：\n在整个引用链路中，只有 ThreadLocal 是采用了弱引用的方式进行声明的。\n2.3.2 扩展 Java 中引用有四种方式。\n强引用： 通过 new 关键字产生的引用关系，无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收被引用的对象；\n软引用： SoftReference 内存空间不足时，发生GC, 垃圾回收器就会回收它, 可用来实现内存敏感的高速缓存。\n弱引用： 声明时，通过 WeakReference 包裹，强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。\n虚引用：虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的 存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚 引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。\nJava中提供这四种引用类型主要有两个目的 ：\n可以让程序员通过代码的方式决定某些对象的生命周期。 有利于JVM进行垃圾回收。 回收前提.md JVM 垃圾回收详解-引用类型 | JavaGuide 3. 源码解读 3.1 从set方法开始 因为set是线程级别的, 所有的操作都在当前线程下, 所以set操作本身就是线程安全的\nThreadLocal.set(T value)\npublic void set(T value) { Thread t = Thread.currentThread(); // 获取当前线程 ThreadLocalMap map = getMap(t);// 获取当前线程的ThreadLocalMap if (map != null) map.set(this, value);// map不为空则调用map的set方法 else createMap(t, value);// map为空则调用createMap方法 } 先看看 map 为空时， createMap 方法是怎么创建 map 的。\nThreadLocal.createMap(Thread t, T firstValue)\nvoid createMap(Thread t, T firstValue) { // 为传入的线程实例化一个map，传入了自身的引用 t.threadLocals = new ThreadLocalMap(this, firstValue); } /** * 构造一个最初包含 (firstKey, firstValue) 的新映射。 * ThreadLocalMaps 是惰性构建的，所以我们只有在至少有一个条目可以放入时才创建一个。 */ ThreadLocalMap(ThreadLocal\u0026lt;?\u0026gt; firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode \u0026amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } 该构造方法主要做了以下操作：\n创建一个默认长度的 Entry 数组 计算出传入的 ThreadLocal 应在数组中的位置 实例化 Entry 放到对应位置上 ThreadLocalMap 元素数置为1 设置要调整大小的下一个值 我们看看 ThreadLocalMap 的基础信息；\n/** * 初始容量 - 必须是 2 的幂次方 */ private static final int INITIAL_CAPACITY = 16; /** * table,长度必须为 2 的幂次方 */ private Entry[] table; /** * table中元素的个数 */ private int size = 0; /** * 要调整大小的下一个大小值。 */ private int threshold; // 默认值为 0 /** * Entry对象继承了弱引用(当发生垃圾回收时就会回收) */ static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } 主要说一下计算索引，firstKey.threadLocalHashCode \u0026amp; (INITIAL_CAPACITY - 1)。\n关于\u0026amp; (INITIAL_CAPACITY - 1),这是取模的一种方式，对于2的幂作为模数取模，用此代替%(2^n)，这也就是为啥容量必须为2的冥，在这个地方也得到了解答，至于为什么可以这样这里不过多解释，原理很简单。\n关于firstKey.threadLocalHashCode：\nprivate final int threadLocalHashCode = nextHashCode(); private static int nextHashCode() { return nextHashCode.getAndAdd(HASH_INCREMENT); } private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; 定义了一个AtomicInteger类型，每次获取当前值并加上HASH_INCREMENT，HASH_INCREMENT = 0x61c88647,关于这个值和斐波那契散列有关，其主要目的就是为了让哈希码能均匀的分布在2的n次方的数组里, 也就是Entry[] table中。\n常见的散列方法是取模散列, 而斐波那契散列是另一种散列方法\n从 ThreadLocal 的实现看散列算法 - 知乎 (zhihu.com) 再看回set方法\nThreadLocalMap.set(ThreadLocal\u003c?\u003e key, Object value)\nThreadLocalMap使用线性探测法来解决哈希冲突，线性探测法的地址增量di = 1, 2, \u0026hellip; , m-1，其中，i为探测次数。该方法一次探测下一个地址，直到有空的地址后插入，若整个空间都找不到空余的地址，则产生溢出。假设当前table长度为16，也就是说如果计算出来key的hash值为14，如果table[14]上已经有值，并且其key与当前key不一致，那么就发生了hash冲突，这个时候将14加1得到15，取table[15]进行判断，这个时候如果还是冲突会回到0，取table[0],以此类推，直到可以插入。所以 可以把table看成一个环形数组。\n在set的时候, 会去判断 entry==null 和 key==null的哈希槽, 并删除这些槽位, 由于使用线性探测方式, 还会挪动冲突的key的位置, 使得相同key紧凑一起\nprivate void set(ThreadLocal\u0026lt;?\u0026gt; key, Object value) { // We don\u0026#39;t use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; // 获取ThreadLocal的hashCode，计算索引位置 int i = key.threadLocalHashCode \u0026amp; (len-1); // 该索引位置上是否有元素，如果有元素的话就进行线性探测 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); // 说明该key已经存在，则覆盖旧值 if (k == key) { e.value = value; return; } /** * table[i]上的key为空，说明被回收了（上面的弱引用中提到过）。 * 这个时候说明改table[i]可以重新使用，用新的key-value将其替换,并删除其他无效的entry */ if (k == null) { replaceStaleEntry(key, value, i); return; } } // 该索引位置上没有元素，则新建Entry tab[i] = new Entry(key, value); int sz = ++size; // 不需要清理 空哈希槽或者槽key为null的， 并且大于等于扩容值，则进行rehash， if (!cleanSomeSlots(i, sz) \u0026amp;\u0026amp; sz \u0026gt;= threshold) // 默认16，以2的倍数扩容 rehash(); } /**java /** * 获取环形数组的下一个索引 */ private static int nextIndex(int i, int len) { return ((i + 1 \u0026lt; len) ? i + 1 : 0); } /** * 获取环形数组的上一个索引 */ private static int prevIndex(int i, int len) { return ((i - 1 \u0026gt;= 0) ? i - 1 : len - 1); } replaceStaleEntry(ThreadLocal\u003c?\u003e key, Object value, int staleSlot)\n// 好复杂, 看看网上文章,以后深入了解吧\nThreadLocal源码分析 - 简书 (jianshu.com) 3.2 ThreadLocal中的get() public T get() { //同set方法类似获取对应线程中的ThreadLocalMap实例 Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } //为空返回初始化值 return setInitialValue(); } /** * 初始化设值的方法，可以被子类覆盖。 */ protected T initialValue() { return null; } private T setInitialValue() { //获取初始化值，默认为null(如果没有子类进行覆盖) T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); //不为空不用再初始化，直接调用set操作设值 if (map != null) map.set(this, value); else //第一次初始化，createMap在上面介绍set()的时候有介绍过。 createMap(t, value); return value; } ThreadLocalMap中的getEntry()\nprivate ThreadLocal.ThreadLocalMap.Entry getEntry(ThreadLocal\u0026lt;?\u0026gt; key) { //根据key计算索引，获取entry int i = key.threadLocalHashCode \u0026amp; (table.length - 1); ThreadLocal.ThreadLocalMap.Entry e = table[i]; if (e != null \u0026amp;\u0026amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); } /** * 通过直接计算出来的key找不到对于的value的时候适用这个方法. */ private ThreadLocal.ThreadLocalMap.Entry getEntryAfterMiss(ThreadLocal\u0026lt;?\u0026gt; key, int i, ThreadLocal.ThreadLocalMap.Entry e) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) return e; if (k == null) //清除无效的entry expungeStaleEntry(i); else //基于线性探测法向后扫描 i = nextIndex(i, len); e = tab[i]; } return null; } 3.3 ThreadLocalMap中的remove() private void remove(ThreadLocal\u0026lt;?\u0026gt; key) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; //计算索引 int i = key.threadLocalHashCode \u0026amp; (len-1); //进行线性探测，查找正确的key for (ThreadLocal.ThreadLocalMap.Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { if (e.get() == key) { //调用weakrefrence的clear()清除引用 e.clear(); //连续段清除 expungeStaleEntry(i); return; } } } 4. 内存泄露 4.1 ThreadLocal 内存泄漏的原因 从上图中可以看出，hreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal不存在外部强引用时，Key(ThreadLocal)势必会被GC回收，这样就会导致ThreadLocalMap中key为null， 而value还存在着强引用，只有thead线程退出以后,value的强引用链条才会断掉。\n但如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：\nThread Ref -\u0026gt; Thread -\u0026gt; ThreaLocalMap -\u0026gt; Entry -\u0026gt; value\n永远无法回收，造成内存泄漏。, 所以泄露的是value值\n4.2 ThreadLocal正确的使用方法 每次使用完ThreadLocal都调用它的remove()方法清除数据 其实调用set()和get() 都有可能也会清除数据\nThreadLocal会在以下过程中清理过期节点：\n调用set()方法时，采样清理、全量清理，扩容时还会继续检查。 调用get()方法，没有直接命中，向后环形查找时。 调用remove()时，除了清理当前Entry，还会向后继续清理。 使用ThreadLocal时，一般建议将其声明为static final的，避免频繁创建ThreadLocal实例。 4.3 总结 由于Thread中包含变量ThreadLocalMap，因此ThreadLocalMap与Thread的生命周期是一样长，如果都没有手动删除对应key，都会导致内存泄漏。\n但是使用弱引用可以多一层保障：弱引用ThreadLocal不会内存泄漏，对应的value在下一次ThreadLocalMap调用set(),get(),remove()的时候会被清除。\n因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。\nThreadLocal的内存泄露？什么原因？如何避免？ - 知乎 (zhihu.com) 5. 使用场景 5.1 场景一：代替参数的显式传递 当我们在写API接口的时候，通常Controller层会接受来自前端的入参，当这个接口功能比较复杂的时候，可能我们调用的Service层内部还调用了 很多其他的很多方法，通常情况下，我们会在每个调用的方法上加上需要传递的参数。\n但是如果我们将参数存入ThreadLocal中，那么就不用显式的传递参数了，而是只需要ThreadLocal中获取即可。\n这个场景其实使用的比较少，一方面显式传参比较容易理解，另一方面我们可以将多个参数封装为对象去传递。\n5.2 场景二：解决线程安全问题 在Spring的Web项目中，我们通常会将业务分为Controller层，Service层，Dao层， 我们都知道@Autowired注解默认使用单例模式，那么不同请求线程进来之后，由于Dao层使用单例，那么负责数据库连接的Connection也只有一个， 如果每个请求线程都去连接数据库，那么就会造成线程不安全的问题，Spring是如何解决这个问题的呢？\n在Spring项目中Dao层中装配的Connection肯定是线程安全的，其解决方案就是采用ThreadLocal方法，当每个请求线程使用Connection的时候， 都会从ThreadLocal获取一次，如果为null，说明没有进行过数据库连接，连接后存入ThreadLocal中，如此一来，每一个请求线程都保存有一份 自己的Connection。于是便解决了线程安全问题\nThreadLocal在设计之初就是为解决并发问题而提供一种方案，每个线程维护一份自己的数据，达到线程隔离的效果。\n5.3 场景三：全局存储用户信息 在现在的系统设计中，前后端分离已基本成为常态，分离之后如何获取用户信息就成了一件麻烦事，通常在用户登录后， 用户信息会保存在Session或者Token中。这个时候，我们如果使用常规的手段去获取用户信息会很费劲，拿Session来说，我们要在接口参数中加上HttpServletRequest对象，然后调用 getSession方法，且每一个需要用户信息的接口都要加上这个参数，才能获取Session，这样实现就很麻烦了。\n在实际的系统设计中，我们肯定不会采用上面所说的这种方式，而是使用ThreadLocal，我们会选择在拦截器的业务中， 获取到保存的用户信息，然后存入ThreadLocal，那么当前线程在任何地方如果需要拿到用户信息都可以使用ThreadLocal的get()方法\n(异步程序中ThreadLocal是不可靠的, 因为threadLocal是线程级别的, 所以只要开了新线程都会丢失信息\n例如, 使用了线程池, future, fegin调用 就会丢失threadlocal里的数据(除非threadLocal是全局的)\n6. 实战 6.1 代码 具体实现流程：\n在登录业务代码中，当用户登录成功时，生成一个登录凭证存储到redis中，将凭证中的字符串保存在cookie中返回给客户端。 使用一个拦截器拦截请求，从cookie中获取凭证字符串与redis中的凭证进行匹配，获取用户信息，将用户信息存储到ThreadLocal中，在本次请求中持有用户信息，即可在后续操作中使用到用户信息。 定义工具类操作[ThreadLocal]（存放，获取，删除用户信息）\npublic class ThreadLocalUtil { /** * 保存用户对象的ThreadLocal 在拦截器操作 添加、删除相关用户数据 */ private static final ThreadLocal\u0026lt;FeginUser\u0026gt; userThreadLocal = new ThreadLocal\u0026lt;FeginUser\u0026gt;(); /** * 添加当前登录用户方法 在拦截器方法执行前调用设置获取用户 * @param user */ public static void addCurrentUser(FeginUser user){ userThreadLocal.set(user); } /** * 获取当前登录用户方法 */ public static FeginUser getCurrentUser(){ return userThreadLocal.get(); } /** * 删除当前登录用户方法 在拦截器方法执行后 移除当前用户对象 */ public static void remove(){ userThreadLocal.remove(); } } 拦截器\n@Component @Slf4j public class UserInfoInterceptor implements HandlerInterceptor { @Autowired private UserInfoUtil userInfoUtil; /** * 请求执行前执行的，将用户信息放入ThreadLocal * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { FeginUser user; try{ user = userInfoUtil.getUser(request); }catch (CustomException e){ log.info(\u0026#34;***************************用户未登录， ThreadLocal无信息***************************\u0026#34;); return true; } if (null!=user) { log.info(\u0026#34;***************************用户已登录，用户信息放入ThreadLocal***************************\u0026#34;); ThreadLocalUtil.addCurrentUser(user); return true; } log.info(\u0026#34;***************************用户未登录， ThreadLocal无信息***************************\u0026#34;); return true; } /** * 接口访问结束后，从ThreadLocal中删除用户信息 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { log.info(\u0026#34;***************************接口调用结束， 从ThreadLocal删除用户信息***************************\u0026#34;); ThreadLocalUtil.remove(); } 6.2 异常处理 6.2.1 feign调用丢失请求头 从请求头中获取登录token, 会有个问题, 如果上游是fegin调用, 则请求头会丢失, 所以需要在fegin调用时手动设置token, 因为feign调用会使用新的http请求且不会携带原来http的header信息\n@Bean public RequestInterceptor requestInterceptor() { //处理feign远程调用丢失请求头问题 return template -\u0026gt; { HttpRequestUtil.getHttpHeader(Constant.AUTHORIZATION) .ifPresent(auth -\u0026gt; template.header(Constant.AUTHORIZATION, auth)); HttpRequestUtil.getHttpHeader(Constant.POWER_MENU_ID) .ifPresent(auth -\u0026gt; template.header(Constant.POWER_MENU_ID, auth)); HttpRequestUtil.getHttpHeader(Constant.X_FLAG) .ifPresent(auth -\u0026gt; template.header(Constant.X_FLAG, auth)); HttpRequestUtil.getHttpHeader(Constant.PLATFORM_FLAG) .ifPresent(auth -\u0026gt; template.header(Constant.PLATFORM_FLAG, auth)); }; } 6.2.2 多线程中丢失请求头 因为threadLocal是线程内部的, 使用多线程后threadLocal会不再有数据\n解决方案:\n手动为子线程里的requst设置请求头 临时存储方案, 将threadLocal中的数据在子线程中再设置一遍 获取用户信息工具类.md spring security 框架默认也没有解决\nspring security 如何在子线程中获取父线程中的用户认证信息（更改安全策略） - precedeforetime - 博客园 (cnblogs.com) 7. 扩展 7.1 InheritableThreadLocal InheritableThreadLocal类继承并重写了ThreadLocal的3个函数：\npublic class InheritableThreadLocal\u0026lt;T\u0026gt; extends ThreadLocal\u0026lt;T\u0026gt; { /** * 该函数在父线程创建子线程，向子线程复制InheritableThreadLocal变量时使用 */ protected T childValue(T parentValue) { return parentValue; } /** * 由于重写了getMap，操作InheritableThreadLocal时， * 将只影响Thread类中的inheritableThreadLocals变量， * 与threadLocals变量不再有关系 */ ThreadLocalMap getMap(Thread t) { return t.inheritableThreadLocals; } /** * 类似于getMap，操作InheritableThreadLocal时， * 将只影响Thread类中的inheritableThreadLocals变量， * 与threadLocals变量不再有关系 */ void createMap(Thread t, T firstValue) { t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); } } 线程间传值实现原理\n线程初始化时将数据从 inheritableThreadLocals 取出并设置\nthread类\npublic class Thread implements Runnable { ......(其他源码) /* * 当前线程的ThreadLocalMap，主要存储该线程自身的ThreadLocal */ ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal，自父线程集成而来的ThreadLocalMap， * 主要用于父子线程间ThreadLocal变量的传递 * 本文主要讨论的就是这个ThreadLocalMap */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; ......(其他源码) } 线程初始化\n/** * 默认情况下，设置inheritThreadLocals可传递 */ private void init(ThreadGroup g, Runnable target, String name, long stackSize) { init(g, target, name, stackSize, null, true); } /** * 初始化一个线程. * 此函数有两处调用， * 1、上面的 init()，不传AccessControlContext，inheritThreadLocals=true * 2、传递AccessControlContext，inheritThreadLocals=false */ private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { ......（其他代码） if (inheritThreadLocals \u0026amp;\u0026amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); ......（其他代码） } 所以只有线程创建时才会 执行值传递\n7.2 TransmittableThreadLocal(TTL) 阿里提供的 在使用线程池等会池化复用线程的执行组件情况下，提供ThreadLocal值的传递功能，解决异步执行时上下文传递的问题。\n其底层是 将数据从父线程中取出来, 再手动设置到子线程中\nTransmittableThreadLocal (TTL) ThreadLocal，一篇文章就够了 - 知乎 (zhihu.com) ThreadLocal源码分析 - 简书 (jianshu.com) ThreadLocal为什么会导致内存泄漏？ - Chen洋 - 博客园 (cnblogs.com) Java-ThreadLocal三种使用场景_用心去追梦的博客-CSDN博客_java threadlocal场景 ThreadLocal存放用户信息（springboot）_神都燕的博客-CSDN博客_threadlocal存储用户信息 InheritableThreadLocal详解 - 简书 (jianshu.com) ThreadLocal 面试夺命11连问_Young丶的博客-CSDN博客 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/threadlocal.html","summary":"[toc] 1. ThreadLocal是什么？ 从名字我们就可以看到ThreadLocal 叫做本地线程变量，意思是说，ThreadLocal 中填充的的是当","title":"ThreadLocal原理以及常见问题"},{"content":"TimingWheel基本原理:\n众所周知寻常的定时器大概有两种，一种是开阻塞线程，另一种是开一个任务队列然后定期扫描。显而易见这两种方式的弊端很明显，前者对线程消耗过大，后者对时间消耗过大（很多未到时间的任务会被多次重复扫描消耗性能）\n为了解决以上两个问题就可以使用TimingWheel数据结构。\n很明显时间轮算法是基于循环链表数据结构。那么他的工作原理具体是怎样的呢？\n能看到图中有个指针，我们假设指针没跳动一下需要10秒，然后现在我们有一个50秒后执行的任务A，由此推断在当前指针指向2的时候，任务A会被存放在槽格7中。当指针跳动到7后取出槽格中的任务队列，此时任务A将会被执行。但是当如果有100秒后的任务需要执行，依然超出了槽格数量是该怎么办呢？很简单，我们为任务实例添加一个属性“圈数”就可以解决。就是说每次装载任务时算出此任务需要指针转动几圈才能够被执行。\n来源 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/timingwheel.html","summary":"TimingWheel基本原理: 众所周知寻常的定时器大概有两种，一种是开阻塞线程，另一种是开一个任务队列然后定期扫描。显而易见这两种方式的弊","title":"timingwheel"},{"content":"关于Https: https=ssl+http\n证书:\nSSL证书需要向国际公认的证书证书认证机构（简称CA，Certificate Authority）申请。 CA机构颁发的证书有3种类型：\n域名型SSL证书（DV SSL）：信任等级普通，只需验证网站的真实性便可颁发证书保护网站； 企业型SSL证书（OV SSL）：信任等级强，须要验证企业的身份，审核严格，安全性更高； 增强型SSL证书（EV SSL）：信任等级最高，一般用于银行证券等金融机构，审核严格，安全性最高，同时可以激活绿色网址栏。 来自 https://www.zhihu.com/question/19578422 也有免费的证书,比如腾讯云的DV SSL 也能自己生产证书,只是不会被其他人承认(也就是说别人访问你的网站会提示证书危险并且浏览器默认不会加载非HTTPS域名下的javascript ),jdk能生成证书,既然别人访问你都会爆红,自己做个证书也没啥用,不如申请免费的玩玩\nTomcat配置Https:\n打开 Tomcat 配置文件 conf\\server.xml。取消注释，并添加三个属性 keystoreFile，keystoreType，keystorePass。 来自 https://blog.csdn.net/gane_cheng/article/details/53001846 http://lixor.iteye.com/blog/1532655 \u0026lt;Connector port=\u0026#34;8443\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; maxThreads=\u0026#34;150\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; clientAuth=\u0026#34;false\u0026#34; sslProtocol=\u0026#34;TLS\u0026#34; keystoreFile=\u0026#34;/你的磁盘目录/订单号.pfx\u0026#34; keystoreType=\u0026#34;PKCS12\u0026#34; keystorePass=\u0026#34;订单号\u0026#34; /\u0026gt; 注:还未实践,暂时只为记录!\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8/tomcat.html","summary":"关于Https: https=ssl+http 证书: SSL证书需要向国际公认的证书证书认证机构（简称CA，Certificate Authority）申请。 CA机构颁发的","title":"Tomcat"},{"content":"[toc]\n1.YARN 概述 YARN YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操 作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序\nYARN 是 Hadoop2.x 版本中的一个新特性。它的出现其实是为了解决第一代 MapReduce 编程 框架的不足，提高集群环境下的资源利用率，这些资源包括内存，磁盘，网络，IO等。Hadoop2.X 版本中重新设计的这个 YARN 集群，具有更好的扩展性，可用性，可靠性，向后兼容性，以 及能支持除 MapReduce 以外的更多分布式计算程序\n1、YARN 并不清楚用户提交的程序的运行机制\n2、YARN 只提供运算资源的调度（用户程序向 YARN 申请资源，YARN 就负责分配资源）\n3、YARN 中的主管角色叫 ResourceManager\n4、YARN 中具体提供运算资源的角色叫 NodeManager\n5、这样一来，YARN 其实就与运行的用户程序完全解耦，就意味着 YARN 上可以运行各种类 型的分布式运算程序（MapReduce 只是其中的一种），比如 MapReduce、Storm 程序，Spark 程序，Tez ……\n6、所以，Spark、Storm 等运算框架都可以整合在 YARN 上运行，只要他们各自的框架中有 符合 YARN 规范的资源请求机制即可\n7、yarn 就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整 合在一个物理集群上，提高资源利用率，方便数据共享\nYARN/MRv2 最基本的想法是将原 JobTracker 主要的资源管理和 Job 调度/监视功能分开作为 两个单独的守护进程。有一个全局的 ResourceManager(RM)和每个 Application 有一个 ApplicationMaster(AM)，Application 相当于 MapReduce Job 或者 DAG jobs。ResourceManager 和 NodeManager(NM)组成了基本的数据计算框架。ResourceManager 协调集群的资源利用， 任何 Client 或者运行着的 applicatitonMaster 想要运行 Job 或者 Task 都得向 RM 申请一定的资 源。ApplicatonMaster 是一个框架特殊的库，对于 MapReduce 框架而言有它自己的 AM 实现， 用户也可以实现自己的 AM，在运行的时候，AM 会与 NM 一起来启动和监视 Tasks。\n2. YARN 的重要概念 2.1 ResourceManager ResourceManager 是基于应用程序对集群资源的需求进行调度的 YARN 集群主控节点，负责 协调和管理整个集群（所有 NodeManager）的资源，响应用户提交的不同类型应用程序的 解析，调度，监控等工作。ResourceManager 会为每一个 Application 启动一个 MRAppMaster， 并且 MRAppMaster 分散在各个 NodeManager 节点\n它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（ApplicationsManager， ASM）\nYARN 集群的主节点 ResourceManager 的职责：\n1、处理客户端请求\n2、启动或监控 MRAppMaster\n3、监控 NodeManager\n4、资源的分配与调度\n2.2 NodeManager NodeManager 是 YARN 集群当中真正资源的提供者，是真正执行应用程序的容器的提供者， 监控应用程序的资源使用情况（CPU，内存，硬盘，网络），并通过心跳向集群资源调度器 ResourceManager 进行汇报以更新自己的健康状态。同时其也会监督 Container 的生命周期 管理，监控每个 Container 的资源使用（内存、CPU 等）情况，追踪节点健康状况，管理日 志和不同应用程序用到的附属服务（auxiliary service）。\nYARN 集群的从节点 NodeManager 的职责：\n1、管理单个节点上的资源\n2、处理来自 ResourceManager 的命令\n3、处理来自 MRAppMaster 的命令\n2.3 MRAppMaster MRAppMaster 对应一个应用程序，职责是：向资源调度器申请执行任务的资源容器，运行 任务，监控整个任务的执行，跟踪整个任务的状态，处理任务失败以异常情况\n2.4 Container Container 容器是一个抽象出来的逻辑资源单位。容器是由 ResourceManager Scheduler 服务 动态分配的资源构成，它包括了该节点上的一定量 CPU，内存，磁盘，网络等信息，MapReduce 程序的所有 Task 都是在一个容器里执行完成的，容器的大小是可以动态调整的\n2.5 ASM 应用程序管理器 ASM 负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协 商资源以启动 MRAppMaster、监控 MRAppMaster 运行状态并在失败时重新启动它等\n2.6 Scheduler 调度器根据应用程序的资源需求进行资源分配，不参与应用程序具体的执行和监控等工作 资源分配的单位就是 Container，调度器是一个可插拔的组件，用户可以根据自己的需求实 现自己的调度器。YARN 本身为我们提供了多种直接可用的调度器，比如 FIFO，Fair Scheduler 和 Capacity Scheduler 等\n3. ARN 架构及各角色职责 4. YARN 作业执行流程 YARN 作业执行流程：\n1、用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。\n2、ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。\n3、MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结束，重复 4 到 7 的步骤。\n4、MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。\n5、一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。\n6、NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。\n7、各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。\n8、应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/yarn.html","summary":"[toc] 1.YARN 概述 YARN YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操 作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的","title":"YARN"},{"content":"1.错误次数太多,用户连接超时(不定期出现)\n原因:太多次重复连接,导致网页缓慢,不能再指定时间(5秒)内连接上\n解决:将代理ip中间件换成新框架的\n2.代理池有卡顿\n3.频繁请求ip,无论成功与否\n4.爬取速度慢:\n原因:不停在爬取重复页面,在爬取帖子详细内容时,重复请求该页面\n解决:目前是注释掉那句代码\n5.数据量不对:\n原因:目前爱卡更改了对发布帖子排序的操作,增加了cookie(oderby参数)来控制是否排序\n以前爬下来的是按回复排序\n解决:增加cookie \u0026lsquo;oderby\u0026rsquo;:1,\n6.被爱卡禁止访问:\n原因:爱卡增加了cookie验证,来做反爬\n解决:增加所需cookie,具体如下:\n\u0026lsquo;_appuv_newcar\u0026rsquo;:\u0026lsquo;f0fafccbbed05fa57d0b404f1be0d158\u0026rsquo;,\n\u0026lsquo;_fwck_newcar\u0026rsquo;:\u0026lsquo;c28879b9110138c43ca7ef25c7e7f52b\u0026rsquo;,\n\u0026lsquo;_appuv_www\u0026rsquo;:\u0026lsquo;37d34d06ad43bde460744eedc6a82c98\u0026rsquo;,\n\u0026lsquo;_fwck_www\u0026rsquo;:\u0026lsquo;b15320548fb602fafa7e384f4c0f568a\u0026rsquo;,\n7.无法进行翻页:\n原因:获取下一页数字的代码错误,例:\nHref:forumdisplay.php?fid=741\u0026amp;orderby=dateline\u0026amp;page=4\n原代码:按 \u0026ldquo;=\u0026rdquo; 分开取第三个,则会取出 dateline\u0026amp;page , 此为错误\n解决: 按 \u0026ldquo;page=\u0026rdquo; 切分, 取第二个\n8.用户禁言论坛修改\n能爬取禁言论坛,修改了xcar.py文件\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/%E9%97%AE%E9%A2%98/%E7%88%B1%E5%8D%A1/%E7%88%B1%E5%8D%A1%E7%9A%84%E9%97%AE%E9%A2%98.html","summary":"1.错误次数太多,用户连接超时(不定期出现) 原因:太多次重复连接,导致网页缓慢,不能再指定时间(5秒)内连接上 解决:将代理ip中间件换成新框","title":"爱卡的问题"},{"content":"Docker 要求 CentOS7 系统的内核版本高于 3.10\n来自* \u0026lt;http://www.runoob.com/docker/centos-docker-install.html \u0026gt;\nuname -r 可查看内核版本\n安装: yum -y install docker\n启动服务: service docker start 或者 systemctl start docker.service (最好用root用户启动,不然启动不了)\n更换镜像地址:\n新版的 Docker 使用 /etc/docker/daemon.json（Linux） 请在该配置文件中加入（没有该文件的话，请先建一个）：\n{ \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://hub-mirror.c.163.com\u0026#34;] } 来自* \u0026lt;http://www.runoob.com/docker/centos-docker-install.html \u0026gt;\n在安装k8s的文章中, 也有描述docker的安装教程\nk8s安装.md ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/docker/%E5%AE%89%E8%A3%85.html","summary":"Docker 要求 CentOS7 系统的内核版本高于 3.10 来自* \u0026lt;http://www.runoob.com/docker/centos-docker-install.html \u0026gt; uname -r 可查看内核版本 安装: yum -y install docker 启动服务: service docker start 或者 systemctl start docker.service (最好用root用户启动,不然启动不了) 更换","title":"安装"},{"content":"[toc]\nK8s搭建流程-使用kubeadm\n1 准备环境 1.1 服务器要求： 建议最小硬件配置：2核CPU、2G内存、20G硬盘，服务器最好可以访问外网，会有从网上拉取镜像需求，如果服务器不能上网，需要提前下载对应镜像并导入节点。\n1.2 软件环境： 软件 版本 操作系统 CentOS 7.7.1908 (Core) Docker 24.0.4 (当前最新版) Kubernetes 1.23 1.3 服务器规划： 服务器名称 服务器IP 备注 master 10.39.40.1 24u/93G/755G 同时作为node节点 node1 10.39.40.2 32u/132G/3.6T node2 10.39.40.3 64u/141G/1.1T 部署使用root账号, 非root账号很多命令相对麻烦\n2. 主机名解析 编辑所有节点服务器的 /etc/hosts 文件 ,添加主机名, 执行命令：\ncat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 10.39.97.19 master 10.39.97.20 node1 EOF 用sudo cat追加文件出错_iteye_2225的博客-CSDN博客 设置每台节点的hostname, 分别执行\nmaster节点:\nhostnamectl set-hostname master\nnode1节点:\nhostnamectl set-hostname node1\n3.时间同步 启动chronyd服务\nsystemctl start chronyd systemctl enable chronyd date 如果没有安装 chronyd , 可以使用 yum install chrony -y 安装\n4.禁用selinux和firewalld服务 关闭firewalld服务,并 设置开机不启动\nsystemctl stop firewalld systemctl disable firewalld 关闭selinux服务，重启后生效\nsed -i 's/enforcing/disabled/' /etc/selinux/config\nselinux - linux的安全系统, 可以尽可能的控制权限最小化 5.禁用swap分区 swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明\nsed -ri 's/.*swap.*/#\u0026amp;/' /etc/fstab # 永久关闭\n6.添加网桥过滤和地址转发功能 cat \u0026gt;\u0026gt; /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF #然后执行 sysctl --system 为使之前的配置生效，重启所有节点服务器\nreboot\n7.docker安装 7.1 安装docker依赖 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n这里使用阿里的镜像一键安装, 默认装最新版, 如果是要指定版本,就采用手动安装的方式, 会帮我们做如下操作\n安装yum-utils\n设置docker仓库镜像为阿里的\n安装 docker-ce / docker-ce-cli / containerd.io / docker-compose / makecache\nCentOS Docker 安装 | 菜鸟教程 (runoob.com) 7.2 配置docker 镜像加速器和默认存储目录 cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;data-root\u0026#34;: \u0026#34;/app/docker-home\u0026#34;, \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;,\u0026#34;https://alzgoonw.mirror.aliyuncs.com\u0026#34;] } EOF 【简记】修改Docker数据目录位置，包含镜像位置 - 东北小狐狸 - 博客园 (cnblogs.com) 7.3 启动Docker # 设置开机启动 systemctl enable docker # 启动docker systemctl start docker 7.4 验证 docker version 检查版本\n可以看到 docker版本, 容器数, 镜像数, root dir 路径, 镜像仓库 等等\ndocker run hello-world\n执行一个 hello-world , 如果能运行成功, 则表示安装成功, 第一次运行要pull 镜像, 可能会慢一点, 等一会它\nUnable to find image \u0026#39;hello-world:latest\u0026#39; locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:9572f7cdcee8591948c2963463447a53466950b3fc15a247fcad1917ca215a2f Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 7.5 安装cri-dockerd 从v1.24起，Docker不能直接作为k8s的容器运行时 ，因为在k8s v1.24版本移除了叫dockershim的组件，这是由k8s团队直接维护而非Docker团队维护的组件，这意味着Docker和k8s的关系不再像原来那般亲密，开发者需要使用其它符合CRI（容器运行时接口 ）的容器运行时工具（如containerd, CRI-O等），当然这并不意味着新版本的k8s彻底抛弃Docker（由于Docker庞大的生态和广泛的群众基础，显然这并不容易办到），在原本安装了Docker的基础上，可以通过补充安装cri-dockerd ，以满足容器运行时接口的条件，从某种程度上说，cri-dockerd就是翻版的dockershim。\n本篇的k8s 刚好在 1.23版本, 所以不用特意安装\nKubernetes最新版2023.07v1.27.4安装和集群搭建保姆级教程 - 知乎 (zhihu.com) 8.kubernetes的Yum源切换成国内源 cat \u0026gt;\u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 9.安装指定版本 kubeadm，kubelet和kubect yum install -y kubelet-1.23.0 kubeadm-1.23.0 kubectl-1.23.0 #设置kubelet开机启动 systemctl enable kubelet 10. 通过 kubeadm 部署Kubernetes 10.1 部署master节点 Kubernetes 初始化，下面的操作只需要在master节点上执行即可\nkubeadm init \\ --apiserver-advertise-address=10.39.40.1 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.23.0 \\ --service-cidr=192.168.128.0/17 \\ --pod-network-cidr=192.168.0.0/17 \\ --ignore-preflight-errors=all \u0026ndash;apiserver-advertise-address #集群通告地址(master 机器IP) \u0026ndash;image-repository #由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址 \u0026ndash;kubernetes-version #K8s版本，与上面安装的一致 \u0026ndash;service-cidr #集群内部service网络 \u0026ndash;pod-network-cidr #集群内部pod网络，与下面部署的CNI网络组件yaml中保持一致\n搭建k8s时service-cidr是根据自己本地得ip地址写得 (aliyun.com) 成功后末尾输出信息如下：\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: #### 这行数据 复制下来, 节点加入进群需要用到此命令 kubeadm join 10.39.40.1:6443 --token 1j3dl9.1z8wjckd94xluwh6 \\ --discovery-token-ca-cert-hash sha256:c464e3ebd7f432f094bacf06269003730f010b431c3fb76acade4f43d4740 创建客户端授权文件\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 授权kubectl 客户端能连接上 k8s, 这里只在master配置, 如果需要在其他节点配置, 就把 /etc/kubernetes/admin.conf 文件复制到其他节点上\n10.2 将node节点加入集群 在所有node节点上执行以下命令(初始化master 得到的命令)\nkubeadm join 10.39.40.1:6443 --token 1j3dl9.1z8wjckd94xluwh6 \\ --discovery-token-ca-cert-hash sha256:c464e3ebd7f432f094bacf06269003730f010b431c3fb76acade4f43d4740 11.部署容器网络（CNI） pod之前需要网络通信, 但是k8s没有具体实现, 只是开了CNI能力, 所以需要其他方案来完善\nCalico是一个纯三层的数据中心网络方案，是目前Kubernetes主流的网络方案。 官方的部署文档使用的是 flannel\nKubernetes CNI网络最强对比：Flannel、Calico、Canal和Weave_文化 \u0026amp; 方法_Rancher_InfoQ精选文章 Master 节点下载Calico YAML 文件\ncurl https://docs.projectcalico.org/v3.7/manifests/calico.yaml -O\n下载完后修改里面定义Pod网络（CALICO_IPV4POOL_CIDR），与前面kubeadm init 的 \u0026ndash;pod-network-cidr 指定的一样\n创建calico网络，Master节点上执行\nkubectl apply -f calico.yaml\n查看网络状态\nkubectl get pods -n kube-system -w\n等待一段时间，calico 和 coredns 相关pod处于ready状态, 这个过程要等好几分钟\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-dbd6bd945-vlqxh 1/1 Running 0 4m51s kube-system calico-node-hkp5s 1/1 Running 0 4m51s kube-system calico-node-ksxg8 1/1 Running 0 4m51s kube-system calico-node-srd2r 1/1 Running 0 4m51s kube-system coredns-7f6cbbb7b8-svhdg 1/1 Running 0 22h kube-system coredns-7f6cbbb7b8-vmm2h 1/1 Running 0 22h ### 上面这些 kube-system etcd-k8s-master 1/1 Running 0 22h kube-system kube-apiserver-k8s-master 1/1 Running 0 22h kube-system kube-controller-manager-k8s-master 1/1 Running 0 22h kube-system kube-proxy-4dvgq 1/1 Running 0 22h kube-system kube-proxy-5jhkj 1/1 Running 0 22h kube-system kube-proxy-5jtwf 1/1 Running 0 22h kube-system kube-scheduler-k8s-master 1/1 Running 0 22h 查看节点状态，处于 ready 状态\n[root@master01 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 21h v1.23.4 node1 Ready \u0026lt;none\u0026gt; 16h v1.23.4 node2 Ready \u0026lt;none\u0026gt; 16h v1.23.4 [root@master01 ~]# 可以看到所有节点的状态已经变为是Ready了。 至此K8s已经部署完成，已可以简单使用，但是缺少NFS，不支持灵活的数据持久化\n12.其他维护事项 重新申请 join token 24小时有效\nkubeadm token create --print-join-command\nmaster 节点同时设置为 node节点\nkubectl taint node master node-role.kubernetes.io/master-\n恢复 master only\nkubectl taint node master node-role.kubernetes.io/master=\u0026quot;\u0026quot;:NoSchedule\nK8s搭建流程-使用kubeadm (flowus.cn) kubernetes(k8s)环境搭建 - 知乎 (zhihu.com) Kubernetes最新版2023.07v1.27.4安装和集群搭建保姆级教程 - 知乎 (zhihu.com) Windows Docker 安装 | 菜鸟教程 (runoob.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/kubernetes/%E5%AE%89%E8%A3%85.html","summary":"[toc] K8s搭建流程-使用kubeadm 1 准备环境 1.1 服务器要求： 建议最小硬件配置：2核CPU、2G内存、20G硬盘，服务器最好可以访问外网，会有","title":"安装"},{"content":" 强烈建议设置SELinux: 修改配置文件(并重启): sudo /etc/selinux/config 改为: SELINUX=disabled或者SELINUX=disabled 继 1.(上一点),注释bindIp这一行\n创建\u0026quot;创建管理员\u0026quot; use admin db.createUser({user:\u0026#39;xkj\u0026#39;,pwd:\u0026#39;a\u0026#39;, roles:[{role:\u0026#39;userAdminAnyDatabase\u0026#39;, db:\u0026#39;admin\u0026#39;}]}) 验证: db.auth(\u0026#39;xkj\u0026#39;,\u0026#39;a\u0026#39;) 创建用户: use DJangoLearn db.createUser({user:\u0026#39;mathartsys\u0026#39;,pwd:\u0026#39;a\u0026#39;, roles:[{role:\u0026#39;dbAdmin\u0026#39;, db:\u0026#39;DJangoLearn\u0026#39;}]}) ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E5%AE%89%E8%A3%85%E5%8F%8A%E5%87%86%E5%A4%87.html","summary":"强烈建议设置SELinux: 修改配置文件(并重启): sudo /etc/selinux/config 改为: SELINUX=disabled或者SELINUX=disabled 继 1.(上","title":"安装及准备"},{"content":"","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/rocketmq/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2.html","summary":"","title":"安装与部署"},{"content":"1. 安装 解压即可,可配置单机和集群版,将配置后的文件夹发到 从机 即可.\n说是集群版,不是flume自己配的,几乎所有配置在自己写的配置文件中,自己配上 组 就可以关联到其他电脑组成集群\n2. 配置 flume.env.sh :\nexport JAVA_HOME=/mysoftware/jdk1.8.0_101\n当运行flume报错说找不到 org.apache.flume.tools.GetJavaProperty 类时,替换hbase配置文件中的HBASE_CLASSPATH\nexport JAVA_CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html","summary":"1. 安装 解压即可,可配置单机和集群版,将配置后的文件夹发到 从机 即可. 说是集群版,不是flume自己配的,几乎所有配置在自己写的配置文件中,自己","title":"安装与配置"},{"content":"[toc]\n来自 :https://www.aboutyun.com/forum.php?mod=viewthread\u0026amp;tid=20620\n这节开始讲解集群搭建：\n这儿选用的linux环境是CentOS-7.0-1406-x86_64-GnomeLive.iso GNOME桌面版。安装虚拟机的过程就不说了，这儿使用的网络模式是NAT模式。目前已aboutyun用户登录master机器。本次我们要搭建的是一个三节点的Hadoop、Spark集群。\n一、Linux环境准备 1. 设置静态ip 2. 关闭SELINUX 修改 /etc/sysconfig/selinux文件\nvim /etc/sysconfig/selinux\n3. 关闭防火墙 sudo systemctl stop firewalld.service #停止firewall sudo systemctl disable firewalld.service #禁止firewall开机启动\n4. 开启ssh sudo systemctl start sshd.service #开启ssh sudo systemctl enablesshd.service #开机启动ssh\n5. 修改hosts sudo vim /etc/hosts\n以下内容加入到hosts文件中：\n6. 修改主机名 sudo vim /etc/hostname\n将文件内容改为master\n7. 配置ntp服务 用于同步时间\nsudo vim /etc/ntp.conf\n设置服务器为以下几个（默认为以下服务器的不用修改）：\nserver 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 保存后执行：\nsudo systemctl start ntpd.service #开启ntp服务\nsudo systemctl enable ntpd.service # 开机运行ntp服务\n8. 克隆节点 9. ssh免密码登录 需要实现在master ssh无密码登录本机、slave1和slave2。在master机器上，执行\nssh-keygen -t rsa\n然后一直回车，这样就生成了aboutyun用户在master上的公钥和秘钥。\n执行 以下命令,将公钥提供给master\nssh-copy-id -i ~/.ssh/id_rsa.pub aboutyun@master\n这样就实现了master使用ssh无密码登录本机。在依次执行以下命令：\nssh-copy-id -i ~/.ssh/id_rsa.pub aboutyun@slave1 ssh-copy-id -i ~/.ssh/id_rsa.pub aboutyun@slave2\n这样就实现了在master上ssh无密码登录slave1和slave2.\n二、安装Java 1. 解压安装包 sudo mkdir /data sudo chmod -R 777 /data tar -zxvf ~/jar/jdk-8u111-linux-x64.tar.gz -C /data 2. 设置环境变量 将以下内容加入到~/.bashrc文件中，\n任意哪个配置文件都可以\nexport JAVA_HOME=/data/jdk1.8.0_111 export PATH=$JAVA_HOME/bin:$PATH export CLASS_PATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:. 然后执行下面的命令:\nsource ~/.bashrc 三、安装scala 1. 解压安装包 tar -zxvf ~/jar/scala-2.11.8.tgz -C /data\n2. 设置环境变量 将以下内容加入到~/.bashrc文件中，\nexport SCALA_HOME=/data/scala-2.11.8 export PATH=$SCALA_HOME/bin:$PATH 然后执行下面的命令:\nsource ~/.bashrc\n四、安装hadoop 1. 解压安装包 tar -zxvf ~/jar/hadoop-2.6.5.tar.gz -C /data\n2. 配置hadoop\n涉及到的配置文件为以下几个：\n${HADOOP_HOME}/etc/hadoop/hadoop-env.sh\n${HADOOP_HOME}/etc/hadoop/yarn-env.sh\n${HADOOP_HOME}/etc/hadoop/slaves\n${HADOOP_HOME}/etc/hadoop/core-site.xml\n${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\n${HADOOP_HOME}/etc/hadoop/mapred-site.xml\n${HADOOP_HOME}/etc/hadoop/yarn-site.xml\n${HADOOP_HOME}/etc/hadoop/yarn-env.sh\n${HADOOP_HOME}/etc/hadoop/slaves\n${HADOOP_HOME}/etc/hadoop/core-site.xml\n${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\n${HADOOP_HOME}/etc/hadoop/mapred-site.xml\n${HADOOP_HOME}/etc/hadoop/yarn-site.xml\n如果有的文件不存在，可以复制相应的template文件获得，例如，mapred-site.xml文件不存在，则可以从mapred-site.xml.template复制一份过来。\n配置文件1：hadoop-env.sh\n指定JAVA_HOME,修改如下\nexport JAVA_HOME=/data/jdk1.8.0_111\n配置文件2：yarn-env.sh\n指定JAVA_HOME,增加如下\nexport JAVA_HOME=/data/jdk1.8.0_111\n配置文件3：slaves\n将所有的从节点加入\n配置文件4：core-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://master:8020\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:///home/aboutyun/hadoop/tmp\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Abase for other temporary directories.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.aboutyun.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;abouyun用户可以代理任意机器上的用户\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.aboutyun.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;abouyun用户代理任何组下的用户\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;io.file.buffer.size\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;131072\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 注意: 需要在本地创建/home/aboutyun/hadoop/tmp目录\n配置文件5：hdfs-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.secondary.http-address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:9001\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:///home/aboutyun/hadoop/namenode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:///home/aboutyun/hadoop/datanode\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.webhdfs.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 注意: 需要在本地创建/home/aboutyun/hadoop/namenode和/home/aboutyun/hadoop/datanode目录\n配置文件6：mapred-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.jobhistory.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:10020\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.jobhistory.webapp.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:19888\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 配置文件7：yarn-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.mapred.ShuffleHandler\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:8032\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.scheduler.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:8030\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.resource-tracker.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:8031\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.admin.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:8033\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.webapp.address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;master:8088\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 3. 设置环境变量\n将以下内容加入到~/.bashrc文件中\nexport HADOOP_HOME=/data/hadoop-2.6.5 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 然后执行以下命令：\nsource ~/.bashrc\n4. 复制到其他节点 1）复制安装目录\n在master机器上：\nscp -r /data/hadoop-2.6.5/ /data/scala-2.11.8/ /data/jdk1.8.0_111/ aboutyun@slave1:~/\n在slave1和slave2机器上：\nsudo mkdir /data sudo chmod 777 /data mv hadoop-2.6.5/ scala-2.11.8/ jdk1.8.0_111/ /data\n2) 复制hadoop日志目录\n在master机器上：\nscp -r ~/hadoop aboutyun@slave1:~/ scp -r ~/hadoop aboutyun@slave2:~/\n3）复制环境变量\n在master机器上：\nscp -r ~/.bashrc aboutyun@slave1:~/ scp -r ~/.bashrc aboutyun@slave2:~/\n在slave1和slave2机器上：\n5. 登录验证\nsource ~/.bashrc\n在master机器上进行如下操作：\n1）格式化hdfs\n`hdfs namenode -format ``\n2）启动hdfs\n`start-dfs.sh```\n在master上使用jps命令\n在slave1和slave2上使用jps命令：\n上面两张图片说明了在master节点上成功启动了NameNode和SecondaryNameNode，在slave节点上成功启动了DataNode，也就说明HDFS启动成功。\n3）启动yarn\nstart-yarn.sh\n在master使用jps命令;\n在slave1和slave2上使用jps命令\n上面两张图片说明成功启动了ResourceManager和NodeManager，也就是说yarn启动成功。\n4）访问WebUI\n在master、slave1和slave2任意一台机器上打开firefox，然后输入http://master:8088/，如果看到如下的图片，就说明我们的hadoop集群搭建成功了。\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html","summary":"[toc] 来自 :https://www.aboutyun.com/forum.php?mod=viewthread\u0026amp;tid=20620 这节开始讲解集群搭建： 这儿选用的linux环境是CentOS-7.0-1406-x86_64-GnomeLive.iso GNOME桌","title":"安装与配置"},{"content":"[toc]\n一.Hive安装包下载 下载地址http://mirrors.hust.edu.cn/apache/\n选择合适的Hive版本进行下载，进到stable-2文件夹可以看到稳定的2.x的版本是2.3.3\n二.Hive安装 使用MySQL做为Hive的元数据库，所以先安装MySQL。参考 https://www.runoob.com/mysql/mysql-install.html 1. 解压\ntar -zxvf apache-hive-2.3.3-bin.tar.gz -C /usr/software\n2.修改 hive-env.sh\ncp -p hive-env.sh.template hive-env.sh \u0026amp;\u0026amp; vim hive-env.sh\n并添加如下内容:\nHADOOP_HOME=/usr/software/hadoop-2.7.3 export HIVE_CONF_DIR=/usr/software/apache-hive-2.1.1-bin/conf 3. 修改hive-site.xml\ncp -p hive-default.xml.template hive-site.xml \u0026amp;\u0026amp; vim hive-site.xml\n修改如下内容:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.cli.print.header\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;!-- false 改为true --\u0026gt; \u0026lt;description\u0026gt;Whether to print the names of the columns in query output.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.cli.print.current.db\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;!-- false 改为true --\u0026gt; \u0026lt;description\u0026gt;Whether to include the current database in the Hive prompt.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; 添加元数据 存放的数据库(mysql)配置\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://namenode01.hadoop.com:3306/hivedb?createDatabaseIfNotExist=true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;root\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 4. 修改log日志路径\ncp -p hive-log4j.properties.template hive-log4j.properties \u0026amp;\u0026amp; vim hive-log4j.properties\n修改如下内容:\nproperty.hive.log.dir = /usr/software/apache-hive-2.1.1-bin/logs\n5.添加mysql连接包\n将 mysql-connector-java-5.1.39.jar 包放到 hive的lib目录下\n6. 添加环境变量\nvim .bash_profile # 添加内容 export HIVE_HOME=/home/hadoop/apps/apache-hive-2.3.3-bin export PATH=$PATH:$HIVE_HOME/bin source .bash_profile 三.验证 hive --help\n出现hive相关信息\n初始化元数据库\nschematool -dbType mysql -initSchema\n注意：当使用的 hive 是 2.x 之前的版本，不做初始化也是 OK 的，当 hive 第一次启动的 时候会自动进行初始化，只不过会不会生成足够多的元数据库中的表。在使用过程中会 慢慢生成。但最后进行初始化。如果使用的 2.x 版本的 Hive，那么就必须手动初始化元 数据库。使用命令：\n使用 hive 命令进入hive\nhttps://www.cnblogs.com/qingyunzong/p/8708057.html#_label0 https://blog.51cto.com/flyfish225/2096888 https://www.cnblogs.com/zhoading/p/11911127.html 四: 连接 4.1 cli方式连接 使用hive命令进入,,用于linux平台命令行查询，查询语句基本跟mysql查询语句类似\n通过 hive \u0026ndash;service serviceName \u0026ndash;help 可以查看某个具体命令的使用方式\n4.2 HiveServer2/beeline 用java等程序实现通过jdbc等驱动的访问hive就用这种起动方式了，这个是程序员最需要的方式了\n1. 修改hadoop中 hdfs-site.xml 配置文件\n加入一条配置信息，表示启用 webhdfs\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.webhdfs.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 2.修改hadoop中 core-site.xml 配置文件\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.hadoop.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.hadoop.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 以上操作做好了之后（最好重启一下HDFS集群）\nhadoop.proxyuser.hadoop.hosts 配置成*的意义，表示任意节点使用 hadoop 集群的代理用户 hadoop 都能访问 hdfs 集群，\nhadoop.proxyuser.hadoop.groups 表示代理用户的组所属\n3. 启动hiveserver2服务\nnohup hiveserver2 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n会启动一个进程: runJar\n4. 启动 beeline 客户端去连接：\nbeeline -u jdbc:hive2//hadoop3:10000 -n hadoop (这种好像要单独设置密码才能使用) 或者 beeline !connect jdbc:hive2://localhost:10000/hivedb 再输数据库用户和密码 -u : 指定元数据库的链接信息\n-n : 指定mysql数据库用户名\n-p: 密码\nhttps://www.iteye.com/blog/491569462-qq-com-1948436 https://www.cnblogs.com/qingyunzong/p/8715925.html#_label1_1 https://www.cnblogs.com/lenmom/p/11218807.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html","summary":"[toc] 一.Hive安装包下载 下载地址http://mirrors.hust.edu.cn/apache/ 选择合适的Hive版本进行下载，进到st","title":"安装与配置"},{"content":"[toc]\n注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和stop-all.sh,而spark启动/关闭用到了这两个脚本,而hadoop中这么命令已经遗弃了\n源笔记:\n1.修改slave文件,配置从机\n如下:\nslave01\nslave02\nslave03\n2.修改spark.env.sh,如下:\nexport JAVA_HOME=/mysoftware/jdk1.8.0_101 export SPARK_MASTER_IP=master export SCALA_HOME=/mysoftware/scala-2.12.3 export HADOOP_HOME=/mysoftware/hadoop-2.7.3 export HADOOP_CONF_DIR=/mysoftware/hadoop-2.7.3/etc/hadoop/ 3.发送到从机即可,集群环境搭建完毕\n一、安装Spark集群 1. 解压安装包 tar -zxvf ~/jar/spark-1.6.3-bin-hadoop2.6.tgz -C /data\n2. 配置spark 涉及到的配置文件有以下几个：\n${SPARK_HOME}/conf/spark-env.sh\n${SPARK_HOME}/conf/slaves\n${SPARK_HOME}/conf/spark-defaults.conf\n这三个文件都是由原始的template文件复制过来的，比如cp spark-env.sh.template spark-env.sh\n配置文件1：spark-env.sh\nJAVA_HOME=/data/jdk1.8.0_111 SCALA_HOME=/data/scala-2.11.8 SPARK_MASTER_HOST=master SPARK_MASTER_PORT=7077 HADOOP_CONF_DIR=/data/hadoop-2.6.5/etc/hadoop # shuffled以及RDD的数据存放目录 SPARK_LOCAL_DIRS=/data/spark_data # worker端进程的工作目录 SPARK_WORKER_DIR=/data/spark_data/spark_works 注意：需要在本地创建/data/spark_data/spark_works目录\n配置文件2：slaves\nmaster slave1 slave2 配置文件3：spark-defaults.conf\nspark.master spark://master:7077 spark.serializer org.apache.spark.serializer.KryoSerializer spark.eventLog.enabled true spark.eventLog.dir file:///data/spark_data/history/event-log spark.history.fs.logDirectory file:///data/spark_data/history/spark-events spark.eventLog.compress true 注意：需要在本地创建/data/spark_data/history/event-log、/data/spark_data/history/spark-events\n3. 复制到其他节点 在master上：\nscp -r /data/spark* aboutyun@slave1:~/ scp -r /data/spark* aboutyun@slave2:~/ 在slave1和slave2上：\nmv ~/spark* /data\n4. 设置环境变量\n将以下内容加入到~/.bashrc文件中，\nexport SPARK_HOME=/data/spark-1.6.3-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH 然后执行以下命令：\nsource ~/.bashrc\n5. 启动验证 在master机器上进行如下操作\n可直接使用 start-all.sh , 但是要注意hadoop的命令, hadoop也有一个一样的命令,但是已经遗弃\n1）启动master\nstart-master.sh\n在master机器上执行jps命令\n上图说明在master节点上成功启动Master进程\n2）启动slave\n在master和slave机器上执行jps命令\n上面的图片说明在每台机器上都成功启动了Worker进程。\n3）访问WebUI\n在master、slave1和slave2这三台中任意一台机器上的浏览器中输入：http://master:8080/，看到如下图片，就说明我们的spark集群安装成功了。\n趟过的坑\n配置core-site.xml和hdfs-site.xml文件时所指定的本地目录一定要自己创建，否则在执行玩格式化hdfs后，启动hdfs会丢失进程。 6. 打包运行 https://www.cnblogs.com/654wangzai321/p/9513488.html 二,高可用(热备) ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html","summary":"[toc] 注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和st","title":"安装与配置"},{"content":"一:\n修改zoo.cfg配置:\ntickTime=2000 initLimit=5 syncLimit=2 dataDir=/opt/zookeeper/server1/data dataLogDir=/opt/zookeeper/server1/dataLog clientPort=2181 server.1=192.168.2.101:2888:3888 server.2=192.168.2.102:2889:3889 server.3=192.168.2.103:2890:3890 参数描述: tickTime：zookeeper中使用的基本时间单位, 毫秒值。 initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个 tickTime 时间间隔数。这里设置为5表名最长容忍时间为 5 * 2000 = 10 秒。 syncLimit：这个配置标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是 2 * 2000 = 4 秒。 dataDir 和 dataLogDir 看配置就知道干吗的了，不用解释。 clientPort：监听client连接的端口号，这里说的client就是连接到Zookeeper的代码程序。 server.{myid}={ip}:{leader服务器交换信息的端口}:{当leader服务器挂了后, 选举leader的端口} maxClientCnxns：对于一个客户端的连接数限制，默认是60，这在大部分时候是足够了。但是在我们实际使用中发现，在测试环境经常超过这个数，经过调查发现有的团队将几十个应用全部部署到一台机器上，以方便测试，于是这个数字就超过了。 二:\n创建的data目录下创建文件 myid (注意,无后缀),并按照上面的配置文件中ip对应的数字\n比如: 在 192.168.2.101机器上的myid文件写1\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html","summary":"一: 修改zoo.cfg配置: tickTime=2000 initLimit=5 syncLimit=2 dataDir=/opt/zookeeper/server1/data dataLogDir=/opt/zookeeper/server1/dataLog clientPort=2181 server.1=192.168.2.101:2888:3888 server.2=192.168.2.102:2889:3889 server.3=192.168.2.103:2890:3890 参数描述: tickTime：zookeeper中使用的基本时间单位, 毫秒值。 initLimit","title":"安装与配置"},{"content":"[toc]\n介绍 安装 单机 集群 Seata-server 的高可用依赖于注册中心、配置中心和数据库来实现。\nServer端存储模式（store.mode）现有file、db、redis三种（后续将引入raft,mongodb），file模式无需改动，直接启动即可，下面专门讲下db启动步骤。\n注： file模式为单机模式，全局事务会话信息内存中读写并持久化本地文件root.data，性能较高;\ndb模式为高可用模式，全局事务会话信息通过db共享，相应性能差些;\nredis模式Seata-Server 1.3及以上版本支持,性能较高,存在事务信息丢失风险,请提前配置合适当前场景的redis持久化配置.\nseata-server 的高可用的实现，主要基于db和注册中心，通过db获取全局事务，实现多实例事务共享。通过注册中心来实现seata-server多实例的动态管理。架构图原理图如 下：\n案例 seata-server 集群搭建 - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8.html","summary":"[toc] 介绍 安装 单机 集群 Seata-server 的高可用依赖于注册中心、配置中心和数据库来实现。 Server端存储模式（store.mode）现有file、db、red","title":"安装与使用"},{"content":"例子:\n将所有方法foo(a,b,c)的实例改为foo(b,a,c)\n`` :%s/foo(([^,]),([^,]),([^,)]*))/foo(\\2,\\1,\\3)/g`\n1.零宽断言\n就是断言匹配字符串的前后规则\n(?=exp)也叫零宽度正预测先行断言[4] ，它断言自身出现的位置的后面能匹配表达式exp。比如\u0008\\w+(?=ing\u0008)，匹配以ing结尾的单词的前面部分(除了ing以外的部分)，如查找I\u0026rsquo;m singing while you\u0026rsquo;re dancing.时，它会匹配sing和danc。\n(?\u0026lt;=exp)也叫零宽度正回顾后发断言[4] ，它断言自身出现的位置的前面能匹配表达式exp。比如(?\u0026lt;=\u0008re)\\w+\u0008会匹配以re开头的单词的后半部分(除了re以外的部分)，例如在查找reading a book时，它匹配ading。\n例子:\n字符串 45678.002017.12.8 正则 \\d+.\\d{2}(?=.) 匹配结果: 2017.12.8 2.负向零宽\n(感觉有点像零宽度正回顾后发断言)\n零宽度负预测先行断言(?!exp)，断言此位置的后面不能匹配表达式exp。例如：\\d{3}(?!\\d)匹配三位数字，而且这三位数字的后面不能是数字；\u0008((?!abc)\\w)+\u0008匹配不包含连续字符串abc的单词。\n例子:\n字符串 45678.002017.12.8 正则 \\d+.\\d{2}(?!.) 匹配结果: 45678.00 字符串 /jp/xkj/jportal/xkj-jportal 正则 /jp/(?.*) 匹配结果 /jp/xkj 替换字符串 /jpxkj/${name} 替换结果 /jpxkj/xkj ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E6%A1%88%E4%BE%8B.html","summary":"例子: 将所有方法foo(a,b,c)的实例改为foo(b,a,c) `` :%s/foo(([^,]),([^,]),([^,)]*))/foo(\\2,\\1,\\3)/g` 1.零宽断言 就是断言匹配字符串的前后规则 (?=exp)也叫零宽度正预测","title":"案例"},{"content":"[TOC]\n一. if的使用 mybatis做if 判断 注意:下面这种写法只适用于 id 类型为字符串.\n\u0026lt;if test=\u0026#34;id != null and id != \u0026#39;\u0026#39; \u0026#34;\u0026gt; id = #{id} \u0026lt;/if\u0026gt; 如果id类型为int 当id=0时 这个判断不会进入.\n可以这样写\u0026lt;if test=\u0026quot;id != null and id != '' or id==0\u0026quot;\u0026gt;\n或者这样写\u0026lt;if test=\u0026quot;id != null \u0026quot;\u0026gt; 来自: https://blog.csdn.net/qq_33626745/article/details/52955626 二. where 用在where条件不确定的情况下,where下的and和or会自动增加和去除,如下案例\n\u0026lt;select id=\u0026#34;getUserList_whereIf\u0026#34; resultMap=\u0026#34;resultMap_User\u0026#34; parameterType=\u0026#34;com.yiibai.pojo.User\u0026#34;\u0026gt; SELECT u.user_id, u.username, u.sex, u.birthday FROM User u \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;username !=null \u0026#34;\u0026gt; u.username LIKE CONCAT(CONCAT(\u0026#39;%\u0026#39;, #{username, jdbcType=VARCHAR}),\u0026#39;%\u0026#39;) \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;sex != null and sex != \u0026#39;\u0026#39; \u0026#34;\u0026gt; AND u.sex = #{sex, jdbcType=INTEGER} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;birthday != null \u0026#34;\u0026gt; AND u.birthday = #{birthday, jdbcType=DATE} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; \u0026lt;/select\u0026gt; ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E6%A0%87%E7%AD%BE%E7%9A%84%E4%BD%BF%E7%94%A8.html","summary":"[TOC] 一. if的使用 mybatis做if 判断 注意:下面这种写法只适用于 id 类型为字符串. \u0026lt;if test=\u0026#34;id != null and id != \u0026#39;\u0026#39; \u0026#34;\u0026gt; id = #{id} \u0026lt;/if\u0026gt; 如果id类型为int 当id=0时","title":"标签的使用"},{"content":"[TOC]\n1. 并发容器类介绍 2. ConcurrentHashMap 2.1 jdk1.7实现 在JDK1.7版本中，ConcurrentHashMap的数据结构是由一个Segment数组和多个HashEntry组成，如下图所示：\nSegment数组的意义就是将一个大的table分割成多个小的table来进行加锁，而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样\nput操作 对于ConcurrentHashMap的数据插入，这里要进行两次Hash去定位数据的存储位置\nstatic class Segment\u0026lt;K,V\u0026gt; extends ReentrantLock implements Serializable { } 从上Segment的继承体系可以看出，Segment实现了ReentrantLock,也就带有锁的功能，\n当执行put操作时，会进行第一次key的hash来定位Segment的位置，如果该Segment还没有初始化，即通过CAS操作进行赋值，然后进行第二次hash操作，找到相应的HashEntry的位置，这里会利用继承过来的锁的特性，在将数据插入指定的HashEntry位置时（链表的尾端），会通过继承ReentrantLock的tryLock（）方法尝试去获取锁，如果获取成功就直接插入相应的位置，如果已经有线程获取该Segment的锁，那当前线程会以自旋的方式去继续的调用tryLock（）方法去获取锁，超过指定次数就挂起，等待唤醒\n简单的说, 会通过两次hash分别找到对应的Segment和hashEntry的位置, 然后再插进去, 每个Segment都是ReentrantLock锁, 所以在Segment上加锁来保证线程安全\n2.2 jdk1.8 实现 JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本\n这些是构成ConcurrentHashMap实现结构的基础，下面看一下基本属性：\n// node数组最大容量：2^30=1073741824 private static final int MAXIMUM_CAPACITY = 1 \u0026lt;\u0026lt; 30 ; // 默认初始值，必须是2的幂数 private static final int DEFAULT_CAPACITY = 16 ; //数组可能最大值，需要与toArray（）相关方法关联 static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8 ; //并发级别，遗留下来的，为兼容以前的版本 private static final int DEFAULT_CONCURRENCY_LEVEL = 16 ; // 负载因子 private static final float LOAD_FACTOR = 0.75f; // 链表转红黑树阀值,\u0026gt; 8 链表转换为红黑树 static final int TREEIFY_THRESHOLD = 8 ; //树转链表阀值，小于等于6（tranfer时，lc、hc=0两个计数器分别++记录原bin、新binTreeNode数量，\u0026lt;=UNTREEIFY_THRESHOLD 则untreeify(lo)） static final int UNTREEIFY_THRESHOLD = 6 ; static final int MIN_TREEIFY_CAPACITY = 64 ; private static final int MIN_TRANSFER_STRIDE = 16 ; private static int RESIZE_STAMP_BITS = 16 ; // 2^15-1，help resize的最大线程数 private static final int MAX_RESIZERS = ( 1 \u0026lt;\u0026lt; ( 32 - RESIZE_STAMP_BITS)) - 1 ; // 32-16=16，sizeCtl中记录size大小的偏移量 private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; // forwarding nodes的hash值 static final int MOVED = - 1 ; // 树根节点的hash值 static final int TREEBIN = - 2 ; // ReservationNode的hash值 static final int RESERVED = - 3 ; // 可用处理器数量 static final int NCPU = Runtime.getRuntime().availableProcessors(); //存放node的数组 transient volatile Node\u0026lt;K,V\u0026gt;[] table; /*控制标识符，用来控制table的初始化和扩容的操作，不同的值有不同的含义 *当为负数时：- 1 代表正在初始化，-N代表有N- 1 个线程正在 进行扩容 *当为 0 时：代表当时的table还没有被初始化 *当为正数时：表示初始化或者下一次进行扩容的大小 */ private transient volatile int sizeCtl; ConcurrentHashMap (int initialCapacity) 构造函数总结下：\n构造函数中并不会初始化哈希表；\n构造函数中仅设置哈希表大小的变量 sizeCtl；\ninitialCapacity 并不是哈希表大小；\n哈希表大小为 initialCapacity*1.5+1 后，向上取最小的 2 的 n 次方。如果超过最大容量一半，那么就是最大容量。\nput操作 final V putVal(K key, V value, boolean onlyIfAbsent){ if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); //两次hash，减少hash冲突，可以均匀分布 int binCount = 0; for (Node\u0026lt;K,V\u0026gt;[] tab = table;;) { Node\u0026lt;K,V\u0026gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable();//这里就是上面构造方法没有进行初始化，在这里进行判断，为null就调用initTable进行初始化，属于懒汉模式初始化 else if ((f = tabAt(tab, i = (n - 1) \u0026amp; hash)) == null) { //如果i位置没有数据，就直接无锁插入 if ( casTabAt(tab, i, null,new Node\u0026lt;K,V\u0026gt;(hash, key, value, null)) ) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) //如果在进行扩容，则先辅助扩容操作 (多线程同步进行扩容) tab = helpTransfer(tab, f); else { V oldVal = null; synchronized (f) { //如果以上条件都不满足，那就要进行加锁操作，也就是存在hash冲突，锁住链表或者红黑树的头结点 if (tabAt(tab, i) == f) { if (fh \u0026gt;= 0) {//表示该节点是链表结构 binCount = 1; for (Node\u0026lt;K,V\u0026gt; e = f;; ++binCount) { K ek; //这里涉及到相同的key进行put就会覆盖原先的value if (e.hash == hash \u0026amp;\u0026amp; ((ek = e.key) == key || (ek != null \u0026amp;\u0026amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) // hashmap 支持仅不存在时插入, 故此判断 e.val = value; break; } Node\u0026lt;K,V\u0026gt; pred = e; if ((e = e.next) == null) { pred.next = new Node\u0026lt;K,V\u0026gt;(hash, key, value, null); //插入链表尾部 break; } } } else if (f instanceof TreeBin) { //红黑树结构 Node\u0026lt;K,V\u0026gt; p; binCount = 2; //红黑树结构旋转插入 if ((p = ((TreeBin\u0026lt;K,V\u0026gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) {//如果链表的长度大于8时就会进行红黑树的转换 if (binCount \u0026gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); // treeifyBin()方法里判断需要数组大小超过64,才会变红黑树, 和hashMap一样 if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); //统计size，并且检查是否需要扩容 return null; } 这个put的过程很清晰，对当前的table进行无条件自循环直到put成功，可以分成以下六步流程来概述\n如果没有初始化就先调用**initTable（）**方法来进行初始化过程 如果没有hash冲突就直接CAS插入 如果还在进行扩容操作就先进行扩容 helpTransfer(tab, f) 如果存在hash冲突，就加锁来保证线程安全，这里有两种情况，一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入， 最后一个如果Hash冲突时会形成Node链表，在链表长度超过8，Node数组超过64时会将链表结构转换为红黑树的结构，break再一次进入循环 如果添加成功就调用**addCount（）**方法统计size，并且检查是否需要扩容 新增槽点值时的线程安全\n此时为了保证线程安全，做了四处优化：\n通过自旋死循环保证一定可以新增成功。 在新增之前，通过 for (Node\u0026lt;K,V\u0026gt;[] tab = table;;)这样的死循环来保证新增一定可以成功，一旦新增成功，就可以退出当前死循环，新增失败的话，会重复新增的步骤，直到新增成功为止。\n当前槽点为空时，通过 CAS 新增。 Java 这里的写法非常严谨，没有在判断槽点为空的情况下直接赋值，因为在判断槽点为空和赋值的瞬间，很有可能槽点已经被其他线程复制了，所以我们采用 CAS 算法，能够保证槽点为空的情况下赋值成功，如果恰好槽点已经被其他线程赋值，当前 CAS 操作失败，会再次执行 for 自旋，再走槽点有值的 put 流程，这里就是自旋 + CAS 的结合。\n当前槽点有值，锁住当前槽点。 put 时，如果当前槽点有值，就是 key 的 hash 冲突的情况，此时槽点上可能是链表或红黑树，我们通过锁住槽点，来保证同一时刻只会有一个线程能对槽点进行修改\nV oldVal = null;//锁定当前槽点，其余线程不能操作，保证了安全 synchronized (f) { ... } 红黑树旋转时，锁住红黑树的根节点，保证同一时刻，当前红黑树只能被一个线程旋转 ConcurrentHashMap核心原理，彻底给整明白了_Java_AI乔治_InfoQ写作社区 transfer() 扩容操作 transfer 方法的主要思路是：\n首先需要把老数组的值全部拷贝到扩容之后的新数组上，先从数组的队尾开始拷贝；\n拷贝数组的槽点时，先把原数组槽点锁住，保证原数组槽点不能操作，成功拷贝到新数组时，把原数组槽点赋值为转移节点；\n这时如果有新数据正好需要 put 到此槽点时，发现槽点为转移节点，就会一直等待，所以在扩容完成之前，该槽点对应的数据是不会发生变化的；\n从数组的尾部拷贝到头部，每拷贝成功一次，就把原数组中的节点设置成转移节点；\n直到所有数组数据都拷贝到新数组时，直接把新数组整个赋值给数组容器，拷贝完成。\n扩容方法主要是通过在原数组上设置转移节点，put 时碰到转移节点时会等待扩容成功之后才能 put 的策略，来保证了整个扩容过程中肯定是线程安全的，因为数组的槽点一旦被设置成转移节点，在没有扩容完成之前，是无法进行操作的\n会使用多线程的方式来做扩容(数据迁移), 通过 ForwardingNode (转移节点) 来表示是否被迁移过 加上 synchronized 来保证线程安全\n这里的多线程不是新开一个线程池去迁移, 而是让 操作数据的线程 暂停操作来帮忙处理数据迁移(例如put时就会有这个处理) (称为辅助扩容或者多线程扩容)\nget操作 ConcurrentHashMap的get操作的流程很简单，也很清晰，可以分为三个步骤来描述\n计算hash值，定位到该table索引位置，如果是首节点符合就返回 如果遇到扩容的时候，会调用标志正在扩容节点ForwardingNode的find方法，查找该节点，匹配就返回 以上都不符合的话，就往下遍历节点，匹配就返回，否则最后就返回null 下图表示有节点成为了链表中的元素\n其中\u0026quot;xkj11\u0026quot; 与\u0026quot;xkj9\u0026quot; hash冲突, 则\u0026quot;11\u0026quot;成为\u0026quot;9\u0026quot;的后驱节点, 所以直接看tab是找不到\u0026quot;xkj11\u0026quot;的, 所以get时会找到\u0026quot;xkj9\u0026quot;(的hash值) 然后比较key值是否相等, 才能确定是否为该值\nConcurrentHashMap底层实现原理(JDK1.7 \u0026amp; 1.8) - 简书 (jianshu.com) 2.3 面试题 ConcurrentHashMap 的 get 方法是否要加锁，为什么？\nget 方法不需要加锁。因为 Node 的元素 value 和指针 next 是用 volatile 修饰的，在多线程环境下线程A修改节点的 value 或者新增节点的时候是对线程B可见的。这也是它比其他并发集合比如 Hashtable、用 Collections.synchronizedMap()包装的 HashMap 效率高的原因之一。\nConcurrentHashMap 支持 key 或者 value 为 null 吗？\n不支持\nConcurrentHashMap 面试题_学习使我可乐的博客-CSDN博客_concurrenthashmap面试题 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E9%9B%86%E5%90%88.html","summary":"[TOC] 1. 并发容器类介绍 2. ConcurrentHashMap 2.1 jdk1.7实现 在JDK1.7版本中，ConcurrentHashMap的数据结构是由一个Segment数组和多个H","title":"并发容器集合"},{"content":"[toc]\n介绍 一种过滤器,用于判断一个元素在一个集合中是否存在.\n它实际上是一个很长的二进制矢量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。\n来自* \u0026lt;https://blog.csdn.net/iam333/article/details/38084137 \u0026gt;\n原理 一个元素通过K个不同的hash函数随机散列到bit数组的K个位置上，\nBloom Filter的一个例子集合S｛x，y，z｝。带有颜色的箭头表示元素经过k（k＝3）hash函数的到在M（bit数组）中的位置。元素W不在S集合中，因为元素W经过k个hash函数得到在M（bit数组）的k个位置中存在值为0的位置。\n向集合S中添加元素x：x经过k个散列函数后，在M中得到k个位置，然后，将这k个位置的值设置为1。\n判断x元素是否在集合S中：x经过k个散列函数后，的到k个位置的值，如果这k个值中间存在为0的，说明元素x不在集合中——元素x曾经插入到过集合S，则M中的k个位置会全部置为1；如果M中的k个位置全为1，则有两种情形。\n情形一：这个元素在这个集合中；\n情形二：曾经有元素插入的时候将这k个位置的值置为1了（第一类错误产生的原因FalsePositive）\n来自* \u0026lt;https://blog.csdn.net/lvsaixia/article/details/51503231 \u0026gt;\n来自* \u0026lt;https://blog.csdn.net/lvsaixia/article/details/51503231 \u0026gt;\n总结 优点\n存储空间和插入/查询时间都是常数，远远超过一般的算法 Hash函数相互之间没有关系，方便由硬件并行实现 不需要存储元素本身，在某些对保密要求非常严格的场合有优势 缺点\n有一定的误识别率 删除困难 来自* \u0026lt;https://www.cnblogs.com/Jack47/p/bloom_filter_intro.html \u0026gt;\n特性:\n过滤器说不存在,那这个元素一定不存在;\n过滤器说存在,这个元素可能存在(有错误率)\nA\u0026amp;Q 1.为什么有错误率?\n由原理决定的,一个元素被映射成多个值,需比较的值也会被映射成多个值,这个两个元素映射后的值可能会有重合,导致误判,但是需比较映射的值有个为0,则该值一定不存在.也可以说,你的集合越大,误判的次数就越大,按数学推理来算大约是0.0005\n2.为什么删除困难?\n​ 映射后的值可能有冲突,你删除一个元素,就必须连同映射后的值一起删除,这就可能影响到其他值(也用到了这个映射值),而且布隆本身就存在误判,万一你要删除的它说不存在呢(实际上存在的)\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8.html","summary":"[toc] 介绍 一种过滤器,用于判断一个元素在一个集合中是否存在. 它实际上是一个很长的二进制矢量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是","title":"布隆过滤器"},{"content":"新版IE有切换版本的功能:\n可以配合虚拟机使用,装最低版本,升级一个版本做个快照,然后就可以慢慢测了\n或者用IETester软件和DebugBar软件,\n来自* \u0026lt;https://blog.csdn.net/qq_27709465/article/details/76972707 \u0026gt;\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%B5%8B%E8%AF%95/%E6%B5%8B%E8%AF%95%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84ie%E6%B5%8F%E8%A7%88%E5%99%A8.html","summary":"新版IE有切换版本的功能: 可以配合虚拟机使用,装最低版本,升级一个版本做个快照,然后就可以慢慢测了 或者用IETester软件和DebugBa","title":"测试不同版本的IE浏览器"},{"content":" 函数 解释 例子 IFNULL(a,b) 如果a为null,则返回b, IFNULL(age,34) Group_concat(id) 将id字段(默认)用 \u0026ldquo;,\u0026rdquo; 连接 . 用 \u0026quot; SEPARATOR \u0026ldquo;可指定连接符号. (配合group使用,效果棒棒哒) SELECT GROUP_CONCAT( CAST(ccrt.RELEASE_AMT AS char) SEPARATOR \u0026lsquo;、\u0026rsquo;) FROM ccrt INSTR(a, b) 这个函数返回字符串b 在 字符串a中的位置, 没有找到字符串返回0，否则返回位置（从1开始）,也可以用来作为是否存在的条件 来自 \u0026lt;https://www.cnblogs.com/gisblogs/p/4011575.html \u0026gt; SELECT * FROM jbp_org_info WHERE INSTR( CONCAT( \u0026lsquo;,\u0026rsquo; , id , \u0026lsquo;,\u0026rsquo; ), ORG_LINK) SUBSTRING_INDEX（str, delim, count） str 需要拆分的字符串delim 分隔符，通过某字符进行拆分count 当 count 为正数，取第 n 个分隔符之前的所有字符； 当 count 为负数，取倒数第 n 个分隔符之后的所有字符。 来自 \u0026lt;https://blog.csdn.net/pjymyself/article/details/81668157 \u0026gt; SUBSTRING_INDEX(\u0026lsquo;7654,7698,7782,7788\u0026rsquo;, \u0026lsquo;,\u0026rsquo; ,2) FIND_IN_SET(str,strlist) 查找str在strlist中的位置,strlist是由\u0026rdquo;,\u0026ldquo;分割的字符串.来自 \u0026lt;https://blog.csdn.net/qq_27682041/article/details/73647706 \u0026gt; TimeStampDiff(type,time1,time2) 计算两个时间,time1与time2相差多少(time1-time2),单位由type字段决定 TimeStampDiff(SECOND,o.IN_TIME,IFNULL(o.OUT_TIME,SYSDATE()))**type有以下选择:**FRAC_SECOND。表示间隔是毫秒SECOND。秒MINUTE。分钟HOUR。小时DAY。天WEEK。星期MONTH。月QUARTER。季度YEAR。年来自 \u0026lt;https://www.cnblogs.com/zhoucx66/p/5629984.html \u0026gt; ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0.html","summary":"函数 解释 例子 IFNULL(a,b) 如果a为null,则返回b, IFNULL(age,34) Group_concat(id) 将id字段(默认)用 \u0026ldquo;,\u0026rdquo; 连接 . 用 \u0026quot; SEPARATOR \u0026ldquo;可指定连接符号. (配合group使用,效果棒棒","title":"常用函数"},{"content":"[TOC]\n在ElasticSearch 2.4版本中，文档存储的介质分为内存和硬盘：内存速度快，但是容量有限；硬盘速度较慢，但是容量很大。同时，ElasticSearch进程自身的运行也需要内存空间，必须保证ElasticSearch进程有充足的运行时内存。为了使ElasticSearch引擎达到最佳性能，必须合理分配有限的内存和硬盘资源。\n一，倒排索引（Inverted Index） ElasticSearch引擎把文档数据写入到倒排索引（Inverted Index）的数据结构中，倒排索引建立的是分词（Term）和文档（Document）之间的映射关系，在倒排索引中，数据是面向词（Term）而不是面向文档的。\n举个例子，文档和词条之间的关系如下图：\n字段值被分析之后，存储在倒排索引中，倒排索引存储的是分词（Term）和文档（Doc）之间的关系，简化版的倒排索引如下图：\n从图中可以看出，倒排索引有一个词条的列表，每个分词在列表中是唯一的，记录着词条出现的次数，以及包含词条的文档。实际上，ElasticSearch引擎创建的倒排索引比这个复杂得多。\n1，段是倒排索引的组成部分 倒排索引是由段（Segment）组成的，段存储在硬盘（Disk）文件中。索引段不是实时更新的，这意味着，段在写入硬盘之后，就不再被更新。在删除文档时，ElasticSearch引擎把已删除的文档的信息存储在一个单独的文件中，在搜索数据时，ElasticSearch引擎首先从段中执行查询，再从查询结果中过滤被删除的文档，这意味着，段中存储着被删除的文档，这使得段中含有”正常文档“的密度降低。多个段可以通过段合并（Segment Merge）操作把“已删除”的文档将从段中物理删除，把未删除的文档合并到一个新段中，新段中没有”已删除文档“，因此，段合并操作能够提高索引的查找速度，但是，段合并是IO密集型操作，需要消耗大量的硬盘IO。\n在ElasticSearch中，大多数查询都需要从硬盘文件（索引的段数据存储在硬盘文件中）中获取数据，因此，在全局配置文件elasticsearch.yml 中，把**结点的路径（Path）**配置为性能较高的硬盘，能够提高查询性能。默认情况下，ElasticSearch使用基于安装目录的相对路径来配置结点的路径，安装目录由属性path.home显示，在home path下，ElasticSearch自动创建config，data，logs和plugins目录，一般情况下不需要对结点路径单独配置。结点的文件路径配置项：\npath.data：设置ElasticSearch结点的索引数据保存的目录，多个数据文件使用逗号隔开，例如，path.data: /path/to/data1,/path/to/data2； path.work：设置ElasticSearch的临时文件保存的目录； 2，分词和原始文本的存储 映射参数index决定ElasticSearch引擎是否对文本字段执行分析操作，也就是说分析操作把文本分割成一个一个的分词，也就是标记流（Token Stream），把分词编入索引，使分词能够被搜索到：\n当index为analyzed时，该字段是分析字段，ElasticSearch引擎对该字段执行分析操作，把文本分割成分词流，存储在倒排索引中，使其支持全文搜索； 当index为not_analyzed时，该字段不会被分析，ElasticSearch引擎把原始文本作为单个分词存储在倒排索引中，不支持全文搜索，但是支持词条级别的搜索；也就是说，字段的原始文本不经过分析而存储在倒排索引中，把原始文本编入索引，在搜索的过程中，查询条件必须全部匹配整个原始文本； 当index为no时，该字段不会被存储到倒排索引中，不会被搜索到； 字段的原始值是否被存储到倒排索引，是由映射参数store决定的，默认值是false，也就是，原始值不存储到倒排索引中。\n映射参数index和store的区别在于：\nstore用于获取（Retrieve）字段的原始值，不支持查询，可以使用投影参数fields，对stroe属性为true的字段进行过滤，只获取（Retrieve）特定的字段，减少网络负载； index用于查询（Search）字段，当index为analyzed时，对字段的分词执行全文查询；当index为not_analyzed时，字段的原始值作为一个分词，只能对字段的原始文本执行词条查询； 3，单个分词的最大长度 如果设置字段的index属性为not_analyzed，原始文本将作为单个分词，其最大长度跟UTF8 编码有关，默认的最大长度是32766Bytes，如果字段的文本超过该限制，那么ElasticSearch将跳过（Skip）该文档，并在Response中抛出异常消息：\noperation[607]: index returned 400 _index: ebrite _type: events _id: 76860 _version: 0 error: Type: illegal_argument_exception Reason: \u0026ldquo;Document contains at least one immense term in field=\u0026ldquo;event_raw\u0026rdquo; (whose UTF8 encoding is longer than the max length 32766), all of which were skipped. Please correct the analyzer to not produce such terms. The prefix of the first immense term is: \u0026lsquo;[112, 114,\u0026hellip; 115]\u0026hellip;\u0026rsquo;, original message: bytes can be at most 32766 in length; got 35100\u0026rdquo; CausedBy:Type: max_bytes_length_exceeded_exception Reason: \u0026ldquo;bytes can be at most 32766 in length; got 35100\u0026rdquo;\n可以在字段中设置ignore_above属性，该属性值指的是字符数量，而不是字节数量；由于一个UTF8字符最多占用3个字节，因此，可以设置\n\u0026ldquo;ignore_above\u0026rdquo;：10000\n这样，超过30000字节之后的字符将会被分析器忽略，单个分词（Term）的最大长度是30000Bytes。\nThe value for ignore_above is the character count, but Lucene counts bytes. If you use UTF-8 text with many non-ASCII characters, you may want to set the limit to 32766 / 3 = 10922 since UTF-8 characters may occupy at most 3 bytes.\n二，列式存储（doc_values） 默认情况下，大多数字段被索引之后，能够被搜索到。倒排索引是由一个有序的词条列表构成的，每一个词条在列表中都是唯一存在的，通过这种数据存储模式，你可以很快查找到包含某一个词条的文档列表。但是，排序和聚合操作采用相反的数据访问模式，这两种操作不是查找词条以发现文档，而是查找文档，以发现字段中包含的词条。ElasticSearch使用列式存储实现排序和聚合查询。\n文档值**（doc_values）**是存储在硬盘上的数据结构，在索引时（index time）根据文档的原始值创建，文档值是一个列式存储风格的数据结构，非常适合执行存储和聚合操作，除了字符类型的分析字段之外，其他字段类型都支持文档值存储。默认情况下，字段的文档值存储是启用的，除了字符类型的分析字段之外。如果不需要对字段执行排序或聚合操作，可以禁用字段的文档值，以节省硬盘空间。\n\u0026#34;mappings\u0026#34;: { \u0026#34;my_type\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;status_code\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;not_analyzed\u0026#34; }, \u0026#34;session_id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;not_analyzed\u0026#34;, \u0026#34;doc_values\u0026#34;: false } } } } 三，顺排索引（fielddata） 字符类型的分析字段，不支持文档值（doc_values），但是，支持fielddata数据结构，fielddata数据结构存储在JVM的堆内存中。相比文档值（数据存储在硬盘上），fielddata字段（数据存储在内存中）的查询性能更高。默认情况下，ElasticSearch引擎在第一次对字段执行聚合或排序查询时（（query-time）），创建fielddata数据结构；在后续的查询请求中，ElasticSearch引擎使用fielddata数据结构以提高聚合和排序的查询性能。\n在ElasticSearch中，倒排索引的各个段（segment）的数据存储在硬盘文件上，从整个倒排索引的段中读取字段数据之后，ElasticSearch引擎首先反转词条和文档之间的关系，创建文档和词条之间的关系，即创建顺排索引，然后把顺排索引存储在JVM的堆内存中。把倒排索引加载到fielddata结构是一个非常消耗硬盘IO资源的过程，因此，数据一旦被加载到内存，最好保持在内存中，直到索引段(segment)的生命周期结束。默认情况下，倒排索引的每个段(segment)，都会创建相应的fielddata结构，以存储字符类型的分析字段值，但是，需要注意的是，分配的JVM堆内存是有限的，Fileddata把数据存储在内存中，会占用过多的JVM堆内存，甚至耗尽JVM赖以正常运行的内存空间，反而会降低ElasticSearch引擎的查询性能。\n1，format属性 fielddata会消耗大量的JVM内存，因此，尽量为JVM设置大的内存，不要为不必要的字段启用fielddata存储。通过format参数控制是否启用字段的fielddata特性，字符类型的分析字段，fielddata的默认值是paged_bytes，这就意味着，默认情况下，字符类型的分析字段启用fielddata存储。一旦禁用fielddata存储，那么字符类型的分析字段将不再支持排序和聚合查询。\n\u0026#34;mappings\u0026#34;: { \u0026#34;my_type\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;fielddata\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;disabled\u0026#34; } } } } } 2，加载属性（loading） loading属性控制fielddata加载到内存的时机，可能的值是lazy，eager和eager_global_ordinals，默认值是lazy。\nlazy：fielddata只在需要时加载到内存，默认情况下，在第一次搜索时，fielddata被加载到内存中；但是，如果查询一个非常大的索引段（Segment），lazy加载方式会产生较大的时间延迟。 eager：在倒排索引的段可用之前，其数据就被加载到内存，eager加载方式能够减少查询的时间延迟，但是，有些数据可能非常冷，以至于没有请求来查询这些数据，但是冷数据依然被加载到内存中，占用紧缺的内存资源。 eager_global_ordinals：按照global ordinals积极把fielddata加载到内存。 四，JVM进程使用的内存和堆内存 1，配置ElasticSearch使用的内存 ElasticSearch使用JAVA_OPTS环境变量（Environment Variable）启动JVM进程，在JAVA_OPTS中，最重要的配置是：-Xmx参数控制分配给JVM进程的最大内存，-Xms参数控制分配给JVM进程的最小内存。通常情况下，使用默认的配置就能满足工程需要。\nES_HEAP_SIZE 环境变量控制分配给JVM进程的堆内存（Heap Memory）大小，顺排索引（fielddata）的数据存储在堆内存（Heap Memory）中。\n2，内存锁定 大多数应用程序尝试使用尽可能多的内存，并尽可能把未使用的内存换出，但是，内存换出会影响ElasticSearch引擎的查询性能，推荐启用内存锁定，禁用ElasticSearch内存的换进换出。\n在全局配置文档 elasticsearch.yml中，设置 bootstrap.memory_lock为ture，这将锁定ElasticSearch进程的内存地址空间，阻止ElasticSearch内存被OS换出（Swap out）。\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%AD%98%E5%82%A8.html","summary":"[TOC] 在ElasticSearch 2.4版本中，文档存储的介质分为内存和硬盘：内存速度快，但是容量有限；硬盘速度较慢，但是容量很大。同时，Ela","title":"存储"},{"content":"1. 调用某个方法时报错\nForbid consumer 10.101.90.181 access service com.jieshun.jht.oms.service.IPropertyManagerService from registry localhost:2181 use dubbo version 2.5.3, Please check registry access list (whitelist/blacklist).\n原因: 大的来说,是连接不到 IPropertyManagerService 类\n具体可以分:\n服务端没启动或者启动失败(也就是说实现类异常) ip写错了,导致提供方注册到其他地方去了 没有在配置文件中指明接口与别名的关系(也有可能你傻逼,写错单词了),补充如下: \u0026lt;dubbo:reference id=\u0026#34;propertyManagerService\u0026#34; interface=\u0026#34;com.jieshun.jht.oms.service.IPropertyManagerService\u0026#34; /\u0026gt; ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E9%94%99%E8%AF%AF.html","summary":"1. 调用某个方法时报错 Forbid consumer 10.101.90.181 access service com.jieshun.jht.oms.service.IPropertyManagerService from registry localhost:2181 use dubbo version 2.5.3, Please check registry access list (whitelist/blacklist). 原因: 大的来说,是连接不到 IPropertyManagerService 类 具体可以分: 服务端没启动或者启动失败(也就是说实现类","title":"错误"},{"content":"[TOC]\n1. 安装lombok后仍不能生产get和set方法: 这分eclipse版本,我用的是sts,一切操作完成后,需要用管理员身份启动sts.exe\n2. 时间格式化配置失效: 配置:\n# 指定日期格式 spring.jackson.date-format=yyyy-MM-dd HH:mm:ss 预计是按指定格式转换,实际上前端返回时间戳\n原因:\n拦截器中继承了 WebMvcConfigurationSupport 或者没有导包 spring-boot-starter-data-rest\n解决:\n(实现) implements WebMvcConfigurer 导包 (项目中是不使用配置……通过java格式化时间字符串) 来自 https://www.jianshu.com/p/7211dfdbbb9d https://www.cnblogs.com/myitnews/p/12329126.html 3. 热部署相关问题 原因: 热部署有自己的加载类的方法,这样导致与jdk的加载类不一样(序列化和反序列化)\n情况:\n启动项目报错. XXXXXXXXXXXXXXXX, is not visible from class loader (大致是类加载的问题) 访问报错: java.lang.ClassCastException: com.xkj.ExampleServiceImpl cannot be cast to com.xkj.ExampleServiceImpl(同一个类,但是类型转换异常) 解决:\n在resources目录下面创建META_INF文件夹，然后创建spring-devtools.properties文件，文件加上类似下面的配置： restart.exclude.companycommonlibs=/mycorp-common-[\\w-]+.jar restart.include.projectcommon=/mycorp-myproj-[\\w-]+.jar \u0026gt; 来自 \u0026lt;https://www.cnblogs.com/ldy-blogs/p/8671863.html\u0026gt; 在启动的Main方法中:\n​\tSystem.setProperty(\u0026quot;spring.devtools.restart.enabled\u0026quot;, \u0026quot;false\u0026quot;);\nConfigurableObjectInputStream配合Thread.currentThread().getContextClassLoader() 来使用 (我不会….)\n来自 https://www.jianshu.com/p/e6d5a3969343 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E9%94%99%E8%AF%AF.html","summary":"[TOC] 1. 安装lombok后仍不能生产get和set方法: 这分eclipse版本,我用的是sts,一切操作完成后,需要用管理员身份启动sts.ex","title":"错误"},{"content":" 错误大意: 必须是对象 使用: 使用聚合操作访问MongoDB时\n解决:在参数前面加上 * , eg. SaleMongo.objects.all().aggregate(*pipeline)\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E9%94%99%E8%AF%AF.html","summary":"错误大意: 必须是对象 使用: 使用聚合操作访问MongoDB时 解决:在参数前面加上 * , eg. SaleMongo.objects.all().aggregate(*pipeline)","title":"错误"},{"content":"1.数据库查出来是正常中文,用MySQLdb查出来却是问号 db = MySQLdb.connect(...); print db.character_set_name() 查看编码集\n解决:\ndb = MySQLdb.connect(..., charset='utf8') 来自:http://blog.csdn.net/hu330459076/article/details/7828512\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%94%99%E8%AF%AF.html","summary":"1.数据库查出来是正常中文,用MySQLdb查出来却是问号 db = MySQLdb.connect(...); print db.character_set_name() 查看编码集 解决: db = MySQLdb.connect(..., charset='utf8') 来自:http://blog.csdn.net/","title":"错误"},{"content":"[TOC]\n一.tomcat找不到web项目(未成功): 新下载的Eclipse, 将项目导入后安装tomcat后部署项目出现There are no resources that can be added or removed from the server. 在网上查来很多方法，说在Properties\u0026ndash;project facets中添加Dynamic Web Module。 但是在我的project中没有Dynamic Web Module,于是又在网上查找相关解决方案，如修改.setting中的文件和.project、 .classpath等方法均无效。 最终在Help\u0026ndash;install new software 中的 Work with 输入Eclipse Repository - http://download.eclipse.org/releases/neon （最后的neon为Eclipse版本） \u0026ndash; Web, XML, Java EE and OSGi Enterprise Development 下 安装有关JAVA EE的插件 （具体名称没有记录）之后在project facets 中就会出现Dynamic Web Module 了。 原文：\u0026lt; https://blog.csdn.net/TnkTechSHL/article/details/81259798\u003e ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/eclipse/%E9%94%99%E8%AF%AF.html","summary":"[TOC] 一.tomcat找不到web项目(未成功): 新下载的Eclipse, 将项目导入后安装tomcat后部署项目出现There are no resources that can be added or removed","title":"错误"},{"content":"搭建环境的第4步中,会有错误,如图:\n原因:是因为要访问的网站在国外,特别慢\n解决:\n方案一: 在hosts文件中加入如下代码:\n​\t50.116.34.243 sublime.wbond.net\n方案二:\n1. 下载这个网站的内容并保存到本地: https://packagecontrol.io/channel_v3.json注意sublime的版本,这个文件默认是3.0.0关键字为: schema_version版本:2.0 2. 打开配置文件,(添加下好的文件路径)Preferences \u0026gt; Package Settings \u0026gt; Package Control \u0026gt; Settings - User添加如下内容(注意文本格式):\u0026quot;channels\u0026quot;: [ \u0026quot;E:/software/sublime/channel_v3.json\u0026quot; ], ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/%E9%94%99%E8%AF%AF.html","summary":"搭建环境的第4步中,会有错误,如图: 原因:是因为要访问的网站在国外,特别慢 解决: 方案一: 在hosts文件中加入如下代码: ​ 50.116.34.243 sublime.wbond.net 方案二: 1. 下载","title":"错误"},{"content":"[toc]\n1.Datanode denied communication with namenode because hostname cannot be resolved( nameNode找不到dateNode**)** 大意是nameNode找不到dateNode,\n原因: 暂未知\n其表现:\n1.在web界面中看得的活着节点是0, 进程都很正常,\n\\2. 日志中能看得报错,具体错误日志在\nless /usr/software/hadoop-2.7.3/logs/hadoop-xkj-datanode-slave1.log\n3.当上传文件时会报错,(大意是,无法复制内容到dataNode上)\n解决:\n先停止hdfs 先检查 /etc/hosts 和 /etc/hostname 两个文件的配置(每个服务器都改),改后需要重启服务器 (参考搭建教程) 删除 dfs.namenode.name.dir 和 dfs.datanode.data.dir 目录的文件 重新格式化NameNode，重新启动NameNode hadoop namenode -format // 该命令已废弃,建议用 hdfs代替,但是还没试\n再启动即可\n(https://blog.csdn.net/JavaDestiny/article/details/87294090 )\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E9%94%99%E8%AF%AF%E5%90%88%E9%9B%86.html","summary":"[toc] 1.Datanode denied communication with namenode because hostname cannot be resolved( nameNode找不到dateNode**)** 大意是nameNode找不到dateNode, 原因: 暂未知 其表现: 1.","title":"错误合集"},{"content":" 来源: https://mp.weixin.qq.com/s/PwnQl6uji85m7BGALmOVrw 数仓建模的过程分为业务建模、领域建模、逻辑建模和物理建模，但是这 些步骤比较抽象。为了便于落地，我根据自己的经验，总结出上面的七个步骤：梳理业务流程、垂直切分、指标体系梳理、实体关系调研、维度梳理、数仓分层以及物理模型建立。每个步骤不说理论，直接放工具、模板和案例。\n1. 业务流程\n找到公司核心业务流程，找到谁，在什么环节，做什么关键动作，得到什么结果。 梳理每个业务节点的客户及关注重点，找到数据在哪。 分域/主题\n决定数仓的建设方式，快速交活，就用自下而上的建设。要全面支撑，就顶层规划，分步实施，交活稍微慢点。\n同时按照业务领域划分主题域。主题域的划分方法有：按业务流划分（推荐）、按需求分、按职责分、按产品功能分等。 指标体系\n指标的意义在于统一语言，统一口径。所以指标的定义必须有严格的标准。否则如无根之水。\n指标可分为原子指标、派生指标和衍生指标，其含义及命名规则举例如下：\n依照指标体系建设标准，开始梳理指标体系。整个体系同样要以业务为核心进行梳理。同时梳理每个业务过程所需的维度。维度就是你观察这个业务的角度，指标就是衡量这个业务结果 好坏的量化结果。 请注意，此时不能被现有数据局限。如果分析出这个业务过程应该有这个指标，但是没有数据，请标注出来，提出收集数据的需求。\n4. 实体关系\n每个业务动作都会有数据产生。我们将能够获取到的数据，提取实体，绘制ER图，便于之后的维度建模。 同样以业务过程为起点向下梳理，此时的核心是业务表。把每张表中涉及的维度、指标都整理出来。 维度整理 维度标准化是将各个业务系统中相同的维度进行统一的过程。其字段名称、代码、名字都可能不一样，我们需要完全掌握，并标准化。 维度的标准尽可能参照国家标准、行业标准。例如地区可以参照国家行政区域代码。\n另外，有些维度存在层级，如区域的省、市、县。绝大多数业务系统中的级联就是多层级维度。\n数仓分层 数据仓库一般分为4层，名字可能会不一样，但是其目的和建设方法基本一致： 每一层采用的建模方法都不一样，其核心是逐层解耦。越到底层，越接近业务发生的记录，越到上层，越接近业务目标。\n依托数仓分层的设计理论，根据实际业务场景，我们就可以梳理出整体的数据流向图。这张图会很清晰的告诉所有人，数据从那来，到哪里去，最终提供什么样的服务。 模型建立 此时才真正进入纯代码阶段。数仓、ETL工具选型；ETL流程开发；cube的建立；任务调度，设定更新方式、更新频率；每日查看日志、监控etl执行情况等等。 前面梳理清楚了，ETL会变的非常清晰\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E6%95%B0%E4%BB%93/%E6%90%AD%E5%BB%BA%E6%95%B0%E4%BB%93.html","summary":"来源: https://mp.weixin.qq.com/s/PwnQl6uji85m7BGALmOVrw 数仓建模的过程分为业务建模、领域建模、逻辑建模和物理建模，但是这 些步骤比较抽象。为了便于落地，我根据自己的经验，总结出上面的七个步骤","title":"搭建数仓"},{"content":"es 的默认打分机制依靠lucene的打分机制- TF/IDF(词频/逆向文档频率)\n计算公式:\nscore(q,d) = queryNorm(q) //归一化因子 · coord(q,d) //协调因子 · ∑ ( tf(t in d) //词频 · idf(t)² //逆向文档频率 · t.getBoost() //权重 · norm(t,d) //字段长度归一值 ) (t in q) 来自:https://blog.csdn.net/paditang/article/details/79098\n词频:\n词在文档中出现的频度是多少？ 频度越高，权重 越高 。 5 次提到同一词的字段比只提到 1 次的更相关。词频的计算方式如下： tf(t in d) = √frequency , 词 t 在文档 d 的词频（ tf ）是该词在文档中出现次数的平方根。\n将参数 index_options设置为docs可以禁用词频统计及词频位置，这个映射的字段不会计算词的出现次数，对于短语或近似查询也不可用。要求精确查询的not_analyzed 字符串字段会默认使用该设置。\n逆向文档频率\n词在集合所有文档里出现的频率是多少？频次越高，权重 越低 。 常用词如 and 或 the 对相关度贡献很少，因为它们在多数文档中都会出现，一些不常见词如 elastic 或 hippopotamus 可以帮助我们快速缩小范围找到感兴趣的文档。逆向文档频率的计算公式如下： idf(t) = 1 + log ( numDocs / (docFreq + 1)) 词 t 的逆向文档频率（idf）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。\n文档长度归一值\n字段的长度是多少？ 字段越短，字段的权重 越高 。如果词出现在类似标题 title 这样的字段，要比它出现在内容 body 这样的字段中的相关度更高。字段长度的归一值公式如下：\nnorm(d) = 1 / √numTerms 字段长度归一值（ norm ）是字段中词数平方根的倒数。\nqueryNorm 查询归化因子：会被应用到每个文档，不能被更改，总而言之，可以被忽略。\ncoord 协调因子： 可以为那些查询词包含度高的文档提供奖励，文档里出现的查询词越多，它越有机会成为好的匹配结果。 协调因子将评分与文档里匹配词的数量相乘，然后除以查询里所有词的数量，如果使用协调因子，评分会变成：\n文档里有 fox → 评分： 1.5 * 1 / 3 = 0.5 文档里有 quick fox → 评分： 3.0 * 2 / 3 = 2.0 文档里有 quick brown fox → 评分： 4.5 * 3 / 3 = 4.5 协调因子能使包含所有三个词的文档比只包含两个词的文档评分要高出很多。 Boost 权重：在查询中设置关键字的权重可以灵活的找到更匹配的文档。\n还有很多很多算法,也能干预这些算法,还可以自己写一个算法,以后再说,\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%89%93%E5%88%86%E6%9C%BA%E5%88%B6.html","summary":"es 的默认打分机制依靠lucene的打分机制- TF/IDF(词频/逆向文档频率) 计算公式: score(q,d) = queryNorm(q) //归一化因子 · coord(q,d) //协调因子 · ∑ ( tf(t in d) //","title":"打分机制"},{"content":" 如何设计高性能、高并发、高可用的系统。 - 一心二念 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%E5%9B%BE.html","summary":"如何设计高性能、高并发、高可用的系统。 - 一心二念 - 博客园 (cnblogs.com)","title":"大型网站架构图"},{"content":"当用户第一次访问应用系统的时候，因为还没有登录，会被引导到认证系统中进行登录；根据用户提供的登录信息，认证系统进行身份校验，如果通过校验，应该返回给用户一个认证的凭据－－ticket；用户再访问别的应用的时候，就会将这个ticket带上，作为自己认证的凭据，应用系统接受到请求之后会把ticket送到认证系统进行校验，检查ticket的合法性。如果通过校验，用户就可以在不用再次登录的情况下访问应用系统2和应用系统3了。\n要实现SSO，需要以下主要的功能：\n所有应用系统共享一个身份认证系统。 统一的认证系统是SSO的前提之一。认证系统的主要功能是将用户的登录信息和用户信息库相比较，对用户进行登录认证；认证成功后，认证系统应该生成统一的认证标志（ticket），返还给用户。另外，认证系统还应该对ticket进行效验，判断其有效性。\n所有应用系统能够识别和提取ticket信息 要实现SSO的功能，让用户只登录一次，就必须让应用系统能够识别已经登录过的用户。应用系统应该能对ticket进行识别和提取，通过与认证系统的通讯，能自动判断当前用户是否登录过，从而完成单点登录的功能。\n总结:做一个认证系统,(所有的访问要经过这个认证系统,除去登录,获取验证码这类的请求),当用户在一个系统登录后,会得到一个ticket(或者说一个唯一标识),以后请求带着这个ticket去访问,由认证系统验证,通过了就正常放行\n来自* \u0026lt;https://baike.baidu.com/item/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95#2 \u0026gt;\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95.html","summary":"当用户第一次访问应用系统的时候，因为还没有登录，会被引导到认证系统中进行登录；根据用户提供的登录信息，认证系统进行身份校验，如果通过校验，应","title":"单点登录"},{"content":"导包:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; //单元测试类 @RunWith(SpringRunner.class) @SpringBootTest @AutoConfigureMockMvc @WebAppConfiguration public class SpringbootDemoApplicationTests { @Autowired private WebApplicationContext context; private MockMvc mvc; @Before public void setUp() throws Exception { // mvc = MockMvcBuilders.standaloneSetup(new CustomerController()).build(); mvc = MockMvcBuilders.webAppContextSetup(context).build();//建议使用这种 } @Test public void contextLoads() throws Exception { MvcResult mvcR = mvc.perform(MockMvcRequestBuilders.get(\u0026#34;/hello\u0026#34;).accept(MediaType.APPLICATION_JSON)) // .andExpect(MockMvcResultMatchers.status().isOk()) // .andDo(MockMvcResultHandlers.print()) .andReturn(); String content = mvcR.getResponse().getContentAsString(); System.out.println(\u0026#34;内容:\u0026#34;+content); } } MockMvc解析\n我在上面代码中进行了标记，我们按照标记进行讲解，这样会更明白一些：\nperform方法其实只是为了构建一个请求，并且返回ResultActions实例，该实例则是可以获取到请求的返回内容。 MockMvcRequestBuilders该抽象类则是可以构建多种请求方式，如：Post、Get、Put、Delete等常用的请求方式，其中参数则是我们需要请求的本项目的相对路径，/则是项目请求的根路径。 param方法用于在发送请求时携带参数，当然除了该方法还有很多其他的方法，大家可以根据实际请求情况选择调用。 andReturn方法则是在发送请求后需要获取放回时调用，该方法返回MvcResult对象，该对象可以获取到返回的视图名称、返回的Response状态、获取拦截请求的拦截器集合等。 我们在这里就是使用到了第4步内的MvcResult对象实例获取的MockHttpServletResponse对象从而才得到的Status状态码。 同样也是使用MvcResult实例获取的MockHttpServletResponse对象从而得到的请求返回的字符串内容。【可以查看rest返回的json数据】 使用Junit内部验证类Assert判断返回的状态码是否正常为200 判断返回的字符串是否与我们预计的一样。 链接：https://www.jianshu.com/p/d8f844711bf4\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95.html","summary":"导包: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; //单元测试类 @RunWith(SpringRunner.class) @SpringBootTest @AutoConfigureMockMvc @WebAppConfiguration public class SpringbootDemoApplicationTests { @Autowired private WebApplicationContext context; private MockMvc mvc; @Before public void setUp() throws Exception { // mvc = MockMvcBuilders.standaloneSetup(new CustomerController()).build(); mvc = MockMvcBuilders.webAppCont","title":"单元测试"},{"content":"倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引(inverted index)。带有倒排索引的文件我们称为倒排索引文件 ，简称倒排文件 (inverted file)。\n例如: 先将很多文章切词, 得到切后的词与来源文章的编号维护成列表,这样根据某个词就能快速找到文章.这就是核心\n列表中也可以保存单词出现的次数,来源文章等等信息, (百度不知道都存了什么,反正肯定很牛逼)\n以单词“拉斯”为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;\u0026lt;4\u0026gt;)，(5;1;\u0026lt;4\u0026gt;)},其含义为在文档3和文档5出现过这个单词，单词频率都为1，单词“拉斯”在两个文档中的出现位置都是4，即文档中第四个单词是“拉斯”。\n原文：https://blog.csdn.net/u012965373/article/details/39118483\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95.html","summary":"倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性","title":"倒排索引"},{"content":"//初始化地图对象，加载地图 var map = new AMap.Map(\u0026#39;mapContainer\u0026#39;, { resizeEnable: true, zoom:11 }); //为地图注册click事件获取鼠标点击出的经纬度坐标 var clickEventListener = map.on(\u0026#39;click\u0026#39;, function(e) { document.getElementById(\u0026#34;longitude\u0026#34;).value = e.lnglat.getLng(); document.getElementById(\u0026#34;latitude\u0026#34;).value = e.lnglat.getLat(); var lnglatXY = [e.lnglat.getLng(), e.lnglat.getLat()]; AMap.service(\u0026#39;AMap.Geocoder\u0026#39;,function(){//回调函数 //实例化Geocoder geocoder = new AMap.Geocoder({ city: \u0026#34;010\u0026#34;,//城市，默认：“全国” }); //逆地理编码 geocoder.getAddress(lnglatXY, function(status, result) { if (status === \u0026#39;complete\u0026#39; \u0026amp;\u0026amp; result.info === \u0026#39;OK\u0026#39;) { //获得了有效的地址信息: document.getElementById(\u0026#34;addressStr\u0026#34;).value = result.regeocode.formattedAddress; } else { //获取地址失败 alertBox(\u0026#34;暂无地址信息！\u0026#34;); } }); }); }); document.getElementById(\u0026#34;query\u0026#34;).onclick = function(e){ var inputAddress = document.getElementById(\u0026#34;inputAddress\u0026#34;).value; placeSearch(inputAddress); }; var MSearch; function placeSearch(inputAddress) { //POI搜索，关键字查询 map.plugin([\u0026#34;AMap.PlaceSearch\u0026#34;], function() { //构造地点查询类 MSearch = new AMap.PlaceSearch({ pageSize:10, pageIndex:1, zoom:11, city:\u0026#34;0755\u0026#34; //城市 }); AMap.event.addListener(MSearch, \u0026#34;complete\u0026#34;, function(data){ var poiArr = data.poiList.pois; var lngX = poiArr[0].location.getLng(); var latY = poiArr[0].location.getLat(); map.setCenter(new AMap.LngLat(lngX, latY)); });//返回地点查询结果 MSearch.search(inputAddress); //关键字查询 }); } ","permalink":"https://xiaokunji.com/zh/%E5%89%8D%E7%AB%AF/js/%E7%82%B9%E5%87%BB%E5%9C%B0%E5%9B%BE%E8%8E%B7%E5%8F%96%E7%BB%8F%E7%BA%AC%E5%BA%A6.html","summary":"//初始化地图对象，加载地图 var map = new AMap.Map(\u0026#39;mapContainer\u0026#39;, { resizeEnable: true, zoom:11 }); //为地图注册click事件获取鼠标点击出的经纬度坐标 var clickEventListener = map.on(\u0026#39;click\u0026#39;, function(e) { document.getElementById(\u0026#34;longitude\u0026#34;).value = e.lnglat.getLng(); document.getElementById(\u0026#34;latitude\u0026#34;).value = e.lnglat.getLat(); var lnglatXY = [e.lnglat.getLng(), e.lnglat.getLat()]; AM","title":"点击地图获取经纬度"},{"content":"[toc]\n1. 前言 我们在使用idea启动debug时可以做好多事情,看变量的值; 随处打断点; 计算变量的值等等, 这些功能全都依赖了java的调试体系JPDA(Java Platform Debugger Architecture), 而java也提供了命令行级别的调试\n2. 调试使用 关键命令\njavac -g src/HelloJDB.java -g 表示展示调试信息\n这样就启动了程序,然后打开调试程序\njdb HelloJDB.class 使用JDB调试Java程序 - 娄老师 - 博客园 (cnblogs.com) 由此可见,idea那么丰富的调试功能基本来源jvm\n3. JPDA JPDA 由三个规范组成：JVMTI（JVM Tool Interface）、JDWP（Java Debug Wire Protocol）、JDI（Java Debug Interface）\n这三个规范的层次由低到高分别是 JVMTI、JDWP、JDI\n这三个规范把调试过程分解成几个概念：调试者（debugger）、被调试者（debuggee）、以及它们中间的通信器\nJPDA 被抽象为三层实现。其中 JVMTI 是 JVM 对外暴露的接口。JDI 是实现了 JDWP 通信协议的客户端，调试器通过它和 JVM 中被调试程序通信。\n模块 层次 编程语言 作用 JVMTI 底层 C 获取及控制当前虚拟机状态 JDWP 中介层 C 定义 JVMTI 和 JDI 交互的数据格式 JDI 高层 Java 提供 Java API 来远程控制被调试虚拟机 IDEA 的 debug 怎么实现？出于这个好奇心，我越挖越深！ - 知乎 (zhihu.com) Java-JPDA 概述 - 江湖小小白 - 博客园 (cnblogs.com) Q\u0026amp;A 为什么在debug中没有重排序了?\n​\t为了保证代码的顺序性,应该是做了禁止重排序的处理(包括指令/编译/内存重排序)\n​ 应该是在JVMTI这一层处理的,因为它包含了 虚拟机中线程、内存、堆、栈、类、方法、变量，事件、定时器处理等等诸多功能\nJava-JPDA 概述 - 江湖小小白 - 博客园 (cnblogs.com) 在debug中怎么实现动态代码的?(改了代码无需重启,甚至直接修改变量值)\n使用了动态字节码技术ASM, 使得代码的修改能直接被jvm识别并执行\n动态字节码技术还有很多,比如CGlib/Aspectj/agent等等, 会单独出一篇动态字节码技术\nIDEA 的 debug 怎么实现？出于这个好奇心，我越挖越深！ - 知乎 (zhihu.com) 我们在启动被 Debug 的 JVM 时，必须添加参数 -agentlib:jdwp=transport=dt_socket,suspend=y,address=localhost:3333，而 -agentlib 选项就指定了我们要加载的 Java Agent，jdwp 是 agent 的名字，在 linux 系统中，我们可以在 jre 目录下找到 jdwp.so 库文件。\nJava 的调试体系 jdpa 组成，从高到低分别为 jdi-\u0026gt;jdwp-\u0026gt;jvmti，我们通过 JDI 接口发送调试指令，而 jdwp 就相当于一个通道，帮我们翻译 JDI 指令到 JVM TI，最底层的 JVM TI 最终实现对 JVM 的操作。\nJava 动态字节码技术 - 枕边书 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%B0%83%E8%AF%95%E4%BD%93%E7%B3%BB.html","summary":"[toc] 1. 前言 我们在使用idea启动debug时可以做好多事情,看变量的值; 随处打断点; 计算变量的值等等, 这些功能全都依赖了java的调试体系JP","title":"调试体系"},{"content":"[toc]\n1.字节码 1.1 什么是字节码？ Java之所以可以“一次编译，到处运行”，一是因为JVM针对各种操作系统、平台都进行了定制，二是因为无论在什么平台，都可以编译生成固定格式的字节码（.class文件）供JVM使用。因此，也可以看出字节码对于Java生态的重要性。之所以被称之为字节码，是因为字节码文件由十六进制值组成，而JVM以两个十六进制值为一组，即以字节为单位进行读取。在Java中一般是用javac命令编译源代码为字节码文件，一个.java文件从编译到运行的示例如图1所示。\n1.2 字节码结构 .java文件通过javac编译后将得到一个.class文件，比如编写一个简单的ByteCodeDemo类，如下图2的左侧部分：\n编译后生成ByteCodeDemo.class文件，打开后是一堆十六进制数，按字节为单位进行分割后展示如图2右侧部分所示。上文提及过，JVM对于字节码是有规范要求的，那么看似杂乱的十六进制符合什么结构呢？JVM规范要求每一个字节码文件都要由十部分按照固定的顺序组成，整体结构如图3所示。接下来我们将一一介绍这十部分：\n（1） 魔数（Magic Number）\n所有的.class文件的前四个字节都是魔数，魔数的固定值为：0xCAFEBABE。魔数放在文件开头，JVM可以根据文件的开头来判断这个文件是否可能是一个.class文件，如果是，才会继续进行之后的操作。\n有趣的是，魔数的固定值是Java之父James Gosling制定的，为CafeBabe（咖啡宝贝），而Java的图标为一杯咖啡。\n魔数是表示一个文件的类型的,图片有图片的魔数,文本有文本的魔数,且魔数是可以更改的\nMagic Number(魔数）是什么_i_can1的博客-CSDN博客_魔数是什么 （2） 版本号\n版本号为魔数之后的4个字节，前两个字节表示次版本号（Minor Version），后两个字节表示主版本号（Major Version）。上图2中版本号为“00 00 00 34”，次版本号转化为十进制为0，主版本号转化为十进制为52，在Oracle官网中查询序号52对应的主版本号为1.8，所以编译该文件的Java版本号为1.8.0。\n（3） 常量池（Constant Pool）\n紧接着主版本号之后的字节为常量池入口。常量池中存储两类常量：字面量与符号引用。字面量为代码中声明为Final的常量值，符号引用如类和接口的全局限定名、字段的名称和描述符、方法的名称和描述符。常量池整体上分为两部分：常量池计数器以及常量池数据区，如下图4所示。\n常量池计数器（constant_pool_count）：由于常量的数量不固定，所以需要先放置两个字节来表示常量池容量计数值。图2中示例代码的字节码前10个字节如下图5所示，将十六进制的24转化为十进制值为36，排除掉下标“0”，也就是说，这个类文件中共有35个常量。 图5 前十个字节及含义\n常量池数据区：数据区是由（constant_pool_count-1）个cp_info结构组成，一个cp_info结构对应一个常量。在字节码中共有14种类型的cp_info（如下图6所示），每种类型的结构都是固定的。 图6 各类型的cp_info\n具体以CONSTANT_utf8_info为例，它的结构如下图7左侧所示。首先一个字节“tag”，它的值取自上图6中对应项的Tag，由于它的类型是utf8_info，所以值为“01”。接下来两个字节标识该字符串的长度Length，然后Length个字节为这个字符串具体的值。从图2中的字节码摘取一个cp_info结构，如下图7右侧所示。将它翻译过来后，其含义为：该常量类型为utf8字符串，长度为一字节，数据为“a”。\n图7 CONSTANT_utf8_info的结构（左）及示例（右）\n其他类型的cp_info结构在本文不再赘述，整体结构大同小异，都是先通过Tag来标识类型，然后后续n个字节来描述长度和（或）数据。先知其所以然，以后可以通过javap -verbose ByteCodeDemo命令，查看JVM反编译后的完整常量池，如下图8所示。可以看到反编译结果将每一个cp_info结构的类型和值都很明确地呈现了出来。\n（4） 访问标志\n常量池结束之后的两个字节，描述该Class是类还是接口，以及是否被Public、Abstract、Final等修饰符修饰。JVM规范规定了如下图9的访问标志（Access_Flag）。需要注意的是，JVM并没有穷举所有的访问标志，而是使用按位或操作来进行描述的，比如某个类的修饰符为Public Final，则对应的访问修饰符的值为ACC_PUBLIC | ACC_FINAL，即0x0001 | 0x0010=0x0011。\n图9 访问标志\n（5） 当前类名\n访问标志后的两个字节，描述的是当前类的全限定名。这两个字节保存的值为常量池中的索引值，根据索引值就能在常量池中找到这个类的全限定名。\n（6） 父类名称\n当前类名后的两个字节，描述父类的全限定名，同上，保存的也是常量池中的索引值。\n（7） 接口信息\n父类名称后为两字节的接口计数器，描述了该类或父类实现的接口数量。紧接着的n个字节是所有接口名称的字符串常量的索引值。\n（8） 字段表\n字段表用于描述类和接口中声明的变量，包含类级别的变量以及实例变量，但是不包含方法内部声明的局部变量。字段表也分为两部分，第一部分为两个字节，描述字段个数；第二部分是每个字段的详细信息fields_info。字段表结构如下图所示：\n图10 字段表结构\n以图2中字节码的字段表为例，如下图11所示。其中字段的访问标志查图9，0002对应为Private。通过索引下标在图8中常量池分别得到字段名为“a”，描述符为“I”（代表int）。综上，就可以唯一确定出一个类中声明的变量private int a。\n图11 字段表示例\n（9）方法表\n字段表结束后为方法表，方法表也是由两部分组成，第一部分为两个字节描述方法的个数；第二部分为每个方法的详细信息。方法的详细信息较为复杂，包括方法的访问标志、方法名、方法的描述符以及方法的属性，如下图所示：\n图12 方法表结构\n方法的权限修饰符依然可以通过图9的值查询得到，方法名和方法的描述符都是常量池中的索引值，可以通过索引值在常量池中找到。而“方法的属性”这一部分较为复杂，直接借助javap -verbose将其反编译为人可以读懂的信息进行解读，如图13所示。可以看到属性中包括以下三个部分：\n“Code区”：源代码对应的JVM指令操作码，在进行字节码增强时重点操作的就是“Code区”这一部分。 “LineNumberTable”：行号表，将Code区的操作码和源代码中的行号对应，Debug时会起到作用（源代码走一行，需要走多少个JVM指令操作码）。 “LocalVariableTable”：本地变量表，包含This和局部变量，之所以可以在每一个方法内部都可以调用This，是因为JVM将This作为每一个方法的第一个参数隐式进行传入。当然，这是针对非Static方法而言。 图13 反编译后的方法表\n（10）附加属性表\n字节码的最后一部分，该项存放了在该文件中类或接口所定义属性的基本信息。\n1.3 字节码操作集合 在上图13中，Code区的红色编号0～17，就是.java中的方法源代码编译后让JVM真正执行的操作码。为了帮助人们理解，反编译后看到的是十六进制操作码所对应的助记符，十六进制值操作码与助记符的对应关系，以及每一个操作码的用处可以查看Oracle官方文档进行了解，在需要用到时进行查阅即可。比如上图中第一个助记符为iconst_2，对应到图2中的字节码为0x05，用处是将int值2压入操作数栈中。以此类推，对0~17的助记符理解后，就是完整的add()方法的实现。\n1.4 操作数栈和字节码 JVM的指令集是基于栈而不是寄存器，基于栈可以具备很好的跨平台性（因为寄存器指令集往往和硬件挂钩），但缺点在于，要完成同样的操作，基于栈的实现需要更多指令才能完成（因为栈只是一个FILO结构，需要频繁压栈出栈）。另外，由于栈是在内存实现的，而寄存器是在CPU的高速缓存区，相较而言，基于栈的速度要慢很多，这也是为了跨平台性而做出的牺牲。\n我们在上文所说的操作码或者操作集合，其实控制的就是这个JVM的操作数栈。为了更直观地感受操作码是如何控制操作数栈的，以及理解常量池、变量表的作用，将add()方法的对操作数栈的操作制作为GIF，如下图14所示，图中仅截取了常量池中被引用的部分，以指令iconst_2开始到ireturn结束，与图13中Code区0~17的指令一一对应：\n1.5 查看字节码工具 如果每次查看反编译后的字节码都使用javap命令的话，好非常繁琐。这里推荐一个Idea插件：jclasslib 。使用效果如图15所示，代码编译后在菜单栏”View”中选择”Show Bytecode With jclasslib”，可以很直观地看到当前字节码文件的类信息、常量池、方法区等信息。\n2. 字节码增强 在上文中，着重介绍了字节码的结构，这为我们了解字节码增强技术的实现打下了基础。字节码增强技术就是一类对现有字节码进行修改或者动态生成全新字节码文件的技术。接下来，我们将从最直接操纵字节码的实现方式开始深入进行剖析。\nCGLIB和javaProxy 是基于对字节码的新增（新增一个完整的字节码，但是要等到需要被操作的类被加载之后才会在系统运行期动态生成代理类，新增类必须用到代理（对性能有一定的影响）\nAspectJ和agent是基于对字节码的修改（修改现有的类的字节码）也就是说被代理的类的class字节码 在装载入jvm的时候，已经被修改了所以性能会很高 修改不需要用到代理\njava 动态字节码技术 - 简书 (jianshu.com) 2.1 cglib 基于继承实现，构建目标类的子类（全新的类），初始化的时候\n2.1.1 ASM 对于需要手动操纵字节码的需求，可以使用ASM，它可以直接生产 .class字节码文件，也可以在类被加载入JVM之前动态修改类行为（如下图17所示）。ASM的应用场景有AOP（Cglib就是基于ASM）、热部署、修改其他jar包中的类等。当然，涉及到如此底层的步骤，实现起来也比较麻烦。接下来，本文将介绍ASM的两种API，并用ASM来实现一个比较粗糙的AOP。但在此之前，为了让大家更快地理解ASM的处理流程，强烈建议读者先对访问者模式 进行了解。简单来说，访问者模式主要用于修改或操作一些数据结构比较稳定的数据，而通过第一章，我们知道字节码文件的结构是由JVM固定的，所以很适合利用访问者模式对字节码文件进行修改。\n2.1.1.1 ASM API 2.1.1.1.1 核心API ASM Core API可以类比解析XML文件中的SAX方式，不需要把这个类的整个结构读取进来，就可以用流式的方法来处理字节码文件。好处是非常节约内存，但是编程难度较大。然而出于性能考虑，一般情况下编程都使用Core API。在Core API中有以下几个关键类：\nClassReader：用于读取已经编译好的.class文件。 ClassWriter：用于重新构建编译后的类，如修改类名、属性以及方法，也可以生成新的类的字节码文件。 各种Visitor类：如上所述，CoreAPI根据字节码从上到下依次处理，对于字节码文件中不同的区域有不同的Visitor，比如用于访问方法的MethodVisitor、用于访问类变量的FieldVisitor、用于访问注解的AnnotationVisitor等。为了实现AOP，重点要使用的是MethodVisitor。 2.1.1.1.2 树形API ASM Tree API可以类比解析XML文件中的DOM方式，把整个类的结构读取到内存中，缺点是消耗内存多，但是编程比较简单。TreeApi不同于CoreAPI，TreeAPI通过各种Node类来映射字节码的各个区域，类比DOM节点，就可以很好地理解这种编程方式。\n2.2 Javassist 使用类加载器启动自定义的类加载器，并加一个类加载监听器，监听器发现目标类被加载时就织入切入逻辑\nJavassist 使用指南（一） - 简书 (jianshu.com) ASM是在指令层次上操作字节码的，阅读上文后，我们的直观感受是在指令层次上操作字节码的框架实现起来比较晦涩。故除此之外，我们再简单介绍另外一类框架：强调源代码层次操作字节码的框架Javassist。\n利用Javassist实现字节码增强时，可以无须关注字节码刻板的结构，其优点就在于编程简单。直接使用java编码的形式，而不需要了解虚拟机指令，就能动态改变类的结构或者动态生成类。其中最重要的是ClassPool、CtClass、CtMethod、CtField这四个类：\nCtClass（compile-time class）：编译时类信息，它是一个class文件在代码中的抽象表现形式，可以通过一个类的全限定名来获取一个CtClass对象，用来表示这个类文件。 ClassPool：从开发视角来看，ClassPool是一张保存CtClass信息的HashTable，key为类名，value为类名对应的CtClass对象。当我们需要对某个类进行修改时，就是通过pool.getCtClass(“className”)方法从pool中获取到相应的CtClass。 CtMethod、CtField：这两个比较好理解，对应的是类中的方法和属性。 了解这四个类后，我们可以写一个小Demo来展示Javassist简单、快速的特点。我们依然是对Base中的process()方法做增强，在方法调用前后分别输出”start”和”end”，实现代码如下。我们需要做的就是从pool中获取到相应的CtClass对象和其中的方法，然后执行method.insertBefore和insertAfter方法，参数为要插入的Java代码，再以字符串的形式传入即可，实现起来也极为简单。\nimport com.meituan.mtrace.agent.javassist.*; public class JavassistTest { public static void main(String[] args) throws NotFoundException, CannotCompileException, IllegalAccessException, InstantiationException, IOException { ClassPool cp = ClassPool.getDefault(); CtClass cc = cp.get(\u0026#34;meituan.bytecode.javassist.Base\u0026#34;); CtMethod m = cc.getDeclaredMethod(\u0026#34;process\u0026#34;); m.insertBefore(\u0026#34;{ System.out.println(\\\u0026#34;start\\\u0026#34;); }\u0026#34;); m.insertAfter(\u0026#34;{ System.out.println(\\\u0026#34;end\\\u0026#34;); }\u0026#34;); Class c = cc.toClass(); cc.writeFile(\u0026#34;/Users/zen/projects\u0026#34;); Base h = (Base)c.newInstance(); h.process(); } } 2.3 java proxy 基于接口的实现，构建目标类的实现类（全新的类），初始化的时候\n2.4 Aspectj aspectJ的使用是在编译期，通过特殊的编译器可以在不改变代码的前提下织入代码 修改目标类的字节，织入代理的字节，在程序编译的时候，不会生成全新的class，\n以前spring的切面使用Aspectj做的,现在它已经淡出了我们的视线,因为 spring自己写了套 aop实现,就是我们常说的spring的两座大山之一, 虽然spring AOP 用了aspectj 的一些注解,但是 实现是spring自己写的,\n2.5 java agent 修改目标类的字节码，在类装载的时候，插入动态代理的字节码，不会生成全新的class\n是通过java.lang.instrument实现，用来在 ClassLoader 加载字节码之前完成对操作字节码的目的.\nAgent就是JVMTI的一种实现，Agent有两种启动方式，\n一是随Java进程启动而启动，经常见到的java -agentlib就是这种方式； (skyworking/ 数列公司等用的这种)\n二是运行时载入，通过attach API，将模块（jar包）动态地Attach到指定进程id的Java进程内。\n诸如jstack打印进程栈、jps列出java进程、jmap做内存dump等功能，都属于Attach API可以发送的指令\njava 动态字节码技术 - 简书 (jianshu.com) 3. 运行时类的重载 3.1 问题引出 上一章重点介绍了两种不同类型的字节码操作框架，且都利用它们实现了较为粗糙的AOP。其实，为了方便大家理解字节码增强技术，在上文中我们避重就轻将ASM实现AOP的过程分为了两个main方法：第一个是利用MyClassVisitor对已编译好的class文件进行修改，第二个是new对象并调用。这期间并不涉及到JVM运行时对类的重加载，而是在第一个main方法中，通过ASM对已编译类的字节码进行替换，在第二个main方法中，直接使用已替换好的新类信息。另外在Javassist的实现中，我们也只加载了一次Base类，也不涉及到运行时重加载类。\n如果我们在一个JVM中，先加载了一个类，然后又对其进行字节码增强并重新加载会发生什么呢？模拟这种情况，只需要我们在上文中Javassist的Demo中main()方法的第一行添加Base b=new Base()，即在增强前就先让JVM加载Base类，然后在执行到c.toClass()方法时会抛出错误，如下图20所示。跟进c.toClass()方法中，我们会发现它是在最后调用了ClassLoader的native方法defineClass()时报错。也就是说，JVM是不允许在运行时动态重载一个类的。\n显然，如果只能在类加载前对类进行强化，那字节码增强技术的使用场景就变得很窄了。我们期望的效果是：在一个持续运行并已经加载了所有类的JVM中，还能利用字节码增强技术对其中的类行为做替换并重新加载。为了模拟这种情况，我们将Base类做改写，在其中编写main方法，每五秒调用一次process()方法，在process()方法中输出一行“process”。\n我们的目的就是，在JVM运行中的时候，将process()方法做替换，在其前后分别打印“start”和“end”。也就是在运行中时，每五秒打印的内容由”process”变为打印”start process end”。那如何解决JVM不允许运行时重加载类信息的问题呢？为了达到这个目的，我们接下来一一来介绍需要借助的Java类库。\nimport java.lang.management.ManagementFactory; public class Base { public static void main(String[] args) { String name = ManagementFactory.getRuntimeMXBean().getName(); String s = name.split(\u0026#34;@\u0026#34;)[0]; //打印当前Pid System.out.println(\u0026#34;pid:\u0026#34;+s); while (true) { try { Thread.sleep(5000L); } catch (Exception e) { break; } process(); } } public static void process() { System.out.println(\u0026#34;process\u0026#34;); } } 3.2 Instrument instrument是JVM提供的一个可以修改已加载类的类库，专门为Java语言编写的插桩服务提供支持。它需要依赖JVMTI的Attach API机制实现，JVMTI这一部分，我们将在下一小节进行介绍。在JDK 1.6以前，instrument只能在JVM刚启动开始加载类时生效，而在JDK 1.6之后，instrument支持了在运行时对类定义的修改。要使用instrument的类修改功能，我们需要实现它提供的ClassFileTransformer接口，定义一个类文件转换器。接口中的transform()方法会在类文件被加载时调用，而在transform方法里，我们可以利用上文中的ASM或Javassist对传入的字节码进行改写或替换，生成新的字节码数组后返回。\n我们定义一个实现了ClassFileTransformer接口的类TestTransformer，依然在其中利用Javassist对Base类中的process()方法进行增强，在前后分别打印“start”和“end”，代码如下：\nimport java.lang.instrument.ClassFileTransformer; public class TestTransformer implements ClassFileTransformer { @Override public byte[] transform(ClassLoader loader, String className, Class\u0026lt;?\u0026gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) { System.out.println(\u0026#34;Transforming \u0026#34; + className); try { ClassPool cp = ClassPool.getDefault(); CtClass cc = cp.get(\u0026#34;meituan.bytecode.jvmti.Base\u0026#34;); CtMethod m = cc.getDeclaredMethod(\u0026#34;process\u0026#34;); m.insertBefore(\u0026#34;{ System.out.println(\\\u0026#34;start\\\u0026#34;); }\u0026#34;); m.insertAfter(\u0026#34;{ System.out.println(\\\u0026#34;end\\\u0026#34;); }\u0026#34;); return cc.toBytecode(); } catch (Exception e) { e.printStackTrace(); } return null; } } 现在有了Transformer，那么它要如何注入到正在运行的JVM呢？还需要定义一个Agent，借助Agent的能力将Instrument注入到JVM中。我们将在下一小节介绍Agent，现在要介绍的是Agent中用到的另一个类Instrumentation。在JDK 1.6之后，Instrumentation可以做启动后的Instrument、本地代码（Native Code）的Instrument，以及动态改变Classpath等等。我们可以向Instrumentation中添加上文中定义的Transformer，并指定要被重加载的类，代码如下所示。这样，当Agent被Attach到一个JVM中时，就会执行类字节码替换并重载入JVM的操作。\nimport java.lang.instrument.Instrumentation; public class TestAgent { public static void agentmain(String args, Instrumentation inst) { //指定我们自己定义的Transformer，在其中利用Javassist做字节码替换 inst.addTransformer(new TestTransformer(), true); try { //重定义类并载入新的字节码 inst.retransformClasses(Base.class); System.out.println(\u0026#34;Agent Load Done.\u0026#34;); } catch (Exception e) { System.out.println(\u0026#34;agent load failed!\u0026#34;); } } } 3.3 JVMTI \u0026amp; Agent \u0026amp; Attach API 上一小节中，我们给出了Agent类的代码，追根溯源需要先介绍JPDA（Java Platform Debugger Architecture）。如果JVM启动时开启了JPDA，那么类是允许被重新加载的。在这种情况下，已被加载的旧版本类信息可以被卸载，然后重新加载新版本的类。正如JDPA名称中的Debugger，JDPA其实是一套用于调试Java程序的标准，任何JDK都必须实现该标准。\nJPDA定义了一整套完整的体系，它将调试体系分为三部分，并规定了三者之间的通信接口。三部分由低到高分别是Java 虚拟机工具接口（JVMTI），Java 调试协议（JDWP）以及 Java 调试接口（JDI），三者之间的关系如下图所示：\n现在回到正题，我们可以借助JVMTI的一部分能力，帮助动态重载类信息。JVM TI（JVM TOOL INTERFACE，JVM工具接口）是JVM提供的一套对JVM进行操作的工具接口。通过JVMTI，可以实现对JVM的多种操作，它通过接口注册各种事件勾子，在JVM事件触发时，同时触发预定义的勾子，以实现对各个JVM事件的响应，事件包括类文件加载、异常产生与捕获、线程启动和结束、进入和退出临界区、成员变量修改、GC开始和结束、方法调用进入和退出、临界区竞争与等待、VM启动与退出等等。\n而Agent就是JVMTI的一种实现，Agent有两种启动方式，\n一是随Java进程启动而启动，经常见到的java -agentlib就是这种方式； (skyworking/ 数列公司等用的这种)\n二是运行时载入，通过attach API，将模块（jar包）动态地Attach到指定进程id的Java进程内。\n(悟空听云产品用的就是这样,现象就是我没写agent,它就自动监听到我的项目了)\nidea也提供了这样方式的debug能力\n在 IDEA 中 DEBUG 的各种奇技淫巧 - 知乎 (zhihu.com) Attach API 的作用是提供JVM进程间通信的能力，比如说我们为了让另外一个JVM进程把线上服务的线程Dump出来，会运行jstack或jmap的进程，并传递pid的参数，告诉它要对哪个进程进行线程Dump，这就是Attach API做的事情。在下面，我们将通过Attach API的loadAgent()方法，将打包好的Agent jar包动态Attach到目标JVM上。具体实现起来的步骤如下：\n定义Agent，并在其中实现AgentMain方法，如上一小节中定义的代码块7中的TestAgent类；\n然后将TestAgent类打成一个包含MANIFEST.MF的jar包，其中MANIFEST.MF文件中将Agent-Class属性指定为TestAgent的全限定名，如下图所示；\n最后利用Attach API，将我们打包好的jar包Attach到指定的JVM pid上，代码如下：\nimport com.sun.tools.attach.VirtualMachine; public class Attacher { public static void main(String[] args) throws AttachNotSupportedException, IOException, AgentLoadException, AgentInitializationException { // 传入目标 JVM pid VirtualMachine vm = VirtualMachine.attach(\u0026#34;39333\u0026#34;); vm.loadAgent(\u0026#34;/Users/zen/operation_server_jar/operation-server.jar\u0026#34;); } } 由于在MANIFEST.MF中指定了Agent-Class，所以在Attach后，目标JVM在运行时会走到TestAgent类中定义的agentmain()方法，而在这个方法中，我们利用Instrumentation，将指定类的字节码通过定义的类转化器TestTransformer做了Base类的字节码替换（通过javassist），并完成了类的重新加载。由此，我们达成了“在JVM运行时，改变类的字节码并重新载入类信息”的目的。 以下为运行时重新载入类的效果：先运行Base中的main()方法，启动一个JVM，可以在控制台看到每隔五秒输出一次”process”。接着执行Attacher中的main()方法，并将上一个JVM的pid传入。此时回到上一个main()方法的控制台，可以看到现在每隔五秒输出”process”前后会分别输出”start”和”end”，也就是说完成了运行时的字节码增强，并重新载入了这个类。\n字节码增强技术探索 - 美团技术团队 (meituan.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%8A%A8%E6%80%81%E5%AD%97%E8%8A%82%E7%A0%81%E6%8A%80%E6%9C%AF.html","summary":"[toc] 1.字节码 1.1 什么是字节码？ Java之所以可以“一次编译，到处运行”，一是因为JVM针对各种操作系统、平台都进行了定制，二是因为无论在什么平","title":"动态字节码技术"},{"content":" 组件 节点 默认端口 配置 用途说明 HDFS DataNode 50010 dfs.datanode.address datanode服务端口，用于数据传输 HDFS DataNode 50075 dfs.datanode.http.address http服务的端口 HDFS DataNode 50475 dfs.datanode.https.address https服务的端口 HDFS DataNode 50020 dfs.datanode.ipc.address ipc服务的端口 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口(web界面) HDFS NameNode 50470 dfs.namenode.https-address https服务的端口 HDFS NameNode 8020 fs.defaultFS 接收Client连接的RPC端口，用于获取文件系统metadata信息。 HDFS journalnode 8485 dfs.journalnode.rpc-address RPC服务 HDFS journalnode 8480 dfs.journalnode.http-address HTTP服务 HDFS ZKFC 8019 dfs.ha.zkfc.port ZooKeeper FailoverController，用于NN HA YARN ResourceManager 8032 yarn.resourcemanager.address RM的applications manager(ASM)端口 YARN ResourceManager 8030 yarn.resourcemanager.scheduler.address scheduler组件的IPC端口 YARN ResourceManager 8031 yarn.resourcemanager.resource-tracker.address IPC YARN ResourceManager 8033 yarn.resourcemanager.admin.address IPC YARN ResourceManager 8088 yarn.resourcemanager.webapp.address WebUI的http服务端口 YARN NodeManager 8040 yarn.nodemanager.localizer.address localizer IPC YARN NodeManager 8042 yarn.nodemanager.webapp.address http服务端口 YARN NodeManager 8041 yarn.nodemanager.address NM中container manager的端口 YARN JobHistory Server 10020 mapreduce.jobhistory.address IPC YARN JobHistory Server 19888 mapreduce.jobhistory.webapp.address http服务端口 HBase Master 60000 hbase.master.port IPC HBase Master 60010 hbase.master.info.port http服务端口 HBase RegionServer 60020 hbase.regionserver.port IPC HBase RegionServer 60030 hbase.regionserver.info.port http服务端口 HBase HQuorumPeer 2181 hbase.zookeeper.property.clientPort HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 HBase HQuorumPeer 2888 hbase.zookeeper.peerport HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 HBase HQuorumPeer 3888 hbase.zookeeper.leaderport HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 Hive Metastore 9083 /etc/default/hive-metastore中export PORT=来更新默认端口 Hive HiveServer 10000 /etc/hive/conf/hive-env.sh中export HIVE_SERVER2_THRIFT_PORT=来更新默认端口 ZooKeeper Server 2181 /etc/zookeeper/conf/zoo.cfg中clientPort= 对客户端提供服务的端口 ZooKeeper Server 2888 /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分 follower用来连接到leader，只在leader上监听该端口。 ZooKeeper Server 3888 /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分 用于leader选举的。只在electionAlg是1,2或3(默认)时需要。 Flume Master/slave 52020 自己的写的配置文件中,如:flume-server.properties 消息传递端口 Kafka Master/slave 9092 server.properties中port属性 kafka对外提供服务,消息传递 Storm master 8888 storm.yaml中ui.port ui进程所用端口(自己修改) Storm slave 6700 -6703 Storm.yaml中supervisor.slots.ports: 可以自己配置,一个端口指定一台电脑,supervisor进程 Spark Master 8080 SparkUI端口 Spark Master 7077 spark-defaults.conf Spark集群运行端口 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E5%88%9D%E5%A7%8B%E7%94%9F%E6%80%81%E5%9C%88/%E7%AB%AF%E5%8F%A3.html","summary":"组件 节点 默认端口 配置 用途说明 HDFS DataNode 50010 dfs.datanode.address datanode服务端口，用于数据传输 HDFS DataNode 50075 dfs.datanode.http.address http服务的端口 HDFS DataNode 50475 dfs.datanode.https.address https服务的端口 HDFS DataNode 50020 dfs.datanode.ipc.address ip","title":"端口"},{"content":"堆表(heap table）和索引组织表（Index Oragnization Table，简称IOT)是两种数据表的存储结构\nOracle支持堆表，也支持索引组织表\nPostgreSQL只支持堆表，不支持索引组织表\nInnodb只支持索引组织表\nMyIASM只支持堆表\n堆表和索引组织表的区别 堆表 堆就是无序数据的集合,索引就是将数据变得有序,在索引中键值有序,数据还是无序的 数据存放在数据里面,索引存放在索引里 堆表中,主键索引和普通索引一样的,叶子节点存放的是指向堆表中数据的指针（可以是一个页编号加偏移量）,指向物理地址,没有回表的说法 堆表中,主键和普通索引基本上没区别,和非空的唯一索引没区别 mysql 的 myisam 引擎，oracle pg 都支持的是堆表 索引组织表 innodb 引擎支持的就是索引组织表 对于主键的索引,叶子节点存放了一整行所有数据,其他索引称为辅助索引(二级索引),它的叶子节点只是存放了键值和主键值 主键包含了一张表的所有数据,因为主键索引的页子节点中保存了每一行的完整记录,包括所有列。如果没有主键,MySQL会自动帮你加一个主键,但是对用户不可见 innodb中数据存放在聚集索引中,换言之,按照主键的方式来组织数据的 其他索引(唯一索引,普通索引)的叶子节点存放该索引列的键值和主键值 不管是什么索引非叶子节点存放的存放的就是键值和指针,不存数据,这个指针在innodb中是6个bit,键值就看数据大小了 在mysql中除了主键索引,其他都是二级索引 , 这些索引都是针对索引树做的二次索引树(因为只存真实数据都在主键里了), 这就是为什么它叫二级索引 以及会出现索引回表, 这全是由于数据存储结构引起的\n深入理解Oracle表(6)：堆组织表(HOT)和索引组织表(IOT)的区别_linwaterbin的博客-CSDN博客_堆组织表 索引组织表 堆表 索引组织表 - mhabbyo - 博客园 (cnblogs.com) 数据库之堆表和索引组织表 - 墨天轮 (modb.pro) 如何理解oracle索引组织表 - 关系型数据库 - 亿速云 (yisu.com) ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%A0%86%E8%A1%A8%E5%92%8C%E7%B4%A2%E5%BC%95%E7%BB%84%E7%BB%87%E8%A1%A8.html","summary":"堆表(heap table）和索引组织表（Index Oragnization Table，简称IOT)是两种数据表的存储结构 Oracle支持堆表，也支持索引组织表 Po","title":"堆表和索引组织表"},{"content":"[toc]\n1. 什么是多版本并发控制（ MVCC ） MySQL 的大多数事务型存储引擎基于提升并发性能的考虑，一般都实现了多版本并发控制（ MVCC ）。MVCC 是行级锁的一个变种，但实际上实现机制有所不同，避免了加锁的操作，因此有了更低的开销和更高的性能。\nMVCC 的实现，是通过保持数据在某个时间点的快照来实现的，不管事务的执行时间有多久，MVCC 保障了每一个事务内看到的数据是一致的，而根据事务开始时间，不同事务看到同一张表，同一份数据可能是不同的。 MVCC的实现，是通过保存数据在某个时间点的快照来实现的。\n需要注意的是，MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作，其他两个级别都和MVCC“不兼容”。因为READ UNCOMMITTED总是读取最新的数据行，而SERIALIZABLE则会对所有读取的行都加锁。\n保存这两个额外的系统版本号，使大多数的读操作都不用加锁，这样的设计使得读数据操作很简单，性能很好，并且只会读取到符合标准的行。不足之处是每行记录都需要额外的存储空间，需要做更多额外的检查工作和维护工作。\n不同存储引擎对 MVCC 的实现机制不尽相同，因为 MVCC 并没有一个统一的标准，下面以 InnoDB 为例，简要介绍 MVCC 的实现原理。\n注: 多版本并发控制不支持myisam存储引擎。估计是因为myism是锁整个表,做不到版本控制\n2. InnoDB 中的 MVCC InnoDB 中的 MVCC，是通过在每行记录后保存两个隐藏的列来实现的。这两个特殊的列一个保存了行的创建时间，一个保存了行的删除时间。当然这里存储的并不是真实的时间，实际上存储的是版本号（ system version number ）。每次开启一个新的事务，系统版本号会开始递增并分配给当前的事务，用于与数据的版本号进行比较。简要介绍下 InnoDB 是如何操作的：\nSELECT：在 InnoDB 中，SELECT 操作有两个额外的条件： 只查找行的创建版本号小于或等于当前事务版本号的数据。这样可以保证事务读取的数据要么是在事务开始之前就已经存在，要么是事务本身插入或者修改过的。 行的删除版本号要么未定义，要么大于当前事务的版本号。这样是为了确保事务读取的数据在事务开始之前未被删除。 INSERT：InnoDB 为每行新增数据设置了当前事务的版本号作为创建版本号，删除版本号留空。 DELETE：InnoDB 为每行删除的数据设置了当前事务的版本号作为删除版本号。 UPDATE：InnoDB 额外插入一条新的记录，将当前事务版本号作为新记录的创建版本号，同时保存当前事务版本号到原来的数据中的删除版本号。 在 InnoDB 中，MVCC 只在 READ COMMITTED 和 REPEATABLE READ 这两个隔离级别下工作。其他的隔离级别和 MVCC 并不兼容，READ UNCOMMITTED 总是读取最新的数据，而 SERIALIZABLE 则会对所有读取的行都进行加锁操作。\n来自: https://blog.csdn.net/qq_35499060/article/details/83590723 以上为简版描述, 但不准确\n2.1 基本概念 MVCC实现原理主要是依赖记录中的 3个隐式字段，undo日志 ， Read View 来实现的。\n隐式字段 每行记录除了我们自定义的字段外，还有数据库隐式定义的 DB_TRX_ID, DB_ROLL_PTR, DB_ROW_ID 等字段\nDB_TRX_ID 6 byte，最近修改(修改/插入)事务 ID：记录创建这条记录/最后一次修改该记录的事务 ID\nDB_ROLL_PTR 7 byte，回滚指针，指向这条记录的上一个版本地址（存储于 rollback segment 里）\nDB_ROW_ID 6 byte，隐含的自增 ID（隐藏主键），如果数据表没有主键，InnoDB 会自动以DB_ROW_ID产生一个聚簇索引\n实际还有一个删除 flag 隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除 flag 变了\n如上图，DB_ROW_ID 是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID 是当前操作该记录的事务 ID ,而 DB_ROLL_PTR 是一个回滚指针，用于配合 undo日志，指向上一个旧版本\nundo日志 undo log 主要分为两种：\ninsert undo log 代表事务在 insert 新记录时产生的 undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃\nupdate undo log ​\t事务在进行 update 或 delete 时产生的 undo log ; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快照读或事务回滚 不涉及该日志时，对应的日志才会被 purge 线程统一清除\npurge\n从前面的分析可以看出，为了实现 InnoDB 的 MVCC 机制，更新或者删除操作都只是设置一下老记录的 deleted_bit ，并不真正将过时的记录删除。\n为了节省磁盘空间，InnoDB 有专门的 purge 线程来清理 deleted_bit 为 true 的记录。为了不影响 MVCC 的正常工作，purge 线程自己也维护了一个read view（这个 read view 相当于系统中最老活跃事务的 read view ）;如果某个记录的 deleted_bit 为 true ，并且 DB_TRX_ID 相对于 purge 线程的 read view 可见，那么这条记录一定是可以被安全清除的。\n因此衍生一个面试题, undo log有什么用途呢？\n事务回滚时使用，保证原子性和一致性。 用于MVCC快照读。 版本链 多个事务并行操作某一行数据时，不同事务对该行数据的修改会产生多个版本，然后通过回滚指针（roll_pointer），连成一个链表，这个链表就称为版本链\n快照读和当前读 快照读： 读取的是记录数据的可见版本（有旧的版本）。不加锁,普通的select语句都是快照读,如：\nselect * from core_user where id \u0026gt; 2; 当前读：读取的是记录数据的最新版本，显式加锁的都是当前读\nselect * from core_user where id \u0026gt; 2 for update; select * from account where id \u0026gt; 2 lock in share mode; Read View 和可见性规则判断 什么是 Read View，说白了 Read View 就是事务进行快照读操作的时候生产的读视图 (Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的 ID (当每个事务开启时，都会被分配一个 ID , 这个 ID 是递增的，所以最新的事务，ID 值越大)\n所以我们知道 Read View 主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个 Read View 读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。\nRead View遵循一个可见性算法，主要是将要被修改的数据的最新记录中的 DB_TRX_ID（即当前事务 ID ）取出来，与系统当前其他活跃事务的 ID 去对比（由 Read View 维护），如果 DB_TRX_ID 跟 Read View 的属性做了某些比较，不符合可见性，那就通过 DB_ROLL_PTR 回滚指针去取出 Undo Log 中的 DB_TRX_ID 再比较，即遍历链表的 DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的 DB_TRX_ID , 那么这个 DB_TRX_ID 所在的旧记录就是当前事务能看见的最新老版本\nRead View是如何保证可见性判断的呢？它也是一种数据结构, 有如下几个重要属性\nm_ids:当前系统中那些活跃(未提交)的读写事务ID, 它数据结构为一个List。\nmin_limit_id:表示在生成ReadView时，当前系统中活跃的读写事务中最小的事务id，即m_ids中的最小值。\nmax_limit_id:表示生成ReadView时，系统中应该分配给下一个事务的id值。(所以系统中目前最大的事务id = max_limit_id - 1)\ncreator_trx_id: 创建当前read view的事务ID\nRead view 匹配条件规则如下：\n如果数据事务ID trx_id \u0026lt; min_limit_id，表明生成该版本的事务在生成Read View前，已经提交(因为事务ID是递增的)，所以该版本可以被当前事务访问。(在当前事务开始前, 数据就已修改并提交了, 所以当前事务能访问) 如果trx_id\u0026gt;= max_limit_id，表明生成该版本的事务在生成ReadView后才生成，所以该版本不可以被当前事务访问。 如果 min_limit_id =\u0026lt;trx_id\u0026lt; max_limit_id,需要分3种情况讨论 （1）.如果m_ids包含trx_id,则代表Read View生成时刻，这个事务还未提交，但是如果数据的trx_id等于creator_trx_id的话，表明数据是自己生成的，因此是可见的。\n（2）如果m_ids包含trx_id，并且trx_id不等于creator_trx_id，则Read View生成时，事务未提交，并且不是自己生产的，所以当前事务也是看不见的；\n（3）.如果m_ids不包含trx_id，则说明你这个事务在Read View生成之前就已经提交了，修改的结果，当前事务是能看见的。\n整体流程 我们模拟一下\n当事务 2 对某行数据执行了快照读，数据库为该行数据生成一个Read View读视图，假设当前事务 ID 为 2，此时还有事务1和事务3在活跃中，事务 4 在事务 2 快照读前一刻提交更新了，所以 Read View 记录了系统当前活跃事务 1，3 的 ID，维护在一个列表上，假设我们称为m_ids 事务 1 事务 2 事务 3 事务 4 事务开始 事务开始 事务开始 事务开始 … … … 修改且已提交 进行中 快照读 进行中 Read View 不仅仅会通过一个列表 trx_list 来维护事务 2执行快照读那刻系统正活跃的事务 ID 列表，还会有两个属性 min_limit_id（ m_ids列表中事务 ID 最小的 ID ），max_limit_id( 快照读时刻系统尚未分配的下一个事务 ID ，也就是目前已出现过的事务ID的最大值 + 1 传送门 ) 。所以在这里例子中 min_limit_id 就是1，max_limit_id就是 4 + 1 = 5，m_ids 集合的值是 1, 3，Read View 如下图 我们的例子中，只有事务 4 修改过该行记录，并在事务 2 执行快照读前，就提交了事务，所以当前该行当前数据的 undo log 如下图所示；我们的事务 2 在快照读该行记录的时候，就会拿该行记录的 DB_TRX_ID 去跟 min_limit_id, max_limit_id和活跃事务 ID 列表( m_ids )进行比较，判断当前事务 2能看到该记录的版本是哪个。 所以先拿该记录 DB_TRX_ID 字段记录的事务 ID 4 去跟 Read View 的 min_limit_id比较，\n看 4 是否小于 min_limit_id( 1 )，所以不符合条件，\n继续判断 4 是否大于等于 max_limit_id( 5 )，也不符合条件，\n最后判断 4 是否处于 m_ids 中的活跃事务, 最后发现事务 ID 为 4 的事务不在当前活跃事务列表中, 符合可见性条件，所以事务 4修改后提交的最新结果对事务 2 快照读时是可见的，所以事务 2 能读到的最新数据记录是事务4所提交的版本，而事务4提交的版本也是全局角度上最新的版本\nRC , RR 级别下的 InnoDB 快照读有什么不同？ RC: read commit , 读已提交\nRR: repeatable read, 可重复读\n正是 Read View 生成时机的不同，从而造成 RC , RR 级别下快照读的结果的不同\n在 RR 级别下的某个事务的对某条记录的第一次快照读会创建一个快照及 Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个 Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个 Read View，所以对之后的修改不可见；\n即 RR 级别下，快照读生成 Read View 时，Read View 会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见\n而在 RC 级别下的，事务中，每次快照读都会新生成一个快照和 Read View , 这就是我们在 RC 级别下的事务中可以看到别的事务提交的更新的原因\n​\n总之在 RC 隔离级别下，是每个快照读都会生成并获取最新的 Read View；而在 RR 隔离级别下，则是同一个事务中的第一个快照读才会创建 Read View, 之后的快照读获取的都是同一个 Read View。\n【MySQL笔记】正确的理解MySQL的MVCC及实现原理 看一遍就理解：MVCC原理详解 MySQL 8.0 MVCC 核心原理解析（核心源码） ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6mvcc.html","summary":"[toc] 1. 什么是多版本并发控制（ MVCC ） MySQL 的大多数事务型存储引擎基于提升并发性能的考虑，一般都实现了多版本并发控制（ MVCC ）。MVCC 是行级锁的一个变种，","title":"多版本并发控制(MVCC)"},{"content":"[toc]\n1. 使用 1.1. 引包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dynamic-datasource-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 用最新版的就好 Maven Repository: dynamic-datasource-spring-boot-starter 不过做开发的经验来看, 应该选 倒数第二个大版本里最新的版本, 这样既能用更多的功能,又能保证没有大bug, 除非最新版本有你需要的功能\n1.2 增加配置 spring: datasource: dynamic: primary: master #设置默认的数据源或者数据源组,默认值即为master, 所以此处不写也可以 strict: false #设置严格模式,默认false不启动. 启动后在未匹配到指定数据源时候回抛出异常,不启动会使用默认数据源. datasource: master: # 第一个数据源的名称 url: jdbc:mysql://127.0.0.1:3306/dynamic username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver db_1: # 第二个数据源的名称 url: jdbc:gbase://127.0.0.1:5258/dynamic username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 如果是多主多从或者一主多从，那么就用分组的功能实现, 即: 数据组名称_xxx，下划线前面的就是数据组名称，后面就是数据源名, 相同组名称的数据源会放在一个组下。切换数据源时，可以指定具体数据源名称，也可以指定组名然后会自动采用负载均衡算法切换,\n例如: 上面配置中的\u0026quot;db_1\u0026quot;, 表示db分组, 1 名称的数据源, 如果再增加一个\u0026quot;db_2\u0026quot;, 则表示这两个数据源是同一个分组, 当在@DS中只指定\u0026quot;db\u0026quot;, 则会在这个分组中采用负载均衡方式选数据源(默认是轮询)\n1.3 使用@DS注解 @DS注解, 是用来表示使用哪个数据源的, 也是切换数据源功能的核心注解, 它可以用来类和方法上, 方法上的优先级更高, 例如 service层, Mapper层,都可以, 如果没有配置,则会使用默认数据源\n@Service @DS(\u0026#34;master\u0026#34;) // 在类上加 public class UserServiceImpl implements UserService { @Autowired private JdbcTemplate jdbcTemplate; public List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; selectAll() { return jdbcTemplate.queryForList(\u0026#34;select * from user\u0026#34;); } @Override @DS(\u0026#34;db_1\u0026#34;) // 在方法上加 public List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; selectByCondition() { return jdbcTemplate.queryForList(\u0026#34;select * from user where age \u0026gt;10\u0026#34;); } } 官方建议不建议@DS放在Mapper上, 所以网上有很多例子是放在service上,\n但是这有个问题, 当在service中需要访问两个数据库时, 就会无法切换数据源, 毕竟@DS是加在service上, 在同一个service上操作, 自然不会切换, 官方也给出解决方案, 在方法上进一步指定数据源, (伪代码)例如:\nclass Person { @Autowired dbService1 db_1; @Autowired dbService2 db_2; void test(){ db_1.save(); // 如果 db_2.save() 不指定数据源, 数据源会使用db_1 db_2.save(); } } @DS(\u0026#34;db_1\u0026#34;) class dbService1 { void save(){ } } @DS(\u0026#34;db_2\u0026#34;) class dbService2 { @DS(\u0026#34;db_2\u0026#34;) void save(){ } } mybatis-plus中多数据源切换@DS注解到底放在哪一层合适？ @DS不建议加在mapper上的原因是? 2. 源码 一般starter自动配置，都是从 META-INF/spring.factories文件中指定自动配置类：\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ com.baomidou.dynamic.datasource.spring.boot.autoconfigure.DynamicDataSourceAutoConfiguration 所以打开 DynamicDataSourceAutoConfiguration 文件\n2.1 Configuration文件, 加载配置与bean注入 // 动态数据源核心自动配置类 @Slf4j @Configuration @EnableConfigurationProperties(DynamicDataSourceProperties.class) // @AutoConfigureBefore 注解的意思是 在DataSourceAutoConfiguration和DruidDataSourceAutoConfigure之前就注册当前bean // 这里特别注意一下, 在 V3.3.3 之前, 没有把DruidDataSourceAutoConfigure加进来, 所以导致项目中有druid时,数据源无法切换或没有druid配置则报错 @AutoConfigureBefore(value = DataSourceAutoConfiguration.class, name = \u0026#34;com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceAutoConfigure\u0026#34;) //引入了Druid的autoConfig和各种数据源连接池的Creator @Import(value = {DynamicDataSourceCreatorAutoConfiguration.class, DynamicDataSourceAopConfiguration.class, DynamicDataSourceAssistConfiguration.class}) @ConditionalOnProperty(prefix = DynamicDataSourceProperties.PREFIX, name = \u0026#34;enabled\u0026#34;, havingValue = \u0026#34;true\u0026#34;, matchIfMissing = true) public class DynamicDataSourceAutoConfiguration { private final DynamicDataSourceProperties properties; //读取多数据源配置，注入到spring容器中 @Bean @ConditionalOnMissingBean public DynamicDataSourceProvider dynamicDataSourceProvider() { Map\u0026lt;String, DataSourceProperty\u0026gt; datasourceMap = properties.getDatasource(); return new YmlDynamicDataSourceProvider(datasourceMap); } //注册自己的动态多数据源DataSource @Bean @ConditionalOnMissingBean public DataSource dataSource(DynamicDataSourceProvider dynamicDataSourceProvider) { DynamicRoutingDataSource dataSource = new DynamicRoutingDataSource(); dataSource.setPrimary(properties.getPrimary()); dataSource.setStrict(properties.getStrict()); dataSource.setStrategy(properties.getStrategy()); dataSource.setProvider(dynamicDataSourceProvider); dataSource.setP6spy(properties.getP6spy()); dataSource.setSeata(properties.getSeata()); return dataSource; } //AOP切面，对DS注解过的方法进行增强，达到切换数据源的目的 @Role(value = BeanDefinition.ROLE_INFRASTRUCTURE) @Bean @ConditionalOnMissingBean public DynamicDataSourceAnnotationAdvisor dynamicDatasourceAnnotationAdvisor(DsProcessor dsProcessor) { DynamicDataSourceAnnotationInterceptor interceptor = new DynamicDataSourceAnnotationInterceptor(properties.isAllowedPublicOnly(), dsProcessor); DynamicDataSourceAnnotationAdvisor advisor = new DynamicDataSourceAnnotationAdvisor(interceptor); advisor.setOrder(properties.getOrder()); return advisor; } //关于分布式事务加强 @Role(value = BeanDefinition.ROLE_INFRASTRUCTURE) @ConditionalOnProperty(prefix = DynamicDataSourceProperties.PREFIX, name = \u0026#34;seata\u0026#34;, havingValue = \u0026#34;false\u0026#34;, matchIfMissing = true) @Bean public Advisor dynamicTransactionAdvisor() { AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut(); pointcut.setExpression(\u0026#34;@annotation(com.baomidou.dynamic.datasource.annotation.DSTransactional)\u0026#34;); return new DefaultPointcutAdvisor(pointcut, new DynamicTransactionAdvisor()); } //动态参数解析器链 @Bean @ConditionalOnMissingBean public DsProcessor dsProcessor() { DsHeaderProcessor headerProcessor = new DsHeaderProcessor(); DsSessionProcessor sessionProcessor = new DsSessionProcessor(); DsSpelExpressionProcessor spelExpressionProcessor = new DsSpelExpressionProcessor(); headerProcessor.setNextProcessor(sessionProcessor); sessionProcessor.setNextProcessor(spelExpressionProcessor); return headerProcessor; } } 我们可以发现，在使用的时候配置的前缀为spring.datasource.dynamic的配置都会被读取到DynamicDataSourceProperties类，作为一个Bean注入到Spring容器。其实这种读取配置文件信息的方式在日常开发中也是很常见的。\n2.1.1 配置类 DynamicDataSourceProperties\n@ConfigurationProperties(prefix = DynamicDataSourceProperties.PREFIX) public class DynamicDataSourceProperties { public static final String PREFIX = \u0026#34;spring.datasource.dynamic\u0026#34;; public static final String HEALTH = PREFIX + \u0026#34;.health\u0026#34;; /** * 必须设置默认的库,默认master */ private String primary = \u0026#34;master\u0026#34;; /** * 是否启用严格模式,默认不启动. 严格模式下未匹配到数据源直接报错, 非严格模式下则使用默认数据源primary所设置的数据源 */ private Boolean strict = false; // ....... } 2.2 注册DataSource 在 DynamicDataSourceAutoConfiguration 类中, 还加载了DataSource, 我们知道 DataSource是spring用来链接数据库的,\n它注册了一个实现类 DynamicRoutingDataSource, 这个类还有一个父类AbstractRoutingDataSource, 我们先看AbstractRoutingDataSource\n/** * 抽象动态获取数据源 * * @author TaoYu * @since 2.2.0 */ public abstract class AbstractRoutingDataSource extends AbstractDataSource { //抽象方法，由子类实现，让子类决定最终使用的数据源 protected abstract DataSource determineDataSource(); //重写getConnection()方法，实现切换数据源的功能 @Override public Connection getConnection() throws SQLException { //这里xid涉及分布式事务的处理 String xid = TransactionContext.getXID(); if (StringUtils.isEmpty(xid)) { //不使用分布式事务，就是直接返回一个数据连接(实现类: ItemDataSource.class) return determineDataSource().getConnection(); } else { String ds = DynamicDataSourceContextHolder.peek(); ConnectionProxy connection = ConnectionFactory.getConnection(ds); return connection == null ? getConnectionProxy(ds, determineDataSource().getConnection()) : connection; } } } 父类很简单, 只是区分了分布式事务的取connect的方式,所以再来看下其子类DynamicRoutingDataSource 的内容\n@Slf4j public class DynamicRoutingDataSource extends AbstractRoutingDataSource implements InitializingBean, DisposableBean { private static final String UNDERLINE = \u0026#34;_\u0026#34;; /** * 所有数据库 */ private final Map\u0026lt;String, DataSource\u0026gt; dataSourceMap = new ConcurrentHashMap\u0026lt;\u0026gt;(); /** * 分组数据库 */ private final Map\u0026lt;String, GroupDataSource\u0026gt; groupDataSources = new ConcurrentHashMap\u0026lt;\u0026gt;(); @Setter private DynamicDataSourceProvider provider; @Setter private Class\u0026lt;? extends DynamicDataSourceStrategy\u0026gt; strategy = LoadBalanceDynamicDataSourceStrategy.class; @Setter private String primary = \u0026#34;master\u0026#34;; @Setter private Boolean strict = false; @Setter private Boolean p6spy = false; @Setter private Boolean seata = false; @Override public DataSource determineDataSource() { // 获得数据源 return getDataSource(DynamicDataSourceContextHolder.peek()); } private DataSource determinePrimaryDataSource() { log.debug(\u0026#34;dynamic-datasource switch to the primary datasource\u0026#34;); return groupDataSources.containsKey(primary) ? groupDataSources.get(primary).determineDataSource() : dataSourceMap.get(primary); } // bean的属性加载完后, 执行该方法 @Override public void afterPropertiesSet() throws Exception { // 检查开启了配置但没有相关依赖 checkEnv(); // 拿到所有的数据源, Key是数据源的名称，Value则是DataSource。 Map\u0026lt;String, DataSource\u0026gt; dataSources = provider.loadDataSources(); // 添加并分组数据源 for (Map.Entry\u0026lt;String, DataSource\u0026gt; dsItem : dataSources.entrySet()) { addDataSource(dsItem.getKey(), dsItem.getValue()); } // 检测默认数据源是否设置 if (groupDataSources.containsKey(primary)) { log.info(\u0026#34;dynamic-datasource initial loaded [{}] datasource,primary group datasource named [{}]\u0026#34;, dataSources.size(), primary); } else if (dataSourceMap.containsKey(primary)) { log.info(\u0026#34;dynamic-datasource initial loaded [{}] datasource,primary datasource named [{}]\u0026#34;, dataSources.size(), primary); } else { throw new RuntimeException(\u0026#34;dynamic-datasource Please check the setting of primary\u0026#34;); } } /** * 获取数据源 * * @param ds 数据源名称 * @return 数据源 */ public DataSource getDataSource(String ds) { if (StringUtils.isEmpty(ds)) { // 没有ds注解, 则默认用 Primary return determinePrimaryDataSource(); } else if (!groupDataSources.isEmpty() \u0026amp;\u0026amp; groupDataSources.containsKey(ds)) { // 如果有组, 则用负载均衡的方式取一个 log.debug(\u0026#34;dynamic-datasource switch to the datasource named [{}]\u0026#34;, ds); return groupDataSources.get(ds).determineDataSource(); } else if (dataSourceMap.containsKey(ds)) { // 没有组, 则直接取 log.debug(\u0026#34;dynamic-datasource switch to the datasource named [{}]\u0026#34;, ds); return dataSourceMap.get(ds); } // 严格模式下直接报错 if (strict) { throw new CannotFindDataSourceException(\u0026#34;dynamic-datasource could not find a datasource named\u0026#34; + ds); } return determinePrimaryDataSource(); } // ..... } 它实现了InitializingBean接口，这个接口需要实现afterPropertiesSet()方法，这是一个Bean的生命周期函数，在Bean初始化的时候做一些操作\n2.2.1 获取所有的DataSource 先看一下它是怎么获得数据源的? provider.loadDataSources() 是抽象方法, 有两个实现\nYmlDynamicDataSourceProvider和AbstractJdbcDataSourceProvider, 在DynamicDataSourceAutoConfiguration 类中, 注入的是YmlDynamicDataSourceProvider, 所以直接看它的实现 , 最后会跳转到AbstractDataSourceProvider\n@Slf4j public abstract class AbstractDataSourceProvider implements DynamicDataSourceProvider { @Autowired private DefaultDataSourceCreator defaultDataSourceCreator; protected Map\u0026lt;String, DataSource\u0026gt; createDataSourceMap( Map\u0026lt;String, DataSourceProperty\u0026gt; dataSourcePropertiesMap) { Map\u0026lt;String, DataSource\u0026gt; dataSourceMap = new HashMap\u0026lt;\u0026gt;(dataSourcePropertiesMap.size() * 2); for (Map.Entry\u0026lt;String, DataSourceProperty\u0026gt; item : dataSourcePropertiesMap.entrySet()) { DataSourceProperty dataSourceProperty = item.getValue(); String poolName = dataSourceProperty.getPoolName(); if (poolName == null || \u0026#34;\u0026#34;.equals(poolName)) { poolName = item.getKey(); } dataSourceProperty.setPoolName(poolName); // 这里的defaultDataSourceCreator.createDataSource()方法使用到适配器模式。因为每种配置数据源创建的DataSource实现类都不一定相同的，所以需要根据配置的数据源类型进行具体的DataSource创建。 dataSourceMap.put(poolName, defaultDataSourceCreator.createDataSource(dataSourceProperty)); } return dataSourceMap; } } 2.2.1.1 构建DataSource 来看一下defaultDataSourceCreator.createDataSource()的逻辑\npublic DataSource createDataSource(DataSourceProperty dataSourceProperty) { DataSourceCreator dataSourceCreator = null; //this.creators是所有适配的DataSourceCreator实现类 for (DataSourceCreator creator : this.creators) { //根据配置匹配对应的dataSourceCreator if (creator.support(dataSourceProperty)) { //如果匹配，则使用对应的dataSourceCreator dataSourceCreator = creator; break; } } if (dataSourceCreator == null) { throw new IllegalStateException(\u0026#34;creator must not be null,please check the DataSourceCreator\u0026#34;); } String publicKey = dataSourceProperty.getPublicKey(); if (StringUtils.isEmpty(publicKey)) { dataSourceProperty.setPublicKey(properties.getPublicKey()); } Boolean lazy = dataSourceProperty.getLazy(); if (lazy == null) { dataSourceProperty.setLazy(properties.getLazy()); } //然后再调用createDataSource方法进行创建对应DataSource DataSource dataSource = dataSourceCreator.createDataSource(dataSourceProperty); // (如果有则运行, 可以自定义)运行 建表语句和数据脚本 this.runScrip(dataSource, dataSourceProperty); return wrapDataSource(dataSource, dataSourceProperty); } 创建DataSource的有很多实现类, 每种数据源都有自己的实现\n对应的全部实现类是放在creator包下：\n创建dataSource就很简单了, 就是把账号/密码/url等参数设置进去\n2.2.2 对数据源分组 再回到之前的，当拿到DataSource的Map集合之后，再做什么呢？\n接着调addDataSource()方法，这个方法是根据下划线\u0026quot;_\u0026ldquo;对数据源进行分组，最后放到groupDataSources成员变量里面。 com.baomidou.dynamic.datasource.DynamicRoutingDataSource#addDataSource\nprivate static final String UNDERLINE = \u0026#34;_\u0026#34;; // 轮询策略 private Class\u0026lt;? extends DynamicDataSourceStrategy\u0026gt; strategy = LoadBalanceDynamicDataSourceStrategy.class; /** * 新数据源添加到分组 * * @param ds 新数据源的名字 * @param dataSource 新数据源 */ private void addGroupDataSource(String ds, DataSource dataSource) { if (ds.contains(UNDERLINE)) { String group = ds.split(UNDERLINE)[0]; GroupDataSource groupDataSource = groupDataSources.get(group); if (groupDataSource == null) { try { //顺便设置负载均衡策略，strategy默认是LoadBalanceDynamicDataSourceStrategy groupDataSource = new GroupDataSource(group, strategy.getDeclaredConstructor().newInstance()); groupDataSources.put(group, groupDataSource); } catch (Exception e) { throw new RuntimeException(\u0026#34;dynamic-datasource - add the datasource named \u0026#34; + ds + \u0026#34; error\u0026#34;, e); } } groupDataSource.addDatasource(ds, dataSource); } } 至此, 项目启动时, 多数据做的事就完了, 那我们执行sql时, 它是怎么切换的呢?\n2.3 切换数据源 在前面的DynamicRoutingDataSource我们知道, 这是mybatisPlus写的获取数据源的类 , 它最终是通过com.baomidou.dynamic.datasource.DynamicRoutingDataSource#determineDataSource获得数据源的, 在 2.2.1 也有描述.\n@Override public DataSource determineDataSource() { // 从threadLocal中获取当前数据源key (就是@DS注解里的字符串) String dsKey = DynamicDataSourceContextHolder.peek(); // 根据key获取数据源 return getDataSource(dsKey); } 那么上面的DynamicDataSourceContextHolder这个类是干嘛的呢？注解@DS的值又是怎么传进来的呢？\n我们先看看这个类\n2.3.1 DynamicDataSourceContextHolder /** * 核心基于ThreadLocal的切换数据源工具类 */ public final class DynamicDataSourceContextHolder { /** * 为什么要用链表存储(准确的是栈) * \u0026lt;pre\u0026gt; * 为了支持嵌套切换，如ABC三个service都是不同的数据源 * 其中A的某个业务要调B的方法，B的方法需要调用C的方法。一级一级调用切换，形成了链。 * 传统的只设置当前线程的方式不能满足此业务需求，必须使用栈，后进先出。 * \u0026lt;/pre\u0026gt; */ private static final ThreadLocal\u0026lt;Deque\u0026lt;String\u0026gt;\u0026gt; LOOKUP_KEY_HOLDER = new NamedThreadLocal\u0026lt;Deque\u0026lt;String\u0026gt;\u0026gt;(\u0026#34;dynamic-datasource\u0026#34;) { @Override protected Deque\u0026lt;String\u0026gt; initialValue() { return new ArrayDeque\u0026lt;\u0026gt;(); } }; /** * 获得当前线程数据源 * * @return 数据源名称 */ public static String peek() { return LOOKUP_KEY_HOLDER.get().peek(); } /** * 设置当前线程数据源 * \u0026lt;p\u0026gt; * 如非必要不要手动调用，调用后确保最终清除 * \u0026lt;/p\u0026gt; * * @param ds 数据源名称 */ public static String push(String ds) { String dataSourceStr = StringUtils.isEmpty(ds) ? \u0026#34;\u0026#34; : ds; LOOKUP_KEY_HOLDER.get().push(dataSourceStr); return dataSourceStr; } // .... } 这里用的是NamedThreadLocal, 比ThreadLocal多了一个名字字段而已, 大家知道ThreadLocal 的特性: 线程隔离/内存泄露风险, 所以 当开多线程会让多数据源切换失败(看你@DS注解打在哪和具体使用); 网上说用到 DynamicDataSourceContextHolder类时, 都是push()和clean()/poll()一起使用.其实就是避免内存泄露\n既然有peek(), 那肯定有push(), 我们来反推一下, 看一下哪些地方在调用push(), 可以发现有两个地方, 分别是\nDynamicDataSourceAnnotationInterceptor 和 MasterSlaveAutoRoutingPlugin,\nMasterSlaveAutoRoutingPlugin是做主从数据库切换的, 我们自然看 DynamicDataSourceAnnotationInterceptor\n2.3.2 DynamicDataSourceAnnotationInterceptor public class DynamicDataSourceAnnotationInterceptor implements MethodInterceptor { /** * The identification of SPEL. */ private static final String DYNAMIC_PREFIX = \u0026#34;#\u0026#34;; private final DataSourceClassResolver dataSourceClassResolver; private final DsProcessor dsProcessor; public DynamicDataSourceAnnotationInterceptor(Boolean allowedPublicOnly, DsProcessor dsProcessor) { dataSourceClassResolver = new DataSourceClassResolver(allowedPublicOnly); this.dsProcessor = dsProcessor; } @Override public Object invoke(MethodInvocation invocation) throws Throwable { //找到@DS注解的属性值，也就是数据源名称 String dsKey = determineDatasourceKey(invocation); //把数据源名称push到当前线程的栈 DynamicDataSourceContextHolder.push(dsKey); try { //执行当前方法 return invocation.proceed(); } finally { //从栈里释放数据源 DynamicDataSourceContextHolder.poll(); } } private String determineDatasourceKey(MethodInvocation invocation) { // 从方法上和类上 获取key String key = dataSourceClassResolver.findDSKey(invocation.getMethod(), invocation.getThis()); // 如果数据源是以 # 开头, 则进入ds处理器, 这里用的是责任链模式 return (!key.isEmpty() \u0026amp;\u0026amp; key.startsWith(DYNAMIC_PREFIX)) ? dsProcessor.determineDatasource(invocation, key) : key; } } 它是invoke方法, 一看就是反射标准写法, 看一下它的父类, MethodInterceptor, 它可以对AOP的切面进行增强,\n简单来说, 如果被切到后, 就会调用这个invoke方法\nSpring动态代理之MethodInterceptor拦截器详解 - 简书 (jianshu.com) 我们继续看切面是如何使用的, 而ds处理器等会讲\n2.3.4 DynamicDataSourceAnnotationAdvisor 直接看 DynamicDataSourceAnnotationInterceptor 在哪里用到了, 会发现回到了DynamicDataSourceAutoConfiguration类\n@Role(value = BeanDefinition.ROLE_INFRASTRUCTURE) @Bean public Advisor dynamicDatasourceAnnotationAdvisor(DsProcessor dsProcessor) { // 创建拦截器 DynamicDataSourceAnnotationInterceptor interceptor = new DynamicDataSourceAnnotationInterceptor(properties.isAllowedPublicOnly(), dsProcessor); // 把拦截器放到Advisor上 DynamicDataSourceAnnotationAdvisor advisor = new DynamicDataSourceAnnotationAdvisor(interceptor); advisor.setOrder(properties.getOrder()); return advisor; } Advisor是Spring AOP的顶层抽象，用来管理Advice和Pointcut\n@Transactional 也是用这种方式做的\n从 AbstractPointcutAdvisor 开始： Spring AOP 之 Advisor、PointcutAdvisor 介绍 DynamicDataSourceAnnotationAdvisor是用于AOP切面编程的，针对注解@DS的切面进行处理：\npublic class DynamicDataSourceAnnotationAdvisor extends AbstractPointcutAdvisor implements BeanFactoryAware { private final Advice advice; private final Pointcut pointcut; // 初始化时调用了构造方法 public DynamicDataSourceAnnotationAdvisor(@NonNull DynamicDataSourceAnnotationInterceptor dynamicDataSourceAnnotationInterceptor) { this.advice = dynamicDataSourceAnnotationInterceptor; this.pointcut = buildPointcut(); } @Override public Pointcut getPointcut() { return this.pointcut; } @Override public Advice getAdvice() { return this.advice; } private Pointcut buildPointcut() { Pointcut cpc = new AnnotationMatchingPointcut(DS.class, true); Pointcut mpc = new AnnotationMethodPoint(DS.class); //方法优于类 return new ComposablePointcut(cpc).union(mpc); } // ..... } 所以总的来说, 当执行的方法或者类被 @DS切到时, 就会执行增强方法, 把数据源key放到threadLocal中, 获取数据源时从threadLocal中拿到key进而拿到数据源\n至此, 多数据源的初始化和使用时切换就结束了\n2.4 数据源的处理器 在com.baomidou.dynamic.datasource.aop.DynamicDataSourceAnnotationInterceptor#determineDatasourceKey处, 有说过ds处理器, 它是干什么的呢?\n我们常用的指定数据源的方式时是 @DS(\u0026quot;master\u0026quot;) @DS(\u0026quot;salve_1\u0026quot;), 这些都是固定字符串, 如果我们需要根据租户来切换数据源呢?或者引入灰度系统后按Header的属性来切换呢? 因此plus的提供了各种数据源的处理器\nDsHeaderProcessor : 从header中获取 DsSessionProcessor: 从session中获取 DsSpelExpressionProcessor: 解析Spel语法获取 常见的@vaule()注解就支持spel语法, 从配置文件中取值或者写默认值都是spel语法\n玩转Spring中强大的spel表达式！ - 知乎 (zhihu.com) 继续 com.baomidou.dynamic.datasource.aop.DynamicDataSourceAnnotationInterceptor#determineDatasourceKey, 看一下它的代码\nprivate String determineDatasourceKey(MethodInvocation invocation) { String key = dataSourceClassResolver.findDSKey(invocation.getMethod(), invocation.getThis()); // dsProcessor.determineDatasource() 从处理器获取数据源 return (!key.isEmpty() \u0026amp;\u0026amp; key.startsWith(DYNAMIC_PREFIX)) ? dsProcessor.determineDatasource(invocation, key) : key; } dsProcessor 反向定位这个变量, 会追踪到com.baomidou.dynamic.datasource.spring.boot.autoconfigure.DynamicDataSourceAutoConfiguration#dsProcessor\n@Bean @ConditionalOnMissingBean public DsProcessor dsProcessor(BeanFactory beanFactory) { // 先header处理器 DsHeaderProcessor headerProcessor = new DsHeaderProcessor(); DsSessionProcessor sessionProcessor = new DsSessionProcessor(); DsSpelExpressionProcessor spelExpressionProcessor = new DsSpelExpressionProcessor(); spelExpressionProcessor.setBeanResolver(new BeanFactoryResolver(beanFactory)); // 再session处理器 headerProcessor.setNextProcessor(sessionProcessor); // 最后spel处理器 sessionProcessor.setNextProcessor(spelExpressionProcessor); return headerProcessor; } 回到使用的地方, 进入方法\ncom.baomidou.dynamic.datasource.processor.DsProcessor\n/** * 决定数据源 * \u0026lt;pre\u0026gt; * 调用底层doDetermineDatasource， * 如果返回的是null则继续执行下一个，否则直接返回 * \u0026lt;/pre\u0026gt; * * @param invocation 方法执行信息 * @param key DS注解里的内容 * @return 数据源名称 */ public String determineDatasource(MethodInvocation invocation, String key) { // key的格式是否匹配 if (matches(key)) { // 解析数据源key String datasource = doDetermineDatasource(invocation, key); if (datasource == null \u0026amp;\u0026amp; nextProcessor != null) { // 下一个处理器 return nextProcessor.determineDatasource(invocation, key); } return datasource; } if (nextProcessor != null) { return nextProcessor.determineDatasource(invocation, key); } return null; } 我们以DsHeaderProcessor 为例:\npublic class DsHeaderProcessor extends DsProcessor { /** * header prefix */ private static final String HEADER_PREFIX = \u0026#34;#header\u0026#34;; @Override public boolean matches(String key) { // 是否 以#header 开头 return key.startsWith(HEADER_PREFIX); } @Override public String doDetermineDatasource(MethodInvocation invocation, String key) { HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); // 从请求头中直接获取(很粗暴,竟然连分隔符都不用), 比如@DS(\u0026#34;#headeruserId\u0026#34;), 这样就会从header中取出userId字段 return request.getHeader(key.substring(8));// 刚好把 #header 这几个字符截断 } } session处理器是以#session 开头\n按顺序处理都不是后, 就会按DsSpelExpressionProcessor 处理\n举一些例子:\n@DS(\u0026quot;#headerUserId\u0026quot;) : 取header中的UserId字段作为数据源key\n@DS(\u0026quot;#sessionTentendId\u0026quot;) : 取session中的TentendId字段作为数据源key\n@DS(\u0026quot;#person\u0026quot;): 把方法上的变量person的值作为数据源key (SPEL语法)\n学习SPEL语法就会发现很多奇妙用法, 比如可与获取方法的参数;方法的返参类型; 类的名称等\n3. Q\u0026amp;A 3.1 @Transactional 和 @DS 注解一起使用时, @DS失效的问题 这是因为 @Transactional 开启事务后, 就不会重新拿数据源,因为@DS也得通过切面去获取数据源, 这样就导致了@DS失效.\n要解决的话, 就在要切换数据源的方法上也打上@DS, 或者多个数据源有修改操作可以都打上事务注解并改变传播机制,(但这其实是分布式事务的范畴, 这样操作不能保证事务了, plus也提供了@DSTransactional 来支持, 不够需要借助seata)\n@Transactional跟@DS动态数据源注解冲突_林蜗牛snail的博客-CSDN博客 关于 @DSTransactional 以后再补充一下\n3.2 低于V3.3.3版本无法切换数据源的问题 这是因为在V3.3.3版本之前, DruidDataSourceAutoConfigure会比DynamicDataSourceAutoConfiguration先注册, 导致数据库有读取druid的配置, 所以会出现 ,没有druid配置启动报错; 数据源无法切换等等原因, 官方在V3.3.3这个版本修复了, 但是这个版本有个其他的重点bug,官方不让下载, 所以github上找不到这个版本的描述, 只在官网有描述\n分两步解决的,\n去除 druid pom坐标 在 DynamicDataSourceAutoConfiguration 上指定优于 DruidDataSourceAutoConfigure加载 解决办法就是排除DruidDataSourceAutoConfigure 类;\n使用dynamic-datasource-spring-boot-starter做多数据源及源码分析_0x2015的博客-CSDN博客 Mybatis-Plus多数据源解析 - 掘金 (juejin.cn) 多数据源官网-版本记录 Mybatis plus的多数据源@DS切换为什么不起作用了，谁的锅，@Transactional ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatisplus/%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E8%A7%A3%E6%9E%90.html","summary":"[toc] 1. 使用 1.1. 引包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dynamic-datasource-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 用最新版的就好 Maven Repository: dynamic-datasource-spring-boot-starter 不过做开发的经验来看, 应该选 倒数第二个大版本里最新的版本, 这样既能用更多的功能,又能保证没有大b","title":"多数据源解析"},{"content":"简单的说: 一个虚拟机上,有很多用户在使用.也就是说,一个机器能满足好几个用户.感觉还不错哈.但这是有问题的,\n1.其物理资源还是共享的,一旦某个用户的操作占用大量资源,那其他用户就会受到很大影响\n2.这里的用户也可以是应用(像k8s),每个应用在一个虚拟机上执行,总有先后,如何保证重要应用优先执行\n3.资源共享,也存在安全问题\n在HBase1.1.0发布之前，HBase同一集群上的用户、表都是平等的，没有优劣之分。这种’大同’社会看起来完美，实际上有很多问题。最棘手的主要有这么两个，其一是某些业务较其他业务重要，需要在资源有限的情况下优先保证核心重要业务的正常运行，其二是有些业务在某些场景下会时常’抽风’，QPS常常居高不下，严重消耗系统资源，导致其他业务无法正常运转。\n这实际上是典型的多租户问题，\n来自: https://blog.csdn.net/lw_ghy/article/details/60779482 k8s\n我认为这也可能成为容器崩溃的核心原因——多租户机制。\nLinux容器在设计之初并没有考虑到安全的隔离沙箱（例如Solaris Zones或者FreeBSD Jails）。相反，容器采用的是共享内核模式，其利用内核功能提供基础性的进程隔离功能。正如Jessie Frazelle在文章中指出，“容器并不真实存在。”\n更麻烦的是，大多数Kubernetes组件无法感知到租户。虽然我们可以使用命名空间以及Pod安全策略，但API本身确实不具备租户感知能力。此外， kubelet或者kube-proxy等内部组件也存在同样的问题。这意味着Kubernetes能够提供的只是一种“软租户”模式。\n抽象泄漏。建立在容器之上的平台会继承容器技术的诸多软租户因素。正如建立在硬多租户虚拟机基础之上的平台（包括VMware、Amazon Web Srevices以及OpenStack等），也都继承了这种硬租户机制一样。\nKubernetes集群本身成了“硬租户”面临的第一道坎，也因此导致大部分用户只能使用“多集群”这一新兴模式，而非“单一共享”集群。相信很多朋友都发现了，谷歌GKE Service的用户往往需要面向多个团队部署数十个Kubernetes集群，有时候每一位开发者都拥有自己的集群。这类作法最终导致了严重的Kube泛滥问题。\n“这类作法最终导致了严重的Kube泛滥问题。”\n原文：https://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/85333601\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E5%A4%9A%E7%A7%9F%E6%88%B7%E6%A8%A1%E5%BC%8F.html","summary":"简单的说: 一个虚拟机上,有很多用户在使用.也就是说,一个机器能满足好几个用户.感觉还不错哈.但这是有问题的, 1.其物理资源还是共享的,一旦某","title":"多租户模式"},{"content":"[toc]\n一. 为啥要泛型? 简单的说,我 new一个list,我想放字符串,数字等多种数据类型,怎么办? 那就整个泛型!(理解意思就行)\nhttps://www.cnblogs.com/lwbqqyumidi/p/3837629.html 二. 什么是泛型？ 泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。\n来自 https://www.cnblogs.com/lwbqqyumidi/p/3837629.html 三. 泛型的使用 泛型类 //此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型(见名知意), T : Type K V : Key Value E : Element ? : 泛型通配符(后面会讲到) 来自 https://segmentfault.com/a/1190000014824002 //在实例化泛型类时，必须指定T的具体类型 public class Generic\u0026lt;T\u0026gt;{ //key这个成员变量的类型为T,T的类型由外部指定 private T key; public Generic(T key) { //泛型构造方法形参key的类型也为T，T的类型由外部指定 this.key = key; } public T getKey(){ //泛型方法getKey的返回值类型为T，T的类型由外部指定 return key; } } 注意：\n泛型的类型参数只能是类类型，不能是简单类型。\n不能对确切的泛型类型使用instanceof操作。如下面的操作是非法的，编译时会出错。 if(ex_num instanceof Generic\u0026lt;Number\u0026gt;){ } 来自https://blog.csdn.net/s10461/article/details/53941091#commentBox 泛型接口 //定义一个泛型接口 public interface Generator\u0026lt;T\u0026gt; { public T next(); } 来自 https://blog.csdn.net/s10461/article/details/53941091#commentBox 泛型方法 /** * 泛型方法的基本介绍 * @param tClass 传入的泛型实参 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间\u0026lt;T\u0026gt;非常重要，可以理解为声明此方法为泛型方法。 * 2）只有声明了\u0026lt;T\u0026gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 * 3）\u0026lt;T\u0026gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 * 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型。 */ public \u0026lt;T\u0026gt; T genericMethod(Class\u0026lt;T\u0026gt; tClass)throws Exception{ T instance = tClass.newInstance(); return instance; } public class GenericTest { //这个类是个泛型类，在上面已经介绍过 public class Generic\u0026lt;T\u0026gt;{ private T key; public Generic(T key) { this.key = key; } //我想说的其实是这个，虽然在方法中使用了泛型，但是这并不是一个泛型方法。 //这只是类中一个普通的成员方法，只不过他的返回值是在声明泛型类已经声明过的泛型。 //所以在这个方法中才可以继续使用 T 这个泛型。 public T getKey(){ return key; } /** * 这个方法显然是有问题的，在编译器会给我们提示这样的错误信息\u0026#34;cannot reslove symbol E\u0026#34; * 因为在类的声明中并未声明泛型E，所以在使用E做形参和返回值类型时，编译器会无法识别。 public E setKey(E key){ this.key = keu } */ } /** * 这才是一个真正的泛型方法。 * 首先在public与返回值之间的\u0026lt;T\u0026gt;必不可少，这表明这是一个泛型方法，并且声明了一个泛型T * 这个T可以出现在这个泛型方法的任意位置. * 泛型的数量也可以为任意多个 * 如：public \u0026lt;T,K\u0026gt; K showKeyName(Generic\u0026lt;T\u0026gt; container){ * ... * } */ public \u0026lt;T\u0026gt; T showKeyName(Generic\u0026lt;T\u0026gt; container){ System.out.println(\u0026#34;container key :\u0026#34; + container.getKey()); //当然这个例子举的不太合适，只是为了说明泛型方法的特性。 T test = container.getKey(); return test; } //这也不是一个泛型方法，这就是一个普通的方法，只是使用了Generic\u0026lt;Number\u0026gt;这个泛型类做形参而已。 public void showKeyValue1(Generic\u0026lt;Number\u0026gt; obj){ Log.d(\u0026#34;泛型测试\u0026#34;,\u0026#34;key value is \u0026#34; + obj.getKey()); } //这也不是一个泛型方法，这也是一个普通的方法，只不过使用了泛型通配符? //同时这也印证了泛型通配符章节所描述的，?是一种类型实参，可以看做为Number等所有类的父类 public void showKeyValue2(Generic\u0026lt;?\u0026gt; obj){ Log.d(\u0026#34;泛型测试\u0026#34;,\u0026#34;key value is \u0026#34; + obj.getKey()); } /** * 这个方法是有问题的，编译器会为我们提示错误信息：\u0026#34;UnKnown class \u0026#39;E\u0026#39; \u0026#34; * 虽然我们声明了\u0026lt;T\u0026gt;,也表明了这是一个可以处理泛型的类型的泛型方法。 * 但是只声明了泛型类型T，并未声明泛型类型E，因此编译器并不知道该如何处理E这个类型。 public \u0026lt;T\u0026gt; T showKeyName(Generic\u0026lt;E\u0026gt; container){ ... } */ /** * 这个方法也是有问题的，编译器会为我们提示错误信息：\u0026#34;UnKnown class \u0026#39;T\u0026#39; \u0026#34; * 对于编译器来说T这个类型并未项目中声明过，因此编译也不知道该如何编译这个类。 * 所以这也不是一个正确的泛型方法声明。 public void showkey(T genericObj){ } */ public static void main(String[] args) { } } 类中的泛型方法 当然这并不是泛型方法的全部，泛型方法可以出现杂任何地方和任何场景中使用。但是有一种情况是非常特殊的，当泛型方法出现在泛型类中时，我们再通过一个例子看一下\npublic class GenericFruit { class Fruit{ @Override public String toString() { return \u0026#34;fruit\u0026#34;; } } class Apple extends Fruit{ @Override public String toString() { return \u0026#34;apple\u0026#34;; } } class Person{ @Override public String toString() { return \u0026#34;Person\u0026#34;; } } class GenerateTest\u0026lt;T\u0026gt;{ public void show_1(T t){ System.out.println(t.toString()); } //在泛型类中声明了一个泛型方法，使用泛型E，这种泛型E可以为任意类型。可以类型与T相同，也可以不同。 //由于泛型方法在声明的时候会声明泛型\u0026lt;E\u0026gt;，因此即使在泛型类中并未声明泛型，编译器也能够正确识别泛型方法中识别的泛型。 public \u0026lt;E\u0026gt; void show_3(E t){ System.out.println(t.toString()); } //在泛型类中声明了一个泛型方法，使用泛型T，注意这个T是一种全新的类型，可以与泛型类中声明的T不是同一种类型。 public \u0026lt;T\u0026gt; void show_2(T t){ System.out.println(t.toString()); } } public static void main(String[] args) { Apple apple = new Apple(); Person person = new Person(); GenerateTest\u0026lt;Fruit\u0026gt; generateTest = new GenerateTest\u0026lt;Fruit\u0026gt;(); //apple是Fruit的子类，所以这里可以 generateTest.show_1(apple); //编译器会报错，因为泛型类型实参指定的是Fruit，而传入的实参类是Person //generateTest.show_1(person); //使用这两个方法都可以成功 generateTest.show_2(apple); generateTest.show_2(person); //使用这两个方法也都可以成功 generateTest.show_3(apple); generateTest.show_3(person); } } 来自 https://blog.csdn.net/s10461/article/details/53941091#commentBox 泛型方法与可变参数 public \u0026lt;T\u0026gt; void printMsg( T... args){ for(T t : args){ Log.d(\u0026#34;泛型测试\u0026#34;,\u0026#34;t is \u0026#34; + t); } } 注意: 泛型没有多态,没有数组\n不能创建一个确切的泛型类型的数组 也就是说下面的这个例子是不可以的：\n`List\u0026lt;String\u0026gt;[] ls = new ArrayList\u0026lt;String\u0026gt;[10];` 来自 \u0026lt;https://blog.csdn.net/s10461/article/details/53941091#commentBox\u0026gt; 静态方法与泛型 public class StaticGenerator\u0026lt;T\u0026gt; { .... .... /** * 如果在类中定义使用泛型的静态方法，需要添加额外的泛型声明（将这个方法定义成泛型方法） * 即使静态方法要使用泛型类中已经声明过的泛型也不可以。 * 如：public static void show(T t){..},此时编译器会提示错误信息： \u0026#34;StaticGenerator cannot be refrenced from static context\u0026#34; */ public static \u0026lt;T\u0026gt; void show(T t){ } } 来自 https://blog.csdn.net/s10461/article/details/53941091#commentBox 通配符 ?\n在某些源码中看到有 ? ,这是什么意思呢? public static \u0026lt;T\u0026gt; void Collections.copy(List\u0026lt;T\u0026gt; dest, List\u0026lt;? extends T\u0026gt; src) { ... } 类型通配符一般是使用？代替具体的类型实参，注意了，重要说三遍！ 此处’？’是类型实参，而不是类型形参 ! 此处’？’是类型实参，而不是类型形参 ！ 此处’？’是类型实参，而不是类型形参 ！ 再直白点的意思就是，此处的？和Number、String、Integer一样都是一种实际的类型，可以把？看成所有类型的父类。是一种真实的类型。 可以解决当具体类型不确定的时候，这个通配符就是 ? ；当操作类型时，不需要使用类型的具体功能时，只使用Object类中的功能。那么可以用 ? 通配符来表未知类型。\n类型通配符上限和类型通配符下限\n类型通配符上限通过形如Box\u0026lt;? extends Number\u0026gt;形式定义，相对应的，类型通配符下限为Box\u0026lt;? super Number\u0026gt;形式，其含义与类型通配符上限正好相反 上限: 只允许继承了Number的类型 下限: 只允许实现了Number的类型 (有利于控制类型嘛,不让你瞎几把 放参数)\n来自 https://www.cnblogs.com/lwbqqyumidi/p/3837629.html 解释一下项目中产生的疑惑:\npublic static boolean isBlank(Collection\u0026lt;?\u0026gt; c) { // 1 // c.add(\u0026#34;3\u0026#34;) 会报错 return null == c || c.isEmpty(); } public static \u0026lt;T\u0026gt; boolean isBlank(Collection\u0026lt;T\u0026gt; c) { // 2 //c.add(\u0026#34;3\u0026#34;) return null == c || c.isEmpty(); } 可以看到两种方法都做到了泛型判空,(随你输入啥集合,都能判断).但是这两种写法有什么区别呢?\n用通配符的情况下,泛型参数是不能被修改的 (我都不知道你是啥类型,一个范围都没有,我改个鸡毛)\nhttps://blog.csdn.net/sinat_32023305/article/details/83215751 基本上所有能用类型通配符（?）解决的问题都能用泛型方法解决，并且泛型方法可以解决的更好.\n通配符使用场景:\n一般只读就用?，要修改就用泛型方法， 在多个参数、返回值之间存在类型依赖关系就应该使用泛型方法，否则就应该是通配符? 具体讲就是，如果一个方法的返回值、某些参数的类型依赖另一个参数的类型就应该使用泛型方法，因为被依赖的类型如果是不确定的?，那么其他元素就无法依赖它），\n例如：\u0026lt;T\u0026gt; void func(List\u0026lt;? extends T\u0026gt; list, T t);\n即第一个参数依赖第二个参数的类型（第一个参数list的类型参数必须是第二个参数的类型或者其子类）；\n可以看到，Java支持泛型方法和?混用；\n这个方法也可以写成：\u0026lt;T, E extends T\u0026gt; void func(List\u0026lt;E\u0026gt; list, T t);\n// 明显意义是一样的，只不过这个list可以修改，而上一个list无法修改 总之就是一旦返回值、形参之间存在类型依赖关系就只能使用泛型方法； 否则就应该使用 ? ；\n一个最典型的应用就是容器赋值方法（Java的API）：public static \u0026lt;T\u0026gt; void Collections.copy(List\u0026lt;T\u0026gt; dest, List\u0026lt;? extends T\u0026gt; src) { ... }\n！！从src拷贝到dest，那么dest最好是src的类型或者其父类，因为这样才能类型兼容，并且src只是读取，没必要做修改，因此使用?还可以强制避免你对src做不必要的修改，增加的安全性\n来自 https://blog.csdn.net/sinat_32023305/article/details/83215751 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%B3%9B%E5%9E%8B.html","summary":"[toc] 一. 为啥要泛型? 简单的说,我 new一个list,我想放字符串,数字等多种数据类型,怎么办? 那就整个泛型!(理解意思就行) https://www.cnblogs.com/lwbqqyumidi/p/3837629.html 二. 什么是泛型？","title":"泛型"},{"content":"[toc]\n1.前言 目前关系数据库 有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式 (4NF）和第五范式 （5NF，又称完美范式）。满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般来说，数据库只需满足第三范式(3NF）就行了。\n2.范式 2.1 第一范式（1NF） 所谓第一范式（1NF）是指在关系模型 中，对于添加的一个规范要求，所有的域都应该是原子性的，即数据库表的每一列都是不可分割的原子数据项，而不能是集合，数组，记录等非原子数据项。即实体中的某个属性有多个值时，必须拆分为不同的属性。在符合第一范式（1NF）表中的每个域值只能是实体的一个属性或一个属性的一部分。简而言之，第一范式就是无重复的域。\n说明：在任何一个关系数据库 中，第一范式（1NF）是对关系模式 的设计基本要求，一般设计中都必须满足第一范式（1NF）。不过有些关系模型中突破了1NF的限制，这种称为非1NF的关系模型。换句话说，是否必须满足1NF的最低要求，主要依赖于所使用的关系模型 。\n简单理解: 表中一个字段应该是原子性的,不可分割的,\n例如: 一个表,有学生名称和分数, 显然学生会有多门学科分数,此时应该把分数拆开成多门分数\n2.2 第二范式（2NF） 在1NF的基础上，非码属性必须完全依赖于候选码（在1NF基础上消除非主属性对主码的部分函数依赖）\n第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或记录必须可以被唯一地区分。选取一个能区分每个实体的属性或属性组，作为实体的唯一标识。\n第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识。简而言之，第二范式就是在第一范式的基础上属性完全依赖于主键。\n例如: 项目表中有经纬度和地址字段, 其实确认了地址字段,经纬度也就确认了,此时经纬度并不完成依赖于项目编码\n2.3 第三范式（3NF） 在2NF基础上，任何非主属性 不依赖于其它非主属性（在2NF基础上消除传递依赖）\n第三范式（3NF）是第二范式（2NF）的一个子集，即满足第三范式（3NF）必须满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个关系中不包含已在其它关系已包含的非主关键字信息。\n例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性，也就是在满足2NF的基础上，任何非主属性不得传递依赖于主属性。\n2.4 巴斯-科德范式（BCNF） Boyce-Codd Normal Form（巴斯-科德范式）\n在3NF基础上，任何非主属性不能对主键子集依赖（在3NF基础上消除对主码子集的依赖）\n巴斯-科德范式（BCNF）是第三范式（3NF）的一个子集，即满足巴斯-科德范式（BCNF）必须满足第三范式（3NF）。通常情况下，巴斯-科德范式被认为没有新的设计规范加入，只是对第二范式与第三范式中设计规范要求更强，因而被认为是修正第三范式，也就是说，它事实上是对第三范式的修正，使数据库冗余度更小。这也是BCNF不被称为第四范式的原因。某些书上，根据范式要求的递增性将其称之为第四范式是不规范，也是更让人不容易理解的地方。而真正的第四范式，则是在设计规范中添加了对多值及依赖的要求。\n2.5 第四范式 设关系R（X，Y，Z），其中X，Y，Z是成对的、不相交属性的集合。若存在非平凡多值依赖，则意味着对R中的每个属性\n存在有函数依赖\n（X必包含键）。那么\n。\n换句话说，当关系R的属性集合X是非平凡多值依赖的域，它就包含关系R的键。则\n。这个定义和BCNF定义唯一的不同点是后者研究非平凡多值依赖 的域。由于函数依赖是多值依赖的特定情况，因此，这直观地说明了4NF比BCNF更强的原因。\n显然，若关系属于4NF，则它必属于BCNF；而属于BCNF的关系不一定属于4NF\n2.6 第五范式 如果关系模式 R中的每一个连接依赖均由R的候选码 所隐含，则称此关系模式符合第五范式。\n链接 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%8C%83%E5%BC%8F.html","summary":"[toc] 1.前言 目前关系数据库 有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式 (4NF","title":"范式"},{"content":"[toc]\n前言 一图解读分布式事务 名词解释 事务：事务是由一组操作构成的可靠的独立的工作单元，事务具备ACID的特性，即原子性、一致性、隔离性和持久性。 本地事务：当事务由资源管理器本地管理时被称作本地事务。本地事务的优点就是支持严格的ACID特性，高效，可靠，状态可以只在资源管理器中维护，而且应用编程模型简单。但是本地事务不具备分布式事务的处理能力，隔离的最小单位受限于资源管理器。 全局事务：当事务由全局事务管理器进行全局管理时成为全局事务，事务管理器负责管理全局的事务状态和参与的资源，协同资源的一致提交回滚。 TX协议：应用或者应用服务器与事务管理器的接口。 XA协议：全局事务管理器与资源管理器的接口。XA是由X/Open组织提出的分布式事务规范。该规范主要定义了全局事务管理器和局部资源管理器之间的接口。主流的数据库产品都实现了XA接口。XA接口是一个双向的系统接口，在事务管理器以及多个资源管理器之间作为通信桥梁。之所以需要XA是因为在分布式系统中从理论上讲两台机器是无法达到一致性状态的，因此引入一个单点进行协调。由全局事务管理器管理和协调的事务可以跨越多个资源和进程。全局事务管理器一般使用XA二阶段协议与数据库进行交互。 AP：应用程序，可以理解为使用DTP（Data Tools Platform）的程序。 RM：资源管理器，这里可以是一个DBMS或者消息服务器管理系统，应用程序通过资源管理器对资源进行控制，资源必须实现XA定义的接口。资源管理器负责控制和管理实际的资源。 TM：事务管理器，负责协调和管理事务，提供给AP编程接口以及管理资源管理器。事务管理器控制着全局事务，管理事务的生命周期，并且协调资源。 两阶段提交协议：XA用于在全局事务中协调多个资源的机制。TM和RM之间采取两阶段提交的方案来解决一致性问题。两节点提交需要一个协调者（TM）来掌控所有参与者（RM）节点的操作结果并且指引这些节点是否需要最终提交。两阶段提交的局限在于协议成本，准备阶段的持久成本，全局事务状态的持久成本，潜在故障点多带来的脆弱性，准备后，提交前的故障引发一系列隔离与恢复难题。 BASE理论：BA指的是基本业务可用性，支持分区失败，S表示柔性状态，也就是允许短时间内不同步，E表示最终一致性，数据最终是一致的，但是实时是不一致的。原子性和持久性必须从根本上保障，为了可用性、性能和服务降级的需要，只有降低一致性和隔离性的要求。 CAP定理：对于共享数据系统，最多只能同时拥有CAP其中的两个，任意两个都有其适应的场景，真是的业务系统中通常是ACID与CAP的混合体。分布式系统中最重要的是满足业务需求，而不是追求高度抽象，绝对的系统特性。C表示一致性，也就是所有用户看到的数据是一样的。A表示可用性，是指总能找到一个可用的数据副本。P表示分区容错性，能够容忍网络中断等故障。 CAP定理 CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性：\n一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用，操作依然可以完成 具体地讲在分布式系统中，一个Web应用至多只能同时支持上面的两个属性。\nCAP 权衡 通过 CAP 理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？\n对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。\n对于涉及到钱财这样不能有一丝让步的场景，C 必须保证。网络发生故障宁可停止服务，这是保证 CA，舍弃 P。貌似这几年国内银行业发生了不下 10 起事故，但影响面不大，报道也不多，广大群众知道的少。还有一种是保证 CP，舍弃 A。例如网络故障是只读不写。\nBASE定理 CAP是分布式系统设计理论，BASE是CAP理论中AP方案的延伸，对于C我们采用的方式和策略就是保证最终一致性；\neBay的架构师Dan Pritchett源于对大规模分布式系统的实践总结，在ACM上发表文章提出BASE理论，BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（StrongConsistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。\nBASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE基于CAP定理演化而来，核心思想是即时无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。\n分布式事务分类 刚性事务 刚性事务：通常无业务改造，强一致性，原生支持回滚/隔离性，低并发，适合短事务。\n原则：刚性事务满足足CAP的CP理论\n刚性事务指的是，要使分布式事务，达到像本地式事务一样，具备数据强一致性，从CAP来看，就是说，要达到CP状态。\n刚性事务：XA 协议（2PC、JTA、JTS）、3PC，但由于同步阻塞，处理效率低，不适合大型网站分布式场景。\n柔性事务 柔性事务指的是，不要求强一致性，而是要求最终一致性，允许有中间状态，也就是Base理论，换句话说，就是AP状态。\n与刚性事务相比，柔性事务的特点为：有业务改造，最终一致性，实现补偿接口，实现资源锁定接口，高并发，适合长事务。\n柔性事务分为：\n补偿型 异步确保型 最大努力通知型。 柔型事务：TCC/FMT、Saga（状态机模式、Aop模式）、本地事务消息、消息事务（半消息）\n刚性事务：XA X/Open DTP(Distributed Transaction Process) 是一个分布式事务模型。这个模型主要使用了两段提交(2PC - Two-Phase-Commit)来保证分布式事务的完整性。\n在X/Open **DTP(Distributed Transaction Process)**模型里面，有三个角色：\nAP: Application，应用程序。也就是业务层。哪些操作属于一个事务，就是AP定义的。\nTM: Transaction Manager，事务管理器。接收AP的事务请求，对全局事务进行管理，管理事务分支状态，协调RM的处理，通知RM哪些操作属于哪些全局事务以及事务分支等等。这个也是整个事务调度模型的核心部分。\nRM：Resource Manager，资源管理器。一般是数据库，也可以是其他的资源管理器，如消息队列(如JMS数据源)，文件系统等。\nXA之所以需要引入事务管理器是因为，在分布式系统中，从理论上讲（参考Fischer等的论文），两台机器理论上无法达到一致的状态，需要引入一个单点进行协调。事务管理器控制着全局事务，管理事务生命周期，并协调资源。资源管理器负责控制和管理实际资源（如数据库或JMS队列）\nXA规范 XA规范(XA Specification) 是X/OPEN 提出的分布式事务处理规范。XA则规范了TM与RM之间的通信接口，在TM与多个RM之间形成一个双向通信桥梁，从而在多个数据库资源下保证ACID四个特性。目前知名的数据库，如Oracle, DB2,mysql等，都是实现了XA接口的，都可以作为RM。\nXA协议的实现 2PC/3PC协议 两阶段提交（2PC）协议是XA规范定义的 数据一致性协议。\n三阶段提交（3PC）协议对 2PC协议的一种扩展。\nSeata Seata 是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式\nSeata AT 模式 Seata AT 模式是增强型2pc模式。\nAT 模式： 两阶段提交协议的演变，没有一直锁表\n一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源 二阶段：提交异步化，非常快速地完成。或回滚通过一阶段的回滚日志进行反向补偿 XA的主要限制 必须要拿到所有数据源，而且数据源还要支持XA协议。目前MySQL中只有InnoDB存储引擎支持XA协议。 性能比较差，要把所有涉及到的数据都要锁定，是强一致性的，会产生长事务。 2PC（标准XA模型） 2PC即Two-Phase Commit，二阶段提交。\n2PC节点角色 二阶段提交协议将节点分为：\n协调者角色(事务管理器Coordinator) 参与者角色（资源管理器Participant） 2PC角色中，事务管理器的角色，负责协调多个数据库（资源管理器）的事务，\n协调者角色(事务管理器Coordinator)，负责向参与者发送指令，收集参与者反馈，做出提交或者回滚决策\n参与者角色（资源管理器Participant），接收协调者的指令执行事务操作，向协调者反馈操作结果，并继续执行协调者发送的最终指令\n详解：两个阶段 广泛应用在数据库领域，为了使得基于分布式架构的所有节点可以在进行事务处理时能够保持原子性和一致性。\n顾名思义，2PC分为两个阶段处理，\n阶段一：提交事务请求、 阶段二：执行事务提交，或者执行中断事务; 如果阶段一超时或者出现异常，2PC的阶段二为：执行中断事务\n说明：绝大部分关系型数据库，都是基于2PC完成分布式的事务处理。\n阶段一：提交事务请求 事务询问。协调者向所有参与者发送事务内容，询问是否可以执行提交操作，并开始等待各参与者进行响应； 执行事务。各参与者节点，执行事务操作，并将Undo和Redo操作计入本机事务日志； 各参与者向协调者反馈事务问询的响应。成功执行返回Yes，否则返回No。 阶段二：提交或者中断事务 这一阶段包含两种情形：\n执行事务提交， 执行中断事务 协调者在阶段二决定是否最终执行事务提交操作。\n所有参与者reply Yes，那么执行事务提交。 当存在某一参与者向协调者发送No响应，或者等待超时。协调者只要无法收到所有参与者的Yes响应，就会中断事务。 执行事务提交 所有参与者reply Yes，那么执行事务提交。\n发送提交请求。协调者向所有参与者发送Commit请求； 事务提交。参与者收到Commit请求后，会正式执行事务提交操作，并在完成提交操作之后，释放在整个事务执行期间占用的资源； 反馈事务提交结果。参与者在完成事务提交后，写协调者发送Ack消息确认； 完成事务。协调者在收到所有参与者的Ack后，完成事务。 执行事务中断 事情总会出现意外，当存在某一参与者向协调者发送No响应，或者等待超时。协调者只要无法收到所有参与者的Yes响应，就会中断事务。\n发送回滚请求。协调者向所有参与者发送Rollback请求； 回滚。参与者收到请求后，利用本机Undo信息，执行Rollback操作。并在回滚结束后释放该事务所占用的系统资源； 反馈回滚结果。参与者在完成回滚操作后，向协调者发送Ack消息； 中断事务。协调者收到所有参与者的回滚Ack消息后，完成事务中断。 2PC二阶段提交的特点 2PC 方案比较适合单体应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低\n2PC具有明显的优缺点： 优点:\n主要体现在实现原理简单； 缺点比较多：\n同步阻塞导致性能问题\n执行过程中，所有参与节点都是事务阻塞型的。所有participant 都处于阻塞状态，各个participant 都在等待其他参与者响应，无法进行其他操作。\n所有分支的资源锁定时间，由最长的分支事务决定。\n另外当参与者锁定公共资源时，处于事务之外的其他第三方访问者，也不得不处于阻塞状态。\n单点故障导致高可用（HA）问题：\n协调者是个单点，一旦出现问题，各个participant 将无法释放事务资源，也无法完成事务操作；\n并且，由于协调者的重要性，一旦协调者发生故障，参与者会一直阻塞下去。\n尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。\n丢失消息导致的数据不一致问题：\n如果协调者向所有参与者发送Commit请求后，发生局部网络异常,或者协调者在尚未给全部的participant发送完Commit请求即出现崩溃，最终导致只有部分participant收到、执行请求。部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。\n过于保守：\n二阶段提交协议没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。\n3PC 针对2PC的缺点，研究者提出了3PC，即Three-Phase Commit。\n作为2PC的改进版，3PC将原有的两阶段过程，重新划分为CanCommit、PreCommit和do Commit三个阶段。\n3PC 协议将 2PC 协议的准备阶段一分为二，从而形成了三个阶段：\n详解：三个阶段 所谓的三个阶段分别是：询问，然后再锁资源，最后真正提交。\n第一阶段：CanCommit 第二阶段：PreCommit 第三阶段：Do Commit 阶段一：CanCommit 事务询问。协调者向所有参与者发送包含事务内容的canCommit的请求，询问是否可以执行事务提交，并等待应答； 各参与者反馈事务询问。正常情况下，如果参与者认为可以顺利执行事务，则返回Yes，否则返回No。 阶段二：PreCommit 在本阶段，协调者会根据上一阶段的反馈情况来决定是否可以执行事务的PreCommit操作。有以下两种可能：\n执行事务预提交 中断事务 执行事务预提交 发送预提交请求。协调者向所有节点发出PreCommit请求，并进入prepared阶段； 事务预提交。参与者收到PreCommit请求后，会开始事务操作，并将Undo和Redo日志写入本机事务日志； 各参与者成功执行事务操作，同时将反馈以Ack响应形式发送给协调者，同事等待最终的Commit或Abort指令。 中断事务 如果任意一个参与者向协调者发送No响应，或者等待超时，协调者在没有得到所有参与者响应时，即可以中断事务。\n中断事务的操作为：\n发送中断请求。 协调者向所有参与者发送Abort请求； 中断事务。无论是participant 收到协调者的Abort请求，还是participant 等待协调者请求过程中出现超时，参与者都会中断事务； coordinator发送Abort的两个场景\n场景1： 任意一个参与者向协调者发送No响应\n场景2： 协调者在没有得到所有参与者响应时\n阶段三：doCommit 在这个阶段，会真正的进行事务提交，同样存在两种可能。\n执行提交 回滚事务 执行提交 coordinator发送提交请求。假如coordinator协调者收到了所有参与者的Ack响应，那么将从预提交转换到提交状态，并向所有参与者，发送doCommit请求； 事务提交。参与者收到doCommit请求后，会正式执行事务提交操作，并在完成提交操作后释放占用资源； 反馈事务提交结果。参与者将在完成事务提交后，向协调者发送Ack消息； 完成事务。协调者接收到所有参与者的Ack消息后，完成事务。 回滚事务 在该阶段，假设正常状态的协调者接收到任一个参与者发送的No响应，或在超时时间内，仍旧没收到反馈消息，就会回滚事务：\n发送中断请求。协调者向所有的参与者发送rollback请求； 事务回滚。参与者收到rollback请求后，会利用阶段二中的Undo消息执行事务回滚，并在完成回滚后释放占用资源； 反馈事务回滚结果。参与者在完成回滚后向协调者发送Ack消息； 回滚事务。协调者接收到所有参与者反馈的Ack消息后，完成事务回滚。 2PC和3PC的区别： 三阶段提交协议在协调者和参与者中都引入 超时机制，并且把两阶段提交协议的第一个阶段拆分成了两步：询问，然后再锁资源，最后真正提交。\n三阶段提交的三个阶段分别为：can_commit，pre_commit，do_commit。\n在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rollback请求时，会在等待超时之后，继续进行事务的提交。\n其实这个应该是基于概率来决定的，\n当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，什么场景会产生PreCommit请求呢？\n协调者产生PreCommit请求的前提条件比较严格：是在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。\n所以，一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了\n一句话概括就是：\n当进入第三阶段时，由于网络超时/网络分区等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。\n3PC主要解决的单点故障问题： 相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，\n因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。\n由于在docommit阶段，participant参与者如果超时，能自己决定提交本地事务，所以，3pc没有2pc那么保守或者说悲观，或者说3pc更加的乐观。\n3PC主要没有解决的数据一致性问题： 但是这种机制，还是有数据一致性问题，或者说，没有彻底解决数据一致性问题。\n‘因为，由于网络原因，协调者发送的rollback命令没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。\n这样就和其他接到rollback命令并执行回滚的参与者之间存在数据不一致的情况。\n\u0026ldquo;3PC相对于2PC而言到底优化了什么地方呢?\u0026rdquo; 相比较2PC而言，3PC对于协调者（Coordinator）和参与者（Participant）都设置了超时时间，而2PC只有协调者才拥有超时机制。\n这解决了一个什么问题呢？\n这个优化点，主要是避免了Participant 参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，\n因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。\n而这种机制也侧面降低了整个事务的阻塞时间和范围。\n另外，通过CanCommit、PreCommit、DoCommit三个阶段的设计，相较于2PC而言，多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的。\n以上就是3PC相对于2PC的一个提高（相对缓解了2PC中的前两个问题），但是3PC依然没有完全解决数据不一致的问题。\n假如在 DoCommit 过程，参与者A无法接收协调者的通信，那么参与者A会自动提交，但是提交失败了，其他参与者成功了，此时数据就会不一致。\n柔性事务的分类 在电商领域等互联网场景下，刚性事务在数据库性能和处理能力上都暴露出了瓶颈。\n柔性事务有两个特性：基本可用和柔性状态。\n基本可用是指分布式系统出现故障的时候允许损失一部分的可用性。 柔性状态是指允许系统存在中间状态，这个中间状态不会影响系统整体的可用性，比如数据库读写分离的主从同步延迟等。柔性事务的一致性指的是最终一致性。 柔性事务主要分为补偿型和通知型，\n补偿型事务又分：TCC、Saga；\n通知型事务分：MQ事务消息、最大努力通知型。\n补偿型事务都是同步的，通知型事务都是异步的。\n通知型事务 通知型事务的主流实现是通过MQ（消息队列）来通知其他事务参与者自己事务的执行状态，引入MQ组件，有效的将事务参与者进行解耦，各参与者都可以异步执行，所以通知型事务又被称为异步事务。\n通知型事务主要适用于那些需要异步更新数据，并且对数据的实时性要求较低的场景，主要包含:\n异步确保型事务和最大努力通知事务两种。\n异步确保型事务：主要适用于内部系统的数据最终一致性保障，因为内部相对比较可控，如订单和购物车、收货与清算、支付与结算等等场景； 最大努力通知：主要用于外部系统，因为外部的网络环境更加复杂和不可信，所以只能尽最大努力去通知实现数据最终一致性，比如充值平台与运营商、支付对接等等跨网络系统级别对接； 异步确保型事务 指将一系列同步的事务操作修改为基于消息队列异步执行的操作，来避免分布式事务中同步阻塞带来的数据操作性能的下降。\nMQ事务消息方案 基于MQ的事务消息方案主要依靠MQ的半消息机制来实现投递消息和参与者自身本地事务的一致性保障。半消息机制实现原理其实借鉴的2PC的思路，是二阶段提交的广义拓展。\n半消息：在原有队列消息执行后的逻辑，如果后面的本地逻辑出错，则不发送该消息，如果通过则告知MQ发送；\n有一些第三方的MQ是支持事务消息的，这些消息队列，支持半消息机制，比如RocketMQ，ActiveMQ。但是有一些常用的MQ也不支持事务消息，比如 RabbitMQ 和 Kafka 都不支持。\n事务发起方首先发送半消息到MQ； MQ通知发送方消息发送成功； 在发送半消息成功后执行本地事务； 根据本地事务执行结果返回commit或者是rollback； 如果消息是rollback, MQ将丢弃该消息不投递；如果是commit，MQ将会消息发送给消息订阅方； 订阅方根据消息执行本地事务； 订阅方执行本地事务成功后再从MQ中将该消息标记为已消费； 如果执行本地事务过程中，执行端挂掉，或者超时，MQ服务器端将不停的询问producer来获取事务状态； Consumer端的消费成功机制有MQ保证； 本地消息表方案 有时候我们目前的MQ组件并不支持事务消息，或者我们想尽量少的侵入业务方。这时我们需要另外一种方案“基于DB本地消息表“。\n本地消息表最初由eBay 提出来解决分布式事务的问题。是目前业界使用的比较多的方案之一，它的核心思想就是将分布式事务拆分成本地事务进行处理。\n发送消息方：\n需要有一个消息表，记录着消息状态相关信息。 业务数据和消息表在同一个数据库，要保证它俩在同一个本地事务。直接利用本地事务，将业务数据和事务消息直接写入数据库。 在本地事务中处理完业务数据和写消息表操作后，通过写消息到 MQ 消息队列。使用专门的投递工作线程进行事务消息投递到MQ，根据投递ACK去删除事务消息表记录 消息会发到消息消费方，如果发送失败，即进行重试。 消息消费方：\n处理消息队列中的消息，完成自己的业务逻辑。 如果本地事务处理成功，则表明已经处理成功了。 如果本地事务处理失败，那么就会重试执行。 如果是业务层面的失败，给消息生产方发送一个业务补偿消息，通知进行回滚等操作。 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。\n本地消息表优缺点：\n优点：\n本地消息表建设成本比较低，实现了可靠消息的传递确保了分布式事务的最终一致性。 无需提供回查方法，进一步减少的业务的侵入。 在某些场景下，还可以进一步利用注解等形式进行解耦，有可能实现无业务代码侵入式的实现。 缺点：\n本地消息表与业务耦合在一起，难于做成通用性，不可独立伸缩。 本地消息表是基于数据库来做的，而数据库是要读写磁盘IO的，因此在高并发下是有性能瓶颈的 MQ事务消息 VS 本地消息表 二者的共性： 1、 事务消息都依赖MQ进行事务通知，所以都是异步的。 2、 事务消息在投递方都是存在重复投递的可能，需要有配套的机制去降低重复投递率，实现更友好的消息投递去重。 3、 事务消息的消费方，因为投递重复的无法避免，因此需要进行消费去重设计或者服务幂等设计。\n二者的区别：\nMQ事务消息：\n需要MQ支持半消息机制或者类似特性，在重复投递上具有比较好的去重处理； 具有比较大的业务侵入性，需要业务方进行改造，提供对应的本地操作成功的回查功能； DB本地消息表：\n使用了数据库来存储事务消息，降低了对MQ的要求，但是增加了存储成本； 事务消息使用了异步投递，增大了消息重复投递的可能性； 最大努力通知 最大努力通知方案的目标，就是发起通知方通过一定的机制，最大努力将业务处理结果通知到接收方。\n最大努力通知型的最终一致性：\n本质是通过引入定期校验机制实现最终一致性，对业务的侵入性较低，适合于对最终一致性敏感度比较低、业务链路较短的场景。\n最大努力通知事务主要用于外部系统，因为外部的网络环境更加复杂和不可信，所以只能尽最大努力去通知实现数据最终一致性，比如充值平台与运营商、支付对接、商户通知等等跨平台、跨企业的系统间业务交互场景；\n而异步确保型事务主要适用于内部系统的数据最终一致性保障，因为内部相对比较可控，比如订单和购物车、收货与清算、支付与结算等等场景。\n普通消息是无法解决本地事务执行和消息发送的一致性问题的。因为消息发送是一个网络通信的过程，发送消息的过程就有可能出现发送失败、或者超时的情况。超时有可能发送成功了，有可能发送失败了，消息的发送方是无法确定的，所以此时消息发送方无论是提交事务还是回滚事务，都有可能不一致性出现。\n所以，通知型事务的难度在于： 投递消息和参与者本地事务的一致性保障。\n因为核心要点一致，都是为了保证消息的一致性投递，所以，最大努力通知事务在投递流程上跟异步确保型是一样的，因此也有两个分支：\n基于MQ自身的事务消息方案 基于DB的本地事务消息表方案 最大努力通知事务在于第三方系统的对接，所以最大努力通知事务有几个特性：\n业务主动方在完成业务处理后，向业务被动方(第三方系统)发送通知消息，允许存在消息丢失。 业务主动方提供递增多挡位时间间隔(5min、10min、30min、1h、24h)，用于失败重试调用业务被动方的接口；在通知N次之后就不再通知，报警+记日志+人工介入。 业务被动方提供幂等的服务接口，防止通知重复消费。 业务主动方需要有定期校验机制，对业务数据进行兜底；防止业务被动方无法履行责任时进行业务回滚，确保数据最终一致性。 MQ事务消息方案 要实现最大努力通知，可以采用 MQ 的 ACK 机制。\n最大努力通知事务在投递之前，跟异步确保型流程都差不多，关键在于投递后的处理。\n因为异步确保型在于内部的事务处理，所以MQ和系统是直连并且无需严格的权限、安全等方面的思路设计。\n业务活动的主动方，在完成业务处理之后，向业务活动的被动方发送消息，允许消息丢失。 主动方可以设置时间阶梯型通知规则，在通知失败后按规则重复通知，直到通知N次后不再通知。 主动方提供校对查询接口给被动方按需校对查询，用于恢复丢失的业务消息。 业务活动的被动方如果正常接收了数据，就正常返回响应，并结束事务。 如果被动方没有正常接收，根据定时策略，向业务活动主动方查询，恢复丢失的业务消息。 本地消息表方案 要实现最大努力通知，可以采用 定期检查本地消息表的机制 。\n发送消息方：\n需要有一个消息表，记录着消息状态相关信息。 业务数据和消息表在同一个数据库，要保证它俩在同一个本地事务。直接利用本地事务，将业务数据和事务消息直接写入数据库。 在本地事务中处理完业务数据和写消息表操作后，通过写消息到 MQ 消息队列。使用专门的投递工作线程进行事务消息投递到MQ，根据投递ACK去删除事务消息表记录 消息会发到消息消费方，如果发送失败，即进行重试。 生产方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 最大努力通知事务 VS 异步确保型事务 最大努力通知事务在我认知中，其实是基于异步确保型事务发展而来适用于外部对接的一种业务实现。他们主要有的是业务差别，如下： • 从参与者来说：最大努力通知事务适用于跨平台、跨企业的系统间业务交互；异步确保型事务更适用于同网络体系的内部服务交付。 • 从消息层面说：最大努力通知事务需要主动推送并提供多档次时间的重试机制来保证数据的通知；而异步确保型事务只需要消息消费者主动去消费。 • 从数据层面说：最大努力通知事务还需额外的定期校验机制对数据进行兜底，保证数据的最终一致性；而异步确保型事务只需保证消息的可靠投递即可，自身无需对数据进行兜底处理。\n通知型事务的问题 通知型事务，是无法解决本地事务执行和消息发送的一致性问题的。\n因为消息发送是一个网络通信的过程，发送消息的过程就有可能出现发送失败、或者超时的情况。超时有可能发送成功了，有可能发送失败了，消息的发送方是无法确定的，所以此时消息发送方无论是提交事务还是回滚事务，都有可能不一致性出现。\n消息发送一致性 消息中间件在分布式系统中的核心作用就是异步通讯、应用解耦和并发缓冲（也叫作流量削峰）。在分布式环境下，需要通过网络进行通讯，就引入了数据传输的不确定性，也就是CAP理论中的分区容错性。\n消息发送一致性是指产生消息的业务动作与消息发送动作一致，也就是说如果业务操作成功，那么由这个业务操作所产生的消息一定要发送出去，否则就丢失。所以，需要借助半消息、本地消息表，保障一致性。\n消息重复发送问题和业务接口幂等性设计 对于未确认的消息，采用按规则重新投递的方式进行处理。\n对于以上流程，消息重复发送会导致业务处理接口出现重复调用的问题。消息消费过程中消息重复发送的主要原因就是消费者成功接收处理完消息后，消息中间件没有及时更新投递状态导致的。如果允许消息重复发送，那么消费方应该实现业务接口的幂等性设计。\n补偿型 什么是补偿模式？\n补偿模式使用一个额外的协调服务来协调各个需要保证一致性的业务服务，协调服务按顺序调用各个业务微服务，如果某个业务服务调用异常（包括业务异常和技术异常）就取消之前所有已经调用成功的业务服务。\n补偿模式大致有TCC，和Saga两种细分的方案\nTCC 事务模型 什么是TCC 事务模型 TCC（Try-Confirm-Cancel）的概念来源于 Pat Helland 发表的一篇名为“Life beyond Distributed Transactions:an Apostate’s Opinion”的论文。\nTCC 分布式事务模型包括三部分：\n1.主业务服务：主业务服务为整个业务活动的发起方，服务的编排者，负责发起并完成整个业务活动。\n2.从业务服务：从业务服务是整个业务活动的参与方，负责提供 TCC 业务操作，实现初步操作(Try)、确认操作(Confirm)、取消操作(Cancel)三个接口，供主业务服务调用。\n3.业务活动管理器：业务活动管理器管理控制整个业务活动，包括记录维护 TCC 全局事务的事务状态和每个从业务服务的子事务状态，并在业务活动提交时调用所有从业务服务的 Confirm 操作，在业务活动取消时调用所有从业务服务的 Cancel 操作。\nTCC 提出了一种新的事务模型，基于业务层面的事务定义，锁粒度完全由业务自己控制，目的是解决复杂业务中，跨表跨库等大颗粒度资源锁定的问题。\nTCC 把事务运行过程分成 Try、Confirm / Cancel 两个阶段，每个阶段的逻辑由业务代码控制，避免了长事务，可以获取更高的性能。\nTCC的工作流程 TCC(Try-Confirm-Cancel)分布式事务模型相对于 XA 等传统模型，其特征在于它不依赖资源管理器(RM)对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务。\nTCC 模型认为对于业务系统中一个特定的业务逻辑，其对外提供服务时，必须接受一些不确定性，即对业务逻辑初步操作的调用仅是一个临时性操作，调用它的主业务服务保留了后续的取消权。如果主业务服务认为全局事务应该回滚，它会要求取消之前的临时性操作，这就对应从业务服务的取消操作。而当主业务服务认为全局事务应该提交时，它会放弃之前临时性操作的取消权，这对应从业务服务的确认操作。每一个初步操作，最终都会被确认或取消。\nTCC 分布式事务模型包括三部分：\nTry 阶段： 调用 Try 接口，尝试执行业务，完成所有业务检查，预留业务资源。\nConfirm 或 Cancel 阶段： 两者是互斥的，只能进入其中一个，并且都满足幂等性，允许失败重试。\nConfirm 操作： 对业务系统做确认提交，确认执行业务操作，不做其他业务检查，只使用 Try 阶段预留的业务资源。\nCancel 操作： 在业务执行错误，需要回滚的状态下执行业务取消，释放预留资源。\nTry 阶段失败可以 Cancel，如果 Confirm 和 Cancel 阶段失败了怎么办？\nTCC 中会添加事务日志，如果 Confirm 或者 Cancel 阶段出错，则会进行重试，所以这两个阶段需要支持幂等；如果重试失败，则需要人工介入进行恢复和处理等。\nTCC事务模型的要求： 可查询操作：服务操作具有全局唯一的标识，操作唯一的确定的时间。 幂等操作：重复调用多次产生的业务结果与调用一次产生的结果相同。一是通过业务操作实现幂等性，二是系统缓存所有请求与处理的结果，最后是检测到重复请求之后，自动返回之前的处理结果。 TCC操作：Try阶段，尝试执行业务，完成所有业务的检查，实现一致性；预留必须的业务资源，实现准隔离性。Confirm阶段：真正的去执行业务，不做任何检查，仅适用Try阶段预留的业务资源，Confirm操作还要满足幂等性。Cancel阶段：取消执行业务，释放Try阶段预留的业务资源，Cancel操作要满足幂等性。TCC与2PC(两阶段提交)协议的区别：TCC位于业务服务层而不是资源层，TCC没有单独准备阶段，Try操作兼备资源操作与准备的能力，TCC中Try操作可以灵活的选择业务资源，锁定粒度。TCC的开发成本比2PC高。实际上TCC也属于两阶段操作，但是TCC不等同于2PC操作。 可补偿操作：Do阶段：真正的执行业务处理，业务处理结果外部可见。Compensate阶段：抵消或者部分撤销正向业务操作的业务结果，补偿操作满足幂等性。约束：补偿操作在业务上可行，由于业务执行结果未隔离或者补偿不完整带来的风险与成本可控。实际上，TCC的Confirm和Cancel操作可以看做是补偿操作。 TCC与2PC对比 TCC其实本质和2PC是差不多的：\nT就是Try，两个C分别是Confirm和Cancel。 Try就是尝试，请求链路中每个参与者依次执行Try逻辑，如果都成功，就再执行Confirm逻辑，如果有失败，就执行Cancel逻辑。 TCC与XA两阶段提交有着异曲同工之妙，下图列出了二者之间的对比\n在阶段1： 在XA中，各个RM准备提交各自的事务分支，事实上就是准备提交资源的更新操作(insert、delete、update等)； 而在TCC中，是主业务活动请求(try)各个从业务服务预留资源。 在阶段2： XA根据第一阶段每个RM是否都prepare成功，判断是要提交还是回滚。如果都prepare成功，那么就commit每个事务分支，反之则rollback每个事务分支。 TCC中，如果在第一阶段所有业务资源都预留成功，那么confirm各个从业务服务，否则取消(cancel)所有从业务服务的资源预留请求。 TCC和2PC不同的是：\nXA是资源层面的分布式事务，强一致性，在两阶段提交的整个过程中，一直会持有资源的锁。基于数据库锁实现，需要数据库支持XA协议，由于在执行事务的全程都需要对相关数据加锁，一般高并发性能会比较差 TCC是业务层面的分布式事务，最终一致性，不会一直持有资源的锁，性能较好。但是对微服务的侵入性强，微服务的每个事务都必须实现try、confirm、cancel等3个方法，开发成本高，今后维护改造的成本也高为了达到事务的一致性要求，try、confirm、cancel接口必须实现幂等性操作由于事务管理器要记录事务日志，必定会损耗一定的性能，并使得整个TCC事务时间拉长 TCC它会弱化每个步骤中对于资源的锁定，以达到一个能承受高并发的目的（基于最终一致性）。\nSAGA长事务模型 SAGA可以看做一个异步的、利用队列实现的补偿事务。\nSaga相关概念 Saga模型是把一个分布式事务拆分为多个本地事务，每个本地事务都有相应的执行模块和补偿模块（对应TCC中的Confirm和Cancel），当Saga事务中任意一个本地事务出错时，可以通过调用相关的补偿方法恢复之前的事务，达到事务最终一致性。\n这样的SAGA事务模型，是牺牲了一定的隔离性和一致性的，但是提高了long-running事务的可用性。\nSaga 模型由三部分组成：\nLLT（Long Live Transaction）：由一个个本地事务组成的事务链**。** 本地事务：事务链由一个个子事务（本地事务）组成，LLT = T1+T2+T3+\u0026hellip;+Ti。 补偿：每个本地事务 Ti 有对应的补偿 Ci。 Saga的执行顺序有两种：\nT1, T2, T3, \u0026hellip;, Tn T1, T2, \u0026hellip;, Tj, Cj,\u0026hellip;, C2, C1，其中0 \u0026lt; j \u0026lt; n Saga 两种恢复策略： 向后恢复（Backward Recovery）：撤销掉之前所有成功子事务。如果任意本地子事务失败，则补偿已完成的事务。如异常情况的执行顺序T1,T2,T3,..Ti,Ci,\u0026hellip;C3,C2,C1。 向前恢复（Forward Recovery）：即重试失败的事务，适用于必须要成功的场景，该情况下不需要Ci。执行顺序：T1,T2,\u0026hellip;,Tj（失败）,Tj（重试）,\u0026hellip;,Ti。 显然，向前恢复没有必要提供补偿事务，如果你的业务中，子事务（最终）总会成功，或补偿事务难以定义或不可能，向前恢复更符合你的需求。\nSaga的使用条件 Saga看起来很有希望满足我们的需求。所有长活事务都可以这样做吗？这里有一些限制：\nSaga只允许两个层次的嵌套，顶级的Saga和简单子事务 在外层，全原子性不能得到满足。也就是说，sagas可能会看到其他sagas的部分结果 每个子事务应该是独立的原子行为 在我们的业务场景下，各个业务环境（如：航班预订、租车、酒店预订和付款）是自然独立的行为，而且每个事务都可以用对应服务的数据库保证原子操作。 补偿也有需考虑的事项：\n补偿事务从语义角度撤消了事务Ti的行为，但未必能将数据库返回到执行Ti时的状态。（例如，如果事务触发导弹发射， 则可能无法撤消此操作） 但这对我们的业务来说不是问题。其实难以撤消的行为也有可能被补偿。例如，发送电邮的事务可以通过发送解释问题的另一封电邮来补偿。\nSAGA模型的解决方案 SAGA模型的核心思想是，通过某种方案，将分布式事务转化为本地事务，从而降低问题的复杂性。\n比如以DB和MQ的场景为例，业务逻辑如下：\n向DB中插入一条数据。 向MQ中发送一条消息。 由于上述逻辑中，对应了两种存储端，即DB和MQ，所以，简单的通过本地事务是无法解决的。那么，依照SAGA模型，可以有两种解决方案。\n方案一：半消息模式。 RocketMQ新版本中，就支持了这种模式。\n方案二：本地消息表 在DB中，新增一个消息表，用于存放消息。如下：\n在DB业务表中插入数据。 在DB消息表中插入数据。 异步将消息表中的消息发送到MQ，收到ack后，删除消息表中的消息。 Saga和TCC对比 Saga和TCC都是补偿型事务，他们的区别为：\n劣势：\nSaga无法保证隔离性； 优势：\n一阶段提交本地事务，无锁，高性能； 事件驱动模式，参与者可异步执行，高吞吐； Saga 对业务侵入较小，只需要提供一个逆向操作的Cancel即可；而TCC需要对业务进行全局性的流程改造； TCC最少通信次数为2n，而Saga为n（n=sub-transaction的数量）。 有些第三方服务没有Try接口，TCC模式实现起来就比较tricky了，而Saga则很简单。 实际案例 背景 下单过程中, 需要创建订单-扣库存-使用优惠券-扣除G豆等等操作, 整个过程需要使用分布式事务(因为订单和这些业务不在同一个数据库中), 此处采用 TCC + 本地消息表 来保证事务, 再加上MQ来异步解耦\n虽然该例子不是完美解决方案, 但其是一个可落地方案\n方案大致描述 先行预扣 库存/优惠券等 订单前置事务因素, 如果失败就写入本地消息表, 并发送mq(异步回退锁定资源) - Try\n库存采用 锁定库存的方式 (锁定库存与可用库存)\n优惠券采用了 分布式锁的方式(也可直接落库)\n库存等资源不够或者网络异常等问题, 只要是预扣不正常都会写入消息表并发送mq\n如果预扣全部成功, 则订单正常流转完成. - Confirm\n该案例中, 订单状态的变更亦是异步处理, 因为前置因素处理完, 则订单是允许生成的\n所以消费时要注意区分 回退和订单状态处理\n确认收款后, 预扣资源才会真正扣除, 此处也采用MQ解耦 - 此代码与下单类似,不重复描述\n消费MQ数据,异步回退锁定的资源, 并修改本地消息表的数据 - cancal\n考虑代码的幂等性和并发性\n定时扫描本地消息表, 做数据兜底, 处理丢mq消息或者写数据库失败的场景 - 此代码暂未完全实现\nCREATE TABLE `order_create_message_0` ( `id` bigint NOT NULL, `user_id` bigint NOT NULL COMMENT \u0026#39;用户id\u0026#39;, `order_id` bigint NOT NULL COMMENT \u0026#39;订单号\u0026#39;, `type` tinyint DEFAULT \u0026#39;1\u0026#39; COMMENT \u0026#39;类型 1-创建订单 2-确认收款\u0026#39;, `message_status` tinyint NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;消息状态（0：未处理，1：处理完成）\u0026#39;, `step_status` char(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;步骤状态（A：未处理，S：处理成功，F：处理失败）（第1位:订单；第2位:库存；第3位:优惠券；第4位：g豆；第5位：明珠卡）\u0026#39;, `num` int NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;消息处理次数\u0026#39;, `created_time` datetime DEFAULT NULL COMMENT \u0026#39;创建时间\u0026#39;, `created_by` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT \u0026#39;创建人\u0026#39;, `modify_time` datetime DEFAULT NULL COMMENT \u0026#39;更新时间\u0026#39;, `modify_by` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT \u0026#39;修改人\u0026#39;, `enable_flag` tinyint DEFAULT NULL COMMENT \u0026#39;是否可用 10-可用 20-删除\u0026#39;, `version` bigint DEFAULT NULL COMMENT \u0026#39;版本号--乐观锁预留字段\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT=\u0026#39;创建订单消息表\u0026#39; 核心代码 订单处理类\npublic ServerResult createOrder(CreateOrderDTO createOrderDTO) { ....// 业务代码处理 // 查询库存/优惠券等资源, 查看是否充足等校验 // 先在redis把优惠券锁定住 log.debug(\u0026#34;用户{}创建订单，冻结优惠券\u0026#34;, createOrderDTO.getUserId()); Map\u0026lt;String, String\u0026gt; lockedCoupon = lockUseCoupon(createOrderDTO.getUserId(), couponInfoMap, couponIdList); ....// 业务代码处理 // 先将订单预写到订单数据库,理论上这段保存逻辑也可以往后写的，但是为了方便追踪问题，先把订单先给预写进订单库 transactionHelper.run(() -\u0026gt; { orderService.saveBatch(orderDoList); orderItemService.saveBatch(allOrderItemDOList); }); // 处理扣减订单的资源：库存、优惠券、g豆、明珠卡等 processOrder(createOrderDTO, couponInfoMap, parentOrderId, orderDoList, allOrderItemDOList); } /** * 锁定用户下单使用的优惠券 * * @param userId 用户id * @param couponInfoMap 优惠券信息Map * @param couponIdList 优惠券id列表 * @return {@link Map}\u0026lt;{@link String}, {@link String}\u0026gt; */ private Map\u0026lt;String, String\u0026gt; lockUseCoupon(String userId, Map\u0026lt;String, CouponInfoDTO\u0026gt; couponInfoMap, List\u0026lt;String\u0026gt; couponIdList) { if (CollUtil.isNotEmpty(couponIdList)) { return new HashMap\u0026lt;\u0026gt;(); } Map\u0026lt;String, String\u0026gt; lockedCouponIdMap = new HashMap\u0026lt;\u0026gt;(); for (String couponId : couponIdList) { CouponInfoDTO couponInfoDTO = couponInfoMap.get(couponId); couponInfoDTO.setCouponCode(couponInfoDTO.getCouponCodeList().get(0)); String lockUserCouponKey = getLockUserCouponKey(userId, couponInfoDTO.getCouponCode()); String value = IdUtil.fastUUID(); boolean lock = redisUtil.tryLock(lockUserCouponKey, value, refreshScopeManage.getLockUserCouponTime()); if (!lock) { // 锁失败了,尽快的解锁已经锁定的券 orderPoolTaskExecutor.execute(() -\u0026gt; unLockInLockedCoupon(lockedCouponIdMap)); // 正在使用 throw new BusinessException(ServerResultCode.ORDER_COUPON_IN_USE); } lockedCouponIdMap.put(lockUserCouponKey, value); } return lockedCouponIdMap; } /** * 处理扣减订单的资源 * 1. 扣减库存 * 2. 扣减 用户优惠券 * 3. 扣减g豆、积分 * 4. 扣减明珠卡 * * @param createOrderDTO 创建订单dto * @param couponInfoMap 优惠券信息Map * @param parentOrderId 父订单id * @param orderDoList 订单做列表 * @param allOrderItemDOList 所有订单项list */ private void processOrder(CreateOrderVo createOrderDTO, Map\u0026lt;String, CouponInfoDTO\u0026gt; couponInfoMap, long parentOrderId, List\u0026lt;OrderDO\u0026gt; orderDoList, List\u0026lt;OrderItemDO\u0026gt; allOrderItemDOList) { // 第1位:订单；第2位:库存；第3位:优惠券；第4位：g豆；第5位：明珠卡 boolean occupyStock = false; boolean occupyCoupon = false; boolean occupyGBean = false; boolean occupyCard = false; try { // 预扣库存 List\u0026lt;SkuInventoryDTO\u0026gt; withholdingList = allOrderItemDOList.stream() .map(orderItemDO -\u0026gt; SkuInventoryDTO.builder().skuId(orderItemDO.getSkuId()).withholding(orderItemDO.getQuantity()).build()) .collect(Collectors.toList()); ServerResult withholdingServerResult = skuInventoryServiceFeign.withholding(SkuInventoryBatchDTO.builder().parentOrderId(parentOrderId).list(withholdingList).build()); occupyStock = true; if (withholdingServerResult.checkNotSuccess()) { log.error(\u0026#34;用户{}创建订单异常，父订单号{}，批量扣减库存失败\u0026#34;, createOrderDTO.getUserId(), parentOrderId); // 批量扣减失败，库存的回退由商品服务保证回退成功 throw new BusinessException(withholdingServerResult); } // 扣减优惠券 if (CollUtil.isNotEmpty(couponInfoMap)) { boolean batchUpdateCoupon = batchDeductionCoupon(createOrderDTO, couponInfoMap, parentOrderId); occupyCoupon = true; if (!batchUpdateCoupon) { log.error(\u0026#34;用户{}创建订单异常，父订单号{}，批量扣减优惠券失败\u0026#34;, createOrderDTO.getUserId(), parentOrderId); throw new BusinessException(ServerResultCode.ORDER_COUPON_UPDATE_STATE_FAILED); } } // 处理g豆 BigDecimal gBeanAmount = StringUtils.isBlank(createOrderDTO.getGBeansAmount()) ? BigDecimal.ZERO : new BigDecimal(createOrderDTO.getGBeansAmount()); if (gBeanAmount.compareTo(BigDecimal.ZERO) \u0026gt; 0) { // 使用了G豆 boolean useGbean = deduceUseGbean(createOrderDTO); occupyGBean = true; if (useGbean) { log.error(\u0026#34;用户{}创建订单异常，父订单号{}，扣减G豆失败\u0026#34;, createOrderDTO.getUserId(), parentOrderId); throw new BusinessException(ServerResultCode.ORDER_DEDUCE_GBEAN_ERROR); } } // 处理明珠卡 if (StringUtils.isNotBlank(createOrderDTO.getCardAmount()) \u0026amp;\u0026amp; StringUtils.isNotBlank(createOrderDTO.getCardId())) { // 使用了明珠卡 boolean useCard = deduceUseCard(createOrderDTO); occupyCard = true; if (useCard) { log.error(\u0026#34;用户{}创建订单异常，父订单号{}，扣减明珠卡失败\u0026#34;, createOrderDTO.getUserId(), parentOrderId); throw new BusinessException(ServerResultCode.ORDER_DEDUCE_CARD_ERROR); } } // 下单成功，对应的优惠资产、库存、g豆已经扣减 OrderCreateMessageDO orderCreateMessageDO = buildOrderCreateMessageDO(parentOrderId, Long.parseLong(createOrderDTO.getUserId())); transactionHelper.run(() -\u0026gt; { // 预创建（用户不可见） 转换为 待付款（用户可见） orderMapper.updateBatchByIdAndUserId(orderDoList, Long.parseLong(createOrderDTO.getUserId()), GetTableNameUtil.getOrderTableName(Long.parseLong(createOrderDTO.getUserId()))); orderItemMapper.updateBatchByIdAndUserId(allOrderItemDOList, Long.parseLong(createOrderDTO.getUserId()), GetTableNameUtil.getOrderItemTableName(Long.parseLong(createOrderDTO.getUserId()))); orderCreateMessageService.save(orderCreateMessageDO); }); // 同步到ES orderListenerService.onCreateOrderSuccess(orderDoList,allOrderItemDOList); // 发送mq消息 -\u0026gt; 处理订单状态(上面线程处理可能失败) rocketMQTemplate.syncSend(refreshScopeManage.getOrderGlobalAsyncTopicName(), JSON.toJSONString(orderCreateMessageDO)); } catch (Exception e) { log.error(StrFormatter.format(\u0026#34;用户{}创建订单异常,父订单号{},失败原因\u0026#34;, createOrderDTO.getUserId(), parentOrderId), e); rollBackGlobalTransaction(parentOrderId, Long.parseLong(createOrderDTO.getUserId()), occupyStock, occupyCoupon, occupyGBean, occupyCard); if (e instanceof BusinessException) { throw e; } throw new BusinessException(\u0026#34;创建订单异常\u0026#34;); } } /** * 全局事务回退 * * @param parentOrderId 父订单id * @param userId 用户id * @param occupyStock 占用库存 * @param occupyCoupon 占领优惠券 * @param occupyGbean 占领gbean * @param occupyCard 占领卡 */ public void rollBackGlobalTransaction(long parentOrderId, long userId, boolean occupyStock, boolean occupyCoupon, boolean occupyGbean, boolean occupyCard) { OrderCreateMessageDO orderCreateMessageDO = buildOrderCreateMessageDO(parentOrderId, userId); // 默认不需要回退 // 第1位:订单；第2位:库存；第3位:优惠券；第4位：g豆；第5位：明珠卡 StringBuilder sb = new StringBuilder(\u0026#34;SSSSS\u0026#34;); if (occupyStock) { // 占用了库存，需要回退库存 sb.replace(1, 2, \u0026#34;A\u0026#34;); } if (occupyCoupon) { // 占用了优惠券，需要回退优惠券 sb.replace(2, 3, \u0026#34;A\u0026#34;); } if (occupyGbean) { // 占用了G豆，需要回退G豆 sb.replace(3, 4, \u0026#34;A\u0026#34;); } if (occupyCard) { // 占用了明珠卡，需要回退明珠卡 sb.replace(4, 5, \u0026#34;A\u0026#34;); } orderCreateMessageDO.setStepStatus(sb.toString()); // 发送mq消息，异步回退 库存、 优惠券、g豆、明珠卡 try { rocketMQTemplate.syncSend(refreshScopeManage.getOrderGlobalAsyncTopicName(), JSON.toJSONString(orderCreateMessageDO)); } catch (Exception e) { // 尽可能的把消息投递出去 log.error(StrFormatter.format(\u0026#34;订单号{}，生产消息失败\u0026#34;, parentOrderId), e); } orderCreateMessageService.save(orderCreateMessageDO); } /** * 预扣库存，即占用库存 +n， 可用库存 -n * * @param query SkuInventoryBatchDTO * @return ServerResult */ @Override public ServerResult withholding(SkuInventoryBatchDTO query) { log.info(\u0026#34;订单：{} 开始预扣库存\u0026#34;, query.getParentOrderId()); LocalDateTime now = LocalDateTime.now(); // 构建扣减日志列表 List\u0026lt;SkuInventoryChangeRecordDO\u0026gt; logs = query.getList() .stream() .map(sku -\u0026gt; SkuInventoryChangeRecordDO.builder() .parentOrderId(query.getParentOrderId()) .skuId(sku.getSkuId()) .quantity(sku.getWithholding()) .status(CommodityConstant.WITHHOLDING_SKU_INVENTORY_SUCCESS) .enableFlag(CommodityConstant.ENABLE) .createTime(now) .modifyTime(now) .version(1L) .build()) .collect(Collectors.toList()); List\u0026lt;SkuInventoryDO\u0026gt; list = getInventoryBySkuId(query); Map\u0026lt;Long, Integer\u0026gt; withholdingMap = query.getList().stream() .collect(Collectors.toMap(SkuInventoryDTO::getSkuId, SkuInventoryDTO::getWithholding)); for (SkuInventoryDO inventoryDO : list) { int withholding = withholdingMap.get(inventoryDO.getSkuId()); if (inventoryDO.getAvailable() \u0026lt; withholding) { return ServerResult.fail(ServerResultCode.WITHHOLDING_SKU_INVENTORY_ERROR); } } transactionHelper.run(() -\u0026gt; { for (SkuInventoryDO inventoryDO : list) { int withholding = withholdingMap.get(inventoryDO.getSkuId()); boolean updateResult = lambdaUpdate() .eq(SkuInventoryDO::getId, inventoryDO.getId()) .eq(SkuInventoryDO::getSkuId, inventoryDO.getSkuId()) .eq(SkuInventoryDO::getLocked, inventoryDO.getLocked()) .eq(SkuInventoryDO::getEnableFlag, CommodityConstant.ENABLE) .ge(SkuInventoryDO::getAvailable, withholding) .set(SkuInventoryDO::getLocked, inventoryDO.getLocked() + withholding) .set(SkuInventoryDO::getAvailable, inventoryDO.getAvailable() - withholding) .set(SkuInventoryDO::getModifyTime, now) .update(); if (!updateResult) { log.error(\u0026#34;订单：{} skuId:{} 预扣库存：{} 失败, 开始回退库存\u0026#34;, query.getParentOrderId(), inventoryDO.getSkuId(), withholding); throw new BusinessException(ServerResultCode.WITHHOLDING_SKU_INVENTORY_ERROR); } // 短信提醒 skuInventoryEarlyWarningMessageProducer.send(generateMessageDTO(inventoryDO)); } skuInventoryChangeRecordService.saveBatch(logs); }); return ServerResult.simpleSuccess(); } 异步处理回退资源类\nMQ 的消费者异步处理\n@Override public void onMessage(OrderCreateMessageDO orderCreateMessage) { log.info(\u0026#34;开始异步处理下单信息：父订单号{}\u0026#34;, orderCreateMessage.getOrderId()); // 步骤状态（A：未处理，S：处理成功，F：处理失败） // （第1位:订单；第2位:库存；第3位:店铺优惠券；第4位：g豆；第5位：明珠卡） String stepStatus = orderCreateMessage.getStepStatus().trim(); if (orderCreateMessage.getMessageStatus() == 1 \u0026amp;\u0026amp; stepStatus.equals(\u0026#34;SSSSS\u0026#34;)) { log.info(\u0026#34;开始异步处理下单信息：父订单号{},订单已经被处理，暂不需要处理\u0026#34;, orderCreateMessage.getOrderId()); return; } // 处理失败次数过多，可能需要人工干预 if (orderCreateMessage.getNum() \u0026gt; 20) { // TODO: 2022/2/25 人工干预 return; } String uuid = IdUtil.simpleUUID(); String key = LOCK_KEY + \u0026#34;:\u0026#34; + orderCreateMessage.getOrderId(); // todo 锁时间需要改成动态配置的 boolean lock = redisUtil.tryLock(key, uuid, 20000); if (!lock) { log.warn(\u0026#34;开始异步处理下单信息：父订单号{},已经有消费者在处理当前订单,放弃本次处理\u0026#34;, orderCreateMessage.getOrderId()); return; } // 查询出父订单下的所有子订单 LambdaQueryWrapper\u0026lt;OrderDO\u0026gt; orderQueryWrapper = Wrappers.lambdaQuery(); orderQueryWrapper.eq(OrderDO::getParentOrderId, orderCreateMessage.getOrderId()) .eq(OrderDO::getUserId,orderCreateMessage.getUserId()); List\u0026lt;OrderDO\u0026gt; subOrderDOList = orderService.list(orderQueryWrapper); // 查询出子订单下的所有订单行 List\u0026lt;Long\u0026gt; orderIds = subOrderDOList.stream().map(OrderDO::getId).collect(Collectors.toList()); LambdaQueryWrapper\u0026lt;OrderItemDO\u0026gt; orderItemDOWrapper = Wrappers.lambdaQuery(); orderItemDOWrapper.in(OrderItemDO::getOrderId, orderIds); List\u0026lt;OrderItemDO\u0026gt; orderItemDOList = orderItemService.list(orderItemDOWrapper); StringBuilder sb = new StringBuilder(stepStatus); try { // oms if (sb.charAt(0) == \u0026#39;A\u0026#39;) { // 如果订单状态全是0，表示下单失败，不同步oms boolean orderFailed = subOrderDOList.stream().noneMatch(orderDO -\u0026gt; orderDO.getState() \u0026gt; 0); if (orderFailed) { // 同步oms } sb.replace(0, 1, \u0026#34;S\u0026#34;); } // 库存 if (sb.charAt(1) == \u0026#39;A\u0026#39;) { List\u0026lt;SkuInventoryDTO\u0026gt; skuInventoryDTOList = orderItemDOList.stream() .map(orderItemDO -\u0026gt; SkuInventoryDTO.builder().skuId(orderItemDO.getSkuId()).returnLocked(orderItemDO.getQuantity()).build()) .collect(Collectors.toList()); ServerResult inventoryResult = skuInventoryServiceFeign.returnLocked(SkuInventoryBatchDTO.builder().list(skuInventoryDTOList).build()); if (inventoryResult.checkSuccess()) { sb.replace(1, 2, \u0026#34;S\u0026#34;); } } // 优惠券 if (sb.charAt(2) == \u0026#39;A\u0026#39;) { } } finally { redisUtil.unLock(key, uuid); } orderCreateMessage.setMessageStatus(\u0026#34;SSSSSS\u0026#34;.equals(sb.toString()) ? 1 : 0); orderCreateMessage.setStepStatus(sb.toString()); orderCreateMessage.setNum(orderCreateMessage.getNum() + 1); LambdaUpdateWrapper\u0026lt;OrderCreateMessageDO\u0026gt; updateWrapper = Wrappers.lambdaUpdate(OrderCreateMessageDO.class); updateWrapper.eq(OrderCreateMessageDO::getId,orderCreateMessage.getId()) .eq(OrderCreateMessageDO::getUserId,orderCreateMessage.getUserId()); orderCreateMessageService.update(orderCreateMessage,updateWrapper); } 分布式事务（ 图解 + 秒懂 + 史上最全 ） - 疯狂创客圈 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%90%86%E8%AE%BA.html","summary":"[toc] 前言 一图解读分布式事务 名词解释 事务：事务是由一组操作构成的可靠的独立的工作单元，事务具备ACID的特性，即原子性、一致性、隔离性和持久性。","title":"分布式事务理论"},{"content":"[toc]\n1. 高效分布式锁 互斥 在分布式高并发的条件下，我们最需要保证，同一时刻只能有一个线程获得锁，这是最基本的一点。\n防止死锁 在分布式高并发的条件下，比如有个线程获得锁的同时，还没有来得及去释放锁，就因为系统故障或者其它原因使它无法执行释放锁的命令,导致其它线程都无法获得锁，造成死锁。\n所以分布式非常有必要设置锁的有效时间，确保系统出现故障后，在一定时间内能够主动去释放锁，避免造成死锁的情况。\n性能 对于访问量大的共享资源，需要考虑减少锁等待的时间，避免导致大量线程阻塞。\n所以在锁的设计时，需要考虑两点。\n锁的颗粒度要尽量小。比如你要通过锁来减库存，那这个锁的名称你可以设置成是商品的ID,而不是任取名称。这样这个锁只对当前商品有效,锁的颗粒度小。\n锁的范围尽量要小。比如只要锁2行代码就可以解决问题的，那就不要去锁10行代码了。\n重入\n我们知道ReentrantLock是可重入锁，那它的特点就是：同一个线程可以重复拿到同一个资源的锁。重入锁非常有利于资源的高效利用。关于这点之后会做演示。\n2. 单机锁 使用setnx()方法获得锁 用eval执行lua脚本删除锁 用lua脚本可以做到原子操作\n详细使用: https://www.cnblogs.com/linjiqin/p/8003838.html redisson也支持单机部署,而且使用更简单\n3. redisson锁 3.1 原理 Redisson是一个基于java编程框架netty进行扩展了的redis。 Redisson是架设在Redis基础上的一个Java驻内存数据网格（In-Memory Data Grid）。充分的利用了Redis键值数据库提供的一系列优势，基于Java实用工具包中常用接口，为使用者提供了一系列具有分布式特性的常用工具类。使得原本作为协调单机多线程并发程序的工具包获得了协调分布式多机多线程并发系统的能力，大大降低了设计和研发大规模分布式系统的难度。同时结合各富特色的分布式服务，更进一步简化了分布式环境中程序相互之间的协作其底层是实现Lock接口实现的。\n加锁和解锁都是通过lua脚本来执行,\n3.1.1 可重入加锁机制 对同一个锁,可以lock多次,对应的也需要unlock多次.\nRedisson可以实现可重入加锁机制的原因，我觉得跟两点有关：\nRedis存储锁的数据类型是 Hash类型 Hash数据类型的key值包含了当前线程信息。 具体值: 这里表面数据类型是Hash类型,Hash类型相当于我们java的 \u0026lt;keyName,\u0026lt;field,value\u0026raquo; 类型,\n这里keyName是指 \u0026lsquo;redisson\u0026rsquo;，\nfield值它的组成是:uuid + 当前线程的ID,\nvalue是重入次数\nuuid是客户端实例化时就创建好了,它是客户端的标识\n重入过程:\nRedisson实现分布式锁(1)\u0026mdash;原理 - 雨点的名字 - 博客园 (cnblogs.com) 3.1.2 (watchDog)看门狗 在持有锁的时间内,业务没有执行完,怎么办?(应该继续拥有锁,知道业务执行完成),所以需要一个线程去监听.\n原理: 额外启动一个线程,每隔10s检测业务是否完成,未完成则续期\n使用: 不设置锁的失效时间,看门狗则自动生效\n只有获得锁时没有指定失效时间,看门狗才会生效,默认失效时间为30s,\n不过要注意,如果业务(比如数据库)出现死锁,导致看门狗一直续期,整个程序就会死锁,这种情况要好生处理,不要让业务出现死锁\nhttps://blog.csdn.net/ice24for/article/details/86177152 https://www.oschina.net/question/1255119_2313008 3.1.3 加锁过程 先竞争锁,成功后设置看门狗,\n尝试获得锁的场景下,如何解决锁竞争?\n使用非公平锁(大部分场景): 使用发布订阅的方式竞争, 订阅后异步等待(有tryLock嘛),在死循环中尝试加锁(默认等7.5s),成功或失败都取消订阅\n使用公平锁 : 在redis中维护一个list做队列, 用队列来排队拿锁 , 整个过程使用lua脚本维护\nRedisson 分布式锁实现之源码篇 → 为什么推荐用 Redisson 客户端 - 云+社区 - 腾讯云 (tencent.com) 【分布式锁】02-使用Redisson实现公平锁原理_meser88的博客-CSDN博客 3.1.4 释放锁过程 重入锁的释放\n锁释放后发布消息\n以上两步用lua脚本完成\n取消看门狗\nRedisson 分布式锁实现之源码篇 → 为什么推荐用 Redisson 客户端 - 云+社区 - 腾讯云 (tencent.com) 公平锁的方式和非公平锁差不多,只是不用发布释放锁的信息,释放后,队列也不删除,一把锁一个队列\nRedisson 分布式锁源码 07：公平锁释放 - 知乎 (zhihu.com) 3.2 使用 # 引入包 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; # 初始化bean @Bean RedissonClient getRedissonClient() { Config config = new Config(); SingleServerConfig serverConfig = config.useSingleServer().setAddress(\u0026#34;redis://\u0026#34;+host+\u0026#34;:\u0026#34;+port).setConnectTimeout(10000) .setTimeout(3000).setIdleConnectionTimeout(10000) .setRetryInterval(1500).setRetryAttempts(3) .setConnectionPoolSize(64).setConnectionMinimumIdleSize(10); if (password != null \u0026amp;\u0026amp; !\u0026#34;\u0026#34;.equals(password)) { serverConfig.setPassword(password); } return Redisson.create(config); } @Autowired private RedissonClient redissonClient; # 获得锁 lock = redissonClient.getLock(\u0026#34;\u0026#34;lockName\u0026#34;\u0026#34;) lockFlag = lock.tryLock(0, 5, TimeUnit.SECONDS); if(lockFlag){ // 一大堆代码 } # 解锁 lock.unlock(); 3.3 工具类 RedissonConfig.java RedissonLockUtil.java 4. Spring Integration 是spring系列的,是一种便捷的事件驱动消息框架用来在系统之间做消息传递的,\n分为 Message -\u0026gt; MessageChannel -\u0026gt; Message Endpoint, 中间还有 Channel Interceptor\nhttps://blog.csdn.net/qq_27808011/article/details/80108622 在分布式锁领域实现方式有\nGemfire JDBC Redis Zookeeper 这里以redis为例 功能和redisson差不多,使用起来也差不多,但貌似不支持看门狗机制\nhttps://blog.csdn.net/qq_29765371/article/details/94752490 https://blog.csdn.net/qq_35529801/article/details/103878784 5. Spring Integration-redis 和 redisson 的区别 内容 redisson Integration-redis 可重入机制 直接在redis中写次数 通过reentrantLock类实现重入机制 获得锁机制 所有线程直接去redis竞争 先在应用内部竞争,成功者再去redis竞争 https://blog.csdn.net/qq_29765371/article/details/94752490 6. 缺陷 由于节点之间是采用异步通信的方式。如果刚刚在 Master 节点上加了锁，但是数据还没被同步到 Salve。这时 Master 节点挂了，它上面的锁就没了，等新的 Master 出来后（主从模式的手动切换或者哨兵模式的一次 failover 的过程），就可以再次获取同样的锁，出现一把锁被拿到了两次的场景。\n加锁的时候会找到一个主节点进行加锁\nhttps://www.cnblogs.com/qdhxhz/p/11046905.html 其解决方案:\nredisson有redLock方案,红锁是多把锁合成一把锁,获得锁时会向全部节点发送lua脚本申请锁,只有获得(n/2+1)个节点的锁时才真正获得锁,(获得锁需要时间,而锁也有过期时间,所以最终时间是过期时间减去获得锁花费的时间)\nhttps://segmentfault.com/a/1190000016976564?utm_source=tag-newest https://www.jianshu.com/p/f302aa345ca8 7. redLock 核心概念就是获得锁需要节点数过半才能获得锁\n假设 有锁的时候死了一台,剩下节点上的锁没有释放, 别人要来拿锁的时候,就凑不齐半数了,\n但是如果死节点获得了锁,死了后马上就启动,也不行,还是会出现分布式锁的问题,所以需要延迟重启,就是等redis上的key都过期了再启动\n需要延迟启动场景:\n假设我们一共有 A、B、C 这三个节点。\n1.客户端 1 在 A，B 上加锁成功。C 上加锁失败。\n2.这时节点 B 崩溃重启了，但是由于持久化策略导致客户端 1 在 B 上的锁没有持久化下来。 客户端 2 发起申请同一把锁的操作，在 B，C 上加锁成功。\n3.这个时候就又出现同一把锁，同时被客户端 1 和客户端 2 所持有了。\n【原创】（求锤得锤的故事）Redis锁从面试连环炮聊到神仙打架。 - why技术 - 博客园 (cnblogs.com) 7.1 加锁过程 假设有5个完全独立的redis主服务器\n获取当前时间戳\nclient尝试按照顺序使用相同的key,value获取所有redis服务的锁，在获取锁的过程中的获取时间比锁过期时间短很多，这是为了不要过长时间等待已经关闭的redis服务。并且试着获取下一个redis实例。\n比如：TTL为5s,设置获取锁最多用1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁\nclient通过获取所有能获取的锁后的时间减去第一步的时间，这个时间差要小于TTL时间并且至少有3个redis实例成功获取锁(过半原则)，才算真正的获取锁成功\n如果成功获取锁，则锁的真正有效时间是 TTL减去第三步的时间差 的时间；比如：TTL 是5s,获取所有锁用了2s,则真正锁有效时间为3s(其实应该再减去时钟漂移);\n如果客户端由于某些原因获取锁失败，便会开始解锁所有redis实例；因为可能已经获取了小于3个锁，必须释放，否则影响其他client获取锁\n7.2 释放锁过程 7.3 缺陷 总的来说因为redLock强依赖时间,而在分布式架构中,时间不是完全可靠的,所以导致了一系列问题\n7.3.1 获得锁期间发生阻塞 作者 Martin 给出这张图，首先我们上一讲说过，RedLock中，为了防止死锁，锁是具有过期时间的。这个过期时间被 Martin 抓住了小辫子。\n如果 Client 1 在持有锁的时候，发生了一次很长时间的 FGC 超过了锁的过期时间。锁就被释放了。 这个时候 Client 2 又获得了一把锁，提交数据。 这个时候 Client 1 从 FGC 中苏醒过来了，又一次提交数据。 这还了得，数据就发生了错误。RedLock 只是保证了锁的高可用性，并没有保证锁的正确性。\n这个时候也许你会说，如果 Client 1 在提交任务之前去查询一下锁的持有者是不自己就能解决这个问题？ 答案是否定的，FGC 会发生在任何时候，如果 FGC 发生在查询之后，一样会有如上讨论的问题。\n那换一个没有 GC 的编程语言？ 答案还是否定的， FGC 只是造成系统停顿的原因之一，IO或者网络的堵塞或波动都可能造成系统停顿。\n7.3.2 发生时间漂移等时间不一致问题 如果某个 Redis Master的系统时间发生了错误，造成了它持有的锁提前过期被释放。\nClient 1 从 A、B、C、D、E五个节点中，获取了 A、B、C三个节点获取到锁，我们认为他持有了锁 这个时候，由于 B 的系统时间比别的系统走得快，B就会先于其他两个节点优先释放锁。 Clinet 2 可以从 B、D、E三个节点获取到锁。在整个分布式系统就造成 两个 Client 同时持有锁了。 没有完美的方案,就看使用者怎么权衡\n看zookeeper方式有没有更好的方案?\nRedis RedLock 完美的分布式锁么？ - 云+社区 - 腾讯云 (tencent.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.html","summary":"[toc] 1. 高效分布式锁 互斥 在分布式高并发的条件下，我们最需要保证，同一时刻只能有一个线程获得锁，这是最基本的一点。 防止死锁 在分布式高并发的条件下，","title":"分布式锁"},{"content":"[toc]\n1. from + size 这是ES分页中最常用的一种方式，与MySQL类似，from指定起始位置，size指定返回的文档数。\nGET kibana_sample_data_flights/_search { \u0026#34;from\u0026#34;: 10, \u0026#34;size\u0026#34;: 2, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;DestWeather\u0026#34;: \u0026#34;Sunny\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; } } ] } 使用简单，且默认的深度分页限制是1万，from + size 大于 10000会报错，可以通过index.max_result_window参数进行修改。\n好处是可以灵活分页, 比如指定页码的跳转\n这种分页方式，在分布式的环境下的深度分页是有性能问题的，一般不建议用这种方式做深度分页，可以用下面将要介绍的两种方式。(以下两种均不能指定页码跳转,只能下滑式翻页)\n理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。\n现在假设我们请求第 1000 页，结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。\n可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。\n2. scroll api 创建一个快照，有新的数据写入以后，无法被查到。每次查询后，输入上一次的 scroll_id\nGET kibana_sample_data_flights/_search?scroll=1m { \u0026#34;size\u0026#34;: 2, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;DestWeather\u0026#34;: \u0026#34;Sunny\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; }, \u0026#34;_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 在返回的数据中，有一个_scroll_id字段，下次搜索的时候带上这个数据，并且使用下面的查询语句。\nPOST _search/scroll { \u0026#34;scroll\u0026#34; : \u0026#34;1m\u0026#34;, \u0026#34;scroll_id\u0026#34; : \u0026#34;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAA6UWWVJRTk9TUXFTLUdnU28xVFN6bEM4QQ==\u0026#34; } 上面的scroll指定搜索上下文保留的时间，1m代表1分钟，还有其他时间可以选择，有d、h、m、s等，分别代表天、时、分钟、秒\n搜索上下文有过期自动删除，但如果自己知道什么时候该删，可以自己手动删除，减少资源占用。\nDELETE /_search/scroll { \u0026#34;scroll_id\u0026#34; : \u0026#34;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAA6UWWVJRTk9TUXFTLUdnU28xVFN6bEM4QQ==\u0026#34; } scroll 本质是创建一个快照,在这个快照中做搜索,所以意味着数据不会被实时更新, 其次上下文是需要保留的,占用了资源,当搜索量大时,上下文就会占据大量资源,官方也不推荐使用这种方式\n不过用在快照场景还是可以的,比如 大量数据导出或者索引重建\n3. search after search after 利用实时有游标来帮我们解决实时滚动的问题。第一次搜索时需要指定 sort，并且保证值是唯一的，可以通过加入 _id 保证唯一性。\n和srcoll很类似,都是记录上次搜索的结尾,然后才开始分页\nGET kibana_sample_data_flights/_search { \u0026#34;size\u0026#34;: 2, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;DestWeather\u0026#34;: \u0026#34;Sunny\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; }, \u0026#34;_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } 在返回的结果中，最后一个文档有类似下面的数据，由于我们排序用的是两个字段，返回的是两个值。\n\u0026#34;sort\u0026#34; : [ 1614561419000, \u0026#34;6FxZJXgBE6QbUWetnarH\u0026#34; ] 第二次搜索，带上这个sort的信息即可，如下\nGET kibana_sample_data_flights/_search { \u0026#34;size\u0026#34;: 2, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;DestWeather\u0026#34;: \u0026#34;Sunny\u0026#34; } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;asc\u0026#34; }, \u0026#34;_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ], \u0026#34;search_after\u0026#34;: [ 1614561419000, \u0026#34;6FxZJXgBE6QbUWetnarH\u0026#34; ] } 三者性能比较\n分页方式 1～10 49000～49010 99000～99010 form…size 8ms 30ms 117ms scroll 7ms 66ms 36ms search_after 5ms 8ms 7ms 参考链接:\n三种方式比较 性能比较 官方文档 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%88%86%E9%A1%B5%E5%AE%9E%E7%8E%B0.html","summary":"[toc] 1. from + size 这是ES分页中最常用的一种方式，与MySQL类似，from指定起始位置，size指定返回的文档数。 GET kibana_sample_data_flights/_search { \u0026#34;from\u0026#34;: 10, \u0026#34;size\u0026#34;: 2, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;DestWeather\u0026#34;: \u0026#34;Sunny\u0026#34; } }, \u0026#34;sort\u0026#34;:","title":"分页实现"},{"content":" 元字符 描述 \\ 将下一个字符标记符、或一个向后引用、或一个八进制转义符。例如，“ ”匹配 。“ ”匹配换行符。序列“\\”匹配“\\”而“(”则匹配“(”。即相当于多种编程语言中都有的“转义字符”的概念。 ^ 匹配输入字行首。如果设置了RegExp对象的Multiline属性，^也匹配“ ”或“\r”之后的位置。 $ 匹配输入行尾。如果设置了RegExp对象的Multiline属性，$也匹配“ ”或“\r”之前的位置。 * 匹配前面的子表达式任意次。例如，zo*能匹配“z”，也能匹配“zo”以及“zoo”。*等价于o{0,} + 匹配前面的子表达式一次或多次(大于等于1次）。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“do”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n\u0026lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o为一组，后三个o为一组。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+”将尽可能多的匹配“o”，得到结果[“oooo”]，而“o+?”将尽可能少的匹配“o”，得到结果 [\u0026lsquo;o\u0026rsquo;, \u0026lsquo;o\u0026rsquo;, \u0026lsquo;o\u0026rsquo;, \u0026lsquo;o\u0026rsquo;] .点 匹配除“ ”之外的任何单个字符。要匹配包括“ ”在内的任何字符，请使用像“[\\s\\S]”的模式。 (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)”。 (?:pattern) 非获取匹配，匹配pattern但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分时很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。 (?=pattern) 非获取匹配，正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如，“Windows(?=95|98|NT|2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?!pattern) 非获取匹配，正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如“Windows(?!95|98|NT|2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。 (?\u0026lt;=pattern) 非获取匹配，反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?\u0026lt;=95|98|NT|2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。 (?\u0026lt;!pattern) 非获取匹配，反向否定预查，与正向否定预查类似，只是方向相反。例如“(?\u0026lt;!95|98|NT|2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。这个地方不正确，有问题此处用或任意一项都不能超过2位，如“(?\u0026lt;!95|98|NT|20)Windows正确，“(?\u0026lt;!95|980|NT|20)Windows 报错，若是单独使用则无限制，如(?\u0026lt;!2000)Windows 正确匹配 x|y 匹配x或y。例如，“z|food”能匹配“z”或“food”(此处请谨慎)。“[zf]ood”则匹配“zood”或“food”。 [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“plin”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。注意:只有连字符在字符组内部时,并且出现在两个字符之间时,才能表示字符的范围; 如果出字符组的开头,则只能表示连字符本身. [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。 \u0008 匹配一个单词边界，也就是指单词和空格间的位置（即正则表达式的“匹配”有两种概念，一种是匹配字符，一种是匹配位置，这里的\u0008就是匹配位置的）。例如，“er\u0008”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。 \\B 匹配非单词边界。“er\\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E7%AC%A6%E5%8F%B7.html","summary":"元字符 描述 \\ 将下一个字符标记符、或一个向后引用、或一个八进制转义符。例如，“ ”匹配 。“ ”匹配换行符。序列“\\”匹配“\\”而“(”则匹配“(”","title":"符号"},{"content":"一些基本概念 接近实时（NRT）\nElasticsearch 是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个很小的延迟（通常是 1 秒）。 集群（cluster）\n代表一个集群，集群中有多个节点（node），其中有一个为主节点，这个主节点是可以通过选举产生的，主从节点是对于集群内部来说的。es的一个概念就是去中心化，字面上理解就是无中心节点，这是对于集群外部来说的，因为从外部来看es集群，在逻辑上是个整体，你与任何一个节点的通信和与整个es集群通信是等价的。 索引（index）\nElasticSearch将它的数据存储在一个或多个索引（index）中。用SQL领域的术语来类比，索引就像数据库，可以向索引写入文档或者从索引中读取文档，并通过ElasticSearch内部使用Lucene将数据写入索引或从索引中检索数据。 ​\t索引分为结构化创建与非结构化创建 其划分依据是 是否有mappings(有则为结构化),创建索引时可以指定. 结构化的好处,es知道了字段的相关信息,可以做一些预处理,提高性能\n来源: https://blog.csdn.net/liuxiao723846/article/details/78444472 https://www.cnblogs.com/hong-fithing/p/11221020.html 文档（document）\n文档（document）是ElasticSearch中的主要实体。对所有使用ElasticSearch的案例来说，他们最终都可以归结为对文档的搜索。文档由字段构成。 映射（mapping）\n所有文档写进索引之前都会先进行分析，如何将输入的文本分割为词条、哪些词条又会被过滤，这种行为叫做映射（mapping）。一般由用户自己定义规则。 类型（type）\n每个文档都有与之对应的类型（type）定义。这允许用户在一个索引中存储多种文档类型，并为不同文档提供类型提供不同的映射。 分片（shards）\n代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引拆分成多个，分布到不同的节点上。构成分布式搜索。分片的数量只能在索引创建前指定，并且索引创建后不能更改。5.X默认不能通过配置文件定义分片\nshard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；\n对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。\nElasticSearch 7.x 不需要优化分片的数量了吗？ - Elastic 中文社区 副本（replicas）\n代表索引副本，es可以设置多个索引的副本，副本的作用一是提高系统的容错性，当个某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负载均衡。\n其本质是对分片进行备份操作, 副本会接收读请求\n扩展阅读: ElasticSearch 如何保证数据一致性,实时性 - 简书 (jianshu.com) 数据恢复（recovery）\n代表数据恢复或叫数据重新分布，es在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。 GET /_cat/health?v #可以看到集群状态 数据源（River）\n代表es的一个数据源，也是其它存储方式（如：数据库）同步数据到es的一个方法。它是以插件方式存在的一个es服务，通过读取river中的数据并把它索引到es中，官方的river有couchDB的，RabbitMQ的，Twitter的，Wikipedia的，river这个功能将会在后面的文件中重点说到。 网关（gateway）\n代表es索引的持久化存储方式，es默认是先把索引存放到内存中，当内存满了时再持久化到硬盘。当这个es集群关闭再重新启动时就会从gateway中读取索引数据。es支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和amazon的s3云存储服务。 自动发现（discovery.zen）\n代表es的自动发现节点机制，es是一个基于p2p的系统，它先通过广播寻找存在的节点，再通过多播协议来进行节点之间的通信，同时也支持点对点的交互。 5.X关闭广播，需要自定义 通信（Transport）\n代表es内部节点或集群与客户端的交互方式，默认内部是使用tcp协议进行交互，同时它支持http协议（json格式）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。 节点间通信端口默认：9300-9400 分片和复制（shards and replicas）\n一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点可能没有这样大的磁盘空间来存储或者单个节点处理搜索请求，响应会太慢。\n为了解决这个问题，Elasticsearch提供了将索引划分成多片的能力，这些片叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引” 可以被放置到集群中的任何节点上。\n分片之所以重要，主要有两方面的原因：\n允许你水平分割/扩展你的内容容量 允许你在分片（位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。\n在一个网络/云的环境里，失败随时都可能发生。在某个分片/节点因为某些原因处于离线状态或者消失的情况下，故障转移机制是非常有用且强烈推荐的。为此， Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。\n复制之所以重要，有两个主要原因：\n在分片/节点失败的情况下，复制提供了高可用性。复制分片不与原/主要分片置于同一节点上是非常重要的。因为搜索可以在所有的复制上并行运行，复制可以扩展你的搜索量/吞吐量 总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（即没有复制） 或多次。一旦复制了，每个索引就有了主分片（作为复制源的分片）和复制分片（主分片的拷贝）。 分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你不能再改变分片的数量。 5.X默认5:1 5个主分片，1个复制分片 默认情况下，Elasticsearch中的每个索引分配5个主分片和1个复制。这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样每个索引总共就有10个分片。\n来自: http://www.cnblogs.com/xiaochina/p/6855591.html Elasticsearch7.X版本 分片和副本数都默认为1\nElasticsearch 7 : 设置索引副本数量和分片数量 - 乐天笔记 (letianbiji.com) 日志类，其特点为：写入频繁，查询较少\n单个分片磁盘存储不要超过50G\n搜索类，其特点为：写入较少，查询频繁\n单个分片磁盘存储不要超过20G\nElasticSearch部署架构和容量规划 (qq.com) （1）关系型数据库中的数据库（DataBase），等价于ES中的索引（Index）\n（2）一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type），\n（3）一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。\n（4）在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。 与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。\n（5）在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET.\nElastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。\n来自:https://blog.csdn.net/laoyang360/article/details/52244917\nhttp://www.ruanyifeng.com/blog/2017/08/elasticsearch.html 缓存类型 Node Query Cache（Filter Context）\n每一个节点有一个Node Query缓存 由该节点的所有Shard共享，只缓存Filter Context 相关内容 Cache采用LRU算法，不会被jvm gc Segment级缓存命中的结果。Segment被合并后，缓存会失效 Shard Query Cache（Cache Query的结果）\n缓存每个分片上的查询结果\n只会缓存设置了size=0的查询对应的结果。不会缓存hits。但是会缓存 Aggregations和Suggestions\nCache Key\nLRU算法，将整个JSON查询串作为Key，与JSON对象的顺序相关，不会被jvm gc\n分片Refresh时候，Shard Request Cache会失效。如果Shard对应的数据频繁发生变化，该缓存的效率会很差\nFielddata Cache\n除了Text类型，默认都采用doc_values。节约了内存。\nAggregation的Global ordinals也保存在Fielddata cache 中\nText类型的字段需要打开Fileddata才能对其进行聚合和排序\nText经过分词，排序和聚合效果不佳，建议不要轻易使用。\nSegment被合并后，会失效\nSegments Cache\n（segments FST数据的缓存），为了加速查询，FST永驻堆内内存，无法被GC回收。该部分内存无法设置大小，长期占用50%~70%的堆内存，只能通过delete index，close index以及force-merge index释放内存\nES底层存储采用Lucene（搜索引擎），写入时会根据原始数据的内容，分词，然后生成倒排索引。查询时，先通过查询倒排索引找到数据地址（DocID）），再读取原始数据（行存数据、列存数据）。\n但由于Lucene会为原始数据中的每个词都生成倒排索引，数据量较大。所以倒排索引对应的倒排表被存放在磁盘上。\n这样如果每次查询都直接读取磁盘上的倒排表，再查询目标关键词，会有很多次磁盘IO，严重影响查询性能。为了解磁盘IO问题，Lucene引入排索引的二级索引FST[Finite State Transducer]。原理上可以理解为前缀树，加速查询\nElasticSearch部署架构和容量规划 (qq.com) Elasticsearch是如何做到快速索引的 InfoQ那篇文章里说Elasticsearch使用的倒排索引比关系型数据库的B-Tree索引快，为什么呢？\n什么是B-Tree索引? 上大学读书时老师教过我们，二叉树查找效率是logN，同时插入新的节点不必移动全部节点，所以用树型结构存储索引，能同时兼顾插入和查询的性能。因此在这个基础上，再结合磁盘的读取特性(顺序读/随机读)，传统关系型数据库采用了B-Tree/B+Tree这样的数据结构：\n为了提高查询的效率，减少磁盘寻道次数，将多个值作为一个数组通过连续区间存放，一次寻道读取多个数据，同时也降低树的高度。\n什么是倒排索引? 继续上面的例子，假设有这么几条数据(为了简单，去掉about, interests这两个field):\nID Name Age Sex 1 Kate 24 Female 2 John 24 Male 3 Bill 29 Male ID是Elasticsearch自建的文档id，那么Elasticsearch建立的索引如下:\nName:\nTerm Posting List Kate 1 John 2 Bill 3 Age:\nTerm Posting List 24 [1,2] 29 3 Sex:\nTerm Posting List Female 1 Male [2,3] Posting List\nElasticsearch分别为每个field都建立了一个倒排索引，Kate, John, 24, Female这些叫term，而[1,2]就是Posting List。Posting list就是一个int的数组，存储了所有符合某个term的文档id。\n通过posting list这种索引方式似乎可以很快进行查找，比如要找age=24的同学，爱回答问题的小明马上就举手回答：我知道，id是1，2的同学。但是，如果这里有上千万的记录呢？如果是想通过name来查找呢？\nTerm Dictionary\nElasticsearch为了能快速找到某个term，将所有的term排个序，二分法查找term，logN的查找效率，就像通过字典查找一样，这就是Term Dictionary。现在再看起来，似乎和传统数据库通过B-Tree的方式类似啊，为什么说比B-Tree的查询快呢？\nTerm Index\nB-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，就像字典里的索引页一样，A开头的有哪些term，分别在哪页，可以理解term index是一颗树：\n这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找\n所以term index不需要存下所有的term，而仅仅是他们的一些前缀与Term Dictionary的block之间的映射关系，再结合FST(Finite State Transducers)的压缩技术，可以使term index缓存到内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘随机读的次数。\nFSTs are finite-state machines that map a term (byte sequence) to an arbitrary output.\n⭕️表示一种状态\n\u0026ndash;\u0026gt;表示状态的变化过程，上面的字母/数字表示状态变化和权重\n将单词分成单个字母通过⭕️和\u0026ndash;\u0026gt;表示出来，0权重不显示。如果⭕️后面出现分支，就标记权重，最后整条路径上的权重加起来就是这个单词对应的序号。\nFSTs are finite-state machines that map a term (byte sequence) to an arbitrary output.\nFST以字节的方式存储所有的term，这种压缩方式可以有效的缩减存储空间，使得term index足以放进内存，但这种方式也会导致查找时需要更多的CPU资源。(存储的是种类,而不是内容,用图的路径来表示内容,那就是图算法了,根据权重找路径,懵逼~)\n压缩技巧 Elasticsearch里除了上面说到用FST压缩term index外，对posting list也有压缩技巧。\n我们再看回最开始的例子，如果Elasticsearch需要对同学的性别进行索引(这时传统关系型数据库已经哭晕在厕所……)，会怎样？如果有上千万个同学，而世界上只有男/女这样两个性别，每个posting list都会有至少百万个文档id。 Elasticsearch是如何有效的对这些文档id压缩的呢？\n增量编码压缩，将大数变小数，按字节存储\n首先，Elasticsearch要求posting list是有序的(为了提高搜索的性能，再任性的要求也得满足)，这样做的一个好处是方便压缩，看下面这个图例：\n原理就是通过增量，将原来的大数变成小数仅存储增量值，再精打细算按bit排好队，最后通过字节存储，而不是大大咧咧的尽管是2也是用int(4个字节)来存储。 (不是很懂最后一行中的绿色底色的8和5是什么意思?)\nRoaring bitmaps 说到Roaring bitmaps，就必须先从bitmap说起。Bitmap是一种数据结构，假设有某个posting list：\n[1,3,4,7,10]\n对应的bitmap就是：\n[1,0,1,1,0,0,1,0,0,1]\n非常直观，用0/1表示某个值是否存在，比如10这个值就对应第10位，对应的bit值是1，这样用一个字节就可以代表8个文档id，(有点像哈希算法)\n旧版本(5.0之前)的Lucene就是用这样的方式来压缩的，但这样的压缩方式仍然不够高效，如果有1亿个文档，那么需要12.5MB的存储空间，这仅仅是对应一个索引字段(我们往往会有很多个索引字段)。于是有人想出了Roaring bitmaps这样更高效的数据结构。\nBitmap的缺点是存储空间随着文档个数线性增长，Roaring bitmaps需要打破这个魔咒就一定要用到某些指数特性：\n将posting list按照65535为界限分块，比如第一块所包含的文档id范围在0~65535之间，第二块的id范围是65536~131071，以此类推。再用\u0026lt;商，余数\u0026gt;的组合表示每一组id，这样每组里的id范围都在0~65535内了，剩下的就好办了，既然每组id不会变得无限大，那么我们就可以通过最有效的方式对这里的id存储。\n为什么是以65535为界限?\u0026quot;\n程序员的世界里除了1024外，65535也是一个经典值，因为它=2^16-1，正好是用2个字节能表示的最大数，一个short的存储单位，注意到上图里的最后一行“If a block has more than 4096 values, encode as a bit set, and otherwise as a simple array using 2 bytes per value”，如果是大块，用节省点用bitset存，小块就豪爽点，2个字节我也不计较了，用一个short[]存着方便。\n那为什么用4096来区分大块还是小块呢？\n个人理解：都说程序员的世界是二进制的，4096*2bytes ＝ 8192bytes \u0026lt; 1KB, 磁盘一次寻道可以顺序把一个小块的内容都读出来，再大一位就超过1KB了，需要两次读。\n联合索引 上面说了半天都是单field索引，如果多个field索引的联合查询，倒排索引如何满足快速查询的要求呢？\n利用跳表(Skip list)的数据结构快速做“与”运算，或者 利用上面提到的bitset按位“与” 先看看跳表的数据结构：\n将一个有序链表level0，挑出其中几个元素到level1及level2，每个level越往上，选出来的指针元素越少，查找时依次从高level往低查找，比如55，先找到level2的31，再找到level1的47，最后找到55，一共3次查找，查找效率和2叉树的效率相当，但也是用了一定的空间冗余来换取的。\n假设有下面三个posting list需要联合索引：\n如果使用跳表，对最短的posting list中的每个id，逐个在另外两个posting list中查找看是否存在，最后得到交集的结果。\n如果使用bitset，就很直观了，直接按位与，得到的结果就是最后的交集。\n总结和思考 Elasticsearch的索引思路:\n将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数(同时也利用磁盘顺序读特性)，结合各种奇技淫巧的压缩算法，用极其苛刻的态度使用内存。\n所以，对于使用Elasticsearch进行索引时需要注意:\n不需要索引的字段，一定要明确定义出来，因为默认是自动建索引的 同样的道理，对于String类型的字段，不需要analysis的也需要明确定义出来，因为默认也是会analysis的 选择有规律的ID很重要，随机性太大的ID(比如java的UUID)不利于查询 来自:https://www.cnblogs.com/dreamroute/p/8484457.html\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D.html","summary":"一些基本概念 接近实时（NRT） Elasticsearch 是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个很小的延迟（通常是 1 秒）。 集","title":"概念介绍"},{"content":"查询和\u0026quot; 01 \u0026ldquo;号的同学学习的课程 完全相同的其他同学的信息\n​\t\u0026ndash; 不存在这样的课程 , 01学了,但是学生x没学 -\u0026gt; 蕴含逻辑运算\n​\t\u0026ndash; (像这种蕴含逻辑运算,涉及到离散,做一些简单倒是可以,比如\u0026quot;完全相同\u0026rdquo;,\u0026ldquo;没有全部拥有的\u0026quot;等全量字眼)\nSELECT * FROM student WHERE NOT EXISTS( SELECT * FROM stucou sc1 WHERE sc1.SId=\u0026#39;01\u0026#39; AND NOT EXISTS( SELECT * FROM stucou sc2 WHERE sc2.SId=student.SId AND sc1.CId=sc2.CId) ); 详解:https://blog.csdn.net/qsyzb/article/details/12525955\n查询学过\u0026quot;张三\u0026quot;老师讲授的全部课程的学生姓名\n​\t\u0026ndash; 不存在这样的课程,张三老师教了,但是学生没学\nSELECT s.* FROM student s WHERE NOT EXISTS ( SELECT 1 FROM teacher t INNER JOIN course c ON c.TId= t.TId WHERE t.Tname=\u0026#39;张三\u0026#39; AND NOT EXISTS ( SELECT 1 FROM stucou sc WHERE c.CId = sc.CId AND sc.SId=s.SId ) ) 对于employees表中，给出奇数行的first_name\nselect e.first_name from employees e where (select count(1) from employees e1 where e.first_name \u0026gt;= e1.first_name)%2!=0 来自: https://www.nowcoder.com/practice/e3cf1171f6cc426bac85fd4ffa786594?tpId=82\u0026tqId=29829\u0026rp=0\u0026ru=%2Fta%2Fsql\u0026qru=%2Fta%2Fsql%2Fquestion-ranking\u0026tPage=4 对所有员工的当前(to_date=\u0026lsquo;9999-01-01\u0026rsquo;)薪水按照salary进行按照1-N的排名，相同salary并列且按照emp_no升序排列\n\u0026ndash; 排名: 小于自己的有多少个,就是排第几,有点类似计数排序\nselect s.emp_no,s.salary,count(distinct s1.salary) as rank from salaries s,salaries s1 where s.to_date=\u0026#39;9999-01-01\u0026#39; and s1.to_date=\u0026#39;9999-01-01\u0026#39; and s.salary\u0026lt;=s1.salary group by s.emp_no order by s.salary desc,s.emp_no asc ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%AB%98%E7%BA%A7sql.html","summary":"查询和\u0026quot; 01 \u0026ldquo;号的同学学习的课程 完全相同的其他同学的信息 ​ \u0026ndash; 不存在这样的课程 , 01学了,但是学生x没学 -\u0026gt; 蕴含逻辑运算 ​ \u0026ndash;","title":"高级SQL"},{"content":"[toc]\n前言 你可能听说过高水位，但不一定听说过Leader Epoch。前者是Kafka中非常重要的概念。而后者是0.11版本中新推出的。主要是为了弥补前者水位机制的一些缺陷。\n1.高水位 1.1 什么是高水位 Kafka的水位不是时间戳，更与时间无关。它是和位置信息绑定的，具体来说，它是用消息位移来表征的。\n这个offset是所有ISR的LEO的最小位置（minimum LEO across all the ISR of this partition），consumer不能读取超过HW的消息，因为这意味着读取到未完全同步（因此没有完全备份）的消息。换句话说就是：HW是所有ISR中的节点都已经复制完的消息.也是消费者所能获取到的消息的最大offset（注意，并不是所有replica都一定有这些消息，而只是ISR里的那些才肯定会有）。\n1.2 高水位的作用 定义消息可见性，用来标识分区下的哪些消息是可以被消费者消费的 帮助kafka完成副本同步 1.3 已提交消息和未提交消息 在分区高水位以下的消息就被认为是已提交消息，反之就是未提交消息 消费者只能消费已提交消息，即位移值小于8的消息。 这里不存在kafka的事务，因为事务机制会影响消息者所能看到的消息的范围，他不只是简单依赖高水位来判断，是依赖于一个名为LSO的位移值来判断事务性消费者的可见性 位移值等于高水位的消息也属于为未提交消息。即高水位的消息也是不能被消费者消费的 LEO表示副本写入下一条消息的位移值。同一个副本对象，起高水位值不会超过LEO 1.4 高水位更新机制\nKafka中所有副本对象都保存一组高水位值和LEO值，但Leader副本中还保留着其他Follower副本的LEO值。\nKafka副本机制在运行过程中，会更新Broker1上Follower副本的高水位和LEO值，同时也会更新Broker0上Leader副本的高水位和LEO以及Follow副本的LEO，但不会更新其HW。\n1.5 副本同步机制解析 当生产者发送一条消息时，Leader和Follower副本对应的高水位是怎么被更新的呢？\nFollower副本也成功地更新LEO为1.此时，Leader和Follower副本的LEO都是1，但各自的高水位依然是0，还没有被更新。他们需要在下一轮的拉取中被更新\n在新一轮的拉去请求中，由于位移值是0的消息已经拉取成功，因此Follower副本这次请求拉去的位移值为1的消息。Leader副本接收此请求后，更新远程副本LEO为1，然后更新Leader高水位为1，然后才会将更新过的高水位值1发送给Follower副本。Follower副本接收到以后，也将自己的高水位值更新为1.至此，一个完整的消息同步周期就结束了。\n总的来说: 第一次拉取只会更新LEO,第二次拉取时才会更新HW\n2. Leader Epoch Follower副本的高水位更新是需要额外一轮的拉取请求才能实现的。若有多个副本的情况下，则需要多轮的拉取请求。也就是说，Leader副本高水位更新和Follower副本高水位更新在时间上是存在错配的。而这种错配往往是数据丢失，数据不一致问题现象的根源。因此kafka社区在0.11版本中引入了Leader Epoch。\n2.1 Leader Epoch的组成 Epoch。一个单调递增的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的Leader被认为是过期的Leader，不能再行使Leader的权力。 起始位移（Start Offset）。Leader副本在该Epoch上写入的首条消息的位移。 Leader Epoch\u0026lt;0,0\u0026gt;和\u0026lt;1,100\u0026gt;。第一个Epoch指的是0版本，位移0开始保存消息，一共保存100条消息。之后Leader发生了变更，版本号增加到1，新版本起始位移为100.\nKafka Broker会在内存中为每个分区都缓存Leader Epoch数据，同时它还会定期的将这信息持久化一个checkpoint文件中。当Leader副本写入消息到磁盘时，Broker会尝试更新这部分缓存，如果该Leader是首次写入消息，那么Broker会向缓存中增加一个Leader Epoch条目，否则就不做更新。\n2.2 Leader Epoch使用 Leader Epoch是怎样防止数据丢失的呢？\n单纯依赖高水位是怎么造成数据丢失的。开始时，副本A和副本B都处于正常状态，A是Leader副本，B是Follower副本。当生产者使用ack=1（默认）往Leader副本A中发送两条消息。且A全部写入成功，此时Kafka会通知生产者说这两条消息写入成功。\n现在假设A,B都写入了这两条消息，而且Leader副本的高水位也已经更新了，但Follower副本高水位还未更新。因为Follower端高水位的更新与Leader端有时间错配。假如现在副本B所在Broker宕机了，那么当它重启回来后，副本B就会执行日志截断操作，将LEO值调整为之前的高水位值，也就是1.所以副本B当中位移值为1的消息就丢失了。副本B中只保留了位移值0的消息。\n当执行完截断操作之后，副本B开始从A中拉取消息，执行正常的消息同步。假如此时副本A所在的Broker也宕机了。那么kafka只能让副本B成为新的Leader，然后副本A重启回来之后，也需要执行日志截断操作，即调整高水位为与B相同的值，也就是1。这样操作之后，位移值为1的那条消息就永远丢失了。\nLeader Epoch机制如何规避这种数据丢失现象呢？\n延续上文场景，引用了Leader Epoch机制之后，Follower副本B重启回来后，需要向A发送一个特殊的请求去获取Leader的LEO值，该例子中为2。当知道Leader LEO为2时，B发现该LEO值不必自己的LEO值小，而且缓存中也没有保存任何起始位移值\u0026gt;2的Epoch条目，因此B无需执行日志截断操作。这是对高水位机制的一次明显改进，即不是依赖于高水位判断是否进行日志截断操作。\n现在，副本A宕机了，B成立新Leader。同样的，在A重启回来后，执行与B逻辑相同的判断，也不需要执行日志截断操作，所以位移值为1的那条消息就全部得以保存。后面当生产者程序向 B 写入新消息时，副本 B 所在的 Broker 缓存中，会生成新的 Leader Epoch 条目：[Epoch=1, Offset=2]。之后，副本 B 会使用这个条目帮助判断后续是否执行日志截断操作。这样，kafka就规避掉了数据丢失的场景。\nhttps://copyfuture.com/blogs-details/2020060309453891120r9ar5i3bhw0px https://www.cnblogs.com/huxi2b/p/7453543.html https://mp.weixin.qq.com/s/17b-uA4vxnU_39xXdM9ihQ ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E9%AB%98%E6%B0%B4%E4%BD%8D%E4%B8%8Eleaderepoch.html","summary":"[toc] 前言 你可能听说过高水位，但不一定听说过Leader Epoch。前者是Kafka中非常重要的概念。而后者是0.11版本中新推出的。主要是为了","title":"高水位与LeaderEpoch"},{"content":"总的来说,让处理请求的时候变短,速度变快\n前端\n浏览器优化技术：合理布局，页面缓存，减少http请求数，页面压缩，减少 cookie 传输。\nCDN\nDNS负载均衡\n动静分离\n动态图片独立提供服务\n加入redis\n服务分流\n代码良好\nhttps://zhuanlan.zhihu.com/p/105160130 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E6%80%A7%E8%83%BD.html","summary":"总的来说,让处理请求的时候变短,速度变快 前端 浏览器优化技术：合理布局，页面缓存，减少http请求数，页面压缩，减少 cookie 传输。 CDN DNS负载均衡 动","title":"高性能"},{"content":"[toc]\n1. 整体架构 RocketMQ 主要由 Broker、NameServer、Producer 和 Consumer 组成的一个集群。\nNameServer：整个集群的注册中心和配置中心，管理集群的元数据。包括 Topic 信息和路由信息、Producer 和 Consumer 的客户端注册信息、Broker 的注册信息。 Broker：负责接收消息的生产和消费请求，并进行消息的持久化和消息的读取。 Producer：负责生产消息。 Consumer：负责消费消息。 broker有很多台,可以分散存储和被访问的压力, 且每个broker可以存在多个slave,当消费者从slave获取消息时,还能分担broker压力,进一步提高消费性能\n生产和消费直连nameServer,可以快速定位到主题在哪个broker上,而且broker上还有这些数据(meta)的缓存, 甚至连nameServer的访问都省略了\n消费集群可以增加数量, 以增加消费性能\n1.1. 网络模型 RocketMQ 使用 Netty 框架实现高性能的网络传输。\nNetty 的高性能传输的体现\n非阻塞 IO Ractor 线程模型 零拷贝。使用 FileChannel.transfer 避免在用户态和内核态之间的拷贝操作；通过 CompositeByteBuf 组合多个 ByteBuffer；通过 slice 获取 ByteBuffer 的切片；通过 wrapper 把普通 ByteBuffer 封装成 netty.ByteBuffer。 RocketMQ 网络模型\nRocketMQ 的 Broker 端基于 Netty 实现了主从 Reactor 模型。架构如下：\n具体流程：\neventLoopGroupBoss 作为 acceptor 负责接收客户端的连接请求 eventLoopGroupSelector 负责 NIO 的读写操作 NettyServerHandler 读取 IO 数据，并对消息头进行解析 disatch 过程根据注册的消息 code 和 processsor 把不同的事件分发给不同的线程。由 processTable 维护（类型为 HashMap） 1.2. 业务线程池隔离 RocketMQ 对 Broker 的线程池进行了精细的隔离。使得消息的生产、消费、客户端心跳、客户端注册等请求不会互相干扰。如下是各个业务执行线程池和 Broker 处理的报文类型的对应关系，从下图中我们也可以看出 Broker 的核心功能点。\n2. 生产者 2.1 三种发送方式 可选 RocketMQ 支持三种消息发送方式：同步发送、异步发送和 One-Way 发送。One-Way 发送时客户端无法确定服务端消息是否投递成功，因此是不可靠的发送方式。异步发送也存在一定几率丢失,\n客户端同步发送消息 时序图:\n2.2. MQClientInstance 的单例模式统一管理维护网络通道，发送消息前只需要做一次服务状态可用性检查即可 2.3. 客户端故障容错机制 MQFaultStrategy 实现了基于 RT 耗时的容错策略。当某个 Broker 的 RT 过大时，认为该 Broker 存在问题，会禁用该 Broker 一段时间\n这样免得再次访问该broker造成性能降低\n3. broker写入 Broker 接收消息时序图\n流程说明\nBroker 通过 Netty 接收 RequestCode 为 SEND_MESSAGE 的请求，并把该请求交给 SendMessageProcessor 进行处理。 SendMessageProcessor 先解析出 SEND_MESSAGE 报文中的消息头信息（Topic、queueId、producerGroup 等），并调用存储层进行处理。 putMessage 中判断当前是否满足写入条件：Broker 状态为 running；Broker 为 master 节点；磁盘状态可写（磁盘满则无法写入）；Topic 长度未超限；消息属性长度未超限；pageCache 未处于繁忙状态（pageCachebusy 的依据是 putMessage 写入 mmap 的耗时，如果耗时超过 1s，说明由于缺页导致页加载慢，此时认定 pageCache 繁忙，拒绝写入）。 从 MappedFileQueue 中选择已经预热过的 MappedFile。 AppendMessageCallback 中执行消息的操作 doAppend，直接对 mmap 后的文件的 bytbuffer 进行写入操作。 3.1 自旋锁减少上下文切换 RocketMQ 的 CommitLog 为了避免并发写入，使用一个 PutMessageLock。PutMessageLock 有 2 个实现版本：PutMessageReentrantLock 和 PutMessageSpinLock。\nPutMessageReentrantLock 是基于 java 的同步等待唤醒机制；PutMessageSpinLock 使用 Java 的 CAS 原语，通过自旋设值实现上锁和解锁。RocketMQ 默认使用 PutMessageSpinLock 以提高高并发写入时候的上锁解锁效率，并减少线程上下文切换次数。\n3.2 MappedFile 预热和零拷贝机制 MappedFile 预热\nRocketMQ 消息写入对延时敏感，为了避免在写入消息时，CommitLog 文件尚未打开或者文件尚未加载到内存引起的 load 的开销，RocketMQ 实现了文件预热机制。项目启动时就开启了线程去预热, 预热代码关键节点如下:\ngraph TD org.apache.rocketmq.broker.BrokerController#initialize --\u0026gt; org.apache.rocketmq.common.ServiceThread#start --\u0026gt; org.apache.rocketmq.store.MappedFile#warmMappedFile Linux 系统在写数据时候不会直接把数据写到磁盘上，而是写到磁盘对应的 PageCache 中，并把该页标记为脏页。当脏页累计到一定程度或者一定时间后再把数据 flush 到磁盘（当然在此期间如果系统掉电，会导致脏页数据丢失）。\n零拷贝\nRocketMQ 选择了 mmap + write 这种零拷贝方式，适用于业务级消息这种小块文件的数据持久化和传输\n而 Kafka 采用的是 sendfile 这种零拷贝方式，适用于系统日志消息这种高吞吐量的大块文件的数据持久化和传输\nJava NIO - 零拷贝实现 | Java 全栈知识体系 (pdai.tech) 3.3 同步和异步刷盘机制 RocketMQ 提供了同步刷盘和异步刷盘两种机制。默认使用异步刷盘机制。\n当 CommitLog 在 putMessage() 中收到 MappedFile 成功追加消息到内存的结果后，便会调用 handleDiskFlush() 方法进行刷盘，将消息存储到文件中。org.apache.rocketmq.store.CommitLog#submitFlushRequest 便会根据两种刷盘策略，调用不同的刷盘服务。\n抽象类 FlushCommitLogService 负责进行刷盘操作，该抽象类有 3 中实现：\nGroupCommitService：同步刷盘\nFlushRealTimeService：异步刷盘\nCommitRealTimeService：异步刷盘并且开启 TransientStorePool\n每个实现类都是一个 ServiceThread 实现类。ServiceThread 可以看做是一个封装了基础功能的后台线程服务。有完整的生命周期管理，支持 start、shutdown、weakup、waitForRunning。\n同步刷盘流程\n所有的 flush 操作都由 GroupCommitService 线程进行处理\n当前接收消息的线程封装一个 GroupCommitRequest，并提交给 GroupCommitService 线程，然后当前线程进入一个 CountDownLatch 的等待\n一旦有新任务进来 GroupCommitService 被立即唤醒，并调用 MappedFile.flush 进行刷盘。底层是调用 mappedByteBuffer.force ()\nflush 完成后唤醒等待中的接收消息线程。从而完成同步刷盘流程\n异步刷盘流程\nRocketMQ 每隔 200ms 进行一次 flush 操作（把数据持久化到磁盘） 当有新的消息写入时候会主动唤醒 flush 线程进行刷盘 当前接收消息线程无须等待 flush 的结果。 3.4 顺序写磁盘 扩展阅读:\nRocketMQ的broker处理消息commit时，加锁应该使用自旋锁还是重入锁以及sendMessageThreadPoolNums-CSDN博客 4. 消费者 消息存储结构\nRocketMQ 的存储结构最大特点：\n所有的消息写入转为顺序写 读写文件分离。通过 ReputMessageService 服务生成 ConsumeQueue 结构说明\nConsumeQueue 与 CommitLog 不同，采用定长存储结构，如下图所示。为了实现定长存储，ConsumeQueue 存储了消息 Tag 的 Hash Code，在进行 Broker 端消息过滤时，通过比较 Consumer 订阅 Tag 的 HashCode 和存储条目中的 Tag Hash Code 是否一致来决定是否消费消息。 ReputMessageService 持续地读取 CommitLog 文件并生成 ConsumeQueue。 4.1 顺序消费与并行消费 串行消费和并行消费最大的区别在于消费队列中消息的顺序性。顺序消费保证了同一个 Queue 中的消费时的顺序性。RocketMQ 的顺序性依赖于分区锁的实现。\n并行消费\n并行消费的实现类为 ConsumeMessageConcurrentlyService。\nPullMessageService 内置一个 scheduledExecutorService 线程池，主要负责处理 PullRequest 请求，从 Broker 端拉取最新的消息返回给客户端。拉取到的消息会放入 MessageQueue 对应的 ProcessQueue。\nConsumeMessageConcurrentlyService 把收到的消息封装成一个 ConsumeRequest，投递给内置的 consumeExecutor 独立线程池进行消费。\nConsumeRequest 调用 MessageListener.consumeMessage 执行用户定义的消费逻辑，返回消费状态。\n如果消费状态为 SUCCESS。则删除 ProcessQueue 中的消息，并提交 offset。\n如果消费状态为 RECONSUME。则把消息发送到延时队列进行重试，并对当前失败的消息进行延迟处理。\n串行消费\n串行消费的实现类为 ConsumeMessageOrderlyService。\nPullMessageService 内置一个 scheduledExecutorService 线程池，主要负责处理 PullRequest 请求，从 Broker 端拉取最新的消息返回给客户端。拉取到的消息会放入 MessageQueue 对应的 ProcessQueue。\nConsumeMessageOrderlyService 把收到的消息封装成一个 ConsumeRequest，投递给内置的 consumeExecutor 独立线程池进行消费。\n消费时首先获取 MessageQueue 对应的 objectLock，保证当前进程内只有一个线程在处理对应的的 MessageQueue, 从 ProcessQueue 的 msgTreeMap 中按 offset 从低到高的顺序取消息，从而保证了消息的顺序性。\nConsumeRequest 调用 MessageListener.consumeMessage 执行用户定义的消费逻辑，返回消费状态。\n如果消费状态为 SUCCESS。则删除 ProcessQueue 中的消息，并提交 offset。\n如果消费状态为 SUSPEND。判断是否达到最大重试次数，如果达到最大重试次数，就把消息投递到死信队列，继续下一条消费；否则消息重试次数 + 1，在延时一段时间后继续重试。\n可见，串行消费如果某条消息一直无法消费成功会造成阻塞，严重时会引起消息堆积和关联业务异常。\n4.2 push消息 RocketMQ支持pull和push操作, 它的push操作是加强版的pull的. 其本质就是一个 长轮询的pull操作, 具体流程如下:\nPullRequest 请求中有个参数 brokerSuspendMaxTimeMillis，默认值为 15s，控制请求 hold 的时长。\nPullMessageProcessor 接收到 Request 后，解析参数，校验 Topic 的 Meta 信息和消费者的订阅关系。对于符合要求的请求，从存储中拉取消息。\n如果拉取消息的结果为 PULL_NOT_FOUND，表示当前 MessageQueue 没有最新消息。\n此时会封装一个 PullRequest 对象，并投递给 PullRequestHoldService 内部线程的 pullRequestTable 中。\nPullRequestHoldService 线程会周期性轮询 pullRequestTable，如果有新的消息或者 hold 时间超时 polling time，就会封装 Response 请求发给客户端。 另外 DefaultMessageStore 中定义了 messageArrivingListener，当产生新的 ConsumeQueue 记录时候，会触发 messageArrivingListener 回调，立即给客户端返回最新的消息。\n长连接机制使得 RocketMQ 的网络利用率非常高效，并且最大限度地降低了消息拉取时的等待开销。实现了毫秒级的消息投递。\nRocketMQ 高性能揭秘 - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/rocketmq/%E9%AB%98%E6%80%A7%E8%83%BD%E4%BF%9D%E8%AF%81.html","summary":"[toc] 1. 整体架构 RocketMQ 主要由 Broker、NameServer、Producer 和 Consumer 组成的一个集群。 NameServer：整个集群的注册中心和配置中","title":"高性能保证"},{"content":"[TOC]\n1. volatile 1.1 volatile的内存语义 在Java中，volatile关键字有特殊的内存语义。volatile主要有以下两个功能：\n保证变量的内存可见性 禁止volatile变量与普通变量重排序（JSR133提出，Java 5 开始才有这个“增强的volatile内存语义”） 这两个功能都是通过内存屏障实现的\npublic class VolatileExample { int a = 0; volatile boolean flag = false; public void writer() { a = 1; // step 1 flag = true; // step 2 } public void reader() { if (flag) { // step 3 System.out.println(a); // step 4 } } } 在这段代码里，我们使用volatile关键字修饰了一个boolean类型的变量flag。\n所谓内存可见性，指的是当一个线程对volatile修饰的变量进行写操作（比如step 2）时，JMM会立即把该线程对应的本地内存中的共享变量的值刷新到主内存；当一个线程对volatile修饰的变量进行读操作（比如step 3）时，JMM会把立即该线程对应的本地内存置为无效，从主内存中读取共享变量的值。\n在这一点上，volatile与锁具有相同的内存效果，volatile变量的写和锁的释放具有相同的内存语义，volatile变量的读和锁的获取具有相同的内存语义。 所以上面的代码是线程安全的\n单纯的赋值操作是原子性的。\n而如果flag变量没有用volatile修饰，在step 2，线程A的本地内存里面的变量就不会立即更新到主内存，那随后线程B也同样不会去主内存拿最新的值，仍然使用线程B本地内存缓存的变量的值a = 0，flag = false。\n工作内存改了值后, 主内存是怎么通知到其他工作内存该值已被修改了?\n答: 基于MESI协议, 缓存一致性协议(MESI只是其中一种)\n状态 描述 M(Modified) 这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。 E(Exclusive) 这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。 S(Shared) 这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中。 I(Invalid) 这行数据无效。 简单来说: cpu为每个变量都记录了状态(MESI四个状态),并通过总线嗅探机制变更每个工作线程中的对应变量的状态,不过这些都是cpu做的,不是JMM做的\n嗅探机制工作原理：每个处理器通过监听在总线上传播的数据来检查自己的缓存值是不是过期了，如果处理器发现自己缓存行对应的内存地址修改，就会将当前处理器的缓存行设置无效状态，当处理器对这个数据进行修改操作的时候，会重新从主内存中把数据读到处理器缓存中。\n注意：基于 CPU 缓存一致性协议，JVM 实现了 volatile 的可见性，但由于总线嗅探机制，会不断的监听总线，如果大量使用 volatile 会引起总线风暴。所以，volatile 的使用要适合具体场景。(synchronize就没有总线风暴)\n总线风暴:需要不断的从主内存嗅探和cas不断循环无效交互导致总线带宽达到峰值,\nvolatile 关键字，你真的理解吗？_Star\u0026rsquo;s Tech Blog-CSDN博客_总线嗅探机制 从volatile关键字到总线风暴_瞎折腾的小码农的博客-CSDN博客 缓存一致性协议（MESI） - 一念永恒乐 - 博客园 (cnblogs.com) 1.2 内存屏障 在linux中也有内存屏障,用的是写屏障 , 它保证了 修改数据和更新指针 之前的线程安全\nJava中的内存屏障是什么 - xyyyn - 博客园 (cnblogs.com) 以下只描述jdk1.8中的内存屏障\n硬件层面，内存屏障分两种：读屏障（Load Barrier）和写屏障（Store Barrier）。内存屏障有两个作用：\n阻止屏障两侧的指令重排序； 强制把写缓冲区/高速缓存中的脏数据等写回主内存，或者让缓存中相应的数据失效。 注意这里的缓存主要指的是CPU缓存，如L1，L2等\n编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。编译器选择了一个比较保守的JMM内存屏障插入策略，这样可以保证在任何处理器平台，任何程序中都能得到正确的volatile内存语义。这个策略是：\n在每个volatile写操作前插入一个StoreStore屏障； 在每个volatile写操作后插入一个StoreLoad屏障； 在每个volatile读操作后插入一个LoadLoad屏障； 在每个volatile读操作后再插入一个LoadStore屏障。 再逐个解释一下这几个屏障。注：下述Load代表读操作，Store代表写操作\nLoadLoad：禁止读和读的重排序 StoreStore：禁止写与写的重排序 LoadStore：禁止读和写的重排序 StoreLoad：禁止写和读的重排序 , 它的开销是四种屏障中最大的（冲刷写缓冲器，清空无效化队列）。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能 Java中的内存屏障是什么_月月鸟的博客-CSDN博客_java 内存屏障 对于连续多个volatile变量读或者连续多个volatile变量写，编译器做了一定的优化来提高性能，比如：\n第一个volatile读;\nLoadLoad屏障；\n第二个volatile读；\nLoadStore屏障\n总之,屏障能少就少,毕竟叠加屏障时,有些屏障就是多余的了\nJMM——volatile与内存屏障_此岸花开-CSDN博客_jmm内存屏障 为什么volatile读前面没有屏障?\n答: 按理来说volatile的读写前后都需要加屏障, 但是volatile读前没有加\n因为JMM认为读比写的概率要高,所以在volatile写后面加屏障次数就会少,性能更好\n从汇编看Volatile的内存屏障 - SegmentFault 思否 更多思考\njava并发编程(二)-volatile写操作前为什么不加LoadStore屏障_公众号:大臭-CSDN博客_loadload屏障 JDK8开始，Java在Unsafe类中提供了三个内存屏障函数。\npublic final class Unsafe{ *** public native void loadFence(); public native void storeFence(); public native void fullFence(); *** } 在JDK9中对JDK定义的三种内存屏障与理论层面划分的四类内存屏障之间的对应进行了说明:\nloadFence = LoadLoad+LoadStore storeFence = StoreStore+LoadStore fullFence = StoreStore+LoadStore+StoreLoad 由于不同的CPU架构不同，重排序的策略不同，所提供的内存屏障也有差异。??? Java中的内存屏障是什么 - xyyyn - 博客园 (cnblogs.com) 2. final 2.1 final关键字的知识点 final成员变量必须在声明的时候初始化或者在构造器中初始化，否则就会报编译错误。final变量一旦被初始化后不能再次赋值(引用不可改变)。 本地变量必须在声明时赋值。 因为没有初始化的过程 接口中声明的所有变量本身是final的。类似于匿名类 在匿名类中所有变量都必须是final变量。java中的String类和Integer类都是final类型的。 final方法不能被重写, final类不能被继承 , final方法调用时使用的是invokespecial指令。 final和abstract这两个关键字是反相关的，final类就不可能是abstract的。 final方法在编译阶段绑定，称为静态绑定(static binding)。 将类、方法、变量声明为final能够提高性能，这样JVM就有机会进行估计，然后优化。 final方法的好处:\n提高了性能，JVM在常量池中会缓存final变量 final变量在多线程中并发安全，无需额外的同步开销 final方法是静态编译的，提高了调用速度 final类创建的对象是只可读的，在多线程可以安全共享 Java中static、final、static final的区别（转） - EasonJim - 博客园 (cnblogs.com) 深入理解final关键字 - 简书 (jianshu.com) 2.2 内存屏障 对于 final 域，编译器和处理器要遵守两个重排序规则：\n在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。(加了一个storestore屏障)\n初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。(加了一个LoadLoad 屏障), 仅针对处理器\n就是说 用 对象的引用,和使用对象中的值 这两个操作不能重排序, 先读对象引用再读final域\n本来这两种操作是存在间接依赖的,大部分处理器都不会重排序,但是有小部分处理仍会重排序(比如 alpha 处理器)\n总结, 从构造函数修改了final域,是能保证线程\u0026quot;安全\u0026quot;的, 构造函数的执行和赋值不能重排序; 构造函数中的final在初始化未完成前,是不可见的\nfinal关键词在各种处理器下的语义:\n深入理解final关键字 - 简书 (jianshu.com) 深入理解Java内存模型（六）——final-InfoQ final的内存语义以及jvm的bug\u0026hellip; - why技术 - 博客园 (cnblogs.com) 2.3 案例 java8 的时间类 LocalDateTime 等时间类就是用final来保证的线程安全,包括它的格式化类\n它整个类都是final的,每次操作LocalDateTime对象都是返回一个新的对象,当然也运用了final对构造函数的一个禁止重排序规则\n高并发之——SimpleDateFormat类的线程安全问题和解决方案_冰河的专栏-CSDN博客 string 类型\n所以string是线程安全的,它每次操作都会返回一个新的对象(不可变的)\nJava String类为什么是final的？\n为了实现字符串池\n为了线程安全\n为了实现String可以创建HashCode不可变性\n它创建的时候HashCode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。\n为什么说String是线程安全的 - 咸咸海风 - 博客园 (cnblogs.com) 3. synchronized 3.1 初识 synchronized 的作用主要有三：\n（1）、原子性：**所谓原子性就是指一个操作或者多个操作，要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。**被synchronized修饰的类或对象的所有操作都是原子的，因为在执行操作之前必须先获得类或对象的锁，直到执行完才能释放。 （2）、可见性：**可见性是指多个线程访问一个资源时，该资源的状态、值信息等对于其他线程都是可见的。 **synchronized对一个类或对象加锁，这个锁的状态对于其他任何线程都是可见的，并且在释放锁之前会将对变量的修改刷新到共享内存当中，保证资源变量的可见性。 （3）、有序性：有序性值程序执行的顺序按照代码先后执行。 synchronized保证了每个时刻都只有一个线程访问同步代码块，也就确定了线程执行同步代码块是分先后顺序的，保证了有序性。 synchronized详解 - 三分恶 - 博客园 (cnblogs.com) 首先需要明确的一点是：Java多线程的锁都是基于对象的，Java中的每一个对象都可以作为一个锁。\n还有一点需要注意的是，我们常听到的类锁其实也是对象锁。\nJava类只有一个Class对象（可以有多个实例对象，多个实例共享这个Class对象），而Class对象也是特殊的Java对象。所以我们常说的类锁，其实就是Class对象的锁。\nsynchronized 是非公平锁，可以重入。\nsynchronized与锁 · 深入浅出Java多线程 (redspider.group) synchronized不能保证代码块内指令重排序, 这也是为什么双检锁中要加volatile\n阿里面试：Java的synchronized 能防止指令重排序吗？_root-CSDN博客 synchronized 是公平锁吗？可以重入吗？详细的来说说 synchronized 3.2 锁原理 Synchronized主要有三种用法：\n（1）、修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁\nsynchronized void method() { //业务代码 } （2）、修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份）。所以，如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。\nsynchronized void staic method() { //业务代码 } （3）、修饰代码块 ：指定加锁对象，对给定对象/类加锁。synchronized(this|object) 表示进入同步代码库前要获得给定对象的锁。synchronized(类.class) 表示进入同步代码前要获得 当前 class 的锁 synchronized(this) { //业务代码 } 简单总结一下：\nsynchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁。\nsynchronized 关键字加到实例方法上是给对象实例上锁。\n// ① 关键字在实例方法上，锁为当前实例 public synchronized void instanceLock() { // code } // ② 关键字在代码块上，锁为括号里面的对象 public void blockLock() { synchronized (this) { // code } } // ③ 关键字在静态方法上，锁为当前Class对象 public static synchronized void classLock() { // code } //④ 关键字在代码块上，锁为括号里面的对象 public void blockLock() { synchronized (this.getClass()) { // code } } //⑤ 关键字在代码块上，锁为括号里面的对象 public void blockLock() { Object o = new Object(); synchronized (o) { // code } } 从效果上来看\n①和② 是等价的, 实例方法是被new出来的对象使用,所以synchronize修饰在方法上和锁定this(当前使用这个方法的人,就是new出来的对象嘛)是一样的.\n④和⑤是等价的, static方法由类调用,所以④和⑤是一样的,这时锁住的对象是class对象\nsynchronized - 八锁问题及扩展_江南烟雨却痴缠丶-CSDN博客 我们这里介绍一下“临界区”的概念。所谓“临界区”，指的是某一块代码区域，它同一时刻只能由一个线程执行。在上面的例子中，如果synchronized关键字在方法上，那临界区就是整个方法内部。而如果是使用synchronized代码块，那临界区就指的是代码块内部的区域。\n3.2.1 synchronized 同步语句块原理 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\u0026#34;synchronized 代码块\u0026#34;); } } } javap -c -s -v -l SynchronizedDemo.class 查看汇编代码\nsynchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置， monitorexit 指令则指明同步代码块的结束位置。**\n当执行 monitorenter 指令时，线程试图获取锁也就是获取 对象监视器 monitor 的持有权。\n第二个 monitorexit 是来处理异常的，正常情况下第一个 monitorexit 之后会执行后面指令，而该指令转向的就是 23 行的return，也就是说正常情况下只会执行第一个 monitorexit 释放锁，然后返回。而如果在执行中发生了异常，第二个 monitorexit 就起作用了，它是由编译器自动生成的，在发生异常时处理异常然后释放掉锁。\n在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由ObjectMonitor实现的。每个对象中都内置了一个 ObjectMonitor对象。\n另外，wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法且这些方法在Object对象就有，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。\n在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。\n在执行 monitorexit 指令后，将锁计数器减 1，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。\nSynchronized原理 - 掘金 (juejin.cn) jvm指令monitorenter，monitorexit与synchronization关键字_Dreamer 科技-CSDN博客_monitorexit 3.2.2 synchronized 修饰方法原理 public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\u0026#34;synchronized 方法\u0026#34;); } } 汇编之后\nsynchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。\n简单总结一下：\nsynchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。\nsynchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。\n不过两者的本质都是对对象监视器 monitor 的获取。\n3.3 synchronized同步概念 3.3.1. Java对象头 在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充。\nsynchronized用的锁是存在Java对象头里的。\nHotspot 有两种对象头：\n数组类型，如果对象是数组类型，则虚拟机用3个字宽 （Word）存储对象头 非数组类型：如果对象是非数组类型，则用2字宽存储对象头。 对象头由两部分组成\nMark Word：存储自身的运行时数据，例如 HashCode、GC 年龄、锁相关信息等内容。 Klass Pointer：类型指针指向它的类元数据的指针。 64 位虚拟机 Mark Word 是 64bit，在运行期间，Mark Word里存储的数据会随着锁标志位的变化而变化。\n扩展阅读: JVM中的Java对象头 - 知乎 (zhihu.com) 3.3.2 监视器（Monitor） 任何一个对象都有一个Monitor与之关联，当且一个Monitor被持有后，它将处于锁定状态。Synchronized在JVM里的实现都是 基于进入和退出Monitor对象来实现方法同步和代码块同步，虽然具体实现细节不一样，但是都可以通过成对的MonitorEnter和MonitorExit指令来实现。\nMonitorEnter指令：插入在同步代码块的开始位置，当代码执行到该指令时，将会尝试获取该对象Monitor的所有权，即尝试获得该对象的锁； MonitorExit指令：插入在方法结束处和异常处，JVM保证每个MonitorEnter必须有对应的MonitorExit； 那什么是Monitor？可以把它理解为 一个同步工具，也可以描述为 一种同步机制，它通常被 描述为一个对象。\n与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质，因为在Java的设计中 ，每一个Java对象自打娘胎里出来就带了一把看不见的锁，它叫做内部锁或者Monitor锁。\n也就是通常说Synchronized的对象锁，MarkWord锁标识位为10，其中指针指向的是Monitor对象的起始地址。无论是ACC_SYNCHRONIZED还是monitorenter、monitorexit都是基于Monitor实现的，在Java虚拟机(HotSpot)中，Monitor是基于C++实现的，由ObjectMonitor实现。\nsynchronized原理及优化 - 简书 (jianshu.com) 3.4 synchronized优化 从JDK5引入了现代操作系统新增加的CAS原子操作（ JDK5中并没有对synchronized关键字做优化，而是体现在J.U.C中，所以在该版本concurrent包有更好的性能 ），从JDK6开始，就对synchronized的实现机制进行了较大调整，包括使用JDK5引进的CAS自旋之外，还增加了自适应的CAS自旋、锁消除、锁粗化、偏向锁、轻量级锁这些优化策略。由于此关键字的优化使得性能极大提高，同时语义清晰、操作简单、无需手动关闭，所以推荐在允许的情况下尽量使用此关键字，同时在性能上此关键字还有优化的空间。\n锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁。但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级。\n实际上HotSpot JVM 是支持锁降级的\n降级的目的和过程：\n因为BasicLocking的实现优先于重量级锁的使用，JVM会尝试在SWT的停顿中对处于“空闲(idle)”状态的重量级锁进行降级(deflate)。这个降级过程是如何实现的呢？我们知道在STW时，所有的Java线程都会暂停在“安全点(SafePoint)”，此时VMThread通过对所有Monitor的遍历，或者通过对所有依赖于MonitorInUseLists值的当前正在“使用”中的Monitor子序列进行遍历，从而得到哪些未被使用的“Monitor”作为降级对象。\n可以降级的Monitor对象：\n重量级锁的降级发生于STW阶段，降级对象就是那些仅仅能被VMThread访问而没有其他JavaThread访问的对象。\nHotspot JVM锁是否可以降级？ - 知乎 (zhihu.com) 3.4.1 偏向锁 在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低，引进了偏向锁。偏向锁使用了一种等到竞争出现才释放锁的机制,一旦出现竞争,就会撤销偏向锁,并加轻量级锁\n当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。\n偏向锁的撤销，需要在某个时间点上没有字节码正在执行时，先暂停拥有偏向锁的线程，然后判断锁对象是否处于被锁定状态。如果线程不处于活动状态，则将对象头设置成无锁状态，并撤销偏向锁；\n偏向锁升级成轻量级锁时，会暂停拥有偏向锁的线程，重置偏向锁标识，这个过程看起来容易，实则开销还是很大的，大概的过程如下：\n在一个安全点（在这个时间点上没有字节码正在执行）停止拥有锁的线程。 遍历线程栈，如果存在锁记录的话，需要修复锁记录和Mark Word，使其变成无锁状态。 唤醒被停止的线程，将当前锁升级成轻量级锁。 所以，如果应用程序里所有的锁通常处于竞争状态，那么偏向锁就会是一种累赘，对于这种情况，我们可以一开始就把偏向锁这个默认功能给关闭：\n-XX:UseBiasedLocking=false 3.4.2 轻量级锁 引入轻量级锁的主要目的是 在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。当关闭偏向锁功能或者多个线程竞争偏向锁导致偏向锁升级为轻量级锁，则会尝试获取轻量级锁。\n（1）轻量级锁加锁\n线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用 CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。若自旋达到一定次数后仍拿不到锁,则会升级为重量级锁,\njvm采用 适应性自旋，简单来说就是线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。 具体值与jvm和操作系统有关\n也有自旋次数,默认是10次\n（2）轻量级锁解锁\n轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成 功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。\n由图可知, 线程1先拿到轻量级锁,导致线程2自旋升级为重量级锁并阻塞,线程1解锁时发现锁已升级,就用重量级锁的方式释放锁,然后唤醒那些阻塞的线程\n3.4.3 重量级锁 重量级锁依赖于操作系统的互斥量（mutex） 实现的，而操作系统中线程间状态的转换需要相对比较长的时间，所以重量级锁效率很低，但被阻塞的线程不会消耗CPU。\n前面说到，每一个对象都可以当做一个锁，当多个线程同时请求某个对象锁时，对象锁会设置几种状态用来区分请求的线程：\nContention List：所有请求锁的线程将被首先放置到该竞争队列 Entry List：Contention List中那些有资格成为候选人的线程被移到Entry List Wait Set：那些调用wait方法被阻塞的线程被放置到Wait Set OnDeck：任何时刻最多只能有一个线程正在竞争锁，该线程称为OnDeck Owner：获得锁的线程称为Owner !Owner：释放锁的线程 当一个线程尝试获得锁时，如果该锁已经被占用，则会将该线程封装成一个ObjectWaiter对象插入到Contention List的队列的队首，然后调用park函数挂起当前线程。\n当线程释放锁时，会从Contention List或EntryList中挑选一个线程唤醒，被选中的线程叫做Heir presumptive即假定继承人，假定继承人被唤醒后会尝试获得锁，但synchronized是非公平的，所以假定继承人不一定能获得锁。这是因为对于重量级锁，线程先自旋尝试获得锁，这样做的目的是为了减少执行操作系统同步操作带来的开销。如果自旋不成功再进入等待队列。这对那些已经在等待队列中的线程来说，稍微显得不公平，还有一个不公平的地方是自旋线程可能会抢占了Ready线程的锁。\n如果线程获得锁后调用Object.wait方法，则会将线程加入到WaitSet中，当被Object.notify唤醒后，会将线程从WaitSet移动到Contention List或EntryList中去。需要注意的是，当调用一个锁对象的wait或notify方法时，如当前锁的状态是偏向锁或轻量级锁则会先膨胀成重量级锁。\n3.4.4 总结锁的升级流程 每一个线程在准备获取共享资源时： 第一步，检查MarkWord里面是不是放的自己的ThreadId ,如果是，表示当前线程是处于 “偏向锁” 。\n第二步，如果MarkWord不是自己的ThreadId，锁升级，这时候，用CAS来执行切换，新的线程根据MarkWord里面现有的ThreadId，通知之前线程暂停，之前线程将Markword的内容置为空。\n第三步，两个线程都把锁对象的HashCode复制到自己新建的用于存储锁的记录空间，接着开始通过CAS操作， 把锁对象的MarKword的内容修改为自己新建的记录空间的地址的方式竞争MarkWord。\n第四步，第三步中成功执行CAS的获得资源，失败的则进入自旋 。\n第五步，自旋的线程在自旋过程中，成功获得资源(即之前获的资源的线程执行完成并释放了共享资源)，则整个状态依然处于 轻量级锁的状态，如果自旋失败 。\n第六步，进入重量级锁的状态，这个时候，自旋的线程进行阻塞，等待之前线程执行完成并唤醒自己。\n可以看简单的锁升级流程 另一角度理解:\nHotSpot VM采用三中不同的方式实现了对象监视器——Object Monitor，并且可以在这三种实现方式中自动切换。\n偏向锁通过在Java对象的对象头markOop中install一个JavaThread指针的方式实现了这个Java对象对此Java线程的偏向，并且只有该偏向线程能够锁定Lock该对象。(偏向锁)\n但是只要有第二个Java线程企图锁定这个已被偏向的对象时，偏向锁就不再满足这种情况了，然后呢JVM就将Biased Locking切换成了Basic Locking(基本对象锁)。(轻量级锁)\nBasic Locking使用CAS操作确保多个Java线程在此对象锁上互斥执行。如果CAS由于竞争而失败(第二个Java线程试图锁定一个正在被其他Java线程持有的对象)，这时基本对象锁因为不再满足需要从而JVM会切换到膨胀锁 - ObjectMonitor。(重量级锁)\n不像偏向锁和基本对象锁的实现，重量级锁的实现需要在Native的Heap空间中分配内存，然后指向该空间的内存指针会被装载到Java对象中去。这个过程我们称之为锁膨胀。\nHotspot JVM锁是否可以降级？ - 知乎 (zhihu.com) 3.4.5 锁消除 锁消除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持，如果判断在一段代码中，堆上的所有数据都不会逃逸出去从而被其他线程访问到，那就可以把它们当做栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行。\n// java.lang.StringBuffer#append(java.lang.String) @Override public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this; } 每个StringBuffer.append（） 方法中都有一个同步块，锁就是 sb 对象。虚拟机观察变量sb，很快就会发现它的动态作用域被限制在 concatString（） 方法内部。也就是说，sb 的所有引用永远不会 “逃逸” 到 concatString（）方法之外，其他线程无法访问到它，因此，虽然这里有锁，但是可以被安全地消除掉，在即时编译之后，这段代码就会忽略掉所有的同步而直接执行了。\n锁消除: 如果加了锁的,但是发现没人会和它竞争,这把锁就会被消除\n3.4.6 锁粗化 原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，只在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快拿到锁。\n大部分情况下，上面的原则都是正确的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。\n上述代码中连续的append（）方法就属于这类情况。如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展 （粗化）到整个操作序列的外部，以上述代码为例，就是扩展到第一个 append（）操作之前直至最后一个 append（）操作之后，这样只需要加锁一次就可以了。\n锁粗化: 发现我们的频繁在一个对象上加锁/释放锁, 就会把这把锁(作用域)放大,全部包起来,一次加锁一次解锁就够了\n9 synchronized与锁 · 深入浅出Java多线程 (redspider.group) synchronized详解 - 三分恶 - 博客园 (cnblogs.com) 扩展阅读: 死磕Synchronized底层实现\u0026ndash;概论 · Issue #12 · farmerjohngit/myblog (github.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%85%B3%E9%94%AE%E8%AF%8D.html","summary":"[TOC] 1. volatile 1.1 volatile的内存语义 在Java中，volatile关键字有特殊的内存语义。volatile主要有以下两个功能： 保证变量的内存可","title":"关键词"},{"content":"[TOC]\n介绍 在我们实际开发中，一个web应用可能要在多个地区使用，面对不同地区的不同语言，为了适应不同的用户，我们可以尝试在前端页面实现多语言的支持，那么同样对于后端返回的一些提示信息，异常信息等，我们后端也可以根据不同的语言环境来进行国际化处理，返回相应的信息。\n但是这里我只用于做返回码,因为天启的返回码用得很太笨重\n因为国际化会以key-val形式读取配置文件,很方便取,其本质通过流将数据写到properties中再写到map中\n@SuppressWarnings({\u0026#34;unchecked\u0026#34;, \u0026#34;rawtypes\u0026#34;}) public PropertyResourceBundle (InputStream stream) throws IOException { Properties properties = new Properties(); properties.load(stream); lookup = new HashMap(properties); } 只能用properties文件\n使用 application.properties\n# 指定默认properties文件, # 默认为 : messages , 形如: # 1. messages.properties 默认文件(在指定语言中找不到配置时会来找这个文件) # 2. messages_zh_CN.properties 简体中文 spring.messages.basename=application-responseCode application-responseCode.properties\nJS001=成功 hello=你好：{0} ， 你的验证码为 ：{1} controller\n@Autowired private MessageSource msg; @RequestMapping(\u0026#34;hello\u0026#34;) public String hello() { String message = msg.getMessage(\u0026#34;JS001\u0026#34;,null,\u0026#34;默认值\u0026#34; LocaleContextHolder.getLocale());// local写啥都一样,反正用默认配置 String message1=msg.getMessage(\u0026#34;hello\u0026#34;, new Object[]{\u0026#34;zhangsan\u0026#34;,\u0026#34;123456\u0026#34;}, LocaleContextHolder.getLocale());// 会替换配置文件中的两个值 return message; // 就是配置文件中的值 } String getMessage(String var1, @Nullable Object[] var2, @Nullable String var3, Locale var4);\n用来从MessageSource获取消息的基本方法。如果在指定的locale中没有找到消息，则使用默认的消息。var2中的参数将使用标准类库中的MessageFormat来作消息中替换值。\n还有两个方法,反正就是封装的嘛,自己看\n来自: https://blog.csdn.net/qq_33619378/article/details/89362747 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%9B%BD%E9%99%85%E5%8C%96.html","summary":"[TOC] 介绍 在我们实际开发中，一个web应用可能要在多个地区使用，面对不同地区的不同语言，为了适应不同的用户，我们可以尝试在前端页面实现多语言的支","title":"国际化"},{"content":"[toc]\n条件函数 assert_true(BOOLEAN condition) 解释 如果condition不为true，则抛出异常，否则返回null\n使用案例 select assert_true(1\u0026lt;2) -- 返回null select assert_true(1\u0026gt;2) -- 抛出异常 coalesce(T v1, T v2, \u0026hellip;) 解释 返回第一个不为null的值，如果都为null，则返回null\n使用案例 select coalesce(null,1,2,null) -- 返回1 select coalesce(1,null) -- 返回1 select coalesce(null,null) -- 返回null if if ( BOOLEAN testCondition, T valueTrue , T valueFalseOrNull )\n解释 如果testCondition条件为true，则返回第一个值，否则返回第二个值\n使用案例 select if(1 is null,0,1) -- 返回1 select if(null is null,0,1) -- 返回0 isnotnull(a) 解释 如果参数a不为null，则返回true，否则返回false\n使用案例 select isnotnull(1) -- 返回true select isnotnull(null) -- 返回false isnull(a) 解释 与isnotnull相反，如果参数a为null，则返回true，否则返回false\n使用案例 select isnull(null) -- 返回true select isnull(1) -- 返回false nullif(a, b) 解释 如果参数a=b，返回null，否则返回a值(Hive2.2.0版本)\n使用案例 select nullif(1,2) -- 返回1 select nullif(1,1) -- 返回null nvl(T value, T default_value) 解释 如果value的值为null，则返回default_value默认值，否则返回value的值。在null值判断时，可以使用if函数给定默认值，也可以使用此函数给定默认值，使用该函数sql特别简洁。\n使用案例 select nvl(1,0) -- 返回1 select nvl(null,0) -- 返回0 日期函数 add_months add_month( DATE | STRING | TIMESTAMP start_date , INT num_months)\n解释 start_date参数可以是string, date 或者timestamp类型，num_months参数时int类型。返回一个日期，该日期是在start_date基础之上加上num_months个月，即start_date之后null_months个月的一个日期。如果start_date的时间部分的数据会被忽略。注意：如果start_date所在月份的天数大于结果日期月的天数，则返回结果月的最后一天的日期。\n使用案例 select add_months(\u0026#34;2020-05-20\u0026#34;,2); -- 返回2020-07-20 select add_months(\u0026#34;2020-05-20\u0026#34;,8); -- 返回2021-01-20 select add_months(\u0026#34;2020-05-31\u0026#34;,1); -- 返回2020-06-30,5月有31天，6月只有30天，所以返回下一个月的最后一天 current_date 解释 返回查询时刻的当前日期\n使用案例 select current_date() -- 返回当前查询日期2020-05-20 current_timestamp() 解释 返回查询时刻的当前时间\n使用案例 select current_timestamp() -- 2020-05-20 14:40:47.273 datediff(STRING enddate, STRING startdate) 解释 返回开始日期startdate与结束日期enddate之前相差的天数\n使用案例 select datediff(\u0026#34;2020-05-20\u0026#34;,\u0026#34;2020-05-21\u0026#34;); -- 返回-1 select datediff(\u0026#34;2020-05-21\u0026#34;,\u0026#34;2020-05-20\u0026#34;); -- 返回1 date_add(DATE startdate, INT days) 解释 在startdate基础上加上几天，然后返回加上几天之后的一个日期\n使用案例 select date_add(\u0026#34;2020-05-20\u0026#34;,1); -- 返回2020-05-21,1表示加1天 select date_add(\u0026#34;2020-05-20\u0026#34;,-1); -- 返回2020-05-19，-1表示减一天 date_sub(DATE startdate, INT days) 解释 在startdate基础上减去几天，然后返回减去几天之后的一个日期,功能与date_add很类似\n使用案例 select date_sub(\u0026#34;2020-05-20\u0026#34;,1); -- 返回2020-05-19,1表示减1天 select date_sub(\u0026#34;2020-05-20\u0026#34;,-1); -- 返回2020-05-21，-1表示加1天 date_format(DATE|TIMESTAMP|STRING ts, STRING fmt) 解释 将date/timestamp/string类型的值转换为一个具体格式化的字符串。支持java的SimpleDateFormat格式，第二个参数fmt必须是一个常量\n使用案例 select date_format(\u0026#39;2020-05-20\u0026#39;, \u0026#39;yyyy\u0026#39;); -- 返回2020 select date_format(\u0026#39;2020-05-20\u0026#39;, \u0026#39;MM\u0026#39;); -- 返回05 select date_format(\u0026#39;2020-05-20\u0026#39;, \u0026#39;dd\u0026#39;); -- 返回20 -- 返回2020年05月20日 00时00分00秒 select date_format(\u0026#39;2020-05-20\u0026#39;, \u0026#39;yyyy年MM月dd日 HH时mm分ss秒\u0026#39;) ; select date_format(\u0026#39;2020-05-20\u0026#39;, \u0026#39;yy/MM/dd\u0026#39;) -- 返回 20/05/20 dayofmonth(STRING date) 解释 返回一个日期或时间的天,与day()函数功能相同\n使用案例 select dayofmonth(\u0026#39;2020-05-20\u0026#39;) -- 返回20 extract(field FROM source) 时间截取函数 解释 提取 day, dayofweek, hour, minute, month, quarter, second, week 或者year的值，field可以选择day, dayofweek, hour, minute, month, quarter, second, week 或者year，source必须是一个date、timestamp或者可以转为 date 、timestamp的字符串。注意：Hive 2.2.0版本之后支持该函数\n使用案例 select extract(year from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回2020，年 select extract(quarter from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回2，季度 select extract(month from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回05，月份 select extract(week from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回21，同weekofyear，一年中的第几周 select extract(dayofweek from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回4,代表星期三 select extract(day from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回20，天 select extract(hour from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回15，小时 select extract(minute from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回21，分钟 select extract(second from \u0026#39;2020-05-20 15:21:34.467\u0026#39;); -- 返回34，秒 year(STRING date) 解释 返回时间的年份,可以用extract函数替代\n使用案例 select year(\u0026#39;2020-05-20 15:21:34\u0026#39;); -- 返回2020 quarter(DATE|TIMESTAMP|STRING a) 解释 返回给定时间或日期的季度，1至4个季度,可以用extract函数替代\n使用案例 select quarter(\u0026#39;2020-05-20 15:21:34\u0026#39;); -- 返回2，第2季度 month(STRING date) 解释 返回时间的月份,可以用extract函数替代\n使用案例 select month(\u0026#39;2020-05-20 15:21:34\u0026#39;) -- 返回5 day(STRING date), 解释 返回一个日期或者时间的天,可以用extract函数替代\n使用案例 select day(\u0026#34;2020-05-20\u0026#34;); -- 返回20 select day(\u0026#34;2020-05-20 15:05:27.5\u0026#34;); -- 返回20 hour(STRING date) 解释 返回一个时间的小时,可以用extract函数替代\n使用案例 select hour(\u0026#39;2020-05-20 15:21:34\u0026#39;);-- 返回15 minute(STRING date) 解释 返回一个时间的分钟值,可以用extract函数替代\n使用案例 select minute(\u0026#39;2020-05-20 15:21:34\u0026#39;); -- 返回21 second(STRING date) 解释 返回一个时间的秒,可以用extract函数替代\n使用案例 select second(\u0026#39;2020-05-20 15:21:34\u0026#39;); --返回34 from_unixtime(BIGINT unixtime [, STRING format]) 解释 将将Unix时间戳转换为字符串格式的时间(比如yyyy-MM-dd HH:mm:ss格式)\n使用案例 select from_unixtime(1589960708); -- 返回2020-05-20 15:45:08 select from_unixtime(1589960708, \u0026#39;yyyy-MM-dd hh:mm:ss\u0026#39;); -- -- 返回2020-05-20 15:45:08 select from_unixtime(1589960708, \u0026#39;yyyy-MM-dd\u0026#39;); -- 返回2020-05-20 from_utc_timestamp(T a, STRING timezone) 解释 转换为特定时区的时间\n使用案例 select from_utc_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;,\u0026#39;PST\u0026#39;); -- 返回2020-05-20 08:21:34.0 select from_utc_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;,\u0026#39;GMT\u0026#39;); -- 返回2020-05-20 15:21:34.0 select from_utc_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;,\u0026#39;UTC\u0026#39;); -- 返回2020-05-20 15:21:34.0 select from_utc_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;,\u0026#39;DST\u0026#39;); -- 返回2020-05-20 15:21:34.0 select from_utc_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;,\u0026#39;CST\u0026#39;); -- 返回2020-05-20 10:21:34.0 last_day(STRING date) 解释 返回给定时间或日期所在月的最后一天，参数可以是\u0026rsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo; 或者 \u0026lsquo;yyyy-MM-dd\u0026rsquo;类型，时间部分会被忽略\n使用案例 select last_day(\u0026#39;2020-05-20 15:21:34\u0026#39;); -- 返回2020-05-31 select last_day(\u0026#39;2020-05-20\u0026#39;); -- 返回2020-05-31 to_date(STRING timestamp) 解释 返回一个字符串时间的日期部分，去掉时间部分，2.1.0之前版本返回的是string，2.1.0版本及之后返回的是date\n使用案例 select to_date(\u0026#39;2020-05-20 15:21:34\u0026#39;); -- 返回2020-05-20 select to_date(\u0026#39;2020-05-20\u0026#39;); -- 返回2020-05-20 to_utc_timestamp(T a, STRING timezone) 解释 转换为世界标准时间UTC的时间戳,与from_utc_timestamp类似\n使用案例 select to_utc_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;, \u0026#39;GMT\u0026#39;); -- 返回2020-05-20 15:21:34.0 trunc(STRING date, STRING format) 截取年或月的第一天 解释 截断日期到指定的日期精度，仅支持月（MONTH/MON/MM）或者年（YEAR/YYYY/YY）\n使用案例 select trunc(\u0026#39;2020-05-20\u0026#39;, \u0026#39;YY\u0026#39;); -- 返回2020-01-01，返回年的1月1日 select trunc(\u0026#39;2020-05-20\u0026#39;, \u0026#39;MM\u0026#39;); -- 返回2020-05-01，返回月的第一天 select trunc(\u0026#39;2020-05-20 15:21:34\u0026#39;, \u0026#39;MM\u0026#39;); -- 返回2020-05-01 unix_timestamp([STRING date [, STRING pattern]]) 解释 参数时可选的，当参数为空时，返回当前Unix是时间戳，精确到秒。可以指定一个具体的日期，转换为Unix时间戳格式\n使用案例 -- 返回1589959294 select unix_timestamp(\u0026#39;2020-05-20 15:21:34\u0026#39;,\u0026#39;yyyy-MM-dd hh:mm:ss\u0026#39;); -- 返回1589904000 select unix_timestamp(\u0026#39;2020-05-20\u0026#39;,\u0026#39;yyyy-MM-dd\u0026#39;); weekofyear(STRING date) 解释 返回一个日期或时间在一年中的第几周，可以用extract替代\n使用案例 select weekofyear(\u0026#39;2020-05-20 15:21:34\u0026#39;); -- 返回21，第21周 select weekofyear(\u0026#39;2020-05-20\u0026#39;); -- 返回21，第21周 next_day(STRING start_date, STRING day_of_week) 解释 参数start_date可以是一个时间或日期，day_of_week表示星期几，比如Mo表示星期一，Tu表示星期二，Wed表示星期三，Thur表示星期四，Fri表示星期五，Sat表示星期六，Sun表示星期日。如果指定的星期几在该日期所在的周且在该日期之后，则返回当周的星期几日期，如果指定的星期几不在该日期所在的周，则返回下一个星期几对应的日期\n使用案例 select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Mon\u0026#39;);-- 返回当前日期的下一个周一日期:2020-05-25 select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Tu\u0026#39;);-- 返回当前日期的下一个周二日期:2020-05-26 select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Wed\u0026#39;);-- 返回当前日期的下一个周三日期:2020-05-27 -- 2020-05-20为周三，指定的参数为周四，所以返回当周的周四就是2020-05-21 select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Th\u0026#39;); select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Fri\u0026#39;);-- 返回周五日期2020-05-22 select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Sat\u0026#39;); -- 返回周六日期2020-05-23 select next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;Sun\u0026#39;); -- 返回周六日期2020-05-24 该函数比较重要：比如取当前日期所在的周一和周日，通过长用在按周进行汇总数据\nselect date_add(next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;MO\u0026#39;),-7); -- 返回当前日期的周一日期2020-05-18 select date_add(next_day(\u0026#39;2020-05-20\u0026#39;,\u0026#39;MO\u0026#39;),-1); -- 返回当前日期的周日日期2020-05-24 months_between(DATE|TIMESTAMP|STRING date1, DATE|TIMESTAMP|STRING date2) 解释 返回 date1 和 date2 的月份差。如果date1大于date2，返回正值，否则返回负值，如果是相减是整数月，则返回一个整数，否则会返回小数\n使用案例 select months_between(\u0026#39;2020-05-20\u0026#39;,\u0026#39;2020-05-20\u0026#39;); -- 返回0 select months_between(\u0026#39;2020-05-20\u0026#39;,\u0026#39;2020-06-20\u0026#39;); -- 返回-1 -- 相差的整数月 select months_between(\u0026#39;2020-06-30\u0026#39;,\u0026#39;2020-05-31\u0026#39;); -- 返回1 -- 非整数月，一个月差一天 select months_between(\u0026#39;2020-06-29\u0026#39;,\u0026#39;2020-05-31\u0026#39;); -- 返回0.93548387 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%87%BD%E6%95%B0.html","summary":"[toc] 条件函数 assert_true(BOOLEAN condition) 解释 如果condition不为true，则抛出异常，否则返回null 使用案例 select assert_true(1\u0026lt;2) -- 返回null select assert_true(1\u0026gt;2) -- 抛出异常 coalesce(T v1, T v2, \u0026hellip;) 解释 返回","title":"函数"},{"content":"红黑树是一种特定类型的二叉树 ，它是在计算机科学中用来组织数据比如数字的块的一种结构。若一棵二叉查找树是红黑树，则它的任一子树必为红黑树. [4]\n红黑树是一种平衡二叉查找树的变体，它的左右子树高差有可能大于 1，所以红黑树不是严格意义上的平衡二叉树 （AVL），但 对之进行平衡的代价较低， 其平均统计性能要强于 AVL 。 [2]\n由于每一颗红黑树都是一颗二叉排序树，因此，在对红黑树进行查找时，可以采用运用于普通二叉排序树上的查找算法，在查找过程中不需要颜色信息。 [5]\n恢复红黑树的属性需要少量(O(log n))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。 虽然插入和删除很复杂，但操作时间仍可以保持为 O(log n) 次 。\n特征:\n节点是红色或黑色。 [3]\n根节点是黑色。 [3]\n所有叶子都是黑色。（叶子是NUIL节点） [3]\n每个红色节点的两个子节点都是黑色。（从每个叶子到根的所有路径上不能有两个连续的红色节点）\n从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 [3]\nhttps://www.cnblogs.com/gofighting/p/5437998.html https://baijiahao.baidu.com/s?id=1641940303518144126\u0026wfr=spider\u0026for=pc https://hacpai.com/article/1578230896592 标准的二叉树: 右子节点比父节点大,左子节点比父节点小,查找的时候就实现了折半查找了\n每次插入和删除时,都会改变树的结构,此时可能会破坏红黑树的规则,则有两种操作来继续保持规则\u0026mdash;-变色,(左/右)旋转\n变色: 把红变黑, 或者黑变红,(尽可能的使其符合规则)\n旋转: 变色不能满足时,则通过旋转,看图\n左旋:\n变成了\n右旋转:\n变成了\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E7%BA%A2%E9%BB%91%E6%A0%91.html","summary":"红黑树是一种特定类型的二叉树 ，它是在计算机科学中用来组织数据比如数字的块的一种结构。若一棵二叉查找树是红黑树，则它的任一子树必为红黑树. [4] 红","title":"红黑树"},{"content":"[TOC]\n前言 和大多数持久化框架一样，MyBatis 提供了一级缓存和二级缓存的支持。默认情况下，MyBatis 只开启一级缓存。\nMyBatis提供了一级缓存和二级缓存\n一级缓存：也称为本地缓存，用于保存用户在一次会话过程中查询的结果，用户一次会话中只能使用一个sqlSession，一级缓存是自动开启的，不允许关闭。 二级缓存：也称为全局缓存，是mapper级别的缓存，是针对一个表的查结果的存储，可以共享给所有针对这张表的查询的用户。也就是说对于mapper级别的缓存不同的sqlsession是可以共享的。 一级缓存 一级缓存是基于 PerpetualCache（MyBatis自带）的 HashMap 本地缓存，作用范围为 session 域内,它存储的SqlSession中的BaseExecutor之中。当 session flush（刷新）或者 close（关闭）之后，该 session 中所有的 cache（缓存）就会被清空。\n在参数和 SQL 完全一样的情况下，我们使用同一个 SqlSession 对象调用同一个 mapper 的方法，往往只执行一次 SQL。因为使用 SqlSession 第一次查询后，MyBatis 会将其放在缓存中，再次查询时，如果没有刷新，并且缓存没有超时的情况下，SqlSession 会取出当前缓存的数据，而不会再次发送 SQL 到数据库。\n由于 SqlSession 是相互隔离的，所以如果你使用不同的 SqlSession 对象，即使调用相同的 Mapper、参数和方法，MyBatis 还是会再次发送 SQL 到数据库执行，返回结果。\n示例:\npackage net.biancheng.test; import java.io.IOException; import java.io.InputStream; import java.util.List; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import org.apache.log4j.Logger; import net.biancheng.po.Website; public class Test { public static Logger logger = Logger.getLogger(Test.class); public static void main(String[] args) throws IOException { InputStream config = Resources.getResourceAsStream(\u0026#34;mybatis-config.xml\u0026#34;); // 根据配置文件构建 SqlSessionFactory ssf = new SqlSessionFactoryBuilder().build(config); SqlSession ss = ssf.openSession(); Website site = ss.selectOne(\u0026#34;net.biancheng.mapper.WebsiteMapper.selectWebsiteById\u0026#34;, 1); logger.debug(\u0026#34;使用同一个sqlsession再执行一次\u0026#34;); Website site2 = ss.selectOne(\u0026#34;net.biancheng.mapper.WebsiteMapper.selectWebsiteById\u0026#34;, 1); // 请注意，当我们使用二级缓存的时候，sqlSession调用了 commit方法后才会生效 ss.commit(); logger.debug(\u0026#34;现在创建一个新的SqlSeesion对象在执行一次\u0026#34;); SqlSession ss2 = ssf.openSession(); Website site3 = ss2.selectOne(\u0026#34;net.biancheng.mapper.WebsiteMapper.selectWebsiteById\u0026#34;, 1); // 请注意，当我们使用二级缓存的时候，sqlSession调用了 commit方法后才会生效 ss2.commit(); } } 结果如下:\nDEBUG [main] - ==\u0026gt; Preparing: SELECT * FROM website WHERE id=? DEBUG [main] - ==\u0026gt; Parameters: 1(Integer) DEBUG [main] - \u0026lt;== Total: 1 DEBUG [main] - 使用同一个sqlsession再执行一次 DEBUG [main] - 现在创建一个新的SqlSeesion对象在执行一次 DEBUG [main] - ==\u0026gt; Preparing: SELECT * FROM website WHERE id=? DEBUG [main] - ==\u0026gt; Parameters: 1(Integer) DEBUG [main] - \u0026lt;== Total: 1 从运行结果可以看出，第一个 SqlSession 实际只发生过一次查询，而第二次查询就从缓存中取出了，也就是 SqlSession 层面的一级缓存。为了克服这个问题，我们往往需要配置二级缓存，使得缓存在 SqlSessionFactory 层面上能够提供给各个 SqlSession 对象共享。\n一级缓存失效的四种情况 sqlsession变了 缓存失效 sqlsession不变,查询条件不同，一级缓存失效 sqlsession不变,中间发生了增删改操作，一级缓存失败 sqlsession不变,手动清除缓存，一级缓存失败 spring结合mybatis后，一级缓存作用：\n在未开启事务的情况之下，每次查询，spring都会关闭旧的sqlSession而创建新的sqlSession,因此此时的一级缓存是不起作用的\n在开启事务的情况之下，spring使用ThreadLocal获取当前资源绑定同一个sqlSession，因此此时一级缓存是有效的。\n由于spring使用了代理, 所以在代理结束时就关闭了sqlsession, 导致一级缓存失效\nSpring整合MyBatis时一级缓存失效问题_xingze_W的博客-CSDN博客_spring整合mybatis一级缓存 mybatis一级缓存失效的四种情况 - 简书 (jianshu.com) 禁用一级缓存 mybatis: configuration: cache-enabled: false #禁用二级缓存 local-cache-scope: statement #默认指定为session级别 指定缓存statement级别后, mybatis会删除缓存\norg.apache.ibatis.executor.BaseExecutor#query(MappedStatement, Object, RowBounds, ResultHandler, CacheKey, BoundSql)\npublic \u0026lt;E\u0026gt; List\u0026lt;E\u0026gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { ErrorContext.instance().resource(ms.getResource()).activity(\u0026#34;executing a query\u0026#34;).object(ms.getId()); if (closed) throw new ExecutorException(\u0026#34;Executor was closed.\u0026#34;); if (queryStack == 0 \u0026amp;\u0026amp; ms.isFlushCacheRequired()) { clearLocalCache(); } List\u0026lt;E\u0026gt; list; try { queryStack++; list = resultHandler == null ? (List\u0026lt;E\u0026gt;) localCache.getObject(key) : null; if (list != null) { handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); } else { list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); } } finally { queryStack--; } if (queryStack == 0) { for (DeferredLoad deferredLoad : deferredLoads) { deferredLoad.load(); } // 此处做了删除 if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) { clearLocalCache(); // issue #482 } } return list; } 二级缓存 二级缓存是全局缓存，作用域超出 session 范围之外，可以被所有 SqlSession 共享。二级缓存的作用域是namespace，也就是作用范围是同一个命名空间\n开启二级缓存后，会使用CachingExecutor装饰Executor，进入一级缓存的查询流程前，先在CachingExecutor进行二级缓存的查询，具体的工作流程如下所示。\n二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。 当开启缓存后，数据的查询执行的流程就是 二级缓存 -\u0026gt; 一级缓存 -\u0026gt; 数据库。\n禁用二级缓存 mybatis: configuration: cache-enabled: false #禁用二级缓存, 默认为true mybatis的二级缓存是本地实现的, 在分布式环境中, 容易产生脏数据 , 所以一般会使用redis这种第三方存储来使用二级缓存\nSpringBoot+Mybatis一级缓存和二级缓存详解 - 郑晓龙 - 博客园 (cnblogs.com) mybatis 二级缓存失效_给我五分钟，带你彻底掌握MyBatis的缓存工作原理_Tfifthe的博客-CSDN博客 MyBatis缓存（一级缓存和二级缓存） (biancheng.net) MyBatis|缓存机制 - 简书 (jianshu.com) springboot+mybatis一级缓存启用/禁用问题_NongYeting的博客-CSDN博客_springboot关闭mybatis缓存 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E7%BC%93%E5%AD%98.html","summary":"[TOC] 前言 和大多数持久化框架一样，MyBatis 提供了一级缓存和二级缓存的支持。默认情况下，MyBatis 只开启一级缓存。 MyBatis提供了一","title":"缓存"},{"content":"[TOC]\n1. 如果判断对象可以回收? 1.1 引用计数算法 定义: 在对象中添加一个引用计数器， 每当有一个地方引用它时， 计数器值就加一； 当引用失效时， 计数器值就减一； 任何时刻计数器为零的对象就是不可能再被使用的\n主流的Java虚拟机里面都没有选用引用计数算法来管理内存， 主要原因是有些场景无法准确标记， 譬如单纯的引用计数就很难解决对象之间相互循环引用的问题\n微软COM（Component Object Model） 技术、 使用ActionScript 3的FlashPlayer、 Python语言以及Squirrel中都使用了引用计数算法进行内存管理\n1.2 可达性分析算法 这个算法的基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集， 从这些节点开始， 根据引用关系向下搜索， 搜索过程所走过的路径称为“引用链”（Reference Chain） ， 如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时， 则证明此对象是不可能再被使用的\n在Java技术体系里面， 固定可作为GC Roots的对象包括以下几种：\n在虚拟机栈（栈帧中的本地变量表） 中引用的对象， 譬如各个线程被调用的方法堆栈中使用到的参数、 局部变量、 临时变量等。 在方法区中类静态属性引用的对象， 譬如Java类的引用类型静态变量。 在方法区中常量引用的对象， 譬如字符串常量池（String Table） 里的引用。 在本地方法栈中JNI（即通常所说的Native方法） 引用的对象。 Java虚拟机内部的引用， 如基本数据类型对应的Class对象， 一些常驻的异常对象（比如NullPointExcepiton、 OutOfMemoryError） 等， 还有系统类加载器。 所有被同步锁（synchronized关键字） 持有的对象。 反映Java虚拟机内部情况的JMXBean、 JVMTI中注册的回调、 本地代码缓存等。 除了这些固定的GC Roots集合以外， 根据用户所选用的垃圾收集器以及当前回收的内存区域不同， 还可以有其他对象“临时性”地加入， 共同构成完整GC Roots集合。 譬如分代收集和局部回收（Partial GC） ， 如果只针对Java堆中某一块区域发起垃圾收集时（如最典型的只针对新生代的垃圾收集） ， 必须考虑到内存区域是虚拟机自己的实现细节（在用户视角里任何内存区域都是不可见的， 更不是孤立封闭的）， 所以某个区域里的对象完全有可能被位于堆中其他区域的对象所引用， 这时候就需要将这些关联区域的对象也一并加入GC Roots集合中去， 才能保证可达性分析的正确性\n1.3 标记后执行finalize() 即使在可达性分析算法中判定为不可达的对象， 也不是“非死不可”的， 这时候它们暂时还处于“缓刑”阶段， 要真正宣告一个对象死亡， 至少要经历两次标记过程： 如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链， 那它将会被第一次标记， 随后进行一次筛选， 筛选的条件是此对象是否有必要执行finalize()方法。 没有必要执行了就直接回收了\n假如对象没有覆盖finalize()方法， 或者finalize()方法已经被虚拟机调用过， 那么虚拟机将这两种情况都视为“没有必要执行”。\n所以基本所有的情况都是直接回收了, 没人会在业务代码里重写finalize()方法\n如果这个对象被判定为确有必要执行finalize()方法， 那么该对象将会被放置在一个名为F-Queue的队列之中， 并在稍后由一条由虚拟机自动建立的、 低调度优先级的Finalizer线程去执行它们的finalize()方法\n这里所说的“执行”是指虚拟机会触发这个方法开始运行， 但并不承诺一定会等待它运行结束。这样做的原因是， 如果某个对象的finalize()方法执行缓慢， 或者更极端地发生了死循环， 将很可能导致F-Queue队列中的其他对象永久处于等待， 甚至导致整个内存回收子系统的崩溃\nfinalize()方法是对象逃脱死亡命运的最后一次机会， 稍后收集器将对F-Queue中的对象进行第二次小规模的标记， 如果对象要在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可， 譬如把自己（this关键字） 赋值给某个类变量或者对象的成员变量， 那在第二次标记时它将被移出“即将回收”的集合； 如果对象这时候还没有逃脱， 那基本上它就真的要被回收了\n总结: 当对象被判断为不可达时, 如果还需要执行finalize()方法, 且在finalize()方法中拯救了自己, 那这次就不会回收这个对象了\n例如:\n/** * 此代码演示了两点： * 1.对象可以在被GC时自我拯救。 * 2.这种自救的机会只有一次， 因为一个对象的finalize()方法最多只会被系统自动调用一次 * * @author zzm */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() { System.out.println(\u0026#34;yes, i am still alive :)\u0026#34;); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\u0026#34;finalize method executed!\u0026#34;); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws Throwable { SAVE_HOOK = new FinalizeEscapeGC(); SAVE_HOOK = null; //对象第一次成功拯救自己, 成功了 System.gc(); // 因为Finalizer方法优先级很低， 暂停0.5秒， 以等待它 Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\u0026#34;no, i am dead :(\u0026#34;); } // 下面这段代码与上面的完全相同， 但是这次自救却失败了SAVE_HOOK = null; System.gc(); // 因为Finalizer方法优先级很低， 暂停0.5秒， 以等待它 Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\u0026#34;no, i am dead :(\u0026#34;); } } } 结果: finalize method executed! yes, i am still alive :) no, i am dead :( 另外一个值得注意的地方就是， 代码中有两段完全一样的代码片段， 执行结果却是一次逃脱成功， 一次失败了。 这是因为任何一个对象的finalize()方法都只会被系统自动调用一次， 如果对象面临下一次回收， 它的finalize()方法不会被再次执行， 因此第二段代码的自救行动失败了\n这是因为判断需要是否执行finalize(), 其中有个条件是,已经执行后就不再执行了\nfinalize()这个方法已经被弃用了, 当初写这个方法也是为了使C/C++程序猿的一种妥协,因为C/C++中有析构函数\n2. 引用类型 无论是通过引用计数算法判断对象的引用数量， 还是通过可达性分析算法判断对象是否引用链可达， 判定对象是否存活都和“引用”离不开关系。\n引用分为强引用（Strongly Re-ference） 、 软引用（Soft Reference） 、 弱引用（Weak Reference） 和虚引用（Phantom Reference） 4种， 这4种引用强度依次逐渐减弱\n2.1 强引用 强引用是最传统的“引用”的定义， 是指在程序代码之中普遍存在的引用赋值， 即类似Object obj=new Object()这种引用关系。 无论任何情况下， 只要强引用关系还存在， 垃圾收集器就永远不会回收掉被引用的对象。\n2.2 软引用 软引用是用来描述一些还有用， 但非必须的对象。 只被软引用关联着的对象， 在系统将要发生内存溢出异常前， 会把这些对象列进回收范围之中进行第二次回收， 如果这次回收还没有足够的内存，才会抛出内存溢出异常。 在JDK 1.2版之后提供了SoftReference类来实现软引用。\n软引用可用来实现内存敏感的(非必须的对象)高速缓存\n内存不够时, 发生垃圾回收, 才会回收软引用\n2.3 弱引用 弱引用也是用来描述那些非必须对象， 但是它的强度比软引用更弱一些， 被弱引用关联的对象只能生存到下一次垃圾收集发生为止。 当垃圾收集器开始工作， 无论当前内存是否足够， 都会回收掉只被弱引用关联的对象。 在JDK 1.2版之后提供了WeakReference类来实现弱引用\n只要是垃圾回收,就会回收弱引用\n2.4 虚引用 虚引用也称为“幽灵引用”或者“幻影引用”， 它是最弱的一种引用关系。 一个对象是否有虚引用的存在， 完全不会对其生存时间构成影响， 也无法通过虚引用来取得一个对象实例。 为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。 在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 通过查看这个类的源码，发现它只有一个构造函数和一个 get() 方法，而且它的 get() 方法仅仅是返回一个null，也就是说将永远无法通过虚引用来获取对象，虚引用必须要和 ReferenceQueue 引用队列一起使用。\npublic class PhantomReference\u0026lt;T\u0026gt; extends Reference\u0026lt;T\u0026gt; { /** * Returns this reference object\u0026#39;s referent. Because the referent of a * phantom reference is always inaccessible, this method always returns * \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt;. * * @return \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; */ public T get() { return null; } public PhantomReference(T referent, ReferenceQueue\u0026lt;? super T\u0026gt; q) { super(referent, q); } } 2.5 引用队列（ReferenceQueue） 引用队列可以与软引用、弱引用以及虚引用一起配合使用，当垃圾回收器准备回收一个对象时，如果发现它还有引用，那么就会在回收对象之前，把这个引用加入到与之关联的引用队列中去。程序可以通过判断引用队列中是否已经加入了引用，来判断被引用的对象是否将要被垃圾回收，这样就可以在对象被回收之前采取一些必要的措施。\n与软引用、弱引用不同，虚引用必须和引用队列一起使用。\n2.6 多引用类型的可达性判断 比较容易理解的是 Java 垃圾回收器会优先清理可达强度低的对象。 那现在问题来了， 若一个对象的引用类型有多个， 那到底如何判断它的可 达性呢？ 其实规则如下： （“单弱多强” ）\n单条引用链的可达性以最弱的一个引用类型来决定； 多条引用链的可达性以最强的一个引用类型来决定； 我们假设图 2 中引用①和③为强引用， ⑤为软引用， ⑦为弱引用， 对于对象 5 按照这两个判断原则， 路径①-⑤取最弱的引用⑤， 因此该路径对对象 5 的引用为软引用。 同样， ③-⑦为弱引用。 在这两条路径之间取最强的引用， 于是对象 5 是一个软可及对象。\n2.7 总结 java四种引用类型以及使用场景详解 - 朱子威 - 博客园 (cnblogs.com) 深入理解Java的四种引用类型强引用（StrongReference）软引用（SoftReference）弱引用（WeakReference）虚引用（PhantomReference）多引用类型的可达 - 云+社区 - 腾讯云 (tencent.com) Carson带你学Java：深入解析引用类型-强、软、弱、虚 - 简书 (jianshu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9B%9E%E6%94%B6%E5%89%8D%E6%8F%90.html","summary":"[TOC] 1. 如果判断对象可以回收? 1.1 引用计数算法 定义: 在对象中添加一个引用计数器， 每当有一个地方引用它时， 计数器值就加一； 当引用失效时， 计数器值就减","title":"回收前提"},{"content":"UserUtils.java package com.gree.ecommerce.utils; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.gree.ecommerce.config.InheritableThreadPoolTaskExecutor; import com.gree.ecommerce.constant.Constant; import com.gree.ecommerce.constant.ServerResultCode; import com.gree.ecommerce.exception.BusinessException; import com.gree.ecommerce.module.user.BaseUserRedisVO; import com.gree.ecommerce.module.user.EocLoginUserRedisVO; import com.gree.ecommerce.module.user.LoginUserRedisVO; import lombok.extern.slf4j.Slf4j; import org.apache.commons.collections4.CollectionUtils; import java.util.Collection; import java.util.List; import java.util.Optional; import java.util.concurrent.TimeUnit; import java.util.function.Supplier; import java.util.stream.Collectors; import java.util.stream.Stream; /** * 用户数据相关工具类 * * @createDate 2021/12/15 */ @Slf4j public class UserUtils { static RedisUtil redisUtil = new RedisUtil(); public UserUtils(RedisUtil ru) { redisUtil = ru; } static Supplier\u0026lt;BusinessException\u0026gt; resultUserNotLoginEx = () -\u0026gt; new BusinessException(ServerResultCode.USER_NOT_LOGGED_IN); /** * 从请求头中获得token =\u0026gt; 再通过redis获取用户信息 * * @return 用户基础信息 * @throws BusinessException token不存在则会抛出 , 返回码为 {@link ServerResultCode#USER_NOT_LOGGED_IN } * @createDate 2021/12/20 */ public static BaseUserRedisVO getBaseUserRedisInfo() { return HttpRequestUtil.getHttpHeader(Constant.AUTHORIZATION) .map(authStr -\u0026gt; redisUtil.get(authStr)) .map(str -\u0026gt; JSON.parseObject(str, BaseUserRedisVO.class)) .orElseGet(() -\u0026gt; Optional.ofNullable( JSON.parseObject(InheritableThreadPoolTaskExecutor.THREAD_USER_INFO_CACHE.get(), BaseUserRedisVO.class)) .orElseThrow(resultUserNotLoginEx) ); } /** * 从请求头中获得token =\u0026gt; 再通过redis获取用户信息 * * @return 商城用户信息 * @throws BusinessException token不存在则会抛出 , 返回码为 {@link ServerResultCode#USER_NOT_LOGGED_IN } * @createDate 2021/12/20 */ public static LoginUserRedisVO getMallUserInfo() { return HttpRequestUtil.getHttpHeader(Constant.AUTHORIZATION) .map(authStr -\u0026gt; redisUtil.get(authStr)) .map(str -\u0026gt; JSON.parseObject(str, LoginUserRedisVO.class)) .orElseGet(()-\u0026gt; Optional.ofNullable(JSON.parseObject(InheritableThreadPoolTaskExecutor.THREAD_USER_INFO_CACHE.get(), LoginUserRedisVO.class)) .orElseThrow(resultUserNotLoginEx)); } /** * 更新商城用户登录的redis信息 * * @param modify 需要修改的属性 * @author zhouMJ **/ public static void modifyMallUserRedisInfo(LoginUserRedisVO modify) { Stream.of(getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_SELLER, modify.getUserId()), getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_MALL, modify.getUserId()), getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_ALL_SALES, modify.getUserId())) .filter(CollectionUtils::isNotEmpty) .flatMap(Collection::stream) .filter(StringUtils::isNotBlank) //查出商城用户的token .forEach(key -\u0026gt; { String redisStr = redisUtil.get(key); if (StringUtils.isNotEmpty(redisStr)) { LoginUserRedisVO redisVO = generateMallModifyVO(redisStr, modify); //修改属性值 redisUtil.setEx(key, JSONObject.toJSONString(redisVO), Constant.REDIS_MALL_USER_LOGIN_TOKEN_TTL, TimeUnit.MINUTES); } }); } /** * 生成商城用户的redis信息修改后的属性对象 * * @param redisStr redis的json串 * @param modify 需要修改的属性 * @return LoginUserRedisVO 商城用户的redis信息修改后的属性对象 **/ private static LoginUserRedisVO generateMallModifyVO(String redisStr, LoginUserRedisVO modify) { LoginUserRedisVO vo = JSONObject.parseObject(redisStr, LoginUserRedisVO.class); if (StringUtils.isNotEmpty(modify.getUserAcc())) { vo.setUserAcc(modify.getUserAcc()); } if (StringUtils.isNotEmpty(modify.getNickName())) { vo.setNickName(modify.getNickName()); } if (StringUtils.isNotEmpty(modify.getMobile())) { vo.setMobile(modify.getMobile()); } if (StringUtils.isNotEmpty(modify.getType())) { vo.setType(modify.getType()); } return vo; } /** * eoc退出登录 */ public static void eocLoginOut() { //删除token信息 redisUtil.delete(getToken()); //删除token群组中的token值 String tokenGroupKey = String.format(Constant.REDIS_USER_TOKEN_LIST_KEY, getBaseUserRedisInfo().getUserId(), getPlatformFlag(), getXFlag()); redisUtil.lRemove(tokenGroupKey, Constant.ONE, getToken()); //删除用户的权限变化标识 delSignUserPower(); } /** * 商城退出登录 */ public static void mallLoginOut() { //删除token信息 redisUtil.delete(getToken()); //删除token群组中的token值 String tokenGroupKey = String.format(Constant.REDIS_USER_TOKEN_LIST_KEY, getBaseUserRedisInfo().getUserId(), getPlatformFlag(), getXFlag()); redisUtil.lRemove(tokenGroupKey, Constant.ONE, getToken()); //如果是卖家中心的,删除用户的权限变化标识 if (getPlatformFlag().equals(Constant.PLATFORM_FLAG_SELLER)) { delSignUserPower(); } } /** * 从请求头中获得token * * @return 请求头token，不存在会返回空字符串 */ public static String getToken() { return HttpRequestUtil.getHttpHeader(Constant.AUTHORIZATION).orElse(\u0026#34;\u0026#34;); } /** * 从请求头中获得权限菜单路由 * * @return 请求头权限菜单路由，不存在会返回空字符串 */ public static String getPowerMenuPath() { return HttpRequestUtil.getHttpHeader(Constant.POWER_MENU_PATH).orElse(\u0026#34;\u0026#34;); } /** * 从请求头中获得端标识 * * @return 请求头端标识 不存在会返回空字符串 */ public static String getXFlag() { return HttpRequestUtil.getHttpHeader(Constant.X_FLAG).orElse(\u0026#34;\u0026#34;); } /** * 从请求头中获得端标识 * * @return 请求头端标识，不存在会返回空字符串 */ public static String getPlatformFlag() { return HttpRequestUtil.getHttpHeader(Constant.PLATFORM_FLAG).orElse(\u0026#34;\u0026#34;); } /** * 从请求头中获得token =\u0026gt; 再通过redis获取用户信息 * * @return eoc用户信息 * @throws BusinessException token不存在则会抛出 , 返回码为 {@link ServerResultCode#USER_NOT_LOGGED_IN } * @createDate 2021/12/20 */ public static EocLoginUserRedisVO getEocUserInfo() { return HttpRequestUtil.getHttpHeader(Constant.AUTHORIZATION) .map(authStr -\u0026gt; redisUtil.get(authStr)) .map(str -\u0026gt; JSON.parseObject(str, EocLoginUserRedisVO.class)) .orElseGet(()-\u0026gt; Optional.ofNullable(JSON.parseObject(InheritableThreadPoolTaskExecutor.THREAD_USER_INFO_CACHE.get(), EocLoginUserRedisVO.class)) .orElseThrow(resultUserNotLoginEx)); } /** * 更新eoc用户登录的redis信息 * * @param modify 需要修改的属性 * @author zhouMJ **/ public static void modifyEocUserRedisInfo(EocLoginUserRedisVO modify) { //查询eoc的token的key集合 List\u0026lt;String\u0026gt; tokenKeys = getPlatAllXflagTokenGroup(getPlatformFlag(), modify.getUserId()); tokenKeys.forEach(key -\u0026gt; { String redisStr = redisUtil.get(key); if (StringUtils.isNotEmpty(redisStr)) { EocLoginUserRedisVO redisVO = generateEocModifyVO(redisStr, modify); //修改属性值 redisUtil.setEx(key, JSONObject.toJSONString(redisVO), Constant.REDIS_SYS_USER_LOGIN_TOKEN_TTL, TimeUnit.MINUTES); } }); } /** * 生成eoc用户的redis信息修改后的属性对象 * * @param redisStr redis的json串 * @param modify 需要修改的属性 * @return LoginUserRedisVO 用户的redis信息修改后的属性对象 **/ private static EocLoginUserRedisVO generateEocModifyVO(String redisStr, EocLoginUserRedisVO modify) { EocLoginUserRedisVO vo = JSONObject.parseObject(redisStr, EocLoginUserRedisVO.class); if (StringUtils.isNotEmpty(modify.getUserAcc())) { vo.setUserAcc(modify.getUserAcc()); } if (StringUtils.isNotEmpty(modify.getEmpName())) { vo.setEmpName(modify.getEmpName()); } if (StringUtils.isNotEmpty(modify.getMobile())) { vo.setMobile(modify.getMobile()); } return vo; } /** * eoc强制清空用户token * * @param userId 用户id **/ public static void eocClearUserToken(Long userId) { redisUtil.delete(getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_EOC, userId)); } /** * eoc标识用户权限变化 * * @param userIds 用户ids **/ public static void eocSignUserPowerUpdate(List\u0026lt;Long\u0026gt; userIds) { List\u0026lt;String\u0026gt; signUserKeys = userIds.stream() //拿到用户下的token组 .map(userId -\u0026gt; getPlatAndXflagTokenGroup(Constant.PLATFORM_FLAG_EOC, Constant.X_REQ_PC, userId)) .flatMap(Collection::stream) //按照token的维度标识权限变化 .map(UserUtils::getSignUserPowerKey) .collect(Collectors.toList()); log.info(\u0026#34;eoc标识用户权限变化eocSignUserPowerUpdate，keys：{}\u0026#34;, signUserKeys); for (String key: signUserKeys) { redisUtil.setEx(key, Constant.TRUE_STR, 7, TimeUnit.HOURS); } } /** * 卖家中心标识用户权限变化 * * @param userIds 用户ids **/ public static void sellerSignUserPowerUpdate(List\u0026lt;Long\u0026gt; userIds) { List\u0026lt;String\u0026gt; signUserKeys = userIds.stream() //拿到用户下的token组 .map(userId -\u0026gt; getPlatAndXflagTokenGroup(Constant.PLATFORM_FLAG_SELLER, Constant.X_REQ_PC, userId)) .flatMap(Collection::stream) //按照token的维度标识权限变化 .map(UserUtils::getSignUserPowerKey) .collect(Collectors.toList()); log.info(\u0026#34;卖家中心标识用户权限变化sellerSignUserPowerUpdate，keys：{}\u0026#34;, signUserKeys); for (String key: signUserKeys) { redisUtil.setEx(key, Constant.TRUE_STR, 7, TimeUnit.HOURS); } } public static String getSignUserPowerKey(String token) { return String.format(Constant.REDIS_USER_TOKEN_POWER_UPDATE_SIGN_KEY, getPlatformFlag(), getXFlag(), token); } /** * 删除用户权限变化标识 **/ public static void delSignUserPower() { redisUtil.delete(getSignUserPowerKey(getToken())); } /** * 卖家中心强制清空用户token * * @param userId 用户id **/ public static void sellerClearUserToken(Long userId) { redisUtil.delete(getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_SELLER, userId)); } /** * 得到平台所有端的token群组 * * @param platform 平台标识 * @param userId 用户id * @return java.util.List\u0026lt;java.lang.String\u0026gt; 平台所有端的token集合 * @author gree **/ public static List\u0026lt;String\u0026gt; getPlatAllXflagTokenGroup(String platform, Long userId) { return Stream.of(getPlatAndXflagTokenGroup(platform, Constant.X_REQ_H5, userId), getPlatAndXflagTokenGroup(platform, Constant.X_REQ_MINI, userId), getPlatAndXflagTokenGroup(platform, Constant.X_REQ_ANDROID, userId), getPlatAndXflagTokenGroup(platform, Constant.X_REQ_IOS, userId), getPlatAndXflagTokenGroup(platform, Constant.X_REQ_PC, userId)) .filter(CollectionUtils::isNotEmpty) .flatMap(Collection::stream) .filter(StringUtils::isNotBlank) .collect(Collectors.toList()); } /** * 得到平台对应端的token群组 * * @param platform 平台标识 * @param xflag 端标识 * @param userId 用户id * @return java.util.List\u0026lt;java.lang.String\u0026gt; 平台对应端的token群组 * @author gree **/ public static List\u0026lt;String\u0026gt; getPlatAndXflagTokenGroup(String platform, String xflag, Long userId) { String key = String.format(Constant.REDIS_USER_TOKEN_LIST_KEY, userId, platform, xflag); return redisUtil.lRange(key, Constant.ZERO, Constant.NEGATIVE_NUMBER_ONE); } /** * 董店注销用户 退出董店、pc商城、全员销售、卖家中心的系统 **/ public static void mallDelUser(Long userId) { List\u0026lt;String\u0026gt; mallUserAllToken = Stream.of(getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_SELLER, userId), getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_MALL, userId), getPlatAllXflagTokenGroup(Constant.PLATFORM_FLAG_ALL_SALES, userId)) .flatMap(Collection::stream) .collect(Collectors.toList()); if (CollectionUtils.isNotEmpty(mallUserAllToken)) { redisUtil.delete(mallUserAllToken); } } } HttpRequestUtil.java package com.gree.ecommerce.utils; import org.springframework.web.context.request.RequestContextHolder; import org.springframework.web.context.request.ServletRequestAttributes; import javax.servlet.http.HttpServletRequest; import java.util.Optional; /** * @author A80080 * @createDate 2022/1/4 */ public class HttpRequestUtil { /** * 获取当前请求session * * @return 返回request */ public static Optional\u0026lt;HttpServletRequest\u0026gt; getHttpServletRequest() { return getServletRequestAttributes().map(ServletRequestAttributes::getRequest); } /** * 获取当前请求Attributes * * @return 返回Attributes */ public static Optional\u0026lt;ServletRequestAttributes\u0026gt; getServletRequestAttributes() { return Optional.ofNullable(RequestContextHolder.getRequestAttributes()) .map(s -\u0026gt; (ServletRequestAttributes) s); } /** * 设置请求Attributes inheritable参数为true * * @return 返回Attributes */ public static Optional\u0026lt;ServletRequestAttributes\u0026gt; setInheritableServletRequestAttributes() { Optional\u0026lt;ServletRequestAttributes\u0026gt; attributesOpt = getServletRequestAttributes(); attributesOpt.ifPresent(servletRequestAttributes -\u0026gt; RequestContextHolder.setRequestAttributes(servletRequestAttributes, true)); return attributesOpt; } /** * 获取当前请求中指定的header * * @param name headerName * @return 指定headerName对应的值 */ public static Optional\u0026lt;String\u0026gt; getHttpHeader(String name) { return getHttpServletRequest().map(request -\u0026gt; request.getHeader(name)); } } ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E8%8E%B7%E5%8F%96%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E5%B7%A5%E5%85%B7%E7%B1%BB.html","summary":"UserUtils.java package com.gree.ecommerce.utils; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.gree.ecommerce.config.InheritableThreadPoolTaskExecutor; import com.gree.ecommerce.constant.Constant; import com.gree.ecommerce.constant.ServerResultCode; import com.gree.ecommerce.exception.BusinessException; import com.gree.ecommerce.module.user.BaseUserRedisVO; import com.gree.ecommerce.module.user.EocLoginUserRedisVO; import com.gree.ecommerce.module.user.LoginUserRedisVO; import lombok.extern.slf4j.Slf4j; import org.apache.commons.collections4.CollectionUtils; import java.util.Collection; import java.util.List; import java.util.Optional; import java.util.concurrent.TimeUnit; import java.util.function.Supplier; import java.util.stream.Collectors; import java.util.stream.Stream; /** * 用户数据相关工具类 * * @createDate 2021/12/15 */ @Slf4j public class UserUtils { static RedisUtil redisUtil = new RedisUtil(); public UserUtils(RedisUtil ru) {","title":"获取用户信息工具类"},{"content":"mysql改为oracle要改动的地方:\n除去limit ​\t①.如果是做分页,直接去掉,用分页插件代替\n​\t②.如果是取第一条,用 where ROWNUM = 1 代替\n表别名写了as的,要去掉as\nSYSDATE() / now() 改成 SYSDATE\nwhere/and条件不能是数字,仅支持boolean, instr函数返回的就是数字, 在后面加上 \u0026gt;0 , =\u0026gt;INSTR(ORG_LINK, #{topOrgId})\u0026gt;0\nCONCAT('%',#{noOrName},'%') 改成 CONCAT( CONCAT('%',#{noOrName}) ,'%') 或者 \u0026lsquo;%\u0026rsquo;||#{noOrName}||\u0026rsquo;%\u0026rsquo;\n批量插入/修改时,sql前面加 begin ,结尾处加 end;\n不能插空值,估计要加配置\nIFNULL 改为 NVl\nDATE_FORMAT 原(%Y-%m-%d)改为 TO_CHAR 时间格式: \u0026lsquo;yyyy-MM-dd hh24:mi:ss\u0026rsquo;\n没有find_in_set函数,可以考虑用instr代替\n没有GROUP_CONCAT函数,用wm_concat代替\n不能使用连表删除/修改,借助 EXISTS 删除 =\u0026gt; https://blog.csdn.net/china_shrimp/article/details/78843256 case when中有取字段和自定义时,需要用 Translate(\u0026lsquo;根目录\u0026rsquo; USING NCHAR_CS) 替换 \u0026lsquo;根目录\u0026rsquo; 因为\u0026rsquo;根目录\u0026rsquo; 是varchar2类型,而目前数据中是nvarchar2类型\n删除insert语句上的 useGeneratedKeys=\u0026ldquo;true\u0026rdquo;\n暂时发现 : 如果是select ,语句后面不允许加 \u0026quot; ; \u0026quot;\n删除时.删除语句 { DELETE H.* FROM JBP_SERVICE_PROJECT H } 改为 { DELETE FROM JBP_SERVICE_PROJECT H }\nOracle 不会自动将字符串转化成日期,如需插入字符串时间,使用 to_date 函数\nOracle 降序排序时默认会把空值放前面,在order by 后面加上 NULLS LAST , 即可实现与mysql一样\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A7%AF%E7%B4%AF.html","summary":"mysql改为oracle要改动的地方: 除去limit ​ ①.如果是做分页,直接去掉,用分页插件代替 ​ ②.如果是取第一条,用 where ROWNUM = 1 代替 表别名","title":"积累"},{"content":"[toc]\nElasticSearch、Logstash和Kiabana均有开箱即用的版本, 也可以使用docker,就不用下载具体的包了\n官网下载:https://www.elastic.co/cn/downloads/\n1. ElasticSearch: 在config中 增加elasticsearch.yml文件如下内容:\n`network.host: 0.0.0.0 # 网络设置,表示大家都能连``\n执行 bin/elasticsearch 即可\n在浏览器中输入 http://localhost:9200/ 返回如下json表示成功:\n{ \u0026#34;name\u0026#34;: \u0026#34;node-1\u0026#34;, \u0026#34;cluster_name\u0026#34;: \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34;: \u0026#34;qgDoFT0_Sa66sYTd_5ETug\u0026#34;, \u0026#34;version\u0026#34;: { \u0026#34;number\u0026#34;: \u0026#34;5.5.3\u0026#34;, \u0026#34;build_hash\u0026#34;: \u0026#34;9305a5e\u0026#34;, \u0026#34;build_date\u0026#34;: \u0026#34;2017-09-07T15:56:59.599Z\u0026#34;, \u0026#34;build_snapshot\u0026#34;: false, \u0026#34;lucene_version\u0026#34;: \u0026#34;6.6.0\u0026#34; }, \u0026#34;tagline\u0026#34;: \u0026#34;You Know, for Search\u0026#34; } 2.Kiabana: 在config中修改 kibana.yml ,如下内容:\nelasticsearch.url: \u0026quot;http://localhost:9200\u0026quot;\n执行 bin/kibana.bat\n在浏览器中访问 http://localhost:5601 会出现页面,说明成功了\n3.Logstash (有点像Flume,有接收数据,处理数据,输出数据.输入/输出都有各种选择)\n直接运行 start.bat 即可\n在config文件夹下,创建logstash.conf文件,写入以下内容:\ninput {// logstash的数据来源 # 从控制台接收, 类型是test(可以不写), stdin{ type =\u0026gt; \u0026#34;test\u0026#34; } #从文件中接收, file{ path=\u0026gt;\u0026#34;/home/jht/jportal-license/jportal/logs/jportal/jportal.log\u0026#34; # 从这个文件中接收,允许写多个文件,写在[]中,用\u0026#34;,\u0026#34;隔开,数组的格式 type =\u0026gt; \u0026#34;jportal\u0026#34; # 类型是jportal start_position =\u0026gt; \u0026#34;beginning\u0026#34; # 表示从文件的起始位置读 } } # 过滤条件,可以处理数据的输出格式 #filter { #\tgrok { # ##patterns_dir 是刚刚创建的patterns文件夹目录，根据创建具体路径配置 # patterns_dir =\u0026gt; \u0026#34;D:/software/ELK/logstash-6.5.0/patterns\u0026#34; # match =\u0026gt; { # \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{JPORTAL}\u0026#34; # } # } #\t#} #输出 output { #输出到控制台,codec(可以不写)表示类型, stdout { codec =\u0026gt; rubydebug } # type是输入源里定义的 if [type] == \u0026#34;system\u0026#34; { #输入到elasticsearch elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;] # es的地址 index =\u0026gt; \u0026#34;system-%{+MM.dd-HH:mm:ss}\u0026#34; # 索引,kiabana会安装index分类,这里可以按照项目或者日志级别等分类 } } if [type] == \u0026#34;jportal\u0026#34; or [type] == \u0026#34;test\u0026#34; { elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;] index =\u0026gt; \u0026#34;jportal-%{+MM.dd-HH:mm:ss}\u0026#34; } } } 4.使用 如果三者改了配置都需要重新启动,不会动态获取配置\n启动Logstash,如果监控的文件中有数据,会输出:\n启动Logstash后,控制会等待输入, 输入 hello:\n控制台会输出:\n{ \u0026#34;type\u0026#34; =\u0026gt; \u0026#34;test\u0026#34;, \u0026#34;@version\u0026#34; =\u0026gt; \u0026#34;1\u0026#34;, \u0026#34;@timestamp\u0026#34; =\u0026gt; 2019-04-29T09:11:51.533Z, \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;hello \u0026#34;, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;V-WYM-063\u0026#34; } 在kiabana中能查询的es中的数据,默认按索引分类.\n先新建个索引,\n查询数据:\n安装head插件,可以管理ES中的数据\na)插件安装方法一\n/usr/share/elasticsearch/bin/plugin install mobz/elasticsearch-head (新版本的plugin命令可能被替代了,自己找找类似的)\nb）插件安装方法二\n首先下载head插件，下载到/usr/loca/src目录下\n下载地址：https://github.com/mobz/elasticsearch-head\n======================================================\nhead插件包百度云盘下载：https://pan.baidu.com/s/1boBE0qj\n提取密码：ifj7\n======================================================\nunzip elasticsearch-head-master.zip\n在/usr/share/elasticsearch/plugins目录下创建head目录\n然后将上面下载的elasticsearch-head-master.zip解压后的文件都移到/usr/share/elasticsearch/plugins/head下\n修改es 的配置文件: elasticsearch.yml配置文件,添加以下内容,\nhttp.cors.enabled: true http.cors.allow-origin: \u0026#34;*\u0026#34; http.cors.allow-credentials: true 启动elasticsearch-head,在elasticsearch-head文件下\nnpm run start\nnpm基于node.js,如果没有则需先装node.js,\n其中还需要grunt命令,如果没有则需安装: npm install -g grunt-cli\n出现以下内容表示成功:\n接着重启elasticsearch服务即可！\n启动head插件\n访问: http://localhost:9100,出现如下界面,并能连接表示成功\nc）插件安装方法三(推荐)\n下载浏览器插件,输入es地址,即可访问es\n插件地址:\nhttps://github.com/liufengji/es-head/blob/master/elasticsearch-head.crx (该插件有问题)\nhttps://chrome.google.com/webstore/detail/elasticsearch-head/ffmkiejjmecolpfloofpjologoblkegm 输入IP地址:\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/%E5%9F%BA%E6%9C%AC%E6%90%AD%E5%BB%BA.html","summary":"[toc] ElasticSearch、Logstash和Kiabana均有开箱即用的版本, 也可以使用docker,就不用下载具体的包了 官网下载:ht","title":"基本搭建"},{"content":"整体类型架构图(待替换)\niterable类型架构图\ndef lazyFunc(x: Int, y: =\u0026gt; Int) = 1 // 表示接受的是一个变量 // x 参数为传值(call by value), y 参数是传名称(call by name) // 传名称相当于惰性参数,使用到该参数时才会求值,用一次求一次; 而传值则会先求值,不管函数内部是否使用或者使用几次 def func(() =\u0026gt; Int) =1 // 表示接受的是一个函数,没有入参,返参是Int的函数 println \u0026#34;23\u0026#34; // 等同于 println(\u0026#34;23\u0026#34;) // 在scala里，函数都可以写成操作符的形式，这使得函数定义更像数学表达式,若参数只有一个，圆括号也是可以省略的。 隐式转换: 1.当调用每个类的方法没有时,编译器会试图隐式转换,从作用域中寻找方法(可能放在伴生对象中) 2.当传函数的参数时,不是函数所需要的类型,会发生隐式转换,在作用域中寻找可以转变类型的函数(不在乎此函数的变量名,只关心入参和返参类型),配合柯西化,可以不传隐式参数\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/scala/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95.html","summary":"整体类型架构图(待替换) iterable类型架构图 def lazyFunc(x: Int, y: =\u0026gt; Int) = 1 // 表示接受的是一个变量 // x 参数为传值(call by value), y 参数是传名称(call","title":"基础语法"},{"content":"[toc]\n1. 并发模型 两种并发模型可以解决这两个问题：\n消息传递并发模型 (CSP-\u0026gt;Go语言 ; Actor模型-\u0026gt;akka框架) 共享内存并发模型 (java) 基于消息传递的并发模型 - 知乎 (zhihu.com) 2. 内存不可见问题 现代计算机为了高效，往往会在高速缓存区中缓存共享变量，因为cpu访问缓存区比访问内存要快得多。\n线程之间的共享变量存在主内存中，每个线程都有一个私有的本地内存，存储了该线程以读、写共享变量的副本。本地内存是Java内存模型的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器等。\nJava线程之间的通信由Java内存模型（简称JMM）控制，从抽象的角度来说，JMM定义了线程和主内存之间的抽象关系。JMM的抽象示意图如图所示：\n所以，线程A无法直接访问线程B的工作内存，线程间通信必须经过主内存。\n注意，根据JMM的规定，线程对共享变量的所有操作都必须在自己的本地内存中进行，不能直接从主内存中读取。\n所以出现了内存不可见问题!\n那么怎么知道这个共享变量的被其他线程更新了呢？这就是JMM的功劳了，也是JMM存在的必要性之一。JMM通过控制主内存与每个线程的本地内存之间的交互，来提供内存可见性保证。\n具体操作如下图:\n6 Java内存模型基础知识 · 深入浅出Java多线程 (redspider.group) 3. 重排序 计算机在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排。\n指令重排对于提高CPU处理性能十分必要。虽然由此带来了乱序的问题，但是这点牺牲是值得的。\n指令重排一般分为以下三种：\n编译器优化重排\n编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。\n指令并行重排\n现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序。\n内存系统重排\n由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。\n指令重排可以保证串行语义一致，但是没有义务保证多线程间的语义也一致。所以在多线程下，指令重排序可能会导致一些问题。\n4. happens-before 一方面，程序员需要JMM提供一个强的内存模型来编写代码；另一方面，编译器和处理器希望JMM对它们的束缚越少越好，这样它们就可以最可能多的做优化来提高性能，希望的是一个弱的内存模型。\nJMM考虑了这两种需求，并且找到了平衡点，对编译器和处理器来说，只要不改变程序的执行结果（单线程程序和正确同步了的多线程程序），编译器和处理器怎么优化都行。\n而对于程序员，JMM提供了happens-before规则（JSR-133规范），满足了程序员的需求——**简单易懂，并且提供了足够强的内存可见性保证。**换言之，程序员只要遵循happens-before规则，那他写的程序就能保证在JMM中具有强的内存可见性。\nhappens-before关系的定义如下：\n如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM也允许这样的重排序。 happens-before关系本质上和as-if-serial语义是一回事。\n总之，如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的，不管它们在不在一个线程。\n天然的happens-before关系\n在Java中，有以下天然的happens-before关系：\n程序顺序规则：一个线程中的每一个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start规则：如果线程A执行操作ThreadB.start()启动线程B，那么A线程的ThreadB.start（）操作happens-before于线程B中的任意操作、 join规则：如果线程A执行操作ThreadB.join（）并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 5. as-if-seial as-if-serial语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。\n为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。\nas-if-serial规则和happens-before规则的区别\nas-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻觉：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻觉：正确同步的多线程程序是按happens-before指定的顺序来执行的。 as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 Java并发理论（二）：as-if-serial规则和happens-before规则详解_少侠露飞的学习笔记-CSDN博客 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html","summary":"[toc] 1. 并发模型 两种并发模型可以解决这两个问题： 消息传递并发模型 (CSP-\u0026gt;Go语言 ; Actor模型-\u0026gt;akka框架) 共享内存并发模","title":"基础知识"},{"content":"常用技术:\nscrapy:一种爬虫框架,我用来爬取了静态页面 Xpath : 用来取路径,根据写的标签路径来取值 beautifulSoup: 与Xpath作用一样,根据标签路径来取值 splash:一个Javascript渲染服务 ","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html","summary":"常用技术: scrapy:一种爬虫框架,我用来爬取了静态页面 Xpath : 用来取路径,根据写的标签路径来取值 beautifulSoup: 与Xpath作用一样,根据标签路径来取值 s","title":"基础知识"},{"content":"[toc]\n位运算 符号 描述 运算规则 \u0026amp; 与 两个位都为1时，结果才为1 | 或 两个位都为0时，结果才为0 ^ 异或 两个位相同为0，相异为1 ~ 取反 0变1，1变0 \u0026laquo; 左移 各二进位全部左移若干位，高位丢弃，低位补0 \u0026raquo; 右移 各二进位全部右移若干位，对无符号数，高位补0，有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补0（逻辑右移） 位运算（\u0026amp;、|、^、~、\u0026raquo;、\u0026laquo;） - 五色风车 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html","summary":"[toc] 位运算 符号 描述 运算规则 \u0026amp; 与 两个位都为1时，结果才为1 | 或 两个位都为0时，结果才为0 ^ 异或 两个位相同为0，相异为1 ~ 取反 0变1，1变0 \u0026laquo; 左移","title":"基础知识"},{"content":"[toc]\n前言 本文档基于elasticsearch 6.4.2 , jdk 1.8 ,三台centos6 服务器,以下所有操作均在jht用户下完成,以10.10.204.167为主服务器\n不要使用root账号安装!!! 使用额外账号安装,例如:jht\n中文官网 1. 节点角色 ES 支持的功能较多（譬如机器学习），为了更好的支持这些功能，es 定义了多种节点角色，一个节点可以同时担任多种角色，默认具有以下角色：\nmaster data data_content data_hot data_warm data_cold data_frozen ingest ml remote_cluster_client transform ES 集群中的节点大致可以分为管理节点、数据节点和任务节点。\n管理节点 ：\n角色配置为 master，能够参与 leader 投票、可以被选为 leader 的节点 被选为 leader 的节点负责集群的管理工作 如果同时配置了 voting_only 角色，该节点只参与选举投票，不做候选人 数据节点：\n角色配置为 data，负责存储数据和提供数据查询、聚合处理的节点 数据节点可以进一步细分：data_content，data_hot，data_warm，data_cold，data_frozen data_content: 存放 document 数据 data_hot： 存放刚写入的时间序列数据（热点数据），要能够快速读写 data_warm：不再经常更新、低频查询的数据 data_cold：极少访问的只读数据 data_frozen：缓存从快照中查询出的数据，如果不存在从快照读取后缓存， 任务节点 类型较多，每个角色负责专门的任务：\n[]： 角色为空，coordinating node，没有任何角色，只负责将收到的请求转发到其它节点 ingest：承担 pipeline 处理任务的节点 remote_cluster_client：跨集群操作时，与其它机器进行通信的节点 ml：执行机器学习任务和处理机器学习 api 的节点，通常建议同时配置角色 remote_cluster_client transform：数据处理节点，对文档数据进行再次加工，通常建议同时配置角色 remote_cluster_client ElasticSearch 零基础入门（1）：基本概念、集群模式、查询语法和聚合语法By李佶澳 (lijiaocn.com) 2. 几种集群模式 基本高可用(数据和管理不分离) Elasticsearch集群要达到基本高可用，一般要至少启动3个节点，3个节点互相连接，单个节点包括所有角色，其中任意节点停机集群依然可用。为什么要至少3个节点？因为集群选举算法奇数法则。\n本文也是介绍的这种\n数据与管理分离 Elasticserach管理节点职责是管理集群元数据、索引信息、节点信息等，自身不设计数据存储与查询，资源消耗低；相反数据节点是集群存储与查询的执行节点。\n管理节点与数据节点分离，各司其职，任意数据节点故障或者全部数据节点故障，集群仍可用；管理节点一般至少启动3个，任意节点停机，集群仍正常运行。\n数据与协调分离 Elasticsearch内部执行查询或者更新操作时，需要路由，默认所有节点都具备此职能，特别是大的查询时，协调节点需要分发查询命令到各个数据节点，查询后的数据需要在协调节点合并排序，这样原有数据节点代价很大，所以分离职责，\n数据节点标签 Elasticsearch给数据节点标签，目的是分离索引数据的分布，在一个集群规模中等以上，索引数据用途多种多样，对于数据节点的资源需求不一样，数据节点的配置可以差异化，有的数据节点配置高做实时数据查询，有的数据节点配置低做历史数据查询，有的数据节点做专门做索引重建。\nElasticsearch集群部署时需要考虑基础硬件条件，集群规模越来越大，需要多个数据中心，多个网络服务、多个服务器机架，多个交换机等组成，索引数据的分布与这些基础硬件条件都密切相关。\n主副分片分离 Elasticsearch集群规模大了之后得考虑集群容灾，若某个机房出现故障，则可以迅速切换到另外的容灾机房。\n跨集群操作 Elasticsearch单个集群规模不能无限增长，理论上可以，实际很危险，通过创建多个分集群分解，集群直接建立直接连接，客户端可以通过一个代理集群访问任意集群，包括代理集群本身数据。\nElasticsearch集群支持异地容灾，采用的是跨集群复制的机制，与同一集群主分片副本分片分离是不同的概念，2个集群完全是独立部署运行，仅数据同步复制。\nElasticsearch集群模式知多少？-阿里云开发者社区 (aliyun.com) 3.1 集群搭建 3.1. 准备 安装jdk 下载elasticsearch安装包并解压 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.2.tar.gz \u0026amp;\u0026amp; tar-xzvf elasticsearch-6.4.2.tar.gz 官网下载地址 3.2. 安装 修改配置\n进入elasticsearch-6.4.2/config ,修改 elasticsearch.yml 文件(已删除多余注释)\n# 集群名称,各节点应保持一致 cluster.name: bdp_es # 节点名称,每台服务器应不一样 node.name: node167 #当主节点挂了后，是否有资格被竞选为主节点 node.master: true #是否为数据节点（能否存储数据） node.data: true path.data: /opt/soft/es-cdh/elasticsearch-6.4.2/data path.logs: /opt/soft/es-cdh/elasticsearch-6.4.2/logs bootstrap.system_call_filter: false network.host: 0.0.0.0 http.port: 9200 transport.tcp.port: 9300 # 集群配置 discovery.zen.ping.unicast.hosts: [\u0026#34;10.10.204.166:9300\u0026#34;, \u0026#34;10.10.204.167:9300\u0026#34;,\u0026#34;10.10.204.165:9300\u0026#34;] #以下三个配置的意思是：每隔30秒向主节点发送一次心跳监测，120秒之内如果没有回应，则算超时 ##连续超时6次，则认为主节点已经挂了 discovery.zen.fd.ping_timeout: 120s discovery.zen.fd.ping_retries: 6 discovery.zen.fd.ping_interval: 30s ##ping节点的响应时间 client.transport.ping_timeout : 60s # 至少几个节点 discovery.zen.minimum_master_nodes: 2 创建 数据和日志目录\nmkdir -p /opt/soft/es-cdh/elasticsearch-6.4.2/data\nmkdir -p /opt/soft/es-cdh/elasticsearch-6.4.2/logs\n修改系统配置 编辑/etc/security/limits.conf文件\n# 配置解释见limits.conf # 追加如下内容,如已存在则更新 * soft nofile 65536 * hard nofile 65536 * soft nproc 16384 * hard nproc 16384 编辑/etc/sysctl.conf\n# 追加如下内容,如已存在则更新 vm.max_map_count = 655360 修使配置生效, 执行命令\nsysctl -p 检查防火墙,必要时打开9200和9300端口\n检查防火墙,必要时打开9200和9300端口\nsudo vim /etc/sysconfig/iptables\n-A INPUT -m state --state NEW -m tcp -p tcp --dport 9200 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 9300 -j ACCEPT #重启防火墙，检查防火墙重启是否成功\nservice iptables restart\n启动时如果仍报错,就继续如下编辑操作(没有则新建)\n编辑 /etc/security/limits.d/90-nproc.conf\n* soft nproc 204800 * hard nproc 204800 编辑/etc/security/limits.d/def.conf\n* soft nofile 204800 * hard nofile 204800 启动单机并检测\n进入es的bin目录,执行\n./elasticsearch 出现一直在等待其他节点相关日志即成功(因为至少启动两个节点)\n搭建其他节点\n将所有文件复制到其他服务器上 scp -r elasticsearch-6.4.2 jht@10.10.204.166:/opt/soft/es-cdh\nscp -r elasticsearch-6.4.2 jht@10.10.204.165:/opt/soft/es-cdh\n修改elasticsearch.yml文件,保证节点名不一样 启动三台服务器并验证\n使用elasticsearch head插件验证\n连接集群中任意一台均可,能连接上即搭建成功\n2.3. 实战 以10.101.204.167为主服务器,在其上写了管理脚本,脚本路径: /home/jht/runShell/esManage.sh\n#!/bin/bash # 简易es管理脚本,可一键启动和停止 # 可改进: 脚本健壮性(如:可重复启动); 命令提取为变量等 start(){ echo \u0026#34;启动es集群.....\u0026#34; esPath=/opt/soft/es-cdh/elasticsearch-6.4.2/bin # ${esPath}/elasticsearch -d // 表示后台启动 nohup ${esPath}/elasticsearch \u0026gt; /home/jht/runShell/logs/es.log 2\u0026gt;\u0026amp;1 \u0026amp; ssh jht@10.10.204.166 \u0026#34;source /etc/profile; nohup ${esPath}/elasticsearch \u0026gt; /home/jht/runShell/logs/es.log 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; ssh jht@10.10.204.165 \u0026#34;source /etc/profile; nohup ${esPath}/elasticsearch \u0026gt; /home/jht/runShell/logs/es.log 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; } stop(){ echo \u0026#34;关闭es集群.....\u0026#34; pgrep -f Elasticsearch | xargs kill -9 ssh jht@10.10.204.166 \u0026#34; pgrep -f Elasticsearch | xargs kill -9\u0026#34; ssh jht@10.10.204.165 \u0026#34; pgrep -f Elasticsearch | xargs kill -9\u0026#34; } case \u0026#34;$1\u0026#34; in \u0026#34;start\u0026#34;) start ;; \u0026#34;stop\u0026#34;) stop ;; *) echo \u0026#34; 允许的参数(必选): { start| stop } \u0026#34; ;; esac 启动es集群 : /home/jht/runShell/esManage.sh start\n关闭es集群: /home/jht/runShell/esManage.sh stop\n参考文章:\n集群搭建 环境配置 安装问题集锦 远程kill问题 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F.html","summary":"[toc] 前言 本文档基于elasticsearch 6.4.2 , jdk 1.8 ,三台centos6 服务器,以下所有操作均在jht用户下完成,以10.10.204.167","title":"集群模式"},{"content":"[toc]\n常用参数说明 配置项 说明 daemonize no Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程（Windows 不支持守护线程的配置为 no ） pidfile /var/run/redis.pid 当 Redis 以守护进程方式运行时，Redis 默认会把 pid 写入 /var/run/redis.pid 文件，可以通过 pidfile 指定 port 6379 指定 Redis 监听端口，默认端口为 6379 bind 127.0.0.1 绑定的主机地址,默认只允许本地访问,将此配置删除,则允许外网访问(但是不建议,因为redis处理快,可能被暴力破解,而且以前也出现过redis漏洞) logfile stdout 日志记录方式，默认为标准输出，也可以写文件路径,如果配置 Redis 为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 /dev/null requirepass password 设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH 命令提供密码，默认关闭 https://www.runoob.com/redis/redis-conf.html 数据类型 Redis支持五种数据类型：\nstring（字符串）， hash（哈希）， list（列表）， set（集合） zset(sorted set：有序集合)。 String string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。\nstring 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。\n// 基本命令 SET runoob \u0026#34;菜鸟教程\u0026#34; GET runoob Hash Redis hash 是一个键值(key=\u0026gt;value)对集合。\nRedis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 每个 hash 可以存储\n2^{32}-1 键值对（40多亿）。\n可以理解为vaule是一个map,里面存很多k-v\nHMSET runoob field1 \u0026#34;Hello\u0026#34; field2 \u0026#34;World\u0026#34; HGET runoob field1 List Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。\n比如可以通过lrange命令，就是从某个元素开始读取多少个元素，可以基于list实现分页查询，这个很棒的一个功能，基于redis实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走 比如可以搞个简单的消息队列，从list头怼进去，从list尾巴那里弄出来 lpush runoob redis lpush runoob mongodb lrange runoob 0 10 set Redis 的 Set 是 string 类型的无序集合。\n集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。集合中最大的成员数为\n2^{32}-1 (4294967295, 每个集合可存储40多亿个成员)。如果有多台服务器产生的值需要放一起去重,就可以用set全局去重了\nsadd runoob redis sadd runoob mongodb sadd runoob mongodb smembers runoob zset Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。比如把分数换成时间戳,就可以按照时间来排序啦\nzset的成员是唯一的,但分数(score)却可以重复。score可以是整数，也可以是浮点数，还可以是+inf表示无穷大，-inf表示负无穷大\nzadd runoob 0 redis zadd runoob 0 mongodb zadd runoob 0 mongodb zadd runoob 1 rabitmq ZRANGEBYSCORE runoob 0 1000 ZRANGEBYSCORE runoob 0 +inf HyperLogLog (不算类型) Redis 在 2.8.9 版本添加了 HyperLogLog 结构。用来计算集合数量,(基数数量,理解为去重后数量),数量值为估算,不是100%准确(这是因为它的(概率)算法),一般用来统计在线用户数,访问量等等\n感觉有set,HyperLogLog就鸡肋了,其实不然,HyperLogLog统计数量时要求内存很小,几乎是恒定的,但是set会随着集合增加而增大所需内存\nPFADD runoobkey \u0026#34;redis\u0026#34; PFADD runoobkey \u0026#34;mongodb\u0026#34; PFCOUNT runoobkey Pub/Sub模式 发布/订阅模式,类似于消息中间件,但实现方式不一样\n原理\nclient-\u0026gt;pubsub_channels 是客户端维护的一个以dict结构的维护的订阅频道哈希表，VAL是NULL,不需要值。\nserver-\u0026gt;pubsub_channels 是服务端维护的一个以dict结构的维护的订阅频道哈希表，VAL是以client维护的双向链表adlist。\n订阅\n订阅流程：SUBSCRIBE命令 SUBSCRIBE channel [channel \u0026hellip;]\n1.首先是将当前订阅的频道channel添加进客户端的pubsub_channels哈希表里面。\n2.然后在将当前订阅的频道channel和对应的client以键值对添加服务端的pubsub_channels的哈希表里。\n3.最后将返回的信息返回给客户端。\n发布\n发布流程：PUBLISH命令 PUBLISH channel message\n1.首先通过频道在服务端的pubsub_channels哈希表里面找到对应的客户端链表。\n2.然后递归循环链表，逐个将消息message发送对应订阅的客户端。\n3.正则匹配的频道 逐个发送对应订阅的客户端。\n缺点:\n消息发布后如果没有消费者消费, 则这条消息会丢失, 消息不会持久化,如果redis宕机数据则丢失 集群模式下, publish命令都会向所有节点进行广播，加重带宽负担(因为集群通信是gossip,所以相当于全部节点都广播了) Redis深度历险-Redis PubSub消息订阅发送_樊先知樊先知的博客-CSDN博客 redis pub/sub原理及实战_iverson2010112228的专栏-CSDN博客 Redis 集群中的 PUB/SUB 相关问题。 - SegmentFault 思否 过期策略 如果假设你设置一个一批key只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 答案是：定期删除+惰性删除\nredis是单线程，收割时间也会占用线程处理时间，如果收割过于频繁，会导致读写出现卡顿。\n1、主库过期策略 1.1、定时扫描\n首先将每个设置了过期时间的key放到一个独立的hash中，默认每秒定时遍历这个hash而不是整个空间：\n并不会遍历所有的key，采用一种简单的贪心策略\n1.1.1、从过期key字典中，随机找20个key。\n1.1.2、删除20个key中过期的key\n1.1.3、如果2中过期的key超过1/4，则重复第一步\n1.1.4、每次处理的时间都不会超过25ms (若超过则直接结束)\n如果有大量的key在同一时间段内过期，就会造成数据库的集中访问，就是缓存雪崩！\n1.2、惰性策略\n客户端访问的时候，会对这个key的过期时间进行检查，如果过期了就立即删除。惰性策略是对定时策略的补充，因为定时策略不会删除所有过期的key\n2、从库过期策略 redis不会扫描从库，删除主库数据的时候，在aof文件里生成一条del指令，在主从同步的时候，从库会执行这条指令，删除过期key。\n所以集群分布式锁算法的漏洞就是这样产生的。(所以得用红锁,但红锁也不是百分百准确,详见分布式锁文章 )\n很简单，就是说，你的过期key，靠定期删除没有被删除掉，还停留在内存里，占用着你的内存呢，除非你的系统去查一下那个key，才会被redis给删除掉。 如果定期删除漏掉了很多过期key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了，咋整？ 答案是：走内存淘汰机制。\nRedis系列六：redis缓存失效策略 - 工作中的点点滴滴 - 博客园 (cnblogs.com) 内存淘汰 最大内存的设置是通过设置maxmemory来完成的，格式为maxmemory bytes ,当目前使用的内存超过了设置的最大内存，就会进行内存释放\nnoeviction：新写入操作会报错(不删除数据),也是默认值 allkeys-lru：在键空间中，移除最近最少使用的key allkeys-random：在键空间中，随机移除某个key volatile-lru：在设置了过期时间的键空间中，移除最近最少使用的key volatile-random：在设置了过期时间的键空间中，随机移除某个key volatile-ttl：在设置了过期时间的键空间中，有更早过期时间的key优先移除 持久化 Redis的一种持久化方式叫快照（snapshotting，RDB）,另一种方式是只追加文件（append-only file,AOF）.默认使用RDB方式\nRedis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 启动时优先从AOF文件中恢复数据 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。\n链接：https://www.jianshu.com/p/65765dd10671\nAOF 文件的体积通常要大于 RDB 文件的体积。\nAOF的速度会慢于RDB方式 ,因为AOF默认是每秒同步一次\nRDB持久化 RDB持久化方式是通过快照(snapshotting)完成的，当符合一定条件时，redis会自动将内存中所有数据以二进制方式生成一份副本并存储在硬盘上。当redis重启时，并且AOF持久化未开启时(默认不开启)，redis会读取RDB持久化生成的二进制文件(默认名称dump.rdb，可通过设置dbfilename修改)进行数据恢复，对于持久化信息可以用过命令“info Persistence”查看。\nRDB生成快照可自动促发，也可以使用命令手动触发，以下是redis触发执行快照条件，后续会对每个条件详细说明：\n客户端执行命令save和bgsave会生成快照； 根据配置文件save m n规则进行自动快照； 主从复制时，从库全量复制同步主库数据，此时主库会执行bgsave命令进行快照； 客户端执行数据库清空命令FLUSHALL时候，触发快照(清空快照)； 客户端执行shutdown关闭redis时，触发快照(保存全部数据,以便下次启动恢复)； 客户端执行save命令，该命令强制redis执行快照，这时候redis处于阻塞状态，不会响应任何其他客户端发来的请求，直到RDB快照文件执行完毕，所以请慎用。 save m n规则说明：在指定的m秒内，redis中有n个键发生改变，则自动触发bgsave。该规则默认也在redis.conf中进行了配置，并且可组合使用(就是写多个)，满足其中一个规则，则触发bgsave 当触发生成快照时, 是通过调用操作系统的fork函数创建子进程去实现的, 它会强制阻塞进程, 导致redis无法对外提供服务\n因为fork函数 可以让子进程共享父进程的数据空间, 子进程不会复制父进程的数据空间，但是会复制内存页表（页表相当于内存的索引、目录）；父进程的数据空间越大，内存页表越大，fork时复制耗时也会越多。\nRedis中存在的两大阻塞：fork阻塞和AOF追加阻塞_u014753478的博客-CSDN博客 AOF持久化 默认情况下，redis是关闭了AOF持久化，开启AOF通过配置appendonly为yes开启(配置文件).AOF实现本质是基于redis通讯协议，将命令以纯文本的方式写入到文件中。\n我们修改配置文件或者在命令行直接使用config set修改，在用config rewrite同步到配置文件。通过客户端修改好处是不用重启redis，AOF持久化直接生效。但不是所有的配置都可以这样,了解一下,还是老实改配置文件吧\nAOF持久化过程\nredisAOF持久化过程可分为以下阶段：\n追加写入\nredis将每一条写命令以redis通讯协议添加至缓冲区aof_buf,这样的好处在于在大量写请求情况下，采用缓冲区暂存一部分命令随后根据策略一次性写入磁盘，这样可以减少磁盘的I/O次数，提高性能。\n同步命令到硬盘 当写命令写入aof_buf缓冲区后，redis会将缓冲区的命令写入到文件，redis提供了三种同步策略，由配置参数appendfsync决定，下面是每个策略所对应的含义：\nappendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘(默认值) appendfsync no #让操作系统决定何时进行同步 文件重写(bgrewriteaof)\n当开启的AOF时，随着时间推移，AOF文件会越来越大,当然redis也对AOF文件进行了优化，即触发AOF文件重写条件（后续会说明）时候，redis将使用bgrewriteaof对AOF文件进行重写。这样的好处在于减少AOF文件大小，同时有利于数据的恢复。 重写时候策略：\n重复或无效的命令不写入文件 过期的数据不再写入文件 多条命令合并写入（当多个命令能合并一条命令时候会对其优化合并作为一个命令写入，例如“RPUSH list1 a RPUSH list1 b\u0026quot; 合并为“RPUSH list1 a b” ） 重写触发条件\nAOF文件触发条件可分为手动触发和自动触发：\n手动触发：客户端执行bgrewriteaof命令。 自动触发：自动触发通过以下两个配置协作生效： auto-aof-rewrite-min-size: AOF文件最小重写大小，只有当AOF文件大小大于该值时候才可能重写,4.0默认配置64mb。 auto-aof-rewrite-percentage：当前AOF文件大小和最后一次重写后的大小之间的比率等于或者等于指定的增长百分比，如100代表当前AOF文件是上次重写的两倍时候才重写。　AOF追加操作会造成阻塞\n在AOF中，如果AOF缓冲区的文件同步策略为everysec，则在主线程中，命令写入aof_buf后调用操作系统write操作内存，write完成后主线程返回；然后fsync操作由专门的文件同步线程每秒调用一次。\n这种做法的问题在于，如果硬盘负载过高，那么fsync操作可能会超过1s；如果Redis主线程持续高速向aof_buf写入命令，硬盘的负载可能会越来越大，IO资源消耗会更快, 此时write操作会被阻塞,直到fsync 处理完成。如果此时Redis异常退出，会导致数据丢失可能远超过1s。\nRedis中存在的两大阻塞：fork阻塞和AOF追加阻塞_u014753478的博客-CSDN博客 RDB-AOF混合持久化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble yes 开启）。\nRedis 5.0 则默认开启\n如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头\n链接：https://www.jianshu.com/p/65765dd10671 https://www.cnblogs.com/wdliu/p/9377278.html 单线程模型 Redis客户端对服务端的每次调用都经历了发送命令，执行命令，返回结果三个过程。其中执行命令阶段，由于Redis是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个队列中，然后逐个被执行。并且多个客户端发送的命令的执行顺序是不确定的。但是可以确定的是不会有两条命令被同时执行，不会产生并发问题，这就是Redis的单线程基本模型。\n一次请求的执行步骤 redis分客户端和服务端，一次完整的redis请求事件有多个阶段（客户端到服务器的网络连接\u0026ndash;\u0026gt;redis读写事件发生\u0026ndash;\u0026gt;redis服务端的数据处理（单线程）\u0026ndash;\u0026gt;数据返回）。平时所说的redis单线程模型，本质上指的是服务端的数据处理阶段，不牵扯网络连接和数据返回，这是理解redis单线程的第一步。接下来，针对不同阶段分别阐述个人的一些理解。\n1：客户端到服务器的网络连接 首先，客户端和服务器是socket通信方式，socket服务端监听可同时接受多个客户端请求，这点很重要，如果不理解可先记住。注意这里可以理解为本质上与redis无关，这里仅仅做网络连接，或者可以理解为，为redis服务端提供网络交互api。\n​ 假设建立网络连接需要30秒（为了更容易理解，所以时间上扩大了N倍）\n2：redis读写事件发生并向服务端发送请求数据 ​ 首先确定一点，redis的客户端与服务器端通信是基于TCP连接，第一阶段仅仅是建立了客户端到服务器的网络连接，然后才是发生第二阶段的读写事件。\n​ 完成了上一个阶段的网络连接，redis客户端开始真正向服务器发起读写事件，假设是set（写）事件，此时redis客户端开始向建立的网络流中送数据，服务端可以理解为给每一个网络连接创建一个线程同时接收客户端的请求数据。\n​ 假设从客户端发数据，到服务端接收完数据需要10秒。\n3：redis服务端的数据处理 ​ 服务端完成了第二阶段的数据接收，接下来开始依据接收到的数据做逻辑处理，然后得到处理后的数据。数据处理可以理解为一次方法调用，带参调用方法，最终得到方法返回值。不要想复杂，重在理解流程。\n​ 假设redis服务端处理数据需要0.1秒\n4：数据返回 ​ 这一阶段很简单，当reids服务端数据处理完后 就会立即返回处理后的数据，没什么特别需要强调的。\n​ 假设服务端把处理后的数据回送给客户端需要5秒。\nredis 单线程的理解 - myseries - 博客园 (cnblogs.com) 运行方式 前面提到 redis是通过socket的方式监听多个客户端的, 每客户端和服务端都会共同经历 建立连接-请求数据-处理数据-返回数据 这几个阶段,\n如果有几个socket同时发起请求, 岂不是得挨个等待? 其实不然\nredis会基于这些建立的连接去探测哪个连接已经接收完了客户端的请求数据（注意：不是探测哪个连接建立好了，而是探测哪个接收完了请求数据），而且这里的探测动作就是单线程的开始，一旦探测到则基于接收到的数据开始数据处理阶段，然后返回数据，再继续探测下一个已经接收完请求数据的网络连接。注意，从探测到数据处理再到数据返回，全程单线程。这应该就是所谓的redis单线程\nredis 单线程的理解 - myseries - 博客园 (cnblogs.com) IO的多路复用 “多路”指的是多个网络连接，“复用”指的是复用同一个线程。\n在redis运行方式 段落中, 说到redis会去探测哪个链接已经接收完数据了, 这个探测就是那个线程\nredis 单线程的理解 - myseries - 博客园 (cnblogs.com) 一篇超详细的文章, 日后再好好消化 彻底搞懂IO多路复用 (qq.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/%E7%AE%80%E4%BB%8B.html","summary":"[toc] 常用参数说明 配置项 说明 daemonize no Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程（Windows 不支持守护线程的配置为 no","title":"简介"},{"content":"[toc]\n1. 前言 Apache Spark 是一个围绕速度、易用性和复杂分析构建的大数据处理框架,Spark是用Scala程序设计语言 编写而成，运行于Java虚拟机（JVM）环境之上\nSpark运行在现有的Hadoop分布式文件系统基础之上（HDFS ）提供额外的增强功能。它支持将Spark应用部署到 现存的Hadoop v1集群（with SIMR – Spark-Inside-MapReduce）或Hadoop v2 YARN集群甚至是Apache Mesos 之中。也有自己的资源管理器(Standalone),可以脱离Hadoop生态圈独立存在\nSpark通过在数据处理过程中成本更低的洗牌（Shuffle）方式，将MapReduce提升到一个更高的层次。\nSpark将中间结果保存在内存中而不是将其写入磁盘，当内存放了足够多的数据时,会放在磁盘上(有存储策略),所以Spark可以用于处理大于集群内存容量总和的数据集\nSpark允许程序开发者使用有向无环图（DAG ）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。\n2. 生态系统 Spark生态圈以Spark Core为核心，从HDFS、Amazon S3和HBase等持久层读取数据，以MESS、YARN和自身携带的Standalone为资源管理器调度Job完成Spark应用程序的计算。 这些应用程序可以来自于不同的组件，如Spark Shell/Spark Submit的批处理、Spark Streaming的实时处理应用、Spark SQL的即时查询、BlinkDB的权衡查询、MLlib/MLbase的机器学习、GraphX的图处理和SparkR的数学计算等等。\nSpark Core**:** 实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集（resilient distributed dataset，简称RDD）的API定义。 Spark Core提供了创建和操作这些集合的多个API。\n来自* \u0026lt;https://www.douban.com/note/536766108/?from=tag \u0026gt;\nSpark Streaming: Spark Streaming 基于微批量方式的计算和处理，可以用于处理实时的流数据。它使用DStream，简单来说就是一个弹性分布式数据集（RDD）系列，处理实时数据。\nSpark SQL: Spark SQL 可以通过JDBC API将Spark数据集暴露出去，而且还可以用传统的BI(商业智能:提供报表展示分析帮助企业作出决策)和可视化工具在Spark数据上执行类似SQL的查询。用户还可以用Spark SQL对不同格式的数据（如JSON，Parquet以及数据库等）执行ETL，将其转化，然后暴露给特定的查询。\nSpark MLlib: MLlib 是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。\nSpark GraphX: GraphX 是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。\n除了这些库以外，还有一些其他的库，如BlinkDB和Tachyon。\nSpark常用术语\n术语 描述 Application Spark的应用程序，包含一个Driver program和若干Executor SparkContext Spark应用程序的入口，负责调度各个运算资源，协调各个Worker Node上的Executor Driver Program 运行Application的main()函数并且创建SparkContext Executor 是为Application运行在Worker node上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。每个Application都会申请各自的Executor来处理任务 Cluster Manager 在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn) Worker Node 集群中任何可以运行Application代码的节点，运行一个或多个Executor进程 Task 运行在Executor上的工作单元(rdd的转换过程) Job SparkContext提交的具体Action操作，常和Action对应,一个JOB包含多个RDD及作用于相应RDD上的各种Operation(理解为一个action表示一个job) Stage 每个Job会被拆分很多组task，每组任务被称为Stage，也称TaskSet(窄依赖是一个stage,宽依赖是一个stage) RDD 是Resilient distributed datasets的简称，中文为弹性分布式数据集;是Spark最核心的模块和类 DAGScheduler 根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler TaskScheduler 将Taskset提交给Worker node集群运行并返回结果 Transformations 是Spark API的一种类型，Transformation返回值还是一个RDD，所有的Transformation采用的都是懒策略，如果只是将Transformation提交是不会执行计算的 Action 是Spark API的一种类型，Action返回值不是一个RDD，而是一个scala集合；计算只有在Action被提交的时候计算才被触发(懒计算)。 worker 集群中任何可以运行Application代码的节点，类似于YARN中的NodeManager节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点； 来自* \u0026lt;http://www.cnblogs.com/shishanyuan/p/4700615.html \u0026gt;\n运行架构图:\n3.弹性分布式数据集 弹性分布式数据集 或RDD（Resilient Distributed Datasets）是Spark框架中的核心概念。可以将RDD视作数据库中的一张表。其中可以保存任何类型的数据。Spark将数据存储在不同分区上的RDD之中,一个RDD中有多个分区,这些分区可以分布在不同节点上(也就是说,RDD是分布存储的),分区的多少涉及对这个RDD进行并行计算的粒度,每个RDD分区计算操作都在一个单独的任务中被执行,分区个数可以自行指定和改变\nRDD可以帮助重新安排计算并优化数据处理过程。\n此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。\nRDD是不可变的,是只可读的。你可以用变换（Transformation）操作修改RDD，但是这个变换所返回的是一个全新的RDD，而原有的RDD仍然保持不变(有点像String的不变性),也就是说,在丢失或者操作失败后都是可以重建的,具有容错。\nspark五大特性(源自rdd类注释)\n是分区(可以存储在不同节点)的集合,因为一个rdd包含了多个分区的数据,把block块的东西整合到rdd(字段名: dependencies 存储在seq数据集中,类型是dependency,便利和取第一个)\n对每个分片并行计算(一般情况下分片大小等于分区大小)(名为computer函数,可重写)\n是其他rdd的依赖集合,就是知道该rdd从哪来,便于回溯(若宕机,存于内存中的rdd会丢失,spark会借由此重算)(字段名: partitions 存储Array中,类型是Partition ,便于通过下标获取)\n(可选)可以重新分区(调节并行度)(一个分区对应一个并行度) (主要是k-v形式的rdd有)\n(可选)给每个分片找到优先数据位置(找最近的数据处理最快嘛)(主要是来源有多个备份的rdd,例如HDFS文件,因为重写了getPreferredLocaltions方法)\n可选: 只有部分类型rdd才有的特性,\nRDD支持两种类型的操作：\n变换（Transformation） 行动（Action） 变换：变换 的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。\n变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce等。\n行动：行动 操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值(Driver会接收到)。\n行动操作包括：reduce，collect，count，first，take，countByKey以及foreach等。\n注:\n只有在行动(action)时才会触发运算,也就是说变换是不会运行的计算的;\nRDD是粗粒度计算的.粗粒度:一个转化或者行动会把整个RDD里面的东西都进行操作\n3.1 RDD依赖关系 由于RDD是粗粒度的操作数据集，每个Transformation操作都会生成一个新的RDD，所以RDD之间就会形成类似流水线的前后依赖关系,进而形成了有向无环图（DAG ）；\n在spark中，RDD之间存在两种类型的依赖关系：窄依赖(Narrow Dependency)和宽依赖(Wide Dependency)\n窄依赖 :是指每个父RDD的一个Partition最多被子RDD的一个Partition所使用，例如map、filter、union等操作都会产生窄依赖；(父RDD的分区都在相同的节点)\n宽依赖 :是指每个父RDD的一个Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；(可能跨越多个节点妈的)\n需要特别说明的是对join操作有两种情况：\n如果两个RDD在进行join操作时，一个RDD的partition仅仅和另一个RDD中已知个数的Partition进行join，那么这种类型的join操作就是窄依赖，例如图1中左半部分的join操作(join with inputs co-partitioned)； 其它情况的join操作就是宽依赖,例如图1中右半部分的join操作(join with inputs not co-partitioned)，由于是需要父RDD的所有partition进行join的转换，这就涉及到了shuffle，因此这种类型的join操作也是宽依赖。 简单的说,就是元素被用几次,只用一次就是窄依赖,超过一次就是宽依赖\n之所以这么区分依赖关系，是因为它们有本质的区别。使用窄依赖时，可以精确知道依赖的上级RDD分区。这样便于回溯。而宽依赖则开销会大。RDD仔细维护者这种依赖关系和计算方法,使得通过重新计算来恢复RDD成为可能。如果链条太长，则恢复代价太大，所以spark又提出一种检查点的机制。\n来自* \u0026lt;http://bbs.pinggu.org/thread-4637506-1-1.html \u0026gt;\n3.2 RDD运行原理 那么 RDD在Spark架构中是如何运行的呢？总高层次来看，主要分为三步：\n创建 RDD 对象 DAGScheduler模块介入运算，计算RDD之间的依赖关系。RDD之间的依赖关系就形成了DAG 每一个JOB被分为多个Stage，划分Stage的一个主要依据是当前计算因子的输入是否是确定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销。 来自* \u0026lt;http://www.cnblogs.com/shishanyuan/p/4721326.html \u0026gt;\n3.2.1 DGA调度 sparkcontext在初始化时,创建了DAG调度与task调度来负责RDD action操作的调度执行。\n3.2.1.1 DAGScheduler DAGSchedule负责spark的最高级别的任务调度调度的力度是stage，它为每个job的所有stage计算一个有向无环图控制他们的并发，便找到一个最佳的路径来执行他们。具体的执行过程是将stage下的task集提交给Taskschedule对象，由它来提交到集群上去申请资源并最终完成执行。\n1.runjob过程\n所有需要执行的RDD action都会调用sparkCcontext.runJob来提交任务，而 SparkContest.runjob调用的是DAGScheduler.runjob, sunjob调用submitjob提交任务,并等待任务结束.提交任务是不是按job的先后顺序提交的,而是倒序,每个job的最后一个操作是action操作，DAG把这最后的action操作操作当做一个stage首先提交，然后逆向逐级递规填补缺少的上级stage，从而生成一颗实现最后action操作的最短有效无环图，然后从头开始计算。\n任务提交后的处理过程大致如下:\nsubmitJob生成新的job id, 发送消息jobsubmitted。\nDAG收到jobSubmitted消息,调用handleJobSubmitted来处理\nhandleJobSubmitted创建一个ResultStage,,并使用submitStage来提交这个ResultStage\n3.2.1.2 TaskSched 相对DAG schedule而言tasked sketches低级别的调度接口。允许实现不同的task调度器。每个task sketched对象只服务于一个sparkleContext的task调度。taskScheduler从DAGScheduler的每个 stage 接受一组task，并负责将他们送到集群上运行他们。\n4.Spark on YARN运行过程 spark运行在不同的资源管理器上有不同的运行过程,这里解释在YARN平台的运行,详情见\nhttp://www.cnblogs.com/shishanyuan/p/4721326.html Spark运行模式\n运行环境 模式 描述 Local 本地模式 常用于本地开发测试，本地还分为local单线程和local-cluster多线程; Standalone 集群模式 典型的Mater/slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现HA On yarn 集群模式 运行在yarn资源管理器框架之上，由yarn负责资源管理，Spark负责任务调度和计算 On mesos 集群模式 运行在mesos资源管理器框架之上，由mesos负责资源管理，Spark负责任务调度和计算 On cloud 集群模式 比如AWS的EC2，使用这个模式能很方便的访问Amazon的S3;Spark支持多种分布式存储系统：HDFS和S3 Spark on YARN模式根据Driver在集群中的位置分为两种模式：\n一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。\n4.1 YARN-Client Yarn-Client模式中，Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问。\n4.2 YARN-Cluster 在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：\n第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；\n第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。\nYARN-Client 与 YARN-Cluster 区别\n理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。\nYARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业； YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。 来自* \u0026lt;http://www.cnblogs.com/shishanyuan/p/4721326.html \u0026gt;\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%8E%9F%E7%90%86.html","summary":"[toc] 1. 前言 Apache Spark 是一个围绕速度、易用性和复杂分析构建的大数据处理框架,Spark是用Scala程序设计语言 编写而成，运行于Java虚拟机（JVM","title":"简介及原理"},{"content":"[toc]\nSystemd 概述 Systemd 简介 历史上，Linux 的启动一直采用 init 进程。这种方法有两个缺点。\n一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。\n二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。\n下面的命令用来启动服务。\n$ sudo /etc/init.d/apache2 start # 或者 $ service apache2 start Systemd 就是为了解决这些问题而诞生的,字母d是守护进程（daemon）的缩写\nSystemd 是一系列工具的集合，其作用也远远不仅是启动操作系统，它还接管了后台服务、结束、状态查询，以及日志归档、设备管理、电源管理、定时任务等许多职责，并支持通过特定事件（如插入特定 USB 设备）和特定端口数据触发的 On-demand（按需）任务。\nSystemd 的后台服务还有一个特殊的身份——它是系统中 PID 值为 1 的进程。\n更少的进程 Systemd 提供了 服务按需启动 的能力，使得特定的服务只有在真定被请求时才启动。\n允许更多的进程并行启动 在 SysV-init 时代，将每个服务项目编号依次执行启动脚本。Ubuntu 的 Upstart 解决了没有直接依赖的启动之间的并行启动。而 Systemd 通过 Socket 缓存、DBus 缓存和建立临时挂载点等方法进一步解决了启动进程之间的依赖，做到了所有系统服务并发启动。对于用户自定义的服务，Systemd 允许配置其启动依赖项目，从而确保服务按必要的顺序运行。\n使用 CGroup 跟踪和管理进程的生命周期 在 Systemd 之间的主流应用管理服务都是使用 进程树 来跟踪应用的继承关系的，而进程的父子关系很容易通过 两次 fork 的方法脱离。\n而 Systemd 则提供通过 CGroup 跟踪进程关系，引补了这个缺漏。通过 CGroup 不仅能够实现服务之间访问隔离，限制特定应用程序对系统资源的访问配额，还能更精确地管理服务的生命周期。\n统一管理服务日志 Systemd 是一系列工具的集合， 包括了一个专用的系统日志管理服务：Journald。这个服务的设计初衷是克服现有 Syslog 服务的日志内容易伪造和日志格式不统一等缺点，Journald 用 二进制格式 保存所有的日志信息，因而日志内容很难被手工伪造。Journald 还提供了一个 journalctl 命令来查看日志信息，这样就使得不同服务输出的日志具有相同的排版格式， 便于数据的二次处理。\nSystemd 架构 Unit 文件 前面说到Systemd 可以管理所有系统资源，那它所管理不同的资源统称为 Unit（单位）。\n在 Systemd 的生态圈中，Unit 文件统一了过去各种不同系统资源配置格式，例如服务的启/停、定时任务、设备自动挂载、网络配置、虚拟内存配置等。而 Systemd 通过不同的文件后缀来区分这些配置文件。\nSystemd 支持的 12 种 Unit 文件类型 .automount：用于控制自动挂载文件系统，相当于 SysV-init 的 autofs 服务 .device：对于 /dev 目录下的设备，主要用于定义设备之间的依赖关系 .mount：定义系统结构层次中的一个挂载点，可以替代过去的 /etc/fstab 配置文件 .path：用于监控指定目录或文件的变化，并触发其它 Unit 运行 .scope：这种 Unit 文件不是用户创建的，而是 Systemd 运行时产生的，描述一些系统服务的分组信息 .service：封装守护进程的启动、停止、重启和重载操作，是最常见的一种 Unit 文件 .slice：用于表示一个 CGroup 的树，通常用户不会自己创建这样的 Unit 文件 .snapshot：用于表示一个由 systemctl snapshot 命令创建的 Systemd Units 运行状态快照 .socket：监控来自于系统或网络的数据消息，用于实现基于数据自动触发服务启动 .swap：定义一个用户做虚拟内存的交换分区 .target：用于对 Unit 文件进行逻辑分组，引导其它 Unit 的执行。它替代了 SysV-init 运行级别的作用，并提供更灵活的基于特定设备事件的启动方式 .timer：用于配置在特定时间触发的任务，替代了 Crontab 的功能 Systemd 目录 Unit 文件按照 Systemd 约定，应该被放置指定的三个系统目录之一中。这三个目录是有优先级的，如下所示，越靠上的优先级越高。因此，在三个目录中有同名文件的时候，只有优先级最高的目录里的那个文件会被使用。\n/etc/systemd/system：系统或用户自定义的配置文件\n/run/systemd/system：软件运行时生成的配置文件\n/usr/lib/systemd/system：系统或第三方软件安装时添加的配置文件。\nCentOS 7：Unit 文件指向该目录\nubuntu 16：被移到了 /lib/systemd/system\nSystemd 默认从目录 /etc/systemd/system/ 读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录 /usr/lib/systemd/system/，真正的配置文件存放在那个目录。\nUnit 和 Target Unit 是 Systemd 管理系统资源的基本单元，可以认为每个系统资源就是一个 Unit，并使用一个 Unit 文件定义。在 Unit 文件中需要包含相应服务的描述、属性以及需要运行的命令。\nTarget 是 Systemd 中用于指定系统资源启动组的方式，相当于 SysV-init 中的运行级别。\n简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于”状态点”，启动某个 Target 就好比启动到某种状态。\nUnit 内容 Unit 文件结构 [Unit] Description=Hello World After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill busybox1 ExecStartPre=-/usr/bin/docker rm busybox1 ExecStartPre=/usr/bin/docker pull busybox ExecStart=/usr/bin/docker run --name busybox1 busybox /bin/ sh -c \u0026#34;while true; do echo Hello World; sleep 1; done\u0026#34; ExecStop=\u0026#34;/usr/bin/docker stop busybox1\u0026#34; ExecStopPost=\u0026#34;/usr/bin/docker rm busybox1\u0026#34; [Install] WantedBy=multi-user.target 如上所示，Systemd 服务的 Unit 文件可以分为三个配置区段：\nUnit 和 Install 段：所有 Unit 文件通用，用于配置服务（或其它系统资源）的描述、依赖和随系统启动的方式 Service 段：服务（Service）类型的 Unit 文件（后缀为 .service）特有的，用于定义服务的具体管理和操作方法 Unit 段 Description：描述这个 Unit 文件的信息 Documentation：指定服务的文档，可以是一个或多个文档的 URL 路径 Requires：依赖的其它 Unit 列表(有多个unit 用空格 隔开, 以下参数均适用)，列在其中的 Unit 模板会在这个服务启动时的同时被启动。并且，如果其中任意一个服务启动失败，这个服务也会被终止 Wants：与 Requires 相似，但只是在被配置的这个 Unit 启动时，触发启动列出的每个 Unit 模块，而不去考虑这些模板启动是否成功 After：与 Requires 相似，但是在后面列出的所有模块全部启动完成以后，才会启动当前的服务 Before：与 After 相反，在启动指定的任务一个模块之间，都会首先确证当前服务已经运行 BindsTo：与 Requires 相似，失败时失败，成功时成功，但是在这些模板中有任意一个出现意外结束或重启时，这个服务也会跟着终止或重启 PartOf：一个 Bind To 作用的子集，仅在列出的任务模块失败或重启时，终止或重启当前服务，而不会随列出模板的启动而启动 OnFailure：当这个模板启动失败时，就会自动启动列出的每个模块 Conflicts：与这个模块有冲突的模块，如果列出的模块中有已经在运行的，这个服务就不能启动，反之亦然 Install 段 这部分配置的目标模块通常是特定运行目标的 .target 文件，用来使得服务在系统启动时自动运行。这个区段可以包含三种启动约束：\nWantedBy：和 Unit 段的 Wants 作用相似，只有后面列出的不是服务所依赖的模块，而是依赖当前服务的模块。它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入 /etc/systemd/system 目录下面以 \u0026lt;Target 名\u0026gt; + .wants 后缀构成的子目录中，如 /etc/systemd/system/multi-user.target.wants/ RequiredBy：和 Unit 段的 Wants 作用相似，只有后面列出的不是服务所依赖的模块，而是依赖当前服务的模块。它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入 /etc/systemd/system 目录下面以 \u0026lt;Target 名\u0026gt; + .required 后缀构成的子目录中 Also：当前 Unit enable/disable 时，同时 enable/disable 的其他 Unit Alias：当前 Unit 可用于启动的别名 Service 段 用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段分为服务生命周期和服务上下文配置两个方面。\n服务生命周期控制相关 Type：定义启动时的进程行为，它有以下几种值：\nType=simple：默认值，执行ExecStart指定的命令，启动主进程\nType=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出\nType=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行\nType=dbus：当前服务通过D-Bus启动\nType=notify：当前服务启动完毕，会通知Systemd，再继续往下执行\nType=idle：若有其他任务执行完毕，当前服务才会运行\ntype 常用的是simple和forking, 他们不能混用,\nsimple适用于前台执行的命令, 例如启动Redis, RocketMQ\nfork适用于后台执行的命令, 有守护进程的, 例如Nginx\nsystemd service 之：服务配置文件编写 (2) | 骏马金龙 (junmajinlong.com) RemainAfterExit：值为 true 或 false（默认）。当配置为 true 时，Systemd 只会负责启动服务进程，之后即便服务进程退出了，Systemd 也仍然会认为这个服务还在运行中。这个配置主要是提供给一些并非常驻内存，而是启动注册后立即退出，然后等待消息按需启动的特殊类型服务使用的。\nExecStart：启动当前服务的命令\nExecStartPre：启动当前服务之前执行的命令\nExecStartPost：启动当前服务之后执行的命令\nExecReload：重启当前服务时执行的命令\nExecStop：停止当前服务时执行的命令\nExecStopPost：停止当其服务之后执行的命令\nRestartSec：自动重启当前服务间隔的秒数\nRestart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括 always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog\nTimeoutStartSec：启动服务时等待的秒数，这一配置对于使用 Docker 容器 而言显得尤为重要，因其第一次运行时可能需要下载镜像，严重延时会容易被 Systemd 误判为启动失败杀死。通常，对于这种服务，将此值指定为 0，从而关闭超时检测\nTimeoutStopSec：停止服务时的等待秒数，如果超过这个时间仍然没有停止，Systemd 会使用 SIGKILL 信号强行杀死服务的进程\n服务上下文配置相关 Environment：为服务指定环境变量 EnvironmentFile：指定加载一个包含服务所需的环境变量的列表的文件，文件中的每一行都是一个环境变量的定义 Nice：服务的进程优先级，值越小优先级越高，默认为 0。其中 -20 为最高优先级，19 为最低优先级 WorkingDirectory：指定服务的工作目录 RootDirectory：指定服务进程的根目录（/ 目录）。如果配置了这个参数，服务将无法访问指定目录以外的任何文件 User：指定运行服务的用户 Group：指定运行服务的用户组 MountFlags：服务的 Mount Namespace 配置，会影响进程上下文中挂载点的信息，即服务是否会继承主机上已有挂载点，以及如果服务运行执行了挂载或卸载设备的操作，是否会真实地在主机上产生效果。可选值为 shared、slaved 或 private shared：服务与主机共用一个 Mount Namespace，继承主机挂载点，且服务挂载或卸载设备会真实地反映到主机上 slave：服务使用独立的 Mount Namespace，它会继承主机挂载点，但服务对挂载点的操作只有在自己的 Namespace 内生效，不会反映到主机上 private：服务使用独立的 Mount Namespace，它在启动时没有任何任何挂载点，服务对挂载点的操作也不会反映到主机上 LimitCPU / LimitSTACK / LimitNOFILE / LimitNPROC 等：限制特定服务的系统资源量，例如 CPU、程序堆栈、文件句柄数量、子进程数量等 注意：如果在 ExecStart、ExecStop 等属性中使用了 Linux 命令，则必须要写出完整的绝对路径。对于 ExecStartPre 和 ExecStartPost 辅助命令，若前面有个 “-” 符号，表示忽略这些命令的出错。因为有些 “辅助” 命令本来就不一定成功，比如尝试清空一个文件，但文件可能不存在。\nUnit 文件占位符和模板 Unit 模板 在现实中，往往有一些应用需要被复制多份运行。例如，用于同一个负载均衡器分流的多个服务实例，或者为每个 SSH 连接建立一个独立的 sshd 服务进程。它们只是一个参数的区别, 如果写很多份文件难以维护, 所以可以通过参数的方式的传入unit文件\nUnit 模板文件的写法与普通的服务 Unit 文件基本相同，不过 Unit 模板的文件名是以 @ 符号结尾的。通过模板启动服务实例时，需要在其文件名的 @ 字符后面附加一个参数字符串。\n例如:\n模版Unit文件: apache@.service\n使用: systemctl start apache@8080.service # 这样就启动了一个端口是8080的服务\napache@.service 模板, 其中 %i 就是占位符\n[Unit] Description=My Advanced Service Template After=etcd.service docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill apache%i ExecStartPre=-/usr/bin/docker rm apache%i ExecStartPre=/usr/bin/docker pull coreos/apache ExecStart=/usr/bin/docker run --name apache%i -p %i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running ExecStop=/usr/bin/docker stop apache1 ExecStopPost=/usr/bin/docker rm apache1 ExecStopPost=/usr/bin/etcdctl rm /domains/example.com/%H:%i [Install] WantedBy=multi-user.target Systemd 在运行服务时，总是会先尝试找到一个完整匹配的 Unit 文件，如果没有找到，才会尝试选择匹配模板。\n例如上面的命令，System 首先会在约定的目录下寻找名为 apache@8080.service 的文件，如果没有找到，而文件名中包含 @ 字符，它就会尝试去掉后缀参数匹配模板文件。对于 apache@8080.service ，systemd 会找到 apache@.service 模板文件，并通过这个模板文件将服务实例化。\nUnit 文件占位符 既然能传参, 有哪些占位符可以使用呢? 如下:\n%n：完整的 Unit 文件名字，包括 .service 后缀名 %p：Unit 模板文件名中 @ 符号之前的部分，不包括 @ 符号 %i：Unit 模板文件名中 @ 符号之后的部分，不包括 @ 符号和 .service 后缀名 %t：存放系统运行文件的目录，通常是 “run” %u：运行服务的用户，如果 Unit 文件中没有指定，则默认为 root %U：运行服务的用户 ID %h：运行服务的用户 Home 目录，即 %{HOME} 环境变量的值 %s：运行服务的用户默认 Shell 类型，即 %{SHELL} 环境变量的值 %m：实际运行节点的 Machine ID，对于运行位置每个的服务比较有用 %b：Boot ID，这是一个随机数，每个节点各不相同，并且每次节点重启时都会改变 %H：实际运行节点的主机名 %v：内核版本，即 “uname -r” 命令输出的内容 %%：在 Unit 模板文件中表示一个普通的百分号 系统管理命令 前面说了这么多, 可知道Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。\nsystemctl是 Systemd 的主命令，用于管理系统。\n几个常用命令\nsystemctl start nginx.server # 依次启动定义在 Unit 文件中的 ExecStartPre、ExecStart 和 ExecStartPost 命令 systemctl status nginx.server # 查看服务状态 systemctl stop nginx.server # 依次停止定义在 Unit 文件中的 ExecStopPre、ExecStop 和 ExecStopPost 命令 systemctl enable nginx.server # 在 /etc/systemd/system/ 建立服务的符号链接，指向 /usr/lib/systemd/system/ 中 , 并设置为服务自启动 systemctl disable nginx.server # 取消服务开机启动 systemctl daemon-reload # 刷新 Unit 文件, 更新unit文件后需要执行 systemd-analyze命令用于查看启动耗时。\nhostnamectl命令用于查看当前主机的信息。\nlocalectl命令用于查看本地化设置。\ntimedatectl命令用于查看当前时区设置。\nloginctl命令用于查看当前登录的用户。\n日志管理 Systemd 通过其标准日志服务 Journald 提供的配套程序 journalctl 将其管理的所有后台进程打印到 std:out（即控制台）的输出重定向到了日志文件。\nSystemd 的日志文件是二进制格式的，必须使用 Journald 提供的 journalctl 来查看，默认不带任何参数时会输出系统和所有后台进程的混合日志。\n默认日志最大限制为所在文件系统容量的 10%，可以修改 /etc/systemd/journald.conf 中的 SystemMaxUse 来指定该最大限制。\n查看所有日志（默认情况下 ，只保存本次启动的日志） journalctl 有时候启动服务时, 报错了, 就会提示应该用什么命令查看日志\n[xiaokj01@master bin]$ systemctl start namesrv.service ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units === Authentication is required to manage system services or units. Authenticating as: xiaokj01 Password: ==== AUTHENTICATION COMPLETE === Job for namesrv.service failed because a timeout was exceeded. See \u0026#34;systemctl status namesrv.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. Unit 案例 nginx [Service] #这里Type一定要写forking Type=forking PIDFile=/usr/local/openresty/nginx/logs/nginx.pid ExecStartPre=/usr/local/openresty/nginx/sbin/nginx -t ExecStart=/usr/local/openresty/nginx/sbin/nginx ExecReload=/bin/kill -s HUP $MAINPID ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target RocketMQ 文件: /lib/systemd/system/rocketMQ.service\n[Unit] Description=rocketMq启动服务 After=network.target rocketMQBroker.service Requires=rocketMQBroker.service [Service] #这里Type一定要写simple Type=simple #ExecStart和ExecStop分别在systemctl start和systemctl stop时候调动 ExecStart=/home/xiaokj01/soft/rocketmq-all-4.8.0-bin-release/bin/mqnamesrv ExecStop=/home/xiaokj01/soft/rocketmq-all-4.8.0-bin-release/bin/mqshutdown namesrv [Install] WantedBy=multi-user.target 文件: /lib/systemd/system/rocketMQBroker.service\n[Unit] Description=rocketMq的broker服务 After=network.target BindsTo=rocketMQ.service Before=rocketMQ.service [Service] #这里Type一定要写simple Type=simple #ExecStart和ExecStop分别在systemctl start和systemctl stop时候调动 ExecStart=/home/xiaokj01/soft/rocketmq-all-4.8.0-bin-release/bin/mqbroker -c /home/xiaokj01/soft/rocketmq-all-4.8.0-bin-release/conf/broker.conf ExecStop=/home/xiaokj01/soft/rocketmq-all-4.8.0-bin-release/bin/mqshutdown broker [Install] WantedBy=multi-user.target 这里把 broker和nameserver 做了依赖关系, 启动 rocketMQ.service 文件 就可以一键启动 MQ\n这里只做了 单nameserver和单broker的场景, 如果需要做集群处理, 应该可以使用模版的方式, 把参数传进去可以实现\n如果不想一键启动, 分开使用(毕竟集群部署下, nameserver和broker不一定会放在一起), 把After, BindsTo 等限制条件删除就行\nRocketMQ系列：使用systemd管理nameserver和broker_rocketmq systemd_公众号-测试生财的博客-CSDN博客 可能是史上最全面易懂的 Systemd 服务管理教程 \u0026mdash;-全面但错误单词较多 Systemd 入门教程：命令篇 - 阮一峰的网络日志 (ruanyifeng.com) systemd service 之：服务配置文件编写 (2) | 骏马金龙 (junmajinlong.com) 干货分享 | Systemd 技术原理\u0026amp;实践（上） - 知乎 (zhihu.com) 八卦: systemd 为什么会有那么大的争议？ - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86/%E7%AE%80%E8%BF%B0systemd.html","summary":"[toc] Systemd 概述 Systemd 简介 历史上，Linux 的启动一直采用 init 进程。这种方法有两个缺点。 一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会","title":"简述systemd"},{"content":"[toc]\n在kafka原理中介绍,kafka在消费组分配分区时,有两种算法: range 和 round-robin和Sticky(0.11.x版本),前两种都存在弊端\n消费者客户端参数partition.asssignment.strategy可以配置多个分配策略，彼此之间以逗号分隔。\nRangeAssignor分配策略(默认使用该策略) RangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。(就是挨个给,轮着来)\n原文链接：https://blog.csdn.net/u013256816/article/details/81123600\nRoundRobin strategy 使用RoundRobin策略有两个前提条件必须满足：\n同一个Consumer Group里面的所有消费者的num.streams必须相等； 每个消费者订阅的主题必须相同。 所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，这里文字可能说不清，看下面的代码应该会明白：\nval allTopicPartitions = ctx.partitionsForTopic.flatMap { case(topic, partitions) =\u0026gt; info(\u0026ldquo;Consumer %s rebalancing the following partitions for topic %s: %s\u0026rdquo; .format(ctx.consumerId, topic, partitions)) partitions.map(partition =\u0026gt; { TopicAndPartition(topic, partition) }) }.toSeq.sortWith((topicPartition1, topicPartition2) =\u0026gt; { /* * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending * up on one consumer (if it has a high enough stream count). */ topicPartition1.toString.hashCode \u0026lt; topicPartition2.toString.hashCode })\n最后按照round-robin风格将分区分别分配给不同的消费者线程。\n在我们的例子里面，加入按照 hashCode 排序完的topic-partitions组依次为\nT1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，\n我们的消费者线程排序为\nC1-0, C1-1, C2-0, C2-1，\n最后分区分配的结果为：\nC1-0 将消费 T1-5, T1-2, T1-6 分区； C1-1 将消费 T1-3, T1-1, T1-9 分区； C2-0 将消费 T1-0, T1-4 分区； C2-1 将消费 T1-8, T1-7 分区；\n链接：https://www.jianshu.com/p/b7647a748329\nSticky strategy Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的：\n分区的分配要尽可能的均匀； 分区的分配尽可能的与上次分配的保持相同。 当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。\n有点像轮询的分配,但是加了一条,\u0026ldquo;尽可能的与上次分配保持相同\u0026rdquo;\n来自: https://blog.csdn.net/u013256816/article/details/81123625 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E7%AE%80%E8%BF%B0%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5.html","summary":"[toc] 在kafka原理中介绍,kafka在消费组分配分区时,有两种算法: range 和 round-robin和Sticky(0.11.x版本),前两种都存","title":"简述分区分配策略"},{"content":"[toc]\n一. 什么是幂等性? 幂等性其实是消息的一致性,分为生产者幂等性和消费者幂等性.\n使用Kafka时,需要保证exactly-once语义。要知道在分布式系统中，出现网络分区是不可避免的，如果kafka broker 在回复ack时，出现网络故障或者是full gc导致ack timeout，producer将会重发，如何保证producer重试时不造成重复or乱序？又或者producer 挂了，新的producer并没有old producer的状态数据，这个时候如何保证幂等？即使Kafka 发送消息满足了幂等，consumer拉取到消息后，把消息交给线程池workers，workers线程对message的处理可能包含异步操作，\n来自 https://www.cnblogs.com/jingangtx/p/11330338.html 就是用来解决数据重复问题，保证kafka单会话单分区内数据不会重复消费在kafka0.11之前通过isr+ack机制可保证数据不丢，却不能保证不重复 有一些情况可能会导致数据重复。比如：网络请求延时导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。\n数据重复的解决方案就是加唯一id，通过id判断数据是否重复\n原文链接：https://blog.csdn.net/qq_37923600/article/details/88583170\n二. 生产者幂等性 保证在发送同一条消息时，在服务端只会被持久化一次，数据不丢不重。\n但是是有条件的\nkafka的幂等性只能保证单会话有效，如果broker挂掉重启，幂等就无效了，因为无法获取之前的状态信息 幂等性不能跨多个Topic-Partition，只能保证单个partition的幂等性。 所以生产者分为了 幂等型producer 和 事务型producer,前者解决了单会话幂等性等问题，后者解决了多会话幂等性\n单回话的意思大概是 发送一次消息表示一次回话吧\n2.1 单回话幂等性 为解决producer重试引起的乱序和重复。Kafka增加了pid和seq。Producer中每个RecordBatch都有一个单调递增的seq; Broker上每个tp也会维护pid-seq的映射，并且每Commit都会更新lastSeq。这样recordBatch到来时，broker会先检查RecordBatch再保存数据：如果batch中 baseSeq(第一条消息的seq)比Broker维护的序号(lastSeq)大1，则保存数据，否则不保存(inSequence方法)。\n简单的说,相当于把存一份标识符,来确定是否已生产了\n在生产者配置文件中加入配置即可实现: enable.idempotence=true\n参考: https://blog.csdn.net/qq_37923600/article/details/88583170 2.2 多回话幂等性 kafka事务引入了transactionId 和Epoch，设置transactional.id后，一个transactionId只对应一个pid, 且Server 端会记录最新的 Epoch 值。这样有新的producer初始化时，会向TransactionCoordinator发送InitPIDRequest请求， TransactionCoordinator 已经有了这个 transactionId对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样当old producer恢复过来请求操作时，将被认为是无效producer抛出异常。 如果没有开启事务，TransactionCoordinator会为新的producer返回new pid，这样就起不到隔离效果，因此无法实现多会话幂等。\n其实就是利用事务,只要没完成就不会自增(原子性),完成操作后手动提交\n2.2.1 实现多会话幂等性 提供了API,使用API即可. 其中分为\n只有写 有写有读(最常见) 只有读(没有实际意义,因为只有读不会发生异常) 这里只描述第二种\n/** * 在一个事务内,即有生产消息又有消费消息 */ public void consumeTransferProduce() { // 1.构建上产者 Producer producer = buildProducer(); // 2.初始化事务(生成productId),对于一个生产者,只能执行一次初始化事务操作 producer.initTransactions(); // 3.构建消费者和订阅主题 Consumer consumer = buildConsumer(); consumer.subscribe(Arrays.asList(\u0026#34;test\u0026#34;)); while (true) { // 4.开启事务 producer.beginTransaction(); // 5.1 接受消息 ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(500); try { // 5.2 do业务逻辑; System.out.println(\u0026#34;customer Message---\u0026#34;); Map\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt; commits = Maps.newHashMap(); for (ConsumerRecord\u0026lt;String, String\u0026gt; record : records) { // 5.2.1 读取消息,并处理消息。print the offset,key and value for the consumer records. System.out.printf(\u0026#34;offset = %d, key = %s, value = %s \u0026#34;, record.offset(), record.key(), record.value()); // 5.2.2 记录提交的偏移量 commits.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset())); // 6.生产新的消息。比如外卖订单状态的消息,如果订单成功,则需要发送跟商家结转消息或者派送员的提成消息 producer.send(new ProducerRecord\u0026lt;String, String\u0026gt;(\u0026#34;test\u0026#34;, \u0026#34;data2\u0026#34;)); } // 7.提交偏移量 producer.sendOffsetsToTransaction(commits, \u0026#34;group0323\u0026#34;); // 8.事务提交 producer.commitTransaction(); } catch (Exception e) { // 7.放弃事务 producer.abortTransaction(); } } } 创建消费者代码，需要：\n将配置中的自动提交属性（auto.commit）进行关闭 而且在代码里面也不能使用手动提交commitSync( )或者commitAsync( ) 设置isolation.level /** * 需要: * 1、关闭自动提交 enable.auto.commit * 2、isolation.level为 * @return */ public Consumer buildConsumer() { Properties props = new Properties(); // bootstrap.servers是Kafka集群的IP地址。多个时,使用逗号隔开 props.put(\u0026#34;bootstrap.servers\u0026#34;, \u0026#34;localhost:9092\u0026#34;); // 消费者群组 props.put(\u0026#34;group.id\u0026#34;, \u0026#34;group0323\u0026#34;); // 设置隔离级别 props.put(\u0026#34;isolation.level\u0026#34;,\u0026#34;read_committed\u0026#34;); // 关闭自动提交 props.put(\u0026#34;enable.auto.commit\u0026#34;, \u0026#34;false\u0026#34;); props.put(\u0026#34;session.timeout.ms\u0026#34;, \u0026#34;30000\u0026#34;); props.put(\u0026#34;key.deserializer\u0026#34;, \u0026#34;org.apache.kafka.common.serialization.StringDeserializer\u0026#34;); props.put(\u0026#34;value.deserializer\u0026#34;, \u0026#34;org.apache.kafka.common.serialization.StringDeserializer\u0026#34;); KafkaConsumer\u0026lt;String, String\u0026gt; consumer = new KafkaConsumer \u0026lt;String, String\u0026gt;(props); return consumer; } 更多参见:http://www.heartthinkdo.com/?p=2040\n三. 消费者幂等性 指的是即使有重复消息,也不能重复处理\n其做法就是做唯一id,相当于天启同步模块中,做个判断,如果项目编号已存在则不做消费处理\nif(cache.contain(msgId)){ // cache中包含msgId，已经处理过 continue; }else { lock.lock(); cache.put(msgId,timeout); commitSync(); lock.unLock(); } // 后续完成所有操作后，删除cache中的msgId，只要msgId存在cache中，就认为已经处理过。Note：需要给cache设置有消息 参考: https://www.cnblogs.com/jingangtx/p/11330338.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E7%AE%80%E8%BF%B0%E5%B9%82%E7%AD%89%E6%80%A7.html","summary":"[toc] 一. 什么是幂等性? 幂等性其实是消息的一致性,分为生产者幂等性和消费者幂等性. 使用Kafka时,需要保证exactly-once语义。要知道","title":"简述幂等性"},{"content":"[toc]\n一、什么是高并发 高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。\n高并发相关常用的一些指标有响应时间（Response Time），吞吐量（Throughput），每秒查询率QPS（Query Per Second），并发用户数等。\n响应时间：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。 吞吐量：单位时间内处理的请求数量。 QPS：每秒响应请求数。在互联网领域，这个指标和吞吐量区分的没有这么明显。 并发用户数：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。 二、如何提升系统的并发能力 加机器的方式就不说了,花钱就完事了\n2.0 常见的互联网分层架构 常见互联网分布式架构如上，分为：\n（1）客户端层：典型调用方是浏览器browser或者手机应用APP\n（2）反向代理层：系统入口，反向代理\n（3）站点应用层：实现核心应用逻辑，返回html或者json\n（4）服务层：如果实现了服务化，就有这一层\n（5）数据-缓存层：缓存加速访问存储\n（6）数据-数据库层：数据库固化数据存储\n2.1 客户端提升性能 资源静态化 - 使用服务端渲染 采用cdn , 加快动态资源加载 一些小点:\n减少http请求 使用浏览器的缓存 减少单个请求的数据传输 2.2 代理层的水平扩展 反向代理层的水平扩展，是通过“DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。\n当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。\n(有多个NG的情况下)\n优化ng的一些小细节,比如:\n启用压缩 使用长链接 增加worker数 NG是七层模型中的第七层-应用层 , 所以又称它为七层负载, 因此性能会有一定的损耗\nLVS(Linux Virtual Server: Linux虚拟服务器) 是七层模型中的第四层-网络层 , 所以又称它为四层负载, 因此性能会更高\nLVS 是在网络栈的四层做请求包的转发，请求包转发之后，由客户端和后端服务直接建立连接，后续的响应包不会再经过 LVS 服务器，所以性能更高, LVS 缺陷是工作在四层，而请求的 URL 是七层的概念，不能针对 URL 做更细致地请求分发，而且 LVS 也没有提供探测后端服务是否存活的机制\n如果你的 QPS 在十万以内，那么可以考虑不引入LVS 而直接使用 Nginx 作为唯一的负载均衡服务器\n来源: 《阿里亿级高并发系统设计》\n主流软件负载均衡器对比(LVS、Nginx、HAproxy) - 知乎 (zhihu.com) LVS负载均衡（LVS简介、三种工作模式、十种调度算法）_chenhuyang的博客-CSDN博客_lvs负载均衡 详情见 优化秒杀过程 2.3 站点层的水平扩展 站点层的水平扩展，是通过“nginx”实现的。通过修改nginx.conf，可以设置多个web后端。\n当web后端成为瓶颈的时候，只要增加服务器数量，新增web服务的部署，在nginx配置中配置上新的web后端，就能扩展站点层的性能，做到理论上的无限高并发。\n(单个NG或者分发到具体某个NG时)\n2.4 服务层 服务层的水平扩展，是通过“服务连接池”实现的。\n站点层通过RPC-client调用下游的服务层RPC-server时，RPC-client中的连接池会建立与下游服务多个连接，当服务成为瓶颈的时候，只要增加服务器数量，新增服务部署，在RPC-client处建立新的下游服务连接，就能扩展服务层性能，做到理论上的无限高并发。用上Eruka之类的服务注册发现功能,使用多节点容灾\n除了水平扩展还有缓存和异步\n缓存:\n下个标题会讲解\n异步:\n引入消息队列从业务级别 解耦 耗时或并发高的功能 在代码中使用多线程到达异步能力 还有很多代码细节,比如\n提升gateway的性能, 改用netty容器启动 , 并设定工作线程和 select(选择)线程数 增加tomcat连接数, 超时时间等 fegin调用采用okhttp的方式,并使用长链接 优化jvm 代码中使用异步,线程池等, 代码质量良好 数据库的链接池 单sql执行, 不阻塞数据库, 提高据库的qps 避免数据库的长事务 详情见 优化秒杀过程 2.5 数据层的水平扩展 在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。\n2.5.1 缓存 使用redis之类的缓存数据库,减少与数据库的交互\n一些优化细节,比如:\n尽量避免大key 减少redis的访问次数, 使用mget和管道 使用netty链接池 2.5.2 数据库 使用读写分离 , 分库分表等操作提高数据库的效率\n引入es, 将复杂查询或者全文类查询通过es来实现\n数据库本身的优化,比如:\n数据库自身的连接池 超时时间 sql要命中索引 https://blog.csdn.net/weixin_42476601/article/details/82220027 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E5%B9%B6%E5%8F%91.html","summary":"[toc] 一、什么是高并发 高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同","title":"简述如何保证高并发"},{"content":"[toc]\n一、什么是高可用 **高可用HA（**High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。\n二、如何保障系统的高可用 应用高可用 我们都知道，单点是系统高可用的大敌，单点往往是系统高可用最大的风险和敌人，应该尽量在系统设计的过程中避免单点。方法论上，高可用保证的原则是“集群化”，或者叫“冗余”：只有一个单点，挂了服务会受影响；如果有冗余备份，挂了还有其他backup能够顶上。\n保证系统高可用，架构设计的核心准则是：冗余。\n有了冗余之后，还不够，每次出现故障需要人工介入恢复势必会增加系统的不可服务实践。所以，又往往是通过“自动故障转移”来实现系统的高可用。\n在流量达到系统可承受的上限时, 还得用 降级/限流/熔断 来保证系统的可用性\n对于服务状态临界值时怎么保证高可用以及数据的安全?\n引入NG的动态负载均衡, 自动剔除不正常的服务\nNginx 模块nginx_upstream_check_module了，这个模块可以让 Nginx 定期地探测后端服务的一个指定的接口，然后根据返回的状态码，来判断服务是否还存活,当探测不存活的次数达到一定阈值时，就自动将这个后端服务从负载均衡服务器中摘除。\nupstream server { server 192.168.1.1:8080; server 192.168.1.2:8080; // 检测 URL check interval=3000 rise=2 fall=5 timeout=1000 type=http default_down=t 5 check_http_send \u0026#34;GET /health_check HTTP/1.0 \u0026#34;; // 检测返回状态码为 200 时认为检测成功 check_http_expect_alive http_2xx; } Nginx 按照上面的方式配置之后，你的业务服务器也要实现一个“/health_check”的接口，在这个接口中返回的 HTTP 状态码，这个返回的状态码可以存储在配置中心中，这样在变更状态码时，就不需要重启服务了\n**在服务刚刚启动时，**可以初始化默认的 HTTP 状态码是 500，这样 Nginx 就不会很快将这个服务节点标记为可用，也就可以等待服务中，依赖的资源初始化完成，避免服务初始启动时的波动\n**在完全初始化之后，**再将 HTTP 状态码变更为 200，Nginx 经过两次探测后，就会标记服务为可用。在服务关闭时，也应该先将 HTTP 状态码变更为 500，等待 Nginx 探测将服务标记为不可用后，前端的流量也就不会继续发往这个服务节点。在等待服务正在处理的请求全部处理完毕之后，再对服务做重启，可以避免直接重启导致正在处理的请求失败的问题。\n或者使用灰度机制(AB区机制)\n发布部署的时候 关闭A区, 发布B区, 待A区发布完全(spring的优雅停机)且测试通过, 再开放A区, 这样A区也能解决服务初始化时的波动 和 关闭服务而丢失服务正在处理事件的情况\n机房高可用 要多机房部署, 机房本身也存在单点问题\n如此就遇到跨机房的数据传输, 数据延迟大体如下:\n北京同地双机房之间的专线延迟一般在 1ms~3ms\n国内异地双机房之间的专线延迟会在 50ms 之内\n同城多活 就是在一个城市部署多个服务节点, 保证存活(包括应用接口, 数据接口等等 )\n鉴于跨机房的数据传输存在延时, 所以应该尽可能的减少跨机房的调用\n总的来说, 所有的操作应该尽量在自己机房处理\n数据库层(mysql) 采用主从同步的方式, 各自机房,查自己的从库即可, 一般有两种方案\n基于存储系统的主从复制，比如 MySQL 和 Redis 基于消息队列的方式 虽然存在跨机房写数据的问题，不过鉴于写数据的请求量不高，所以在性能上是可以容忍的。\n服务间的调用, 也应优先调用自己机房的服务 (使用注册中心分组订阅的能力)\n异地多活 如果发生城市级的天灾人祸时, 同城多活就会出现问题, 所以要在多个城市部署多个节点 来保证存活\n大前提和同城多活一样, 业务处理尽量在自己机房处理\n同城多机房方案可以允许有跨机房数据写入的发生，但是数据的读取，和服务的调用应该尽量保证在同一个机房中。\n异地多活方案则应该避免跨机房同步的数据写入和读取，而是采取异步的方式，将数据从一个机房同步到另一个机房。\n三、常见的互联网分层架构 常见互联网分布式架构如上，分为：\n（1）客户端层：典型调用方是浏览器browser或者手机应用APP\n（2）反向代理层：系统入口，反向代理\n（3）站点应用层：实现核心应用逻辑，返回html或者json\n（4）服务层：如果实现了服务化，就有这一层\n（5）数据-缓存层：缓存加速访问存储\n（6）数据-数据库层：数据库固化数据存储\n整个系统的高可用，又是通过每一层的冗余+自动故障转移来综合实现的。\n和实现高并发一样,在每一层都做处理\n四,总结 高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。\n方法论上，高可用是通过冗余+自动故障转移来实现的。\n整个互联网分层系统架构的高可用，又是通过每一层的冗余+自动故障转移来综合实现的，具体的：\n（1）【客户端层】到【反向代理层】的高可用，是通过反向代理层的冗余实现的，常见实践是keepalived + virtual IP自动故障转移\n（2）【反向代理层】到【站点层】的高可用，是通过站点层的冗余实现的，常见实践是nginx与web-server之间的存活性探测与自动故障转移\n（3）【站点层】到【服务层】的高可用，是通过服务层的冗余实现的，常见实践是通过service-connection-pool来保证自动故障转移\n（4）【服务层】到【缓存层】的高可用，是通过缓存数据的冗余实现的，常见实践是缓存客户端双读双写，或者利用缓存集群的主从数据同步与sentinel保活与自动故障转移；更多的业务场景，对缓存没有高可用要求，可以使用缓存服务化来对调用方屏蔽底层复杂性\n（5）【服务层】到【数据库“读”】的高可用，是通过读库的冗余实现的，常见实践是通过db-connection-pool来保证自动故障转移\n（6）【服务层】到【数据库“写”】的高可用，是通过写库的冗余实现的，常见实践是keepalived + virtual IP自动故障转移\nhttps://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==\u0026mid=2651959728\u0026idx=1\u0026sn=933227840ec8cdc35d3a33ae3fe97ec5\u0026chksm=bd2d046c8a5a8d7a13551124af36bedf68f7a6e31f6f32828678d2adb108b86b7e08c678f22f\u0026scene=21#wechat_redirect ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E5%8F%AF%E7%94%A8.html","summary":"[toc] 一、什么是高可用 **高可用HA（**High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系","title":"简述如何保证高可用"},{"content":"[toc]\n1. 适配器模式 类比插座,有欧洲标准,中国标准,为了在欧洲使用中国的插头,所以需要一个适配器,做到兼容\n在正常开发中,一般是针对一个已有的类,想要用它的功能,由于各种原因,不能直接用,需要写一个中间类,自己调用这个中间类,由中间类去调用已有的类\n优点：\n1.通过适配器，客户端可以直接调用同一接口，因而对客户端来说是透明的。这样做更简单，更直接，更紧凑。\n2.复用了现存的类，解决了现存类和复用环境要求不一致的问题。\n3.将目标类和适配者类解耦，通过引入一个适配器现有的适配者类，而无须修改原有代码。\n一个对象适配器可以把多个不同的适配器类适配到同一个目标，也就是说，同一个适配器可以把适配者类和他的子类都适配到目标接口。\n缺点：\n1.对于适配器来说，更换适配器的实现过程比较复杂 。\n2. 代理模式 优点:\n\\1. 动态代理采用在运行时动态生成代码的方式,取消了对被代理类的扩展限制,遵循了开闭原则\n\\2. 若动态代理要对目标类的增强逻辑进行扩展,结合策略模式,只需要新增策略类便可完成,无需修改代理类的代码\n3.在编码时，代理逻辑与业务逻辑是互相独立的，没有耦合\nhttps://blog.csdn.net/wb_snail/article/details/80632038 2.1. 静态代理 定义：为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介的作用。\n作用：增强一个类中的某个方法.对程序进行扩展，Spring框架中AOP。\n2.2 动态代理 1、动态代理：(它与装饰者模式有点相似，它比装饰者模式还要灵活)\n动态代理它可以直接给某一个目标(被代理 对象)对象(实现了某个或者某些接口)生成一个代理对象，而不需要代理类存在，如上图中经理人需要存在。 动态代理与代理模式原理是一样的，只是它没有具体的代理类，直接通过反射生成了一个代理对象 2、动态代理的分类\njdk提供一个Proxy类可以直接给实现接口类的对象直接生成代理对象 API: 调用, 缺少什么参数,传什么 spring中动态代理:cglib 继承 3. 单例模式 一个类能返回对象一个引用(永远是同一个)和一个获得该实例的方法（必须是静态方法，通常使用getInstance这个名 称）；当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用；同时我们 还将该类的构造函数定义为私有方法，这样其他处的代码就无法通过调用该类的构造函数来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例。\n优点：\n1.在单例模式中，活动的单例只有一个实例，对单例类的所有实例化得到的都是相同的一个实例。这样就 防止其它对象对自己的实例化，确保所有的对象都访问一个实例\n2.单例模式具有一定的伸缩性，类自己来控制实例化进程，类就在改变实例化进程上有相应的伸缩性。\n3.提供了对唯一实例的受控访问。\n4.由于在系统内存中只存在一个对象，因此可以 节约系统资源，当 需要频繁创建和销毁的对象时单例模式无疑可以提高系统的性能。\n5.允许可变数目的实例。\n6.避免对共享资源的多重占用。\n缺点：\n1.不适用于变化的对象，如果同一类型的对象总是要在不同的用例场景发生变化，单例就会引起数据的错误，不能保存彼此的状态。\n2.由于单利模式中没有抽象层，因此单例类的扩展有很大的困难。\n3.单例类的职责过重，在一定程度上违背了“单一职责原则”。\n4.滥用单例将带来一些负面问题，如为了节省资源将数据库连接池对象设计为的单例类，可能会导致共享连接池对象的程序过多而出现连接池溢出；如果实例化的对象长时间不被利用，系统会认为是垃圾而被回收，这将导致对象状态的丢失\n适用场景：\n单例模式只允许创建一个对象，因此节省内存，加快对象访问速度，因此对象需要被公用的场合适合使用，如多个模块使用同一个数据源连接对象等等。如：\n1.需要频繁实例化然后销毁的对象。\n2.创建对象时耗时过多或者耗资源过多，但又经常用到的对象。\n3.有状态的工具类对象。\n4.频繁访问数据库或文件的对象。\n以下都是单例模式的经典使用场景：\n1.资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如上述中的日志文件，应用配置。\n2.控制资源的情况下，方便资源之间的互相通信。如线程池等。\n4. 策略模式 在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。\n优点： 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。\n缺点： 1、策略类会增多。 2、所有策略类都需要对外暴露。\n使用场景： 1、如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 2、一个系统需要动态地在几种算法中选择一种。 3、如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。\n案例: 天启api中每个接口都需要验签,共通实现同一个验签方法,(抽取一个共同方法),入参选择各自的请求类(验签参数在请求类中),在具体执行验签函数时,则会选择请求类中的验签函数(因为优先使用子类的继承方法)\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%AE%80%E8%BF%B0%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html","summary":"[toc] 1. 适配器模式 类比插座,有欧洲标准,中国标准,为了在欧洲使用中国的插头,所以需要一个适配器,做到兼容 在正常开发中,一般是针对一个已有的类,想","title":"简述设计模式"},{"content":"[TOC]\n前言 Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。\n服务器初始化启动。 服务器运行期间无法和Leader保持连接。 leader的选择机制，zookeeper提供了三种方式：\nLeaderElection AuthFastLeaderElection FastLeaderElection （最新默认,也是本文主角） 下面就两种情况进行分析讲解。\n服务器[启动]时期的Leader选举 个人简述: 初始启动时,每台机器都投自己,然后通知其他机器(通知的这个过程就叫投票),通知本质是发送消息,携带了(leaderId,zxid,Epoch),初始值就是(1,0,0),\n(每个机器都投票,都判别)然后其他机器收到消息,进行判断,用收到的消息和自己的消息判别,\n首先看是否为本轮选举(别人是新选举就更新自己,自己是新选举就通知它更新),然后比较zxid,如果一样再比较leaderId(都是取大值,所以配置文件中要求brokenId不能重复),比较后的结果如果与本地不同,则更新本地并通知其他机器,反之不动. 服务器还会判断,是否收集到了所有服务器的选举状态或者是否有过半机器(此时还会等200ms,看有没有新的消息),根据结果设置自己该成为leader还是follower,然后退出选举,kafka就能正常用了,如果已经产生leader,新加入的机器自动成为follower,同步数据即可\n说明:\nleaderId:机器所认为的leader ID,这个ID值就是配置文件中的brokenId zxid: 数据值,每次kafka中更新值就会更新这个值(zxid越大,数据越新) Epoch: 逻辑时钟值, 用于判断是否为本轮选举,每次选举都自动递增 若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下\n(1) 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。\n(2) 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。\n(3) 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下\n优先检查ZXID。ZXID比较大的服务器优先作为Leader。 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。\n(4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。\n(5) 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。\n以上这是两台的场景,\n如果是三台, 按顺序启动,leader也会是server2, 因为在第二轮投票时 server1和server2会投给server2, 达到过半,(server3会投给自己) (集群中没有leader时每个节点优先投给自己)\n如果是四台, 按顺序启动, leader也会是server4, 但会有三轮选举. 在第二轮中, 每个节点的票数: server1:0 ; server:2 ; server3:1 ; server4:1\nserver2票数没有过半, 所以需要会发起一轮投票, 而server4的myid最大, 所以大家都会投给它,\n服务器[运行]时期的Leader选举 个人简述: 集群运行过程中,leader突然宕机了,其他机器就把自己的状态变成LOOKING状态,开始选举新的leader(如同新启动时的那种),但是不同的是,每个broken上的zxid可能是不同的(可能数据还未完全同步)\n在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致。假设正在运行的有Server1、Server2、Server3三台服务器，当前Leader是Server2，若某一时刻Leader挂了，此时便开始Leader选举。选举过程如下\n(1) 变更状态。Leader挂后，余下的非Observer服务器都会讲自己的服务器状态变更为LOOKING，然后开始进入Leader选举过程。\n(2) 每个Server会发出一个投票。在运行期间，每个服务器上的ZXID可能不同，此时假定Server1的ZXID为123，Server3的ZXID为122；在第一轮投票中，Server1和Server3都会投自己，产生投票(1, 123)，(3, 122)，然后各自将投票发送给集群中所有机器。\n(3) 接收来自各个服务器的投票。与启动时过程相同。\n(4) 处理投票。与启动时过程相同，此时，Server1将会成为Leader。\n(5) 统计投票。与启动时过程相同。\n(6) 改变服务器的状态。与启动时过程相同。\n参考:\nhttps://www.cnblogs.com/shuaiandjun/p/9383655.html https://www.cnblogs.com/leesf456/p/6107600.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E7%AE%80%E8%BF%B0%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.html","summary":"[TOC] 前言 Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选","title":"简述选举机制"},{"content":"Dubbo是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。简单的说，dubbo就是个服务框架，如果没有分布式的需求，其实是不需要用的，只有在分布式的时候，才有dubbo这样的分布式服务框架的需求，并且本质上是个服务调用的东东，说白了就是个远程服务调用的分布式框架\n其核心部分包含:\n远程通讯: 提供对多种基于长连接的NIO框架抽象封装，包括多种线程模型，序列化，以及“请求-响应”模式的信息交换方式。\n集群容错: 提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。\n自动发现: 基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器。\n来自* \u0026lt;https://blog.csdn.net/wilsonke/article/details/39896595 \u0026gt;\n架构\n节点角色说明\n节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 来自* \u0026lt;http://dubbo.apache.org/zh-cn/docs/user/preface/architecture.html \u0026gt;\n调用关系说明:\n服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 来自* \u0026lt;http://dubbo.apache.org/zh-cn/docs/user/preface/architecture.html \u0026gt;\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E4%BB%8B%E7%BB%8D.html","summary":"Dubbo是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。简单的说，dubbo就是个服务框架","title":"介绍"},{"content":"MyBatis-Plus （简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。\n特性\n无侵入：只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑 损耗小：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作 强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求 支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错 支持多种数据库：支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer2005、SQLServer 等多种数据库 支持主键自动生成：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题 支持 XML 热加载：Mapper 对应的 XML 支持热加载，对于简单的 CRUD 操作，甚至可以无 XML 启动 支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作 支持自定义全局通用操作：支持全局通用方法注入（ Write once, use anywhere ） 支持关键词自动转义：支持数据库关键词（order、key\u0026hellip;\u0026hellip;）自动转义，还可自定义关键词 内置代码生成器：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用 内置分页插件：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List 查询 内置性能分析插件：可输出 Sql 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询 内置全局拦截插件：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作 内置 Sql 注入剥离器：支持 Sql 注入剥离，有效预防 Sql 注入攻击 官网写的超详细: https://mp.baomidou.com/ 扩展阅读: MyBatis-Plus的BaseMapper实现原理 - 掘金 (juejin.cn) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatisplus/%E4%BB%8B%E7%BB%8D.html","summary":"MyBatis-Plus （简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 特性 无侵入：只做增强不做改变，引入它不会对现有工程","title":"介绍"},{"content":"[toc]\nSeata 是什么? Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。\nSeata 整体工作流程 Seata 对分布式事务的协调和控制，主要是通过 XID 和 3 个核心组件实现的。\nXID XID 是全局事务的唯一标识，它可以在服务的调用链路中传递，绑定到服务的事务上下文中。\n核心组件 Seata 定义了 3 个核心组件：\nTC（Transaction Coordinator）：事务协调器，它是事务的协调者（这里指的是 Seata 服务器），主要负责维护全局事务和分支事务的状态，驱动全局事务提交或回滚。 TM（Transaction Manager）：事务管理器，它是事务的发起者，负责定义全局事务的范围，并根据 TC 维护的全局事务和分支事务状态，做出开始事务、提交事务、回滚事务的决议。 RM（Resource Manager）：资源管理器，它是资源的管理者（这里可以将其理解为各服务使用的数据库）。它负责管理分支事务上的资源，向 TC 注册分支事务，汇报分支事务状态，驱动分支事务的提交或回滚。 以上三个组件相互协作，TC 以 Seata 服务器（Server）形式独立部署，TM 和 RM 则是以 Seata Client 的形式集成在微服务中运行，其整体工作流程如下图。\nSeata 的整体工作流程如下：\nTM 向 TC 申请开启一个全局事务，全局事务创建成功后，TC 会针对这个全局事务生成一个全局唯一的 XID； XID 通过服务的调用链传递到其他服务; RM 向 TC 注册一个分支事务，并将其纳入 XID 对应全局事务的管辖； TM 根据 TC 收集的各个分支事务的执行结果，向 TC 发起全局事务提交或回滚决议； TC 调度 XID 下管辖的所有分支事务完成提交或回滚操作。 四种模式 Seata 提供了 AT、TCC、SAGA 和 XA 四种事务模式，可以快速有效地对分布式事务进行控制。\n在这四种事务模式中使用最多，最方便的就是 AT 模式。与其他事务模式相比，AT 模式可以应对大多数的业务场景，且基本可以做到无业务入侵，开发人员能够有更多的精力关注于业务逻辑开发。\nAT 模式 AT: Automatic Transaction\nAT 模式的前提 任何应用想要使用 Seata 的 AT 模式对分布式事务进行控制，必须满足以下 2 个前提：\n必须使用支持本地 ACID 事务特性的关系型数据库，例如 MySQL、Oracle 等； 应用程序必须是使用 JDBC 对数据库进行访问的 JAVA 应用。 整体机制 两阶段提交协议的演变：\n一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 二阶段： 提交异步化，非常快速地完成。 回滚通过一阶段的回滚日志进行反向补偿。 工作机制 以一个示例来说明整个 AT 分支的工作过程。\n业务表：product\nField Type Key id bigint(20) PRI name varchar(100) since varchar(100) AT 分支事务的业务逻辑：\nupdate product set name = \u0026#39;GTS\u0026#39; where name = \u0026#39;TXC\u0026#39;; 一阶段 过程：\n解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = \u0026lsquo;TXC\u0026rsquo;）等相关的信息。 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。 select id, name, since from product where name = \u0026#39;TXC\u0026#39;; 得到前镜像：\nid name since 1 TXC 2014 执行业务 SQL：更新这条记录的 name 为 \u0026lsquo;GTS\u0026rsquo;。\n查询后镜像：根据前镜像的结果，通过 主键 定位数据。\nselect id, name, since from product where id = 1; 得到后镜像：\nid name since 1 GTS 2014 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。 { \u0026#34;branchId\u0026#34;: 641789253, \u0026#34;undoItems\u0026#34;: [{ \u0026#34;afterImage\u0026#34;: { \u0026#34;rows\u0026#34;: [{ \u0026#34;fields\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: 4, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: 12, \u0026#34;value\u0026#34;: \u0026#34;GTS\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;since\u0026#34;, \u0026#34;type\u0026#34;: 12, \u0026#34;value\u0026#34;: \u0026#34;2014\u0026#34; }] }], \u0026#34;tableName\u0026#34;: \u0026#34;product\u0026#34; }, \u0026#34;beforeImage\u0026#34;: { \u0026#34;rows\u0026#34;: [{ \u0026#34;fields\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: 4, \u0026#34;value\u0026#34;: 1 }, { \u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;type\u0026#34;: 12, \u0026#34;value\u0026#34;: \u0026#34;TXC\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;since\u0026#34;, \u0026#34;type\u0026#34;: 12, \u0026#34;value\u0026#34;: \u0026#34;2014\u0026#34; }] }], \u0026#34;tableName\u0026#34;: \u0026#34;product\u0026#34; }, \u0026#34;sqlType\u0026#34;: \u0026#34;UPDATE\u0026#34; }], \u0026#34;xid\u0026#34;: \u0026#34;xid:xxx\u0026#34; } 提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 全局锁 。 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。 将本地事务提交的结果上报给 TC。 CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) NOT NULL, `context` varchar(128) NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime NOT NULL, `log_modified` datetime NOT NULL, `ext` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 二阶段-回滚 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句： update product set name = \u0026#39;TXC\u0026#39; where id = 1; 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。 第三点中是无法处理的情况, seata使用 @GlobalLock+@Transactional或 @GlobalTransaction 来加了全局锁, 在回滚完成前, 其他请求将被阻塞, 如果真的出现了这种情况, 可自定义实现FailureHandler做邮件通知或其他\nSeata常见问题 脏写导致数据回滚失败？ 二阶段-提交 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。 总结:\n第一阶段是把 所有要修改的数据存在seata的UNDO_LOG表里(修改前的数据),并拿到全局锁\n第二阶段 看执行结果决定是回滚还是提交\n​\t2.1 如果是回滚: 则把UNDO_LOG里对应的数据写回去\n​ 2.2 如果是提交: 则删除UNDO_LOG中的数据\nTCC 模式 什么是 TCC TCC 是分布式事务中的二阶段提交协议，它的全称为 Try-Confirm-Cancel，即资源预留（Try）、确认操作（Confirm）、取消操作（Cancel），他们的具体含义如下：\nTry：对业务资源的检查并预留； Confirm：对业务处理进行提交，即 commit 操作，只要 Try 成功，那么该步骤一定成功； Cancel：对业务处理进行取消，即回滚操作，该步骤回对 Try 预留的资源进行释放。 TCC 是一种侵入式的分布式事务解决方案，以上三个操作都需要业务系统自行实现，对业务系统有着非常大的入侵性，设计相对复杂，但优点是 TCC 完全不依赖数据库，能够实现跨数据库、跨应用资源管理，对这些不同数据访问通过侵入式的编码方式实现一个原子操作，更好地解决了在各种复杂业务场景下的分布式事务问题。\nSeata TCC 模式 Seata TCC 模式跟通用型 TCC 模式原理一致，我们先来使用 Seata TCC 模式实现一个分布式事务：\n假设现有一个业务需要同时使用服务 A 和服务 B 完成一个事务操作，我们在服务 A 定义该服务的一个 TCC 接口：\npublic interface TccActionOne { @TwoPhaseBusinessAction(name = \u0026#34;DubboTccActionOne\u0026#34;, commitMethod = \u0026#34;commit\u0026#34;, rollbackMethod = \u0026#34;rollback\u0026#34;) public boolean prepare(BusinessActionContext actionContext, @BusinessActionContextParameter(paramName = \u0026#34;a\u0026#34;) String a); public boolean commit(BusinessActionContext actionContext); public boolean rollback(BusinessActionContext actionContext); } 同样，在服务 B 定义该服务的一个 TCC 接口：\npublic interface TccActionTwo { @TwoPhaseBusinessAction(name = \u0026#34;DubboTccActionTwo\u0026#34;, commitMethod = \u0026#34;commit\u0026#34;, rollbackMethod = \u0026#34;rollback\u0026#34;) public void prepare(BusinessActionContext actionContext, @BusinessActionContextParameter(paramName = \u0026#34;b\u0026#34;) String b); public void commit(BusinessActionContext actionContext); public void rollback(BusinessActionContext actionContext); } 在业务所在系统中开启全局事务并执行服务 A 和服务 B 的 TCC 预留资源方法：\n@GlobalTransactional public String doTransactionCommit(){ //服务A事务参与者 tccActionOne.prepare(null,\u0026#34;one\u0026#34;); //服务B事务参与者 tccActionTwo.prepare(null,\u0026#34;two\u0026#34;); } 以上就是使用 Seata TCC 模式实现一个全局事务的例子，可以看出，TCC 模式同样使用 @GlobalTransactional 注解开启全局事务，而服务 A 和服务 B 的 TCC 接口为事务参与者，Seata 会把一个 TCC 接口当成一个 Resource，也叫 TCC Resource。\nTCC 接口可以是 RPC，也可以是 JVM 内部调用，意味着一个 TCC 接口，会有发起方和调用方两个身份，以上例子，TCC 接口在服务 A 和服务 B 中是发起方，在业务所在系统中是调用方。如果该 TCC 接口为 Dubbo RPC，那么调用方就是一个 dubbo:reference，发起方则是一个 dubbo:service。\nSeata 启动时会对 TCC 接口进行扫描并解析，如果 TCC 接口是一个发布方，则在 Seata 启动时会向 TC 注册 TCC Resource，每个 TCC Resource 都有一个资源 ID；如果 TCC 接口时一个调用方，Seata 代理调用方，与 AT 模式一样，代理会拦截 TCC 接口的调用，即每次调用 Try 方法，会向 TC 注册一个分支事务，接着才执行原来的 RPC 调用。\n当全局事务决议提交/回滚时，TC 会通过分支注册的的资源 ID 回调到对应参与者服务中执行 TCC Resource 的 Confirm/Cancel 方法。\n如何控制异常 在 TCC 模型执行的过程中，还可能会出现各种异常，其中最为常见的有空回滚、幂等、悬挂等。下面我讲下 Seata 是如何处理这三种异常的。\n如何处理空回滚 什么是空回滚？\n空回滚指的是在一个分布式事务中，在没有调用参与方的 Try 方法的情况下，TM 驱动二阶段回滚调用了参与方的 Cancel 方法。\n那么空回滚是如何产生的呢？\n如上图所示，全局事务开启后，参与者 A 分支注册完成之后会执行参与者一阶段 RPC 方法，如果此时参与者 A 所在的机器发生宕机，网络异常，都会造成 RPC 调用失败，即参与者 A 一阶段方法未成功执行，但是此时全局事务已经开启，Seata 必须要推进到终态，在全局事务回滚时会调用参与者 A 的 Cancel 方法，从而造成空回滚。\n要想防止空回滚，那么必须在 Cancel 方法中识别这是一个空回滚，Seata 是如何做的呢？\nSeata 的做法是新增一个 TCC 事务控制表，包含事务的 XID 和 BranchID 信息，在 Try 方法执行时插入一条记录，表示一阶段执行了，执行 Cancel 方法时读取这条记录，如果记录不存在，说明 Try 方法没有执行。\n如何处理幂等 幂等问题指的是 TC 重复进行二阶段提交，因此 Confirm/Cancel 接口需要支持幂等处理，即不会产生资源重复提交或者重复释放。\n那么幂等问题是如何产生的呢？\n如上图所示，参与者 A 执行完二阶段之后，由于网络抖动或者宕机问题，会造成 TC 收不到参与者 A 执行二阶段的返回结果，TC 会重复发起调用，直到二阶段执行结果成功。\nSeata 是如何处理幂等问题的呢？\n同样的也是在 TCC 事务控制表中增加一个记录状态的字段 status，该字段有 3 个值，分别为：\ntried：1 committed：2 rollbacked：3 二阶段 Confirm/Cancel 方法执行后，将状态改为 committed 或 rollbacked 状态。当重复调用二阶段 Confirm/Cancel 方法时，判断事务状态即可解决幂等问题。\n如何处理悬挂 悬挂指的是二阶段 Cancel 方法比 一阶段 Try 方法优先执行，由于允许空回滚的原因，在执行完二阶段 Cancel 方法之后直接空回滚返回成功，此时全局事务已结束，但是由于 Try 方法随后执行，这就会造成一阶段 Try 方法预留的资源永远无法提交和释放了。\n那么悬挂是如何产生的呢？\n如上图所示，在执行参与者 A 的一阶段 Try 方法时，出现网路拥堵，由于 Seata 全局事务有超时限制，执行 Try 方法超时后，TM 决议全局回滚，回滚完成后如果此时 RPC 请求才到达参与者 A，执行 Try 方法进行资源预留，从而造成悬挂。\nSeata 是怎么处理悬挂的呢？\n在 TCC 事务控制表记录状态的字段 status 中增加一个状态：\nsuspended：4 当执行二阶段 Cancel 方法时，如果发现 TCC 事务控制表有相关记录，说明二阶段 Cancel 方法优先一阶段 Try 方法执行，因此插入一条 status=4 状态的记录，当一阶段 Try 方法后面执行时，判断 status=4 ，则说明有二阶段 Cancel 已执行，并返回 false 以阻止一阶段 Try 方法执行成功。\nSaga模式 Saga 模式是一种补偿协议。在 Saga 模式中，在分布式事务内有多个参与者，每个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向和逆向回滚操作。 如下图所示，T1~T3 都是正向的业务流程，都对应着一个冲正逆向操作 C1~C3。\n在分布式事务执行过程中，会依次执行各参与者的正向操作：\n如果所有正向操作均执行成功，则分布式事务提交；\n如果任何一个正向操作执行失败，则分布式事务会退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。\nSaga 模式的优势：\n在一阶段提交本地数据库事务，无锁，高性能 参与者可以采用事件驱动异步执行，高吞吐 补偿服务即正向服务的\u0026quot;反向\u0026quot;操作，易于理解，易于实现 Saga 模式也存在很明显的缺点：在一阶段已经提交了本地数据库事务，且没有进行\u0026quot;预留\u0026quot;动作，所以不能保证隔离性，不容易进行并发控制。与 AT 模式和 TCC 模式相比，Saga 模式的适用场景有限。\nA模式 在 XA 模式中，需要在 Seata 定义的分布式事务范围内，利用事务资源实现对 XA 协议的支持，以 XA 协议的机制来管理分支事务。\n本质上，Seata 的 AT、TCC、Saga模式都是补偿型的。事务处理机制构建在框架或应用中。事务资源本身对分布式事务是无感知的。而在 XA 模式下，事务资源对分布式事务是可感知的。\nSeata：Spring Cloud Alibaba分布式事务组件（非常详细） (biancheng.net) Seata 中文官网 深度剖析 Seata TCC 模式（一） Seata之模式简介 - 带翅膀的猫 (chengpengper.cn) 动手实践Seata四种模式（XA、AT、TCC、SAGA）_小钟要学习！！！的博客-CSDN博客_seata xa 模式 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E4%BB%8B%E7%BB%8D.html","summary":"[toc] Seata 是什么? Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA","title":"介绍"},{"content":"根据Istio官方文档的介绍，Istio在服务网络中主要提供了以下关键功能：\n流量管理：控制服务之间的流量和API调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。 可观察性：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。 策略执行：将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。策略的更改是通过配置网格而不是修改应用程序代码。 服务身份和安全：为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。 平台支持：Istio旨在在各种环境中运行，包括跨云、Kubernetes、Mesos等。最初专注于Kubernetes，但很快将支持其他环境。 集成和定制：策略执行组件可以扩展和定制，以便与现有的ACL、日志、监控、配额、审核等解决方案集成。 Istio针对可扩展性进行了设计，以满足不同的部署需要。这些功能极大的减少了应用程序代码、底层平台和策略之间的耦合，使得微服务更加容易实现。\n下图为Istio的架构设计图，主要包括了Envoy、Pilot、Mixer和Istio-Auth等。\nEnvoy: 扮演Sidecar的功能，协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集与流量相关的性能指标。\nPilot: 负责部署在Service Mesh中的Envoy实例的生命周期管理。本质上是负责流量管理和控制，将流量和基础设施扩展解耦，这是Istio的核心。可以把Pilot看做是管理Sidecar的Sidecar, 但是这个特殊的Sidacar并不承载任何业务流量。Pilot让运维人员通过Pilot指定它们希望流量遵循什么规则，而不是哪些特定的pod/VM应该接收流量。有了Pilot这个组件，我们可以非常容易的实现 A/B 测试和金丝雀Canary测试。\nMixer: Mixer在应用程序代码和基础架构后端之间提供通用中介层。它的设计将策略决策移出应用层，用运维人员能够控制的配置取而代之。应用程序代码不再将应用程序代码与特定后端集成在一起，而是与Mixer进行相当简单的集成，然后Mixer负责与后端系统连接。Mixer可以认为是其他后端基础设施（如数据库、监控、日志、配额等）的Sidecar Proxy。\nIstio-Auth: 提供强大的服务间认证和终端用户认证，使用交互TLS，内置身份和证书管理。可以升级服务网格中的未加密流量，并为运维人员提供基于服务身份而不是网络控制来执行策略的能力。Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括基于属性和角色的访问控制以及授权钩子）来控制和监视访问服务、API或资源的访问者。\nIstio的设计理念先进，功能也比较强大，加之Google、IBM的影响力让Istio迅速传播，让广大开发者认识到了Istio项目在Service Mesh领域的重要性。但是Istio目前版本也存在了一些不足：\n目前的Istio大部分能力与Kubernetes是强关联的。而我们在构建微服务的时候往往是希望服务层与容器层是解耦的，服务层在设计上需要能够对接多种容器层平台。 Istio至今未有稳定版本，截至本文编写时为止，Istio的最新版本为0.8版本，预计在2018年内会发布1.0版本。 来自: https://blog.csdn.net/dylloveyou/article/details/81951286 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/servicemesh/istio/%E4%BB%8B%E7%BB%8D.html","summary":"根据Istio官方文档的介绍，Istio在服务网络中主要提供了以下关键功能： 流量管理：控制服务之间的流量和API调用的流向，使得调用更可靠，","title":"介绍"},{"content":"前言: 微服务:将系统很多功能拆分成一个个小服务(就是很多工程)\n服务网格:服务有多了,如果服务之间随意通信,就会形成蜘蛛网,乱七八糟,不好管理,(目前捷顺是人为避免server之间调用,各个业务系统之间也没有联系,唯一的联系是项目和组织,这些都在B门户,通过API接口来通信,柳波说,华为的微服务也是用api的方式通信,多达几千个接口)服务网格就是处理服务之间的通信,让程序猿只需关注业务,类比于现在把网络分发和程序,程序不需要关注网络怎么分发,因为已经抽了tcp/ip出来,如图\n服务网格就是在每个服务\u0026quot;旁边\u0026quot;加个东西,这个东西专门用来出流量处理与通信之类的,这个东西叫sidecar(边车,抗日剧中那种三轮车,很形象吧),如果有很多服务,每个服务\u0026quot;旁边\u0026quot;有个sidercar,sidecar用来通信,就形成了网格的样子,如图\n注: 蓝色的是sidecar,绿色的就是(微服务)应用\n第一代Service Mesh 的 代表为 Linkerd和Envoy\n第二代Service Mesh 的 代表为 Istio\n目前 第一代 都在为 Istio做支持,就是说第一代基本放弃竞争了,和Istio共同推进Service Mesh的发展,更牛逼的是Istio是由 Google、IBM 和 Lyft 联合开发\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/servicemesh/%E4%BB%8B%E7%BB%8D.html","summary":"前言: 微服务:将系统很多功能拆分成一个个小服务(就是很多工程) 服务网格:服务有多了,如果服务之间随意通信,就会形成蜘蛛网,乱七八糟,不好管理","title":"介绍"},{"content":"spring boot 想杜绝spring mvc辣么多xml配置,所有在spring boot中没有几乎没有xml文件(当然你用xml也兼容),全部用properties 和 注解去代替\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E4%BB%8B%E7%BB%8D.html","summary":"spring boot 想杜绝spring mvc辣么多xml配置,所有在spring boot中没有几乎没有xml文件(当然你用xml也兼容),全部用proper","title":"介绍"},{"content":"[TOC]\nSpringCloud是基于SpringBoot的一整套实现微服务的框架。它提供了微服务开发所需的配置管理、服务发现、断路器、智能路由、微代理、控制总线、全局锁、决策竞选、分布式会话和集群状态管理等组件。最重要的是，基于SpringBoot，会让开发微服务架构非常方便。列举只是部分,更多参照官网: https://spring.io/projects/spring-cloud 来自 https://blog.lqdev.cn/2018/09/04/SpringCloud/chapter-one/ 核心组件 SpringCloudGateway Spring Cloud Gateway是Spring官方基于Spring 5.0，Spring Boot 2.0和Project Reactor等技术开发的网关，Spring Cloud Gateway旨在为微服务架构提供一种简单而有效的统一的API路由管理方式。\nSpring Cloud Gateway作为Spring Cloud生态系中的网关，目标是替代Netflix ZUUL，其不仅提供统一的路由方式，并且基于Filter链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。\nSpringCloudNetflix 这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka,Hystrix,Zuul… 太多了\nNetflix Eureka 服务中心，云端服务发现，一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。\nNetflix Hystrix 熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。比如突然某个小弟生病了(服务达到处理上限或者因为网络问题连不通等等,就是这个小弟不能正常处理请求的情况)，但是你还需要它的支持，然后调用之后它半天没有响应，你却不知道，一直在等等这个响应；有可能别的小弟也正在调用你的武功绝技，那么当请求多之后，就会发生严重的阻塞影响老大的整体计划。这个时候Hystrix就派上用场了，当Hystrix发现某个小弟不在状态不稳定立马马上让它下线，让其它小弟来顶上来，或者给你说不用等了这个小弟今天肯定不行，该干嘛赶紧干嘛去别在这排队了。\nNetflix Zuul Zuul是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和Netflix流应用的 Web 网站后端所有请求的前门。当其它门派来找大哥办事的时候一定要先经过zuul,看下有没有带刀子什么的给拦截回去，或者是需要找那个小弟的直接给带过去。\nSpringCloudConfig 俗称的配置中心，配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。就是以后大家武器、枪火什么的东西都集中放到一起，别随便自己带，方便以后统一管理、升级装备。\nSpringCloudBus 事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。相当于水浒传中日行八百里的神行太保戴宗，确保各个小弟之间消息保持畅通。\nSpringCloudforCloudFoundry Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题 其实就是与CloudFoundry进行集成的一套解决方案，抱了Cloud Foundry的大腿。\nSpringCloudCluster Spring Cloud Cluster将被Spring Integration取代。提供在分布式系统中的集群所需要的基础功能支持，如：选举、集群的状态一致性、全局锁、tokens等常见状态模式的抽象和实现。 如果把不同的帮派组织成统一的整体，Spring Cloud Cluster已经帮你提供了很多方便组织成统一的工具。\nSpringCloudConsul Consul是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对. Spring Cloud Consul封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。\n来自 https://blog.lqdev.cn/2018/09/04/SpringCloud/chapter-one/ SpringCloud Sleuth/zipkin Spring-Cloud-Sleuth是Spring Cloud的组成部分之一，为SpringCloud应用实现了一种分布式追踪解决方案，其兼容了Zipkin, HTrace和log-based追踪\n来自:https://blog.csdn.net/u010257992/article/details/52474639\nzipkin是链路监控的界面,官方好像没有专门的文档,一般和sleuth一起使用,当然,链路监控不止zipkin,也有其他第三方的也能和sleuth结合.zipkin可以展示某一个请求的链路,各个服务之间的调用情况等等,\nspringboot2.X以后不建议自己写服务端了(可以理解为链路监控中心,其他被链路的,像jportal就是客户端),直接运行官方的jar包即可,在客户端引zipkin包和配置即可完成,暂没有实践,以后有机会再来吧\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springcloud/%E4%BB%8B%E7%BB%8D.html","summary":"[TOC] SpringCloud是基于SpringBoot的一整套实现微服务的框架。它提供了微服务开发所需的配置管理、服务发现、断路器、智能路由、微","title":"介绍"},{"content":"Docker 中包括三个基本的概念：\nImage（镜像） Container（容器） Repository（仓库 ) 镜像是 Docker 运行容器的前提，仓库是存放镜像的场所，可见镜像更是 Docker 的核心。\n来自链接 \u0026gt;\nDocker 面向对象 容器 对象 镜像 类 来自* \u0026lt;http://www.runoob.com/docker/docker-architecture.html \u0026gt;\nDocker 镜像(Images) Docker 镜像是用于创建 Docker 容器的模板。 Docker 容器(Container) 容器是独立运行的一个或一组应用。 Docker 客户端(Client) Docker 客户端通过命令行或者其他工具使用 Docker API (https://docs.docker.com/reference/api/docker_remote_api ) 与 Docker 的守护进程通信。 Docker 主机(Host) 一个物理或者虚拟的机器用于执行 Docker 守护进程和容器。 Docker 仓库(Registry) Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。Docker Hub(https://hub.docker.com ) 提供了庞大的镜像集合供使用。 Docker Machine Docker Machine是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、 Digital Ocean、Microsoft Azure。 来自* \u0026lt;http://www.runoob.com/docker/docker-architecture.html \u0026gt;\nImage（镜像）\n那么镜像到底是什么呢？Docker 镜像可以看作是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。\n镜像不包含任何动态数据，其内容在构建之后也不会被改变。镜像（Image）就是一堆只读层（read-only layer）的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助读者理解镜像的定义：\n从左边我们看到了多个只读层，它们重叠在一起。除了最下面一层，其他层都会有一个指针指向下一层。这些层是 Docker 内部的实现细节，并且能够在主机的文件系统上访问到。\n统一文件系统（Union File System）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角。\n这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。我们可以在图片的右边看到这个视角的形式。\n来自 链接 \u0026gt;\n来自* \u0026lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg \u0026gt;\nContainer（容器）\n容器（Container）的定义和镜像（Image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的(估计是用来生成镜像的)。\n来自 链接 \u0026gt;\nRepository（仓库）\nDocker 仓库是集中存放镜像文件的场所。镜像构建完成后，可以很容易的在当前宿主上运行。\n但是， 如果需要在其他服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry（仓库注册服务器）就是这样的服务。\n有时候会把仓库（Repository）和仓库注册服务器（Registry）混为一谈，并不严格区分。\nDocker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。\n实际上，一个 Docker Registry 中可以包含多个仓库（Repository），每个仓库可以包含多个标签（Tag），每个标签对应着一个镜像。\n所以说，镜像仓库是 Docker 用来集中存放镜像文件的地方，类似于我们之前常用的代码仓库。\n通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。\n我们可以通过\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 Latest 作为默认标签。\n仓库又可以分为两种形式：\nPublic（公有仓库） Private（私有仓库） Docker Registry 公有仓库是开放给用户使用、允许用户管理镜像的 Registry 服务。\n一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。\n除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。\n当用户创建了自己的镜像之后就可以使用 Push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 Pull 下来就可以了。\n来自链接 \u0026gt;\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/docker/%E4%BB%8B%E7%BB%8D.html","summary":"Docker 中包括三个基本的概念： Image（镜像） Container（容器） Repository（仓库 ) 镜像是 Docker 运行容器的前提，仓库是存放镜像的场所","title":"介绍"},{"content":"[TOC]\n一.概念 日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。\n通常，日志被分散在储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。\n集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。\n通过我们需要对日志进行集中化管理，将所有机器上的日志信息收集、汇总到一起。完整的日志数据具有非常重要的作用：\n1）信息查找。通过检索日志信息，定位相应的bug，找出解决方案。\n2）服务诊断。通过对日志信息进行统计、分析，了解服务器的负荷和服务运行状态，找出耗时请求进行优化等等。\n3）数据分析。如果是格式化的log，可以做进一步的数据分析，统计、聚合出有意义的信息，比如根据请求中的商品id，找出TOP10用户感兴趣商品。\n开源实时日志分析ELK平台能够完美的解决我们上述的问题，ELK由ElasticSearch、Logstash和Kiabana三个开源工具组成：\n1）ElasticSearch是一个基于Lucene的开源分布式搜索服务器。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是第二流行的企业搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。在elasticsearch中，所有节点的数据是均等的。\nElasticsearch本身就是分布式的，因此即便你只有一个节点，Elasticsearch默认也会对你的数据进行分片和副本操作，当你向集群添加新数据时，数据也会在新加入的节点中进行平衡。\n2）Logstash是一个完全开源的工具，它可以对你的日志进行收集、过滤、分析，支持大量的数据获取方法，并将其存储供以后使用（如搜索）。说到搜索，logstash带有一个web界面，搜索和展示所有日志。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。\n3）Kibana 是一个基于浏览器页面的Elasticsearch前端展示工具，也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。\nELK工作原理展示图：\n如上图：Logstash收集AppServer产生的Log，并存放到ElasticSearch集群中，而Kibana则从ES集群中查询数据生成图表，再返回给Browser。\nLogstash工作原理：\nLogstash事件处理有三个阶段：inputs → filters → outputs。是一个接收，处理，转发日志的工具。支持系统日志，webserver日志，错误日志，应用日志，总之包括所有可以抛出来的日志类型。\nInput：输入数据到logstash。\n一些常用的输入为：\nfile：从文件系统的文件中读取，类似于tail -f命令\nsyslog：在514端口上监听系统日志消息，并根据RFC3164标准进行解析\nredis：从redis service中读取\nbeats：从filebeat中读取\nFilters：数据中间处理，对数据进行操作。\n一些常用的过滤器为：\ngrok：解析任意文本数据，Grok 是 Logstash 最重要的插件。它的主要作用就是将文本格式的字符串，转换成为具体的结构化的数据，配合正则表达式使用。内置120多个解析语法。\nmutate：对字段进行转换。例如对字段进行删除、替换、修改、重命名等。\ndrop：丢弃一部分events不进行处理。\nclone：拷贝 event，这个过程中也可以添加或移除字段。\ngeoip：添加地理信息(为前台kibana图形化展示使用)\nOutputs：outputs是logstash处理管道的最末端组件。一个event可以在处理过程中经过多重输出，但是一旦所有的outputs都执行结束，这个event也就完成生命周期。\n一些常见的outputs为：\nelasticsearch：可以高效的保存数据，并且能够方便和简单的进行查询。\nfile：将event数据保存到文件中。\ngraphite：将event数据发送到图形化组件中，一个很流行的开源存储图形化展示的组件。\nCodecs：codecs 是基于数据流的过滤器，它可以作为input，output的一部分配置。Codecs可以帮助你轻松的分割发送过来已经被序列化的数据。\n一些常见的codecs：\njson：使用json格式对数据进行编码/解码。\nmultiline：将汇多个事件中数据汇总为一个单一的行。比如：java异常信息和堆栈信息。\n这里，需要重点关注filter部分，下面列举几个常用的插件，实际使用中根据自身需求从官方文档中查找适合自己业务的插件并使用即可，当然也可以编写自己的插件。\n**grok：**是Logstash最重要的一个插件，用于将非结构化的文本数据转化为结构化的数据。grok内部使用正则语法对文本数据进行匹配，为了降低使用复杂度，其提供了一组pattern，我们可以直接调用pattern而不需要自己写正则表达式，参考源码grok-patterns。grok解析文本的语法格式是%{SYNTAX:SEMANTIC}，SYNTAX是pattern名称，SEMANTIC是需要生成的字段名称，使用工具Grok Debugger可以对解析语法进行调试。例如，在下面的配置中，我们先使用grok对输入的原始nginx日志信息（默认以message作为字段名）进行解析，并添加新的字段request_path_with_verb（该字段的值是verb和request_path的组合），然后对request_path字段做进一步解析。\n**kv：**用于将某个字段的值进行分解，类似于编程语言中的字符串Split。在下面的配置中，我们将request_args字段值按照“\u0026amp;”进行分解，分解后的字段名称以“request_args_”作为前缀，并且丢弃重复的字段。\n**geoip：**用于根据IP信息生成地理位置信息，默认使用自带的一份GeoLiteCity database，也可以自己更换为最新的数据库，但是需要数据格式需要遵循Maxmind的格式（参考GeoLite），似乎目前只能支持legacy database，数据类型必须是.dat。下载GeoLiteCity.dat.gz后解压， 并将文件路径配置到source中即可。\n**translate：**用于检测某字段的值是否符合条件，如果符合条件则将其翻译成新的值，写入一个新的字段，匹配pattern可以通过YAML文件来配置。例如，在下面的配置中，我们对request_api字段翻译成更加易懂的文字描述。\nfilter { grok { match =\u0026gt; {\u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IPORHOST:client_ip} \\\u0026#34;%{TIMESTAMP_ISO8601:timestamp}\\\u0026#34; \\\u0026#34;%{WORD:verb} %{NOTSPACE:request_path} HTTP/%{NUMBER:httpversion}\\\u0026#34; %{NUMBER:response_status:int} %{NUMBER:response_body_bytes:int} \\\u0026#34;%{DATA:user_agent}\\\u0026#34; \\\u0026#34;%{DATA:http_referer}\\\u0026#34; \\\u0026#34;%{NOTSPACE:http_x_forwarder_for}\\\u0026#34; \\\u0026#34;%{NUMBER:request_time:float}\\\u0026#34; \\\u0026#34;%{DATA:upstream_resopnse_time}\\\u0026#34; \\\u0026#34;%{DATA:http_cookie}\\\u0026#34; \\\u0026#34;%{DATA:http_authorization}\\\u0026#34; \\\u0026#34;%{DATA:http_token}\\\u0026#34;\u0026#34;} add_field =\u0026gt; {\u0026#34;request_path_with_verb\u0026#34; =\u0026gt; \u0026#34;%{verb} %{request_path}\u0026#34;} } grok { match =\u0026gt; {\u0026#34;request_path\u0026#34; =\u0026gt; \u0026#34;%{URIPATH:request_api}(?:\\?%{NOTSPACE:request_args}|)\u0026#34;} add_field =\u0026gt; {\u0026#34;request_annotation\u0026#34; =\u0026gt; \u0026#34;%{request_api}\u0026#34;} } kv { prefix =\u0026gt; \u0026#34;request_args_\u0026#34; field_split =\u0026gt; \u0026#34;\u0026amp;\u0026#34; source =\u0026gt; \u0026#34;request_args\u0026#34; allow_duplicate_values =\u0026gt; false } geoip { source =\u0026gt; \u0026#34;client_ip\u0026#34; database =\u0026gt; \u0026#34;/home/elktest/geoip_data/GeoLiteCity.dat\u0026#34; } translate { field =\u0026gt; request_path destination =\u0026gt; request_annotation regex =\u0026gt; true exact =\u0026gt; true dictionary_path =\u0026gt; \u0026#34;/home/elktest/api_annotation.yaml\u0026#34; override =\u0026gt; true } } ======================ELK整体方案=======================\nELK中的三个系统分别扮演不同的角色，组成了一个整体的解决方案。Logstash是一个ETL工具，负责从每台机器抓取日志数据，对数据进行格式转换和处理后，输出到Elasticsearch中存储。Elasticsearch是一个分布式搜索引擎和分析引擎，用于数据存储，可提供实时的数据查询。Kibana是一个数据可视化服务，根据用户的操作从Elasticsearch中查询数据，形成相应的分析结果，以图表的形式展现给用户。\nELK的安装很简单，可以按照\u0026quot;下载-\u0026gt;修改配置文件-\u0026gt;启动\u0026quot;方法分别部署三个系统，也可以使用docker来快速部署。具体的安装方法这里不详细介绍，下面来看一个常见的部署方案，如下图所示，部署思路是：\n1）在每台生成日志文件的机器上，部署Logstash，作为Shipper的角色，负责从日志文件中提取数据，但是不做任何处理，直接将数据输出到Redis队列(list)中；\n2）需要一台机器部署Logstash，作为Indexer的角色，负责从Redis中取出数据，对数据进行格式化和相关处理后，输出到Elasticsearch中存储；\n3）部署Elasticsearch集群，当然取决于你的数据量了，数据量小的话可以使用单台服务，如果做集群的话，最好是有3个以上节点，同时还需要部署相关的监控插件；\n4）部署Kibana服务，提供Web服务。\n在前期部署阶段，主要工作是Logstash节点和Elasticsearch集群的部署，而在后期使用阶段，主要工作就是Elasticsearch集群的监控和使用Kibana来检索、分析日志数据了，当然也可以直接编写程序来消费Elasticsearch中的数据。\n在上面的部署方案中，我们将Logstash分为Shipper和Indexer两种角色来完成不同的工作，中间通过Redis做数据管道，为什么要这样做？为什么不是直接在每台机器上使用Logstash提取数据、处理、存入Elasticsearch？\n首先，采用这样的架构部署，有三点优势：\n第一，降低对日志所在机器的影响，这些机器上一般都部署着反向代理或应用服务，本身负载就很重了，所以尽可能的在这些机器上少做事；\n第二，如果有很多台机器需要做日志收集，那么让每台机器都向Elasticsearch持续写入数据，必然会对Elasticsearch造成压力，因此需要对数据进行缓冲，同时，这样的缓冲也可以一定程度的保护数据不丢失；\n第三，将日志数据的格式化与处理放到Indexer中统一做，可以在一处修改代码、部署，避免需要到多台机器上去修改配置。\n其次，我们需要做的是将数据放入一个消息队列中进行缓冲，所以Redis只是其中一个选择，也可以是RabbitMQ、Kafka等等，在实际生产中，Redis与Kafka用的比较多。由于Redis集群一般都是通过key来做分片，无法对list类型做集群，在数据量大的时候必然不合适了，而Kafka天生就是分布式的消息队列系统。\nhttps://www.cnblogs.com/kevingrace/p/5919021.html ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/%E4%BB%8B%E7%BB%8D.html","summary":"[TOC] 一.概念 日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的","title":"介绍"},{"content":"[toc]\n一、Kubernetes是什么？ 官方文档中描述为:\nKubernetes一个用于容器集群的自动化部署、扩容以及运维的开源平台。通过Kubernetes,你可以快速有效地响应用户需求;快速而有预期地部署你的应用;极速地扩展你的应用;无缝对接新应用功能;节省资源，优化硬件资源的使用。为容器编排管理提供了完整的开源方案。\n介绍一下其中提到的几个词:\n容器\n我们现在常说的容器一般是指Docker容器，通过容器隔离的特性和宿主机进行解耦，使我们的服务不需要依赖于宿主机而运行，与宿主机互不影响，Docker容器十分轻量。而kubernetes则负责管理服务中所有的Docker容器，创建、运行、重启与删除容器。\n快速响应\n个人理解为两个方面。\n一、新增或者修改需求时，可以快速进行部署测试(CICD)；\n二、kubernetes可以根据不同条件进行动态扩缩容，举个栗子，用户访问量突然由1000人上升到100000人时，现有的服务已经无法支撑，kubernetes会自动将用户服务模块增加更多实例以保证当前的系统访问量。\n扩展\n在快速响应的特点中已经有所提及，这里再补充一点: Kubernetes内部有完善的注册发现机制，当某个服务的实例增加时，kubernetes会自动将其加入服务列表中，免除在传统运维中需要人工维护服务列表的问题。\n对接新应用\nkubernetes是一个通用的容器编排框架，支持不同类型的语言，或者是语言无关的，新增加的应用都会以一个新的对象进行接入。\n硬件资源\n这一点我觉得是kubernetess很基本但是非常重要的一个优点了，kubernetes在部署应用时会自动检查各个服务器的cpu与内存使用量，同时会根据服务申请的cpu与内存资源，将服务部署到最合适的服务器。(其实这就是容器调度的核心功能了)\n小知识: 因kubernetes名字过长，一般简称为k8s，因为k与s之间有8个字母，故而称之。\n二、Kubernetes解决了什么问题？ 下面以几个case进行阐述，便于理解。\n服务器环境\nkubernetes是使用Docker进行容器管理的，所以天生具备Docker的所有特性，只需要使用相应环境的Docker镜像就可以运行服务，还需要关心宿主机是redhat、centos还是ubuntu，只要在宿主机上安装Docker环境即可，相比传统运维，减少了各种依赖环境的冲突，降低运维成本，也方便整体服务的迁移。\n服务器资源管理\n对于kubernetes来说，是不关心有几台服务器的，每个服务器都是一个资源对象(Node)，kubernetes关心的是这个Node上有多少可用的cpu和内存。例如现在有两台服务器\nserver01 (4c16g), 已用(2c7.5G)\nserver02 (4c16g), 已用(3c13G)\n现在有一个服务ServiceA需要部署，ServiceA申明自己运行需要至少3G内存，这时kubernetes会根据调度策略将其部署到server01上，很明显server01的资源是更加充足的。实际上kubernetes的调度策略要复杂的多，kubernetes会监控整体服务器资源的状态进行调度，而以前的运维方式只能由人工判断资源使用。\n服务容灾恢复\n说简单点，就是服务挂了之后，能够自动恢复。例如现在有一个ServiceA，运行在server01上，kubernetes会通过内部的kubelet组件监控ServiceA服务进程的状态，一旦发现进程丢失(服务本身挂掉或者整个server01的服务器挂掉)，就会尝试换一台资源充足的服务器重新部署ServiceA并启动，这样就可以确保我们的服务一直是可用状态，而不需要人工维护。\n硬件资源利用\n前面已经说过，kubernetes会根据节点(Node)的CPU与内存资源的可用量对服务进行部署调度，在调度策略中，可以配置不同的调度策略。例如现在有两台服务器：\nserver01 (4c16g), 已用(3c7.5G) server02 (4c16g), 已用(1c13G) 需要部署两个服务\nserviceA-Java, 申请2G内存，0.5CPU单位 ServiceB-Nginx, 申请200M内存,申请1CPU单位 这里kubernetes如果讲道理的话，会将ServiceA-Java部署到server01，将serviceB-Nginx部署到server02。这里server01的内存和server02的CPU资源都得到了充分的利用。经过个人实践，相比之前的部署方式，kubernetes节省了很多资源，资源利用是非常高效的。\n原文：https://blog.csdn.net/kingboyworld/article/details/80966107\n版本管理与滚动升级\n版本管理 kubernetes在部署服务时，会记录部署服务的版本，我们可以很容易的进行上次版本或跨版本回退。\n滚动升级 kubernetes在进行服务升级时，采用的默认策略是先将一部分新的服务启动，确定服务正常后，停止一部分旧服务，进行新老服务的替换，之后再启动一些新的服务，停止一部分旧服务，直到旧服务全部停止，即切换完成。滚动省级的过程中，极大的减少了服务切换的间隔时间。\n其它\n上面所说的是kubernetes的主体功能，kubernetes还有很多其他重要的特性解决了之前运维的痛点，例如DNS解析、自动负载、存储声明等等。\n原文：https://blog.csdn.net/kingboyworld/article/details/80966107\n三、kubernetes特点 网络模型\nkubernetes采用了三层网络模型，分为PodIP,ClusterIP,NodeIP。用简单的话来说，kubernetes在内部使用自己的网络进行通讯，这样做一个最直接的好处是我们不用再担心端口冲突的问题。\n举个栗子: 我们在server01上部署两个一样的服务serviceA-1,serviceA-2,两个服务的端口都是8080，这个时候有一个服务是无法启动的，因为端口被占用了，而在kubernetes中，两个服务在不同的Docker容器中,每个Docker容器都有自己的IP,这时就不会出现端口占用的问题了。\n为什么要有三层网络有三个IP呢？其实每个IP的作用是不一样的：\nNodeIP NodeIP是最好理解的，就是每个服务器的IP。例如server01的IP是192.168.1.2，有一个服务实例的IP申明类型为NodeIP，端口申明为30222,那么我们就可以通过192.168.1.2:30222访问到这个服务实例。\nPodIP PodIP的作用可以简单理解为每个服务自己特有的IP,就像上面说的可以解决端口冲突的问题，同时也是每个服务的唯一标识。PodIP是无法通过外网访问的，只能在服务内部进行访问。\nClusterIP(可以按照下面访问的进行理解，但实际有所区别) 中文叫集群IP。集群IP可以简单理解为是对同一个服务的多个实例(每个实例有自己的PodIP)组成的集群的入口IP，换句话说，是对多个实例的负载IP。举个栗子：\n有三个实例:\nserviceA-1 172.22.1.2 serviceA-2 172.22.1.3 serviceA-3 172.22.1.4 有一个ClusterIP 172.23.2.23指向了serviceA服务，那么我们访问172.23.2.23则会负载转向到172.22.1.2、172.22.1.3、172.22.1.4中的其中一个服务\n对象\n在kubernetes中，万物皆对象。路由(Ingress)、服务(Service)、部署(Deployment)、存储(Storage/PV/PVC)、容器(Pod)、角色(Role)、账户(Accoutn)、配置(ConfigMap)等等。通过管理这些对象来管理整个kubernetes集群。\n注意：此处说的服务(Service),不同于上文提到的服务(开发的项目模块)\n声名式管理\nkubernetes采用声名式进行资源管理，也就是从结果来看问题。举个栗子，现在需要部署十个ServiceA\n面向过程: 部署ServiceA-01,再部署ServiceA02…..ServiceA-10，强调的是过程，用代码来表示的话就是while(serviceA.count \u0026lt; 10) {serviceA.count++}\n面向结果(声明式):不管是同时部署还是挨个部署，总之要部署十个ServiceA服务。用代码来表示的话就是kubernetes.addServiceA(10),不用管内部的细节怎么处理，只要最终的结果。\n四、Kubernetes常用相关概念 部署 - Deployment Deployment 的作用是管理和控制 Pod 和 ReplicaSet，管控它们运行在用户期望的状态中。哎，打个形象的比喻，Deployment 就是包工头，主要负责监督底下的工人 Pod 干活，确保每时每刻有用户要求数量的 Pod 在工作。如果一旦发现某个工人 Pod 不行了，就赶紧新拉一个 Pod 过来替换它。\nReplicaSets ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。ReplicaSet 的作用就是管理和控制 Pod，管控他们好好干活。但是，ReplicaSet 受控于 Deployment。形象来说，ReplicaSet 就是总包工头手下的小包工头。\n从 K8S 使用者角度来看，用户会直接操作 Deployment 部署服务，而当 Deployment 被部署的时候，K8S 会自动生成要求的 ReplicaSet 和 Pod。在**K8S 官方文档 中也指出用户只需要关心 Deployment 而不操心 ReplicaSet**：\nReplicationController\nReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。其实“ReplicationController 是 ReplicaSet 的前身”，官方也推荐用 Deployment 取代 ReplicationController 来部署服务。\n容器组 - Pod\nPod是Kubernetes中的最小管理单元，Pods和Docker中的容器可以理解为包含关系，在Pods中可以包含有多个Docker容器，例如有ServiceA和ServiceB,ServiceA高度依赖ServiceB(需要共享主机的相同文件),这时就可以将ServiceA与ServiceB放在同一个Pods中，当做一个整体来管理。如果分开部署当然也可以，不过会消耗额外的资源或者产生其他不必要的麻烦。例如我们的应用服务/redis等 都是pod\n服务 - Service Service是一个对象，并不是我们常说的“服务”的含义, 这个对象有自己的IP，也就是ClusterIP, 是若干个 Pod 的流量入口、流量均衡器。Service 是 K8S 服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”。举个例子，我们的一个服务 A，部署了 3 个备份，也就是 3 个 Pod；对于用户来说，只需要关注一个 Service 的入口就可以，而不需要操心究竟应该请求哪一个 Pod。优势非常明显：一方面外部用户不需要感知因为 Pod 上服务的意外崩溃、K8S 重新拉起 Pod 而造成的 IP 变更，外部用户也不需要感知因升级、变更服务带来的 Pod 替换而造成的 IP 变化，另一方面，Service 还可以做流量负载均衡。\n路由 - Ingress Ingress 是整个 K8S 集群的接入层，复杂集群内外通讯。无论是容器组还是Service，外网都是无法直接访问的，Ingress就可以通过一个负载IP与Kubernetes集群内部进行通讯，一般会和Service对象进行配合使用。\n配置项 - ConfigMap 简单理解为一个管理配置的对象，可以将项目的配置写入到ConfgiMap中，项目中的配置使用相应的变量名就可以读取相应的变量值。\nVolume 数据卷 K8S 支持很多类型的 volume 数据卷挂载，具体请参见**K8S 卷 **。前文就“如何理解 volume”提到：“需要手动 mount 的磁盘”，此外，有一点可以帮助理解：数据卷 volume 是 Pod 内部的磁盘资源。\nvolumeMounts volume 是 K8S 的对象，对应一个实体的数据卷；而 volumeMounts 只是 container 的挂载点，对应 container 的其中一个参数。但是，volumeMounts 依赖于 volume，只有当 Pod 内有 volume 资源的时候，该 Pod 内部的 container 才可能有 volumeMounts。\nContainer 容器 一个 Pod 内可以有多个容器 container。\n在 Pod 中，容器也有分类\n标准容器 Application Container。 初始化容器 Init Container。 边车容器 Sidecar Container。 临时容器 Ephemeral Container。 一般来说，我们部署的大多是标准容器（ Application Container）。\nnamespace 命名空间 和前文介绍的所有的概念都不一样，namespace 跟 Pod 没有直接关系，而是 K8S 另一个维度的对象。或者说，前文提到的概念都是为了服务 Pod 的，而 namespace 则是为了服务整个 K8S 集群的。\nKubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为namespace 。\n比如我有 2 个业务 A 和 B，那么我可以创建 ns-a 和 ns-b 分别部署业务 A 和 B 的服务，如在 ns-a 中部署了一个 deployment，名字是 hello，返回用户的是“hello a”；在 ns-b 中也部署了一个 deployment，名字恰巧也是 hello，返回用户的是“hello b”（要知道，在同一个 namespace 下 deployment 不能同名；但是不同 namespace 之间没有影响）\n五、Kubernetes结构 Kubernetes由Master节点和Worker节点组成。master节点是Kubernetes的大脑，而woker节点则是kubernetes中实际运行服务的劳动者。\nMaster 主要由ETCD/Controller Manager/Api Server/Schedular能成，\nETCD 主要负责存储各个woker节点的状态和其它相关数据，可以理解为kubernetes的数据库。\nController Manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等, K8S 所有 Worker Node 的监控器。Controller Manager 有很多具体的 Controller。Controller 负责监控和调整在 Worker Node 上部署的服务的状态，比如用户要求 A 服务部署 2 个副本，那么当其中一个服务挂了的时候，Controller 会马上调整，让 Scheduler 再选择一个 Worker Node 重新部署服务。\nScheduler K8S 所有 Worker Node 的调度器, 负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上\nAPI Server。K8S 的请求入口服务\nAPI Server 负责接收 K8S 所有请求（来自 UI 界面或者 CLI 命令行工具），然后，API Server 根据用户的具体请求，去通知其他组件干活。\nWorker 主要由kubelet和kube-proxy组成，一般还会安装kube-dns组件。\nkubelet Worker Node 的监视器，以及与 Master Node 的通讯器, 负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；\nkube-proxy 负责为Service提供cluster内部的服务发现和负载均衡；\nkube-dns 负责为整个集群提供DNS服务，通过Service名称访问相应的服务\nContainer Runtime。\nWorker Node 的运行环境。即安装了容器化所需的软件环境确保容器化程序能够跑起来，比如 Docker Engine。大白话就是帮忙装好了 Docker 运行环境。\nLogging Layer。K8S 的监控状态收集器。\nLogging Layer 负责采集 Node 上所有服务的 CPU、内存、磁盘、网络等监控项信息。\nKubernetes 入门\u0026amp;进阶实战 - 知乎 (zhihu.com) 最新、最全、最详细的 K8S 学习笔记总结（2021最新版） - 知乎 (zhihu.com) 一、Kubernetes简介-是什么？_KimZing的博客-CSDN博客 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/kubernetes/%E4%BB%8B%E7%BB%8D.html","summary":"[toc] 一、Kubernetes是什么？ 官方文档中描述为: Kubernetes一个用于容器集群的自动化部署、扩容以及运维的开源平台。通过Kuber","title":"介绍"},{"content":"[toc]\n1. 简介 RocketMQ是一个纯Java、分布式、队列模型的开源消息中间件，前身是MetaQ，是阿里参考Kafka特点研发的一个队列模型的消息中间件，后开源给apache基金会成为了apache的顶级开源项目，具有高性能、高可靠、高实时、分布式特点\n2. 组成 他主要有四大核心组成部分：NameServer、Broker、Producer以及Consumer四部分。\nRocketMQ架构上主要分为四部分，如上图所示:\nProducer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳\nConsumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳. Consumer既可以从Master订阅消息，也可以从Slave订阅消息. Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。\nNameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。\n​\t主要包括两个功能：\nBroker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；\n路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。\nNameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。\nBrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。\nRemoting Module：整个Broker的实体，负责处理来自clients端的请求。\nClient Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息\nStore Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。\nHA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。\nIndex Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。\n集群工作流程：\n启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。\nBroker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。\n收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。\n如果是自动创建主题, 则会沿用默认主题TBW102的配置,它在哪,自动创建的主题就在哪\nRocketMQ自动创建topic - 简书 (jianshu.com) Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。\nConsumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。\nNameServer：\n主要负责对于源数据的管理，包括了对于Topic和路由信息的管理。\nNameServer压力不会太大，平时主要开销是在维持心跳和提供Topic-Broker的关系数据。\n但有一点需要注意，Broker向NameServer发心跳时， 会带上当前自己所负责的所有Topic信息，如果Topic个数太多（万级别），会导致一次心跳中，就Topic的数据就几十M，网络情况差的话， 网络传输失败，心跳失败，导致NameServer误认为Broker心跳失败。\nNameServer 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。\n每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。\nProducer\n消息生产者，负责产生消息，一般由业务系统负责产生消息。\nRocketMQ 提供了三种方式发送消息：同步、异步和单向\n同步发送：同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。\n异步发送：异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。\n单向发送：单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。\nBroker\n消息中转角色，负责存储消息，转发消息。\nBroker是具体提供业务的服务器，单个Broker节点与所有的NameServer节点保持长连接及心跳，并会定时将Topic信息注册到NameServer，顺带一提底层的通信和连接都是基于Netty实现的。\nBroker负责消息存储，以Topic为纬度支持轻量级的队列，单机可以支撑上万队列规模，支持消息推拉模型。\nConsumer\n消息消费者，负责消费消息，一般是后台系统负责异步消费。\nConsumer也由用户部署，支持PUSH和PULL两种消费模式，支持集群消费和广播消息，提供实时的消息订阅机制。\nPull：拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。(默认20s拉取一次)\nPush：推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。\npush 方式其本质是加快版的pull(长轮询), 从结果来看就像是push一样\n消息领域模型\nMessage\nMessage（消息）就是要传输的信息。\n一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题。\nTopic\nTopic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。\n一个 Topic 也可以被 0个、1个、多个消费者订阅。\nTag\nTag（标签）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。\n标签有助于保持您的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。\nGroup\n分组，一个组可以订阅多个Topic。\n分为ProducerGroup，ConsumerGroup，代表某一类的生产者和消费者，一般来说同一个服务可以作为Group，同一个Group一般来说发送和消费的消息都是一样的\nQueue\n在Kafka中叫Partition，每个Queue内部是有序的，在RocketMQ中分为读和写两种队列，一般来说读写队列数量一致，如果不一致就会出现很多问题。\n读写队列，则是在做路由信息时使用。在消息发送时，使用写队列个数返回路由信息，而消息消费时按照读队列个数返回路由信息。在物理文件层面，只有写队列才会创建文件。举个例子：写队列个数是8，设置的读队列个数是4.这个时候，会创建8个文件夹，代表0 1 2 3 4 5 6 7，但在消息消费时，路由信息只返回4，在具体拉取消息时，就只会消费0 1 2 3这4个队列中的消息，4 5 6 7中的信息压根就不会被消费。反过来，如果写队列个数是4，读队列个数是8，在生产消息时只会往0 1 2 3中生产消息，消费消息时则会从0 1 2 3 4 5 6 7所有的队列中消费，当然 4 5 6 7中压根就没有消息 ，假设消费group有两个消费者，事实上只有第一个消费者在真正的消费消息(0 1 2 3)，第二个消费者压根就消费不到消息。由此可见，只有readQueueNums\u0026gt;=writeQueueNums,程序才能正常进行\n简单来说就是, 读队列表示消费的数量, 写队列表示生产的数量\nrocketmq设置读写队列数的目的在于方便队列的缩容和扩容。思考一个问题，一个topic在每个broker上创建了128个队列，现在需要将队列缩容到64个，怎么做才能100%不会丢失消息，并且无需重启应用程序？\n最佳实践：先缩容写队列128-\u0026gt;64，写队列由0 1 2 \u0026hellip;\u0026hellip;127缩至 0 1 2 \u0026hellip;\u0026hellip;..63。等到64 65 66\u0026hellip;\u0026hellip;127中的消息全部消费完后，再缩容读队列128-\u0026gt;64.(同时缩容写队列和读队列可能会导致部分消息未被消费)\nrocketmq中的读写队列_八荒六合唯我独尊-CSDN博客_rocketmq读写队列 Message Queue\nMessage Queue（消息队列），主题被划分为一个或多个子主题，即消息队列。\n一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。\n消息的物理管理单位。一个Topic下可以有多个Queue，Queue的引入使得消息的存储可以分布式集群化，具有了水平扩展能力。\ntopic下有消息队列, 消息队列又分为读/写队列, 应该是这样吧?????\nOffset\n在RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用Offset 来访问，Offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限。\n也可以认为 Message Queue 是一个长度无限的数组，Offset 就是下标。\n消息消费模式\n消息消费模式有两种：Clustering（集群消费）和Broadcasting（广播消费）。\n默认情况下就是集群消费，该模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。\n而广播消费消息会发给消费者组中的每一个消费者进行消费。\nMessage Order\nMessage Order（消息顺序）有两种：Orderly（顺序消费）和Concurrently（并行消费）。\n顺序消费表示消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列。\n并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。\n3. 生产者 3.1 三种发送消息方式 rocketmq支持三种发送消息的方式，分别是同步发送（sync），异步发送（async）和直接发送（oneway）\n同步发送 sync 发送消息采用同步模式，这种方式只有在消息完全发送完成之后才返回结果，此方式存在需要同步等待发送结果的时间代价。\n这种方式具有内部重试机制，即在主动声明本次消息发送失败之前，内部实现将重试一定次数，默认为2次（DefaultMQProducer＃getRetryTimesWhenSendFailed）。 发送的结果存在同一个消息可能被多次发送给给broker，这里需要应用的开发者自己在消费端处理幂等性问题。\n异步发送 async 发送消息采用异步发送模式，消息发送后立刻返回，当消息完全完成发送后，会调用回调函数sendCallback来告知发送者本次发送是成功或者失败。异步模式通常用于响应时间敏感业务场景，即承受不了同步发送消息时等待返回的耗时代价。\n同同步发送一样，异步模式也在内部实现了重试机制，默认次数为2次（DefaultMQProducer#getRetryTimesWhenSendAsyncFailed）。发送的结果同样存在同一个消息可能被多次发送给给broker，需要应用的开发者自己在消费端处理幂等性问题。\n直接发送 one-way 采用one-way发送模式发送消息的时候，发送端发送完消息后会立即返回，不会等待来自broker的ack来告知本次消息发送是否完全完成发送。这种方式吞吐量很大，但是存在消息丢失的风险，所以其适用于不重要的消息发送，比如日志收集。\n4. 消费者 5. broker 5.1 动态、增减 Broker 扩容\n对集群进行扩容的时候，可以动态增加 Broker 角色的机器 。 只增加 Broker 不会对原有的 Topic 产生影响，原来创建好的 Topic 中数据的读写依然在原来的那些 Broker 上进行 。\n集群扩容后,\n一种是可以把新建的 Topic 指定到新的 Broker 机器上，均衡利用资源；\n另一种方式是通过 updateTopic 命令更改现有的 Topic 配置，在新加的 Broker 上创建新的队列 。 比如 TestTopic 是现有的一个 Topic ，因为数据量增大需要扩容，新增的一个 Broker 机器地址是 192 . 168.0.1:10911 ，这个时候执行下面的命令： sh ./bin/mqadmin updateTopic -b 192.168.0.1:10911 -t TestTopic -n 192.168.0.100:9876 ，结果是在新增的 Broker 机器上，为 TestTopic 新创建了 8个读写队列 。\n缩容\n当某个 Topic 有多个 Master Broker，停了其中一个，这时候是否会丢失消息呢？\n答案和 Producer 使用的发送消息的方式有关，\n如果使用同步方式 send ( msg ）发送，在DefaultMQProducer 内部有个自动重试逻辑，其中一个 Broker 停了，会自动向另一个 Broker 发消息，不会发生丢消息现象。\n如果使用异步方式发送 send ( msg, callback ），或者用 sendOneWay 方式，会丢失切换过程中的消息 。\n因为在异步和 sendOneWay 这两种发送方式下，Producer.setRetryTimesWhensendFailed 设置不起作用，发送失败不会重试 。DefaultMQProducer 默认每 30 秒到 NameServer 请求最新的路由消息， Producer如果获取不到已停止的 Broker 下的队列信息，后续就自动不再向这些队列发送消息 。\n5.2 消息存储 5.2.1 消息存储整体架构 消息存储架构图中主要有下面三个跟消息存储相关的文件构成。\n(1) CommitLog：消息主体以及元数据的存储主体，存储Producer端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G, 文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件；\n(2) ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能，由于RocketMQ是基于主题topic的订阅模式，消息消费是针对主题进行的，如果要遍历commitlog文件中根据topic检索消息是非常低效的。Consumer即可根据ConsumeQueue来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引,而不是存储生产者的消息本身，保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量offset(8字节)，消息大小size(4字节)和消息Tag的HashCode值(8字节)。consumequeue文件可以看成是基于topic的commitlog索引文件，故consumequeue文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样consumequeue文件采取定长设计，每一个条目共20个字节，分别为8字节的commitlog物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个ConsumeQueue文件大小约5.72M；\n(3) IndexFile：IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。Index文件的存储位置是：$HOME \\store\\index${fileName}，文件名fileName是以创建时的时间戳命名的，固定的单个IndexFile文件大小约为400M，一个IndexFile可以保存 2000W个索引，IndexFile的底层存储设计为在文件系统中实现HashMap结构，故rocketmq的索引文件其底层实现为hash索引。\nindexFile 这个文件用来加快消息查询的速度 。 按照Message Key查询消息就会用到这个文件\nindexFile落盘并不是采取定时刷盘机制，而是每更新一次索引文件就会将上一次的改动刷写到磁盘 。 见文件org.apache.rocketmq.store.index.IndexService#flush\n在上面的RocketMQ的消息存储整体架构图中可以看出，RocketMQ采用的是混合型的存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。RocketMQ的混合型存储结构(多个Topic的消息实体内容都存储于一个CommitLog中)针对Producer和Consumer分别采用了数据和索引部分相分离的存储结构，Producer发送消息至Broker端，然后Broker端使用同步或者异步的方式对消息刷盘持久化，保存至CommitLog中。只要消息被刷盘持久化至磁盘文件CommitLog中，那么Producer发送的消息就不会丢失。正因为如此，Consumer也就肯定有机会去消费这条消息。当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，如果一个消息拉取请求未拉取到消息，Broker允许等待30s的时间，只要这段时间内有新消息到达，将直接返回给消费端。这里，RocketMQ的具体做法是，使用Broker端的后台服务线程—ReputMessageService不停地分发请求并异步构建ConsumeQueue（逻辑消费队列）和IndexFile（索引文件）数据。\n5.2.2 页缓存与内存映射 页缓存（PageCache)是OS对文件的缓存，用于加速对文件的读写。一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于OS使用PageCache机制对读写访问操作进行了性能优化，将一部分的内存用作PageCache。对于数据的写入，OS会先写入至Cache内，随后通过异步的方式由pdflush内核线程将Cache内的数据刷盘至物理磁盘上。对于数据的读取，如果一次读取文件时出现未命中PageCache的情况，OS从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取。\n在RocketMQ中，ConsumeQueue逻辑消费队列存储的数据较少，并且是顺序读取，在page cache机制的预读取作用下，Consume Queue文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。而对于CommitLog消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取，严重影响性能。如果选择合适的系统IO调度算法，比如设置调度算法为“Deadline”（此时块存储采用SSD的话），随机读的性能也会有所提升。\n另外，RocketMQ主要通过MappedByteBuffer对文件进行读写操作。其中，利用了NIO中的FileChannel模型将磁盘上的物理文件直接映射到用户态的内存地址中（这种Mmap的方式减少了传统IO将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（正因为需要使用内存映射机制，故RocketMQ的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存）。\n6. topic 一个topic可以分布在多broker上, 所以只有多个broker合起来才是一个完整的topic内容\n5. RocketMQ 集群部署模式 0. 单 master 模式\n​ 也就是只有一个 master 节点，称不上是集群，一旦这个 master 节点宕机，那么整个服务就不可用，适合个人学习使用。\n多 master 模式 多个 master 节点组成集群，单个 master 节点宕机或者重启对应用没有影响。 优点：所有模式中性能最高 缺点：单个 master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息的实时性就受到影响。 注意：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 queue 应该分布在集群中各个节点，而不是只在某各节点上，否则，该节点宕机会对订阅该 topic 的应用造成影响。 多 master 多 slave 异步复制模式 在多 master 模式的基础上，每个 master 节点都有至少一个对应的 slave。master 节点可读可写，但是 slave 只能读不能写，类似于 mysql 的主备模式。 优点： 在 master 宕机时，消费者可以从 slave 读取消息，消息的实时性不会受影响，性能几乎和多 master 一样。 缺点：使用异步复制的同步方式有可能会有消息丢失的问题。 多 master 多 slave 同步双写模式 同多 master 多 slave 异步复制模式类似，区别在于 master 和 slave 之间的数据同步方式。 优点：同步双写的同步模式能保证数据不丢失。 缺点：发送单个消息 RT 会略长，性能相比异步复制低10%左右。 刷盘策略：同步刷盘和异步刷盘（指的是节点自身数据是同步还是异步存储） 同步方式：同步双写和异步复制（指的一组 master 和 slave 之间数据的同步） 注意：要保证数据可靠，需采用同步刷盘和同步双写的方式，但性能会较其他方式低。 几个核心参数之间可按需组合 =\u0026gt; master和slave的数量/ 刷盘策略/ master和salve之间的同步方式\nA\u0026amp;Q commitLog的消息是怎么同步到consumerQueue中的?\nRocketMQ 通过开启 一个线程 ReputMessageServcie 来准实时转发 CommitLog 文件更新事件 ， 相应 的任务处理器根据转发的消息及时更新 ConsumeQueue 、 IndexFile 文件 。broker启动时就会启动同步线程,\nbroker启动后会取 math.max(所有队列中物理偏移量,commitLog低水位), 得应该从哪个位置开始追加队列消息 然后就会启动线程, 每1毫秒执行一次 从commitLog里取出消息来,通过调用 doDispatch方法 。 最终将分别调用 CommitLogDispatcherBuildConsumeQueue （构建消息消费队列 ）、CommitLogDispatcherBuildlndex （构建索引文件） 消息消 费 队列转发任务实现类为 ： CommitLogDispatcherBuildConsumeQueue ，内部最终将调用 putMessagePositioninfo 方法 , 根据消息 主题与 队列 ID ，先获取对应的 ConumeQueue, 依 次将消息偏移量、消息长度、 tag的 hashcode 写入到 ByteBuffer 中，并根据 consumeQueueOffset 计算 ConumeQu eue 中的物理地址，将内 容追加到 ConsumeQueue 的内存映射文件中（本操作只追击并不刷盘 ）， ConumeQueue 的刷盘方式固定为异步刷盘模式 总结, broker启动后就会启动监听线程,每1毫秒执行一次, 把消息从commitlog取出来,分别去构建consumerQueue和indexFile,\n构建consumerQueue的消息写到MappedByteBuffer中(MappedFile类),只写内存不刷盘\nbroker 的高可用\n《浅入浅出》-RocketMQ - 知乎 (zhihu.com) rocketmq/docs/cn · apache/rocketmq (github.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/rocketmq/%E4%BB%8B%E7%BB%8D.html","summary":"[toc] 1. 简介 RocketMQ是一个纯Java、分布式、队列模型的开源消息中间件，前身是MetaQ，是阿里参考Kafka特点研发的一个队列模型的消","title":"介绍"},{"content":"1.hadoop:\nlucene (全文检索) \u0026ndash;\u0026gt;Nutch(搜索引擎)\nGFS(谷歌文件系统) \u0026mdash;\u0026gt;HDFS hadoop分布式文件系统\nMapReduce(数据的处理算法) \u0026ndash;\u0026gt;MapReduce 分布式计算\nbigtable(列式数据库) \u0026mdash;\u0026gt;HBase\n主要包括:\n分布式存储系统:HDFS\n资源管理系统 :YARN \u0026ndash;\u0026gt;负责集群资源的统一管理和调度((hadoop2.0才有))\n分布式计算框架: MapReduce\n元数据:\n对数据信息描述的数据(比如,某个资源在哪个块,起始地址是什么,大小为多少,)\nnamenode 持久化 有两个文件:fsimage,edits\nSecondaryNameNode 合并fsimage,edits两个文件,加快启动速度\nDN(dateNote) 和NN(namenode)保持心跳机制\n副本放置策略:\n找负载比较低的\n放在不与第一个副本同一个地方\n放在第二个副本同一个机架\nhadoop作业流程:\n客户向ResourceManager提交作业\nResourceManager的ApplicationManager通知一个NodeManager启动contrainer并在contrainer中启动ApplicationMaster负责这次作业\nApplicationMaster向ApplicationManager注册\nApplicationMaster向ResourceManager的ResourceSchedule轮询申请资源\n申请到资源后,通知相应的NodeManager启动作业\nNodeManager启动contrainer并执行相应的Map/Reduce Task\n执行的Task向ApplicationMaster汇报作业情况\n作业执行完成后,Application想ApplicationMaster注销作业\n注:namenode: ResourceManager -\u0026gt; ResourceSchedule , ApplicationManager ;\ndatanode : NodeManager ; ApplicationManager ; MapTask ;ReduceTask\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86.html","summary":"1.hadoop: lucene (全文检索) \u0026ndash;\u0026gt;Nutch(搜索引擎) GFS(谷歌文件系统) \u0026mdash;\u0026gt;HDFS hadoop分布式文件系统 MapReduce(数据的处理","title":"介绍及原理"},{"content":"","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86.html","summary":"","title":"介绍及原理"},{"content":"[toc]\n1. 前言 Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。已有的Hive系统虽然也提供了SQL语义，但由于Hive底层执行使用的是MapReduce引擎，仍然是一个批处理过程，难以满足查询的交互性。相比之下，Impala的最大特点也是最大卖点就是它的快速。但也依赖Hive (impala元数据都存储在Hive的metastore中,或者说两者元数据共存)\n来自* \u0026lt;https://baike.baidu.com/item/Impala/7458017?fr=aladdin \u0026gt;\n2.Impala核心组件 2.1 Impala Daemon Impala的核心组件是运行在各个节点上面的impalad这个守护进程（Impala daemon），它负责读写数据文件，接收从impala-shell、Hue、JDBC、ODBC等接口发送的查询语句，并行化查询语句和分发工作任务到Impala集群的各个节点上，同时负责将本地计算好的查询结果发送给协调器节点（coordinator node）。\n你可以向运行在任意节点的Impala daemon提交查询，这个节点将会作为这个查询的协调器（coordinator node），其他节点将会传输部分结果集给这个协调器节点。由这个协调器节点构建最终的结果集。在做实验或者测试的时候为了方便，我们往往连接到同一个Impala daemon来执行查询，但是在生产环境运行产品级的应用时，我们应该循环（按顺序）的在不同节点上面提交查询，这样才能使得集群的负载达到均衡。\nImpala daemon不间断的跟statestore进行通信交流，从而确认哪个节点是健康的能接收新的工作任务。它同时接收catalogd daemon（从Impala 1.2之后支持）传来的广播消息来更新元数据信息，当集群中的任意节点create、alter、drop任意对象、或者执行INSERT、LOAD DATA的时候触发广播消息。\n2.2 Impala Statestore Impala Statestore检查集群各个节点上Impala daemon的健康状态，同时不间断地将结果反馈给各个Impala daemon。这个服务的物理进程名称是statestored，在整个集群中我们仅需要一个这样的进程即可。如果某个Impala节点由于硬件错误、软件错误或者其他原因导致离线，statestore就会通知其他的节点，避免其他节点再向这个离线的节点发送请求。\n由于statestore是当集群节点有问题的时候起通知作用，所以它对Impala集群并不是有关键影响的。如果statestore没有运行或者运行失败，其他节点和分布式任务会照常运行，只是说当节点掉线的时候集群会变得没那么健壮。当statestore恢复正常运行时，它就又开始与其他节点通信并进行监控。\n2.3 Impala Catalog Imppalla catalog服务将SQL语句做出的元数据变化通知给集群的各个节点，catalog服务的物理进程名称是catalogd，在整个集群中仅需要一个这样的进程。由于它的请求会跟statestore daemon交互，所以最好让statestored和catalogd这两个进程在同一节点上。\nImpala 1.2中加入的catalog服务减少了REFRESH和INVALIDATE METADATA语句的使用。在之前的版本中，当在某个节点上执行了CREATE DATABASE、DROP DATABASE、CREATE TABLE、ALTER TABLE、或者DROP TABLE语句之后，需要在其它的各个节点上执行命令INVALIDATE METADATA来确保元数据信息的更新。同样的，当你在某个节点上执行了INSERT语句，在其它节点上执行查询时就得先执行REFRESH table_name这个操作，这样才能识别到新增的数据文件。需要注意的是，通过Impala执行的操作带来的元数据变化，有了catalog就不需要再执行REFRESH和INVALIDATE METADATA，但如果是通过Hive进行的建表、加载数据，则仍然需要执行REFRESH和INVALIDATE METADATA来通知Impala更新元数据信息。\n来自* \u0026lt;http://www.cnblogs.com/chenz/articles/3947147.html \u0026gt;\n3. Impala执行步骤 Impala执行的查询有以下几个步骤：\n1. 客户端通过ODBC、JDBC、或者Impala shell向Impala集群中的任意节点发送SQL语句，这个节点的impalad实例作为这个查询的协调器（coordinator）。\n![img](F:\\学习资料\\个人笔记\\MDImages\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/impala/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86.html","summary":"[toc] 1. 前言 Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级","title":"介绍及原理"},{"content":"[toc]\n1.介绍 ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。\n来自: 链接 \u0026gt;\n2.ZooKeeper的设计目标 简单的数据结构 : Zookeeper 使得分布式程序能够通过一个共享的树形结构的名字空间来进行相互协调，即Zookeeper 服务器内存中的数据模型由一系列被称为ZNode的数据节点组成，Zookeeper 将全量的数据存储在内存中，以此来提高服务器吞吐、减少延迟的目的。 可以构建集群 : Zookeeper 集群通常由一组机器构成，组成 Zookeeper 集群的而每台机器都会在内存中维护当前服务器状态，并且每台机器之间都相互通信。 顺序访问 : 对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序。 高性能 : Zookeeper 和Redis一样全量数据存储在内存中，100%读请求压测QPS 12-13W 3.关于 ZooKeeper 的一些重要概念 3.1 Zookeeper 集群： Zookeeper 是一个由多个 server 组成的集群,一个 leader，多个 follower。（这个不同于我们常见的Master/Slave模式）leader 为客户端服务器提供读写服务，除了leader外其他的机器只能提供读服务。 每个 server 保存一份数据副本全数据一致，分布式读 follower，写由 leader 实施更新请求转发，由 leader 实施更新请求顺序进行，来自同一个 client 的更新请求按其发送顺序依次执行数据更新原子性，一次数据更新要么成功，要么失败。全局唯一数据视图，client 无论连接到哪个 server，数据视图都是一致的实时性，在一定事件范围内，client 能读到最新数据。\n3.2 集群角色 ①.Leader：是整个 Zookeeper 集群工作机制中的核心 。Leader 作为整个 ZooKeeper 集群的主节点，负责响应所有对 ZooKeeper 状态变更的请求。 主要工作：\n事务请求的唯一调度和处理，保障集群处理事务的顺序性。 集群内各服务器的调度者。 Leader 选举是 Zookeeper 最重要的技术之一，也是保障分布式数据一致性的关键所在,选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪 我们以三台机器为例，在服务器集群初始化阶段，当有一台服务器Server1启动时候是无法完成选举的，当第二台机器 Server2 启动后两台机器能互相通信，每台机器都试图找到一个leader，于是便进入了 leader 选举流程.\n来自 https://blog.lqdev.cn/2018/09/09/SpringCloud/chapter-three/ 。\n每个 server 发出一个投票 投票的最基本元素是（SID-服务器id,ZXID-事物id） 接受来自各个服务器的投票 处理投票 优先检查 ZXID(数据越新ZXID越大),ZXID比较大的作为leader，ZXID一样的情况下比较SID 统计投票 这里有个过半的概念，大于集群机器数量的一半，即大于或等于（n/2+1）,我们这里的由三台，大于等于2即为达到“过半”的要求。 这里也有引申到为什么 Zookeeper 集群推荐是单数。 集群数量 至少正常运行数量 允许挂掉的数量 2 2的半数为1，半数以上最少为2 0 3 3的半数为1.5，半数以上最少为2 1 4 4的半数为2，半数以上最少为3 1 5 5的半数为2.5，半数以上最少为3 2 6 6的半数为3，半数以上最少为4 2 通过以上可以发现，3台服务器和4台服务器都最多允许1台服务器挂掉，5台服务器和6台服务器都最多允许2台服务器挂掉,明显4台服务器成本高于3台服务器成本，6台服务器成本高于5服务器成本。这是由于半数以上投票通过决定的。\n5.改变服务器状态 一旦确定了 leader，服务器就会更改自己的状态，且一般不会再发生变化，比如新机器加入集群、非 leader 挂掉一台。\n②.Follower ：是 Zookeeper 集群状态的跟随者。他的逻辑就比较简单。除了响应本服务器上的读请求外，follower 还要处理leader 的提议，并在 leader 提交该提议时在本地也进行提交。另外需要注意的是，leader 和 follower 构成ZooKeeper 集群的法定人数，也就是说，只有他们才参与新 leader的选举、响应 leader 的提议。\n③.Observer ：服务器充当一个观察者的角色。如果 ZooKeeper 集群的读取负载很高，或者客户端多到跨机房，可以设置一些 observer 服务器，以提高读取的吞吐量。Observer 和 Follower 比较相似，只有一些小区别：首先 observer 不属于法定人数，即不参加选举也不响应提议，也不参与写操作的“过半写成功”策略；其次是 observer 不需要将事务持久化到磁盘，一旦 observer 被重启，需要从 leader 重新同步整个名字空间。\n④.Learner: Observer和Follower统称为学习者,因为同步数据时他们都会接受leader的同步\n3.3.会话（Session） Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session 的 sessionTimeout 值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。\n3.3.1 会话（Session） 在Zookeeper客户端与服务端成功完成连接创建后，就创建了一个会话，Zookeeper会话在整个运行期间的生命周期中，会在不同的会话状态中之间进行切换，这些状态可以分为CONNECTING、CONNECTED、RECONNECTING、RECONNECTED、CLOSE等。\n一旦客户端开始创建Zookeeper对象，那么客户端状态就会变成CONNECTING状态，同时客户端开始尝试连接服务端，\n连接成功后，客户端状态变为CONNECTED，\n通常情况下，由于断网或其他原因，客户端与服务端之间会出现断开情况，这时，Zookeeper客户端会自动进行重连服务，同时客户端状态再次变成CONNCTING，\n直到重新连上服务端后，状态又变为CONNECTED，\n在通常情况下，客户端的状态总是介于CONNECTING 和CONNECTED 之间。\n但是，如果出现诸如会话超时、权限检查或是客户端主动退出程序等情况，客户端的状态就会直接变更为CLOSE状态。\n3.3.2 会话创建 Session是Zookeeper中的会话实体，代表了一个客户端会话，其包含了如下四个属性\nsessionID。会话ID，唯一标识一个会话，每次客户端创建新的会话时，Zookeeper都会为其分配一个全局唯一的sessionID。 TimeOut。会话超时时间，客户端在构造Zookeeper实例时，会配置sessionTimeout参数用于指定会话的超时时间，Zookeeper客户端向服务端发送这个超时时间后，服务端会根据自己的超时时间限制最终确定会话的超时时间。 TickTime。下次会话超时时间点，为了便于Zookeeper对会话实行”分桶策略”管理，同时为了高效低耗地实现会话的超时检查与清理，Zookeeper会为每个会话标记一个下次会话超时时间点，其值大致等于当前时间加上TimeOut。 isClosing。标记一个会话是否已经被关闭，当服务端检测到会话已经超时失效时，会将该会话的isClosing标记为”已关闭”，这样就能确保不再处理来自该会话的心情求了。 Zookeeper为了保证请求会话的全局唯一性，在SessionTracker初始化时，调用initializeNextSession方法生成一个sessionID，之后在Zookeeper运行过程中，会在该sessionID的基础上为每个会话进行分配，初始化算法如下\npublic static long initializeNextSession(long id) { long nextSid = 0; // 无符号右移8位是为了避免左移24后，再右移8位出现负数而无法通过高8位确定sid值 nextSid = (System.currentTimeMillis() \u0026laquo; 24) \u0026raquo;\u0026gt; 8; nextSid = nextSid | (id \u0026laquo; 56); return nextSid; }\n3.3.3 会话管理 Zookeeper的会话管理主要是通过SessionTracker来负责，其采用了分桶策略（将类似的会话放在同一区块中进行管理）进行管理，以便Zookeeper对会话进行不同区块的隔离处理以及同一区块的统一处理。\n3.4 数据节点 Znode 在Zookeeper中，“节点\u0026quot;分为两类，\n第一类同样是指构成集群的机器，我们称之为机器节点； 第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。\n3.4.1 节点类型 在Zookeeper中，node可以分为持久节点 和 临时节点 和 顺序节点 三大类。\n可以通过组合生成如下四种类型节点\nPERSISTENT 持久节点,节点创建后便一直存在于Zookeeper服务器上，直到有删除操作来主动清楚该节点。 PERSISTENT_SEQUENTIAL 持久顺序节点,相比持久节点，其新增了顺序特性，每个父节点都会为它的第一级子节点维护一份顺序，用于记录每个子节点创建的先后顺序。在创建节点时，会自动添加一个数字后缀，作为新的节点名，该数字后缀的上限是整形的最大值。 EPEMERAL 临时节点，临时节点的生命周期与客户端会话绑定，客户端失效，节点会被自动清理。同时，Zookeeper规定不能基于临时节点来创建子节点，即临时节点只能作为叶子节点。 3.5 版本——保证分布式数据原子性操作 每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。\nversion– 当前数据节点数据内容的版本号 cversion– 当前数据子节点的版本号 aversion– 当前数据节点ACL变更版本号 上述各版本号都是表示修改次数，如version为1表示对数据节点的内容变更了一次。即使前后两次变更并没有改变数据内容，version的值仍然会改变。version可以用于写入验证，类似于CAS。\n3.6 watcher事件监听器 ZooKeeper允许用户在指定节点上注册一些Watcher，当数据节点发生变化的时候，ZooKeeper服务器会把这个变化的通知发送给感兴趣的客户端,客户端是先收到变更通知,后收到修改后的数据\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86.html","summary":"[toc] 1.介绍 ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组","title":"介绍及原理"},{"content":"[toc]\n1. flume是什么？ flume是分布式的，可靠的，高可用的，用于对不同来源的大量的日志数据进行有效收集、聚集和移动，并以集中式的数据存储的系统。\n整个系统分为三层：Agent层(客户机)，Collector层(中心服务器)和Store层(存储服务器)。\n来自* \u0026lt;http://www.aboutyun.com/thread-8317-1-1.html \u0026gt;\nflume由agent组成,agent由source，channel，sink组成，Agent使用JVM 运行Flume\nsource：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据, (数据来源)包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义\nchannel: source组件把数据收集来以后，临时存放在channel中,即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 它可以和任意数量的source和sink链接 来自 http://www.cnblogs.com/gongxijun/p/5656778.html sink：sink组件是用于把数据发送到目的地的组件，从channel接受内容,目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 Thrift: thrift是一个软件框架，用来进行可扩展且跨语言的服务的开发(Facebook)。\nAvro:Avro是一个数据序列化系统，设计用于支持大批量数据交换的应用(Apache),flume内置\nevent: 传输数据的基本单位。一个完整的event包括：event headers、event body、event信息(即文本文件中的单行记录)\nevent将传输的数据进行封装，如果是文本文件，通常是一行记录，\nevent也是事务的基本单位。event从source，流向channel，再到sink，\n本身为一个字节数组并可携带headers(头信息)信息。\nevent代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。\nFlume提供了三种级别的可靠性保障，从强到弱依次分别为：\nend-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。）， Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送）， Best effort（数据发送到接收方后，不会进行确认） 来自 \u0026lt;http://www.cnblogs.com/oubo/archive/2012/05/25/2517751.html \u0026gt;\n2. 关于flume启动 1)、flume组件启动顺序：channels——\u0026gt;sinks——\u0026gt;sources，关闭顺序：sources——\u0026gt;sinks——\u0026gt;channels；\n2)、自动加载配置文件功能，会先关闭所有组件，再重启所有组件；\n3)、关于AbstractConfigurationProvider中的Map\u0026lt;Class\u0026lt;? extends Channel\u0026gt;, Map\u0026lt;String, Channel\u0026raquo; channelCache这个对象，始终存储着agent中得所有channel对象，因为在动态加载时，channel中可能还有未消费完的数据，但是需要对channel重新配置，所以用以来缓存channel对象的所有数据及配置信息；\n4)、通过在启动命令中添加\u0026quot;no-reload-conf\u0026quot;参数为true来取消自动加载配置文件功能；\n来自* \u0026lt;http://www.cnblogs.com/lxf20061900/p/4012847.html \u0026gt;\n3. flume的运行过程 ​\t把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。\n为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),\n待(全部)数据真正到达目的地(sink)后，flume在删除自己缓存的数据。\nflume可以支持多级flume的agent,\n即flume可以前后相继，例如sink可以将数据写到下一个agent的source中，这样的话就可以连成串了，可以整体处理了。flume还支持扇入(fan-in)、扇出(fan-out)。所谓扇入就是source可以接受多个输入，所谓扇出就是sink可以将数据输出多个目的地destination中。\n来自* \u0026lt;http://blog.csdn.net/a2011480169/article/details/51544664 \u0026gt;\nprivate void test(){ return ; } ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%8E%9F%E7%90%86.html","summary":"[toc] 1. flume是什么？ flume是分布式的，可靠的，高可用的，用于对不同来源的大量的日志数据进行有效收集、聚集和移动，并以集中式的数据存储的","title":"介绍与原理"},{"content":"[toc]\n查询默认垃圾收集器 java -XX:+PrintCommandLineFlags -version\n结果如下:\n-XX:InitialHeapSize=257798976 -XX:MaxHeapSize=4124783616 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC -XX:+PrintCommandLineFlags java version \u0026#34;1.8.0_211\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 可以看到 jdk1.8 使用的ParallelGC , Parallel 垃圾收集器，即 PS Scavenge(Parallel Scavenge) 和 PS MarkSweep( Parallel Old)。\n注意这里的 + 号，有 +/- 号表示启用/关闭，没有的表示是配置属性。\nJDK8 及以前使用的是Parallel 垃圾收集器，jdk9 到 jdk16使用的都是 G1 收集器。\n虽然推出了 ZGC, 但 可能是由于稳定性考虑没有马上更换\nCMS默认情况下没有用在任何版本的回收器, CMS 使用标记清除算法，如果用作默认确实不怎么合适。\n【Java】JVM - 各版本默认垃圾收集器 - 掘金 (juejin.cn) 【Java】JVM - 垃圾收集器 - 掘金 (juejin.cn) 彻夜研究了2天终于知道 JDK8 默认的GC 收集器了 - 掘金 (juejin.cn) ZGC 与CMS中的ParNew和G1类似，ZGC也采用标记-复制算法，\n不过ZGC对该算法做了重大改进：ZGC在标记、转移和重定位阶段几乎都是并发的，这是ZGC实现停顿时间小于10ms目标的最关键原因。\nZGC通过染色指针和读屏障技术，解决了转移过程中准确访问对象的问题，实现了并发转移\n新一代垃圾回收器ZGC的探索与实践 - 美团技术团队 (meituan.com) CMS CMS存在的问题 使用的标记-清除算法，可能存在大量空间碎片。 并发清理, 内存不够会出现Concurrent Mode Failure失败而导致垃圾收集器降级为Serial Old, 产生STW 对CPU资源非常敏感。在并发阶段，会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS默认启动的回收线程数是（CPU数量+3）/4 CMS收集器的工作流程(步骤)是什么样的? 初始标记: 只标记和GC Roots能直连的对象,速度快,会发生(stop the world) 并发标记: 和应用线程并发执行,遍历初始标记阶段标记过的对象,标记这些对象的可达对象。 重新标记: 由于并发标记是和应用线程是并发执行的,所以有些标记过的对象发生了变化。这个过程比初始标记用时长,但是比并发标记阶段用时短。会发生(stop the world) 并发清除: 和应用线程一起运行。基于标记对象,直接清理对象。 总结CMS常见面试题_入门小站的博客-CSDN博客_cms面试题 关于CMS收集器的问题？ - 知乎 (zhihu.com) JVM 面试必问的 CMS，你懂了吗？_web13618542420的博客-CSDN博客 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8.html","summary":"[toc] 查询默认垃圾收集器 java -XX:+PrintCommandLineFlags -version 结果如下: -XX:InitialHeapSize=257798976 -XX:MaxHeapSize=4124783616 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC -XX:+PrintCommandLineFlags java version \u0026#34;1.8.0_211\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 可以看到 jdk1.8 使用的ParallelGC , Parallel 垃圾收集器，即","title":"垃圾回收器"},{"content":"[toc]\n1. 什么是类的加载机制 ​\t类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。\n类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误\n加载.class文件的方式\n从本地系统中直接加载 通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将Java源文件动态编译为.class文件 2. 类的生命周期 其中类加载的过程包括了加载、验证、准备、解析、初始化五个阶段。在这五个阶段中，加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持Java语言的运行时绑定（也成为动态绑定或晚期绑定）。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。\n2.1 加载：查找并加载类的二进制数据 加载时类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情：\n1、通过一个类的全限定名来获取其定义的二进制字节流。\n2、将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。\n3、在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。\n相对于类加载的其他阶段而言，加载阶段（准确地说，是加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。\n2.2 连接 验证：确保被加载的类的正确性 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作：\n文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。\n元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。\n字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。\n符号引用验证：确保解析动作能正确执行。\n验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。\n准备：为类的静态变量分配内存，并将其初始化为默认值\n准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：\n(1)、这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。\n(2)、这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。\n假设一个类变量的定义为：public static int value = 3；\n那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器（）方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。\n· 这里还需要注意如下几点：\n对基本数据类型来说，对于类变量（static）和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。\n详见地址 ​\t(3)、如果类字段的字段属性表中存在ConstantValue属性，即同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值。\n假设上面的类变量value被定义为： public static final int value = 3；\n编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为3。回忆上一篇博文中对象被动引用的第2个例子，便是这种情况。我们可以理解为static final常量在编译期就将其结果放入了调用它的类的常量池中\n解析：把类中的符号引用转换为直接引用 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。\n符号引用: 就是一组符号来描述目标，可以是任何字面量。\n直接引用: 就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。\n2.3 初始化 ​\t初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式：\n①声明类变量是指定初始值\n②使用静态代码块为类变量指定初始值\nJVM初始化步骤\n假如这个类还没有被加载和连接，则程序先加载并连接该类 假如该类的直接父类还没有被初始化，则先初始化其直接父类 假如类中有初始化语句，则系统依次执行这些初始化语句 类初始化时机：只有当对类的主动使用的时候才会导致类的初始化，类的主动使用包括以下六种：\n创建类的实例，也就是new的方式 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（如Class.forName(“com.shengsiyuan.Test”)） 初始化某个类的子类，则其父类也会被初始化 Java虚拟机启动时被标明为启动类的类（Java Test），直接使用java.exe命令来运行某个主类 2.4 结束生命周期 在如下几种情况下，Java虚拟机将结束生命周期\n执行了System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止 3. 类加载器 类加载器可以大致划分为以下三类：\n启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\\jre\\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库（如rt.jar，所有的java.*开头的类均被Bootstrap ClassLoader加载）。启动类加载器是无法被Java程序直接引用的。\n扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\\jre\\lib目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。\n应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。\n应用程序都是由这三种类加载器互相配合进行加载的，如果有必要，我们还可以加入自定义的类加载器。因为JVM自带的ClassLoader只是懂得从本地文件系统加载标准的java class文件，因此如果编写了自己的ClassLoader，便可以做到如下几点：\n1）在执行非置信代码之前，自动验证数字签名。\n2）动态地创建符合用户特定需要的定制化构建类。\n3）从特定的场所取得java class，例如数据库中和网络中。\n启动类加载器 它使用C++实现（这里仅限于Hotspot，也就是JDK1.5之后默认的虚拟机，有很多其他的虚拟机是用Java语言实现的），是虚拟机自身的一部分；\n其他类加载器都由Java语言实现，独立于虚拟机之外，并且全部继承自抽象类java.lang.ClassLoader，这些类加载器需要由启动类加载器加载到内存中之后才能去加载其他的类。\n这几种类加载器的层次关系如下图所示\n注意：这里父类加载器并不是通过继承关系来实现的，而是采用组合实现的\nJVM类加载机制\n全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 4. 类的加载 类加载有三种方式：\n1、命令行启动应用时候由JVM初始化加载\n2、通过Class.forName()方法动态加载\n3、通过ClassLoader.loadClass()方法动态加载\n5、双亲委派模型 ​\t双亲委派模型的工作流程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。\n5.1 为什么需要双亲委派 对于任意一个类，都需要由加载它的类加载器和这个类本身来一同确立其在Java虚拟机中的唯一性。\n判断一个类是否相同，通常用equals()方法，isInstance()方法和isAssignableFrom()方法。来判断，对于同一个类，如果没有采用相同的类加载器来加载，在调用的时候，会产生意想不到的结果\nspring Boot项目,加入热部署依赖后,就会出现这种问题,明明是同一个类,却报错java.lang.ClassCastException,因为热部署是通过自定义的类加载器的方式,动态的刷新内存做到的\n详见地址 5.2 双亲委派机制 1、当AppClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。\n2、当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。\n3、如果BootStrapClassLoader加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），会使用ExtClassLoader来尝试加载；\n4、若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。\n双亲委派模型意义：\n系统类防止内存中出现多份同样的字节码\n保证Java程序安全稳定运行\nClassLoader源码分析：\npublic Class\u0026lt;?\u0026gt; loadClass(String name)throws ClassNotFoundException { return loadClass(name, false); } protected synchronized Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve)throws ClassNotFoundException { // 首先判断该类型是否已经被加载 Class c = findLoadedClass(name); if (c == null) { //如果没有被加载，就委托给父类加载或者委派给启动类加载器加载 try { if (parent != null) { //如果存在父类加载器，就委派给父类加载器加载 c = parent.loadClass(name, false); } else { //如果不存在父类加载器，就检查是否是由启动类加载器加载的类，通过调用本地方法native Class findBootstrapClass(String name) c = findBootstrapClass0(name); } } catch (ClassNotFoundException e) { // 如果父类加载器和启动类加载器都不能完成加载任务，才调用自身的加载功能 c = findClass(name); } } if (resolve) { resolveClass(c); } return c; } 5.3 为什么需要破坏双亲委派 ​\t因为在某些情况下父类加载器需要委托子类加载器去加载class文件。受到加载范围的限制，父类加载器无法加载到需要的文件，以Driver接口为例，由于Driver接口定义在jdk当中的，而其实现由各个数据库的服务商来提供，比如mysql的就写了MySQL Connector，那么问题就来了，DriverManager（也由jdk提供）要加载各个实现了Driver接口的实现类，然后进行管理，但是DriverManager由启动类加载器加载，只能记载JAVA_HOME的lib下文件，而其实现是由服务商提供的，由系统类加载器加载，这个时候就需要启动类加载器来委托子类来加载Driver实现，从而破坏了双亲委派，这里仅仅是举了破坏双亲委派的其中一个情况。\n详情地址 参考地址:\n类的加载机制 双亲委派机制 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6.html","summary":"[toc] 1. 什么是类的加载机制 ​ 类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个j","title":"类加载机制"},{"content":"MySQL逻辑架构\nMySQL逻辑架构图\nMySQL逻辑架构分四层\n1.连接层：主要完成一些类似连接处理，授权认证及相关的安全方案。\n2.服务层：在 MySQL据库系统处理底层数据之前的所有工作都是在这一层完成的，包括权限判断，SQL接口，SQL解析，SQL分析优化， 缓存查询的处理以及部分内置函数执行(如日期,时间,数学运算,加密)等等。各个存储引擎提供的功能都集中在这一层，如存储过程，触发器，视图等。\n3.引擎层：是底层数据存取操作实现部分，由多种存储引擎共同组成。真正负责MySQL中数据的存储和提取。就像Linux众多的文件系统 一样。每个存储引擎都有自己的优点和缺陷。服务器是通过存储引擎API来与它们交互的。这个接口隐藏 了各个存储引擎不同的地方。对于查询层尽可能的透明。这个API包含了很多底层的操作。如开始一个事物，或者取出有特定主键的行。存储引擎不能解析SQL，互相之间也不能通信。仅仅是简单的响应服务器 的请求。\n4.存储层：将数据存储于裸设备的文件系统之上，完成与存储引擎的交互。\nhttps://www.cnblogs.com/ChangAn223/p/10686639.html https://www.bilibili.com/video/BV12b411K7Zu?p=186 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84%E5%9B%BE.html","summary":"MySQL逻辑架构 MySQL逻辑架构图 MySQL逻辑架构分四层 1.连接层：主要完成一些类似连接处理，授权认证及相关的安全方案。 2.服务层：在","title":"逻辑架构图"},{"content":"[toc]\n用法一：常量 public enum Color { RED, GREEN, BLANK, YELLOW } 也可以写构造方法,给枚举加值\npublic enum Color { RED(\u0026#34;红色\u0026#34;, 1), GREEN(\u0026#34;绿色\u0026#34;, 2), BLANK(\u0026#34;白色\u0026#34;, 3), YELLO(\u0026#34;黄色\u0026#34;, 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) { this.name = name; this.index = index; } // 普通方法 , 配个switch使用更佳 public static String getName(int index) { for (Color c : Color.values()) { if (c.getIndex() == index) { return c.name; } } return null; } // get set 方法 public String getName() { return name; } public void setName(String name) { this.name = name; } public int getIndex() { return index; } public void setIndex(int index) { this.index = index; } } 用法二：switch public class TrafficLight { Signal color = getName(\u0026#34;2\u0026#34;); public void change() { switch (color) { case RED: color = Signal.GREEN; break; case YELLOW: color = Signal.RED; break; case GREEN: color = Signal.YELLOW; break; } } } 用法三：实现接口 所有的枚举都继承自java.lang.Enum类。由于Java 不支持多继承，所以枚举对象不能再继承其他类。 可以把基础枚举定义成接口,其他枚举的就实现这个基础枚举(感觉略微有点鸡肋)\npublic interface Behaviour { void print(); String getInfo(); } public enum Color implements Behaviour{ RED(\u0026#34;红色\u0026#34;, 1), GREEN(\u0026#34;绿色\u0026#34;, 2), BLANK(\u0026#34;白色\u0026#34;, 3), YELLO(\u0026#34;黄色\u0026#34;, 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) { this.name = name; this.index = index; } //接口方法 @Override public String getInfo() { return this.name; } //接口方法 @Override public void print() { System.out.println(this.index+\u0026#34;:\u0026#34;+this.name); } } 用法四：使用接口组织枚举 public interface Food { enum Coffee implements Food{ BLACK_COFFEE,DECAF_COFFEE,LATTE,CAPPUCCINO } enum Dessert implements Food{ FRUIT, CAKE, GELATO } } @Test void test(){ Food food = Food.DessertEnum.CAKE; System.out.println(food); food = CoffeeEnum.BLACK_COFFEE; System.out.println(food); } 在使用上,里面的枚举也可以不用实现接口,只是这样返回值就不是接口类型而是枚举类型. 实现了接口,就能让所有的枚举返回同一个类型,方便管理 (这种方式下,怎么优雅的给枚举写msg呢? 不然还是不好用)\n来自: https://blog.csdn.net/qq_27093465/article/details/52180865 https://www.jianshu.com/p/46dbd930f6a2 https://blog.csdn.net/u013276277/article/details/80766808 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%9E%9A%E4%B8%BE.html","summary":"[toc] 用法一：常量 public enum Color { RED, GREEN, BLANK, YELLOW } 也可以写构造方法,给枚举加值 public enum Color { RED(\u0026#34;红色\u0026#34;, 1), GREEN(\u0026#34;绿色\u0026#34;","title":"枚举"},{"content":"[TOC]\nBeanFactory和ApplicationContext有什么区别？ Bean工厂(BeanFactory)是Spring框架最核心的接口，提供了高级Ioc的配置机制．\n应用上下文(ApplicationContext)建立在BeanFacotry基础之上，提供了更多面向应用的功能，如果国际化，属性编辑器，事件等等．\nbeanFactory是spring框架的基础设施，是面向spring本身，ApplicationContext是面向使用Spring框架的开发者，几乎所有场合都会用到ApplicationContext.\nhttps://www.jianshu.com/p/b82ceb084adf bean加载过程 根据bean名称 查询缓存中是否已存在(因为是单例)\ncontext.getBean(\u0026quot;person\u0026quot;, Person.class);\n确认该bean在工厂中是否已定义,解决循环依赖问题,有定义则完善该bean并返回\n如果没有就会创建个新的,并放到bean池中\n根据配置文件(xml.yaml)给bean设置属性(处理循环依赖问题(仅仅是单例的情况下)) 初始化bean,如果有自定义的bean初始化方法,则会在这里执行 https://segmentfault.com/a/1190000012887776#item-4-3 .\nhttps://www.jianshu.com/p/b82ceb084adf Spring 框架中都用到了哪些设计模式 代理模式—在AOP和remoting中被用的比较多。\n单例模式—在spring配置文件中定义的bean默认为单例模式。\n模板方法—用来解决代码重复的问题 比如. RestTemplate, JmsTemplate, JpaTemplate。 前端控制器—Srping提供了DispatcherServlet来对请求进行分发。 视图帮助(View Helper )—Spring提供了一系列的JSP标签，高效宏来辅助将分散的代码整合在视图里。 依赖注入—贯穿于BeanFactory / ApplicationContext接口的核心理念。\n工厂模式—BeanFactory用来创建对象的实例。\nBuilder模式- 自定义配置文件的解析bean是时采用builder模式，一步一步地构建一个beanDefinition\n策略模式：Spring 中策略模式使用有多个地方，如 Bean 定义对象的创建以及代理对象的创建等。这里主要看一下代理对象创建的策略模式的实现。 前面已经了解 Spring 的代理方式有两个 Jdk 动态代理和 CGLIB 代理。这两个代理方式的使用正是使用了策略模式。\nSpring框架中单例beans是线程安全的吗？ 不是，Spring框架中的单例beans不是线程安全的。\nspring bean的生命周期 Spring启动，查找并加载需要被Spring管理的bean，进行Bean的实例化 Bean实例化后对将Bean的引入和值注入到Bean的属性中 如果Bean实现了BeanNameAware接口的话，Spring将Bean的Id传递给setBeanName()方法 如果Bean实现了BeanFactoryAware接口的话，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入 如果Bean实现了ApplicationContextAware接口的话，Spring将调用Bean的setApplicationContext()方法，将bean所在应用上下文引用传入进来 如果Bean实现了BeanPostProcessor接口，Spring就将调用他们的postProcessBeforeInitialization()方法。Spring 的 AOP 就是利用它实现的。 如果Bean 实现了InitializingBean接口，Spring将调用他们的afterPropertiesSet()方法。类似的，如果bean使用init-method声明了初始化方法，该方法也会被调用 如果Bean 实现了BeanPostProcessor接口，Spring就将调用他们的postProcessAfterInitialization()方法。 此时，Bean已经准备就绪，可以被应用程序使用了。他们将一直驻留在应用上下文中，直到应用上下文被销毁。 如果bean实现了DisposableBean接口，Spring将调用它的destory()接口方法，同样，如果bean使用了destory-method 声明销毁方法，该方法也会被调用。 简版:\nBean 的实例化 Bean 属性赋值 Bean 的初始化 Bean 的使用 Bean 的销毁 java对象创建实例化和初始化的区别\n实例化：在堆中申请内存空间，属性都是默认值\n初始化：给对象的属性进行赋值操作或者初始化方法的调用\nSpring Bean生命周期 (biancheng.net) 面试官：请你说一下 Bean 的生命周期 - 知乎 (zhihu.com) java类的初始化和实例化区别 - pu20065226 - 博客园 (cnblogs.com) Spring 中3种依赖注入的方式： 基于 field 注入（属性注入）\n基于 setter 注入\n基于 constructor 注入（构造器注入）\nspring 新版本推荐使用构造器注入, 如下\nprivate final Svc svc; public HelpService( Svc svc) { this.svc = svc; } 可以使用 lombok的 @RequiredArgsConstructor 或者 @AllArgsConstructor 来自动生成构造方法, 虽然这样做也容易 违反单一责任原则\n@RequiredArgsConstructor // @AllArgsConstructor public class MdmProjectController { private final MdmProjectService mdmProjectService; } field 注入，就是在bean的变量上使用注解进行依赖注入。本质上是通过反射的方式直接注入到field。这是我平常开发中看的最多也是最熟悉的一种方式，同时，也正是 Spring 团队所不推荐的方式。\n源码位置: org/springframework/beans/factory/annotation/AutowiredAnnotationBeanPostProcessor.java:621 5.1.7.RELEASE\n@Override protected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable { //获取要注入的字段 Field field = (Field) this.member; Object value; //如果字段的值有缓存 if (this.cached) { //从缓存中获取字段值value value = resolvedCachedArgument(beanName, this.cachedFieldValue); } //没有缓存 else { //创建一个字段依赖描述符 DependencyDescriptor desc = new DependencyDescriptor(field, this.required); desc.setContainingClass(bean.getClass()); Set\u0026lt;String\u0026gt; autowiredBeanNames = new LinkedHashSet\u0026lt;\u0026gt;(1); Assert.state(beanFactory != null, \u0026#34;No BeanFactory available\u0026#34;); //获取容器中的类型转换器 TypeConverter typeConverter = beanFactory.getTypeConverter(); try { //核心！获取注入的值 value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter); } catch (BeansException ex) { throw new UnsatisfiedDependencyException(null, beanName, new InjectionPoint(field), ex); } // .... 省略代码 if (value != null) { //显式使用JDK的反射机制，设置自动的访问控制权限为允许访问 ReflectionUtils.makeAccessible(field); //为字段赋值 field.set(bean, value); } } 为什么不推荐使用Field注入?\n违反单一责任原则 添加新的依赖项非常容易。添加6个、10个甚至12个依赖项没有问题。当使用构造函数注入时，在某一点之后，构造函数参数的数量会变得过高，并且很明显会出现问题。依赖太多通常意味着类有太多的责任。这可能违反了单一职责原则和关注点分离，这表明类需要进一步的检查和重构。当直接注入字段时，没有这样的警告，因为这种方法可以无限扩展。\n依赖隐藏 使用依赖注入容器意味着类不再负责管理自己的依赖项。获取依赖项的职责是从类中提取的。由其他人现在负责提供依赖项——依赖注入容器或在测试中手动分配它们。当类不再负责获取其依赖项时，它应该使用公共接口(方法或构造函数)清楚地与它们通信。\n就是说 类的成员变量(依赖项)被\u0026quot;隐藏\u0026quot;了, 按照 控制翻转 的思想, 成员变量的赋值应该在初始化类时(显示)处理 , 但是由于 field注入是反射做的, 就不是类的主动操作了,\n依赖注入容器耦合 DI框架的核心思想之一是托管类不应该依赖于所使用的DI容器。换句话说，它应该只是一个普通的POJO，可以独立地实例化它，前提是将所有必需的依赖项传递给它。通过这种方式，您可以在单元测试中实例化它，而不需要启动DI容器，并单独测试它(使用的容器更像是集成测试)。如果没有容器耦合，则可以将该类作为托管或非托管类使用，甚至可以切换到新的DI框架。\n就是说, 要用bean应该从容器中取, 不能自己额外去取, 反射操作就是自己去取了\n不变性 与构造函数不同，Field注入不能用于将依赖项分配给最终字段。\n会造成循环依赖\n2, 3, 4,5 几点都是由于 field方式是反射做的, 才造成了这种情况, 使用Field 注入不会有功能上的问题, 只是违背了spring的设计思想\nSpring注解@Autowired源码分析 - 腾讯云开发者社区-腾讯云 (tencent.com) 关于Spring注入方式的几道面试题，你能答上么？ - 知乎 (zhihu.com) IDEA 提示 Field injection is not recommended_Ongoing蜗牛的博客-CSDN博客 科普文: 大白话讲解Spring的@bean注解 - 知乎 (zhihu.com) 一类注解是用于注册Bean: @Component , @Repository , @ Controller , @Service , @Configration, @bean 等等\n一类注解是用于使用Bean @Autowired , @Resource 等等\n本题中的注入方式, 指的是 bean注入的某个类中, 而 @service 等注解是注入的容器中\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E9%9D%A2%E8%AF%95.html","summary":"[TOC] BeanFactory和ApplicationContext有什么区别？ Bean工厂(BeanFactory)是Spring框架最核心的接","title":"面试"},{"content":"[toc]\nspringboot 启动过程 启动的流程主要分为两大阶段：\n初始化SpringApplication运行SpringApplication (在这里就扫描了所有的spring.factories,并缓存在内存中) 运行SpringApplication的过程 其中运行SpringApplication的过程又可以细分为以下几个部分：\n1）SpringApplicationRunListeners 引用启动监控模块,\n2）ConfigrableEnvironment配置环境模块和监听：包括创建配置环境、加载属性配置文件和配置监听\n3）ConfigrableApplicationContext配置应用上下文：包括配置应用上下文对象、配置基本属性和刷新应用上下文, 其中最核心代码refreshContext(context) 刷新上下文, 将通过工程模式产生应用上下文中所需的bean。实现spring-boot-starter-*(mybatis、redis等)自动化配置的关键，包括spring.factories的加载、bean的实例化等核心工作\n详解面试官经常问的SpringBoot启动流程机制 - 云+社区 - 腾讯云 (tencent.com) 高级面试题\u0026ndash;SpringBoot启动流程解析_hfmbook的博客-CSDN博客_springboot启动流程面试 @SpringBootApplication 注解 @SpringBootApplication是一个复合注解，包括@ComponentScan，和@SpringBootConfiguration，@EnableAutoConfiguration。\n@SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到spring容器中，并且实例名就是方法名。 @EnableAutoConfiguration的作用启动自动的配置，@EnableAutoConfiguration注解的意思就是Springboot根据你添加的jar包来配置你项目的默认配置，比如根据spring-boot-starter-web ，来判断你的项目是否需要添加了webmvc和tomcat，就会自动的帮你配置web项目中所需要的默认配置。 @ComponentScan，扫描当前包及其子包下被@Component，@Controller，@Service，@Repository注解标记的类并纳入到spring容器中进行管理。是以前的\u0026lt;context:component-scan\u0026gt;（以前使用在xml中使用的标签，用来扫描包配置的平行支持）。 所以SpringBootApplication做了三件事, 能够识别并加载@bean的实例 / 开启自动去读配置(自己业务加的那些配置) / 扫描各种类对象并加载\n@SpringBootApplication注解分析 - duanxz - 博客园 (cnblogs.com) 自动装配机制 1、main方法中SpringApplication.run(HelloBoot.class,args)的执行流程中有refreshContext(context)。\n2、而这个refreshContext(context)内部会解析，配置类上自动装配功能的注解@EnableAutoConfiguration中的，@EnableAutoConfiguration中的，使用@Import引入类AutoConfigurationImportSelector。\n3、AutoConfigurationImportSelector这个类中的方法SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()会读取jar包中的/项目中的META-INF/spring.factories文件。\n4、spring.factories配置了自动装配的类，最后根据配置类的条件，自动装配Bean。\nSpringBoot自动装配原理 - 简书 (jianshu.com) （Spring Boot的自动装配原理及流程）_一碗谦谦粉的博客-CSDN博客_springboot自动装配面试 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E9%9D%A2%E8%AF%95.html","summary":"[toc] springboot 启动过程 启动的流程主要分为两大阶段： 初始化SpringApplication运行SpringApplication (在这里就扫描了所有的","title":"面试"},{"content":"[toc]\n在做分布式集群时候一般会产生什么问题？ 并发性。加锁解决。\n缺乏全局时钟。\n通信异常。引起数据丢失或者接收数据延迟的问题。\n网络分区，也叫脑裂。\n三态。成功和失败以外的第三种状态，叫超时态。\n节点故障。节点越多，发生故障的几率越大。\ncap不能同时满足。\n一致性问题。\n资源倾斜问题。忙的忙死，闲的闲死。\n扩容和缩容问题。\n分布式幂等性问题。\nsession共享问题。\n分布式全局生成id问题。\n来自: https://www.yuque.com/fudadajiagoushimeiriyiti/rqui93/ecylig 度量指标 QPS\nQuery Per Second: 每秒响应请求数，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准, 即每秒的响应请求数，也即是最大吞吐能力。\nTPS\nTransactions Per Second，也就是事务数/秒。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。\nQPS基本类似于TPS，但是不同的是，对于一个页面的一次访问，形成一个TPS；但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“QPS”之中。\n并发用户数\n并发用户数：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。\nRT\nresponse Time：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。一般而言，用户感知友好的高并发系统，时延应该控制在250毫秒以内。\nPV\nPV（Page View）：页面访问量，即页面浏览量或点击量，用户每次刷新即被计算一次。可以统计服务一天的访问日志得到。\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%9D%A2%E8%AF%95.html","summary":"[toc] 在做分布式集群时候一般会产生什么问题？ 并发性。加锁解决。 缺乏全局时钟。 通信异常。引起数据丢失或者接收数据延迟的问题。 网络分区，也叫脑裂。 三","title":"面试"},{"content":"[TOC]\ncpu 100% 怎么排查? 找到最耗CPU的进程: top命令查看进程运行信息列表,键入P (大P)，进程按照CPU使用率从高到低排序。 找到最耗CPU的线程：top -Hp 进程PID命令查看该进程下所有的线程,键入P (大P)，进程按照CPU使用率从高到低排序。 将线程PID转化为16进制:printf “%X ” 线程PID。是因为堆栈里，线程id是用16进制表示的(十进制6524转换为十六进制就是197c)。 查看堆栈，找到线程: jstack 进程PID |grep 线程PID转换后的16进制 -C10(显示匹配行前后各10行) \u0026ndash;color。例如:jstack 6505 |grep 0x197c -C10 \u0026ndash;color Linux-cpu100%排查_听风的小男孩的博客-CSDN博客_cpu排查 linux CPU 100% 异常排查实践与总结 - leejun2005的个人页面 - OSCHINA - 中文开源技术交流社区 ","permalink":"https://xiaokunji.com/zh/linux%E7%B3%BB%E7%BB%9F/%E9%9D%A2%E8%AF%95.html","summary":"[TOC] cpu 100% 怎么排查? 找到最耗CPU的进程: top命令查看进程运行信息列表,键入P (大P)，进程按照CPU使用率从高到低排序。 找到最耗CPU的线程","title":"面试"},{"content":"[toc]\n1. sql优化 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。\n应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：\nselect id from t where num is null 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：\nselect id from t where num=0 应尽量避免在 where 子句中使用!=或\u0026lt;\u0026gt;操作符，否则将引擎放弃使用索引而进行全表扫描。\n应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描(如果or的每个条件都有索引,则or会走索引)，如：\nselect id from t where num=10 or num=20 可以这样查询：\nselect id from t where num=10 union all select id from t where num=20 in 和 not in 也要慎用，否则会导致全表扫描，如：\nselect id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了：\nselect id from t where num between 1 and 3 全模糊查询也将导致全表扫描(不使用左模糊会走索引)：\nselect id from t where name like '%abc%' 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：\nselect id from t where num/2=100 应改为:\nselect id from t where num=100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：\nselect id from t where substring(name,1,3)='abc'--name以abc开头的id 应改为:\nselect id from t where name like 'abc%' 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。\n在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。\n不要写一些没有意义的查询，如需要生成一个空表结构：\nselect col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：\ncreate table #t(...) 很多时候用 exists 代替 in 是一个好的选择：\nselect num from a where num in(select num from b) 用下面的语句替换：\nselect num from a where exists(select 1 from b where num=a.num) https://blog.csdn.net/qq_38789941/article/details/83744271 2. 数据库优化 并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。\n索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。\n一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。\n尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。\n这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。\n尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。\n任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。\n避免频繁创建和删除临时表，以减少系统表资源的消耗。\n临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。\n在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，\n以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。\n如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。\n尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。\n使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。\n与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。 在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。\n尽量避免大事务操作，提高系统并发能力。\n尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。\nhttps://blog.csdn.net/qq_38789941/article/details/83744271 连表的字段编码集应该一样 , 不一致会导致索引失效, 因为做了隐式转换 mysql的utf8 不是完整的utf-8, 是压缩版的, utf8只支持每个字符最多三个字节，而真正的 UTF-8 是每个字符最多四个字节。utf8mb4才是真正的ytf-8, 所以编码集应该统一用这个, 编码集还涉及到 排序规则 , 它表示各个图形字符之间的大小比较规则，比如：是否区分大小写，区分全角和半角等。\nhttps://blog.csdn.net/wzngzaixiaomantou/article/details/119720356 https://blog.csdn.net/CSDN_WYL2016/article/details/119881723 https://www.infoq.cn/article/in-mysql-never-use-utf8-use-utf8 https://blog.csdn.net/u010476739/article/details/118653156 3. 悲观锁和乐观锁 乐观锁：\n乐观锁不是数据库自带的，需要我们自己去实现。乐观锁是指操作数据库时（更新操作），想法很乐观，认为这次的操作不会导致冲突，在操作数据室，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了。\n通常是这样实现的：在表中的数据进行操作时（更新），会给数据表中加一个版本（version）字段，每操作一次将那条记录的版本号加1。也就是先查询出那条记录，获取出version字段，如果涛对那条记录进行操作（更新），则先判断此刻version的值是否与刚刚查出来的version的值相等，如果相等，则说明这段期间，没有其他程序对齐进行操作，则可以执行更新，将version字段的值加1；如果更新时发现此刻的version值与刚刚获取出来的verison的值不相等，则说明这段期间已经有其他程序对齐进行操作了，则不进行更新操作。\n例子：\n1.查询出商品信息\nselect * from goods where id=#{id} 2.根据商品信息生成订单\n3.修改商品status为2\nupdate goods set status = 2,version = version +1 where id=#{id}\n悲观锁：\n与乐观所相对应的就是悲观锁。悲观锁就是在操作数据时，认为此操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作，这单跟java中的synchronized很相似，所以悲观锁需要耗费较多的时间。另外与乐观锁相对应的，悲观锁是由数据库自己实现的，要用的时候，我们直接调用数据库的相关语句就可以 了。\n由悲观锁涉及到的两个锁概念：共享锁与排他锁。\n4. 共享锁与排他锁 共享锁：\n共享锁指的就是对于多个不同的事务，对同一个资源共享同一个锁。相当于对于同一把门，它拥有多个钥匙一样。对于悲观锁，一般数据库已经实现了，共享锁也属于悲观锁的一种，共享锁，也叫读锁。就是我们对数据进行读取操作的时候，其实是不会改变数据的值的。\n所以我们可以给数据库增加读锁，获得读锁的事务就可以读取数据了。当数据库已经被别人增加了读锁的时候，其他新来的事务也可以读数据，但是不能写。\n也就是说，如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。\n用法:\n在查询语句后面增加LOCK IN SHARE MODE，Mysql会对查询结果中的每行都加共享锁。\nSELECT ... LOCK IN SHARE MODE; 当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。\n排它锁：\n排它锁与共享锁相对应，就是指对于多个不同的事务，对同一个资源只能有一把锁。 排他锁，也叫写锁。就是我们对数据进行写操作的时候，要先获得写锁，获得写锁的事务既可以写数据也可以读数据。但是，如果数据库已经被别人增加了排他写锁，那么后面的事务是无法在获得该数据库的任何锁的。\n也就是说，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。\n用法 在查询语句后面增加FOR UPDATE，Mysql会对查询结果中的每行都加排他锁\nSELECT ... FOR UPDATE; 当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。\n来自: https://www.cnblogs.com/fanghl/p/11316540.html 5. 索引的目的是什么？ 快速访问数据表中的特定信息，提高检索速度 创建唯一性索引，保证数据库表中每一行数据的唯一性。 加速表和表之间的连接 使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间 6. 索引对数据库系统的负面影响是什么？ 创建索引和维护索引需要耗费时间，这个时间随着数据量的增加而增加； 索引需要占用物理空间，不光是表需要占用数据空间，每个索引也需要占用物理空间； 当对表进行增、删、改、的时候索引也要动态维护，这样就降低了数据的维护速度。 7. 为数据表建立索引的原则有哪些？ 在最频繁使用的、用以缩小查询范围的字段上建立索引。 在频繁使用的、需要排序的字段上建立索引 对于查询中很少涉及的列或者重复值比较多的列，不宜建立索引。 对于一些特殊的数据类型，不宜建立索引，比如文本字段（text）等 原文链接：https://blog.csdn.net/qq_34988624/article/details/85838850\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%9D%A2%E8%AF%95.html","summary":"[toc] 1. sql优化 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致","title":"面试"},{"content":"[toc]\nRedis的单线程指什么 Redis 单线程主要指的是网络IO和键值对读写是由一个线程来完成的，Redis在处理客户端的请求的时候包括获取(Socket)，解析，执行，内容返回(Socket写)等由一个顺序串行的主线程处理，这即为单线程\n但Redis的其它功能，比如持久化，异步删除，集群数据同步等等，都是由额外的线程执行的，是多线程\n即Redis工作线程是单线程的，但是，整个Redis来说是多线程的\nRedis单线程为什么很快 1、基于内存操作：Redis的所有数据都存在内存中，因此所有的运算都是内存级别\n2、数据结构简单：Redis的数据结构简单，查找和操作时间大部分时间复杂度是O(1)\n3、多路复用和非阻塞：Redis使用多路复用功能来监听多个Socket链接客服端，这样可以一个线程处理多个请求，减少线程切换带来的开销，同时也避免了IO阻塞操作\n4、避免上下文切换：因为是单线程模型，因此避免了多线程切换和多线程竞争，减少时间和性能的消耗\nRedis系列第一讲：Redis是单线程吗 - 掘金 (juejin.cn) 谈谈redis的对象机制（redisObject) Redis的5种基础数据类型，在底层是采用对象机制实现的。\nRedis的每种对象其实都由对象结构(redisObject) 与 对应编码的数据结构组合而成，而每种对象类型对应若干编码方式，不同的编码方式所对应的底层数据结构是不同的。\nredisObject 是 Redis 类型系统的核心, 数据库中的每个键、值, 以及 Redis 本身处理的参数, 都表示为这种数据类型。\nRedis相关知识\u0026mdash;-对象机制_小舟~的博客-CSDN博客 redis 优化措施 避免大key的存在\n设置合理的淘汰策略\n内存尽可能大; 淘汰策略改为随机淘汰，随机淘汰比 LRU 要快很多（视业务情况调整）\n谨慎fork操作带来的阻塞\n控制 Redis 实例的内存：尽量在 10G 以下，执行 fork 的耗时与实例大小有关，实例越大，耗时越久\n关闭操作系统的内存大页\n应用在申请内存时,linux会给一个完整的内存页大小, 申请的多了, 分配内存也就大了 , 耗时也就长了\n对于 Redis 这种对性能和延迟极其敏感的数据库来说，我们希望 Redis 在每次申请内存时，耗时尽量短\n开启AOF后设置 重写时不刷盘\n# AOF rewrite 期间，AOF 后台子线程不进行刷盘操作 # 相当于在这期间，临时把 appendfsync 设置为了 none no-appendfsync-on-rewrite yes 建议使用 固态硬盘, 加快数据落磁盘\n关闭swap 内存\n操作系统为了缓解内存不足对应用程序的影响，允许把一部分内存中的数据换到磁盘上，以达到应用程序对内存使用的缓冲，这些内存数据被换到磁盘上的区域，就是 Swap。\nRedis进阶 - 性能调优：Redis性能调优详解 | Java 全栈知识体系 (pdai.tech) 热key的处理 利用二级缓存\n将热key加载到jvm中, 利用本地缓存抗住\n备份热key\n将热key多备份几份,用后缀区分, 这样请求就会分发到不同的key上了\n检测热key\n经验预估, 比如秒杀key 客户端收集并统计 redis 自带的命令 , monitor和hotkeys 但都有性能问题 【原创】谈谈redis的热key问题如何解决 - 孤独烟 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/%E9%9D%A2%E8%AF%95.html","summary":"[toc] Redis的单线程指什么 Redis 单线程主要指的是网络IO和键值对读写是由一个线程来完成的，Redis在处理客户端的请求的时候包括获取(Socke","title":"面试"},{"content":"[toc]\nRocketMQ如何保证消息不丢失？ Producer端如何保证消息不丢失 采取send()「同步发消息」，发送结果是同步感知的。 发送失败后可以**「重试」**，设置重试次数。默认3次。 producer.setRetryTimesWhenSendFailed(10); 「集群部署」，比如发送失败了的原因可能是当前Broker宕机了，重试的时候会发送到其他Broker上。 Broker端如何保证消息不丢失 修改刷盘策略为同步刷盘。默认情况下是异步刷盘的。 flushDiskType = SYNC_FLUSH 集群部署，主从模式，高可用。 Consumer端如何保证消息不丢失 完全消费正常后在进行手动ack确认。 RocketMQ如果订阅关系不一致会怎样？ 订阅关系一致指的是同一个消费者Group ID下所有Consumer实例所订阅的Topic、Tag必须完全一致。如果订阅关系不一致，消息消费的逻辑就会混乱，甚至导致消息丢失。问题如下:\n第一个问题: 订阅以集群为单位，如果集群中消费者1订阅了topicA， 消费者2订阅topicB，那么会覆盖，二者只有一个成功订阅。\n更新订阅信息时，订阅信息是按照消费组存放的，这步骤就会导致同一个消费组内的各个消费者客户端的订阅信息相互被覆盖。\n第二个问题: 就是集群中会进行分摊，比如说消费者1 订阅了topicA ，来了100个topicA的消息， 也会分50个给消费者2 。\n因为在同一个消费组中, 所以会分给其他消费者, 但是其他消费者又没订阅, 所以会消费失败, 进而消息重试\nkafka就没有这个毛病, 因为kafka在消费架构上简单, 不像RocketMQ一样有 消费组 和 集群/广播消费 的维度\nRocketMQ 可以在同一个消费组中 设置集群消费还是广播消费, (当然组不同的话, 也能实现广播消费)\nkafka只有消费组, 而组里的消费者只能是竞争关系(一个分区只能被一个消费者消费,对应 RocketMQ的集群消费), 想要 广播消费 就用不同的组来消费\n订阅关系一致 (alibabacloud.com) RocketMQ的订阅关系(topic tag和GID) RocketMQ为什么要保证订阅关系的一致性？ - 简书 (jianshu.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/rocketmq/%E9%9D%A2%E8%AF%95.html","summary":"[toc] RocketMQ如何保证消息不丢失？ Producer端如何保证消息不丢失 采取send()「同步发消息」，发送结果是同步感知的。 发送失败后可","title":"面试"},{"content":"秒杀系统注意点:\n严格防止超卖：库存100件你卖了120件，凉凉 事物性: 用户的金额/优惠券,库存要保持一致 防止黑产：防止不怀好意的人群通过各种技术手段把你本该下发给群众的利益全收入了囊中。 保证用户体验：高并发下，别网页打不开了，支付不成功了，购物车进不去了 抗住并发量,使用CDN节点,减轻前端压力\n尽量把流量控制在上游,因为抢单都是疯狂点击的或者直接爬虫类,这样对系统来说,很多都是重复的请求,徒增系统压力\n对于前端层,界面操作的,前端可以控制下来许多,禁止重点,或者多次点击只发生一次\n对于网关层,对url做去重处理,用一些关键参数(如用户id)做hash,可利用布隆过滤器增加效率; 也可以做页面缓存,对同一个ip段数量多的返回同一个页面; 加上熔断机制,对于严重超额的请求直接拒绝\n对于服务层,请求不直接进入数据层,先经过缓存,确认当前数据,再经过消息队列,一批一批的下放到数据层\n对于数据层, 采用分库分表,对热门区域/热门商品等维度进行拆分\n事物保证, 可以利用redis单线程速度快的特性来控制库存等问题; 可以用消息队列来保证事物的最终一致性\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E7%A7%92%E6%9D%80.html","summary":"秒杀系统注意点: 严格防止超卖：库存100件你卖了120件，凉凉 事物性: 用户的金额/优惠券,库存要保持一致 防止黑产：防止不怀好意的人群通过各种","title":"秒杀"},{"content":"注:Flume框架对hadoop和zookeeper的依赖只是在jar包上，并不要求flume启动时必须将hadoop和zookeeper服务也启动\n来自* \u0026lt;http://www.cnblogs.com/oubo/archive/2012/05/25/2517751.html \u0026gt;\n启动flume服务器,需要用一个服务端的配置文件\nflume-ng agent --conf conf --conf-file /mysoftware/flume-1.7.0/conf/flume-server.properties --name a1 -Dflume.root.logger=INFO,console \u0026gt; /mysoftware/flume-1.7.0/logs/flume-server.log 2\u0026gt;\u0026amp;1 \u0026amp; 启动flume客户端,需要用一个客户端的配置文件\nflume-ng agent --conf conf --conf-file /mysoftware/flume-1.7.0/conf/flume-client.properties --name agent1 -Dflume.root.logger=INFO,console \u0026gt; /mysoftware/flume-1.7.0/logs/flume-client.log 2\u0026gt;\u0026amp;1 \u0026amp; 这里的用到了avro,貌似必须先启动服务器,后启动客户端(这里是从客户端发消息到服务端)\n以下是两个例子:\n注:文件均需自己创建!,两个配置文件\n1. 从网络端口接收,存入HDFS\nflume-server.properties: 服务端 (从avro端口到hdfs)\n#set Agent name a1.sources = r1 a1.channels = c1 a1.sinks = k1 #set channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # other node,nna to nns source a1.sources.r1.type = avro a1.sources.r1.bind = master a1.sources.r1.port = 52020 a1.sources.r1.interceptors = i1 #指定传输中event的head(头信息)，常用timestamp a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Collector a1.sources.r1.interceptors.i1.value = master a1.sources.r1.channels = c1 #set sink to hdfs a1.sinks.k1.type=hdfs a1.sinks.k1.hdfs.path=hdfs://master:9000/flume/logdfs a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=TEXT a1.sinks.k1.hdfs.rollInterval=1 a1.sinks.k1.channel=c1 a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d a1.sinks.k1.hdfs.fileSuffix=.txt flume-client.properties: 客户端 (从命令结果中avro端口)\n#agent1 name agent1.channels = c1 agent1.sources = r1 agent1.sinks = k1 #set gruop #agent1.sinkgroups = g1 #set channel agent1.channels.c1.type = memory agent1.channels.c1.capacity = 1000 agent1.channels.c1.transactionCapacity = 100 #set source agent1.sources.r1.channels = c1 agent1.sources.r1.type = exec #指定源的类型,为一个命令 agent1.sources.r1.command = tail -F /mysoftware/flume-1.7.0/logdfs/flumetest.log agent1.sources.r1.interceptors = i1 i2 agent1.sources.r1.interceptors.i1.type = static agent1.sources.r1.interceptors.i1.key = Type agent1.sources.r1.interceptors.i1.value = LOGIN agent1.sources.r1.interceptors.i2.type = timestamp # set sink1 agent1.sinks.k1.channel = c1 agent1.sinks.k1.type = avro #作为中间传递者,因为它具有很快捷的传输 agent1.sinks.k1.hostname = master agent1.sinks.k1.port = 52020 # set sink2 #(可以有多个sink) #agent1.sinks.k2.channel = c1 #agent1.sinks.k2.type = avro #agent1.sinks.k2.hostname = slave1 #agent1.sinks.k2.port = 52020 #set sink group #agent1.sinkgroups.g1.sinks = k1 k2 #set failover #设置容错 #agent1.sinkgroups.g1.processor.type = failover #load_balance 还有个是负载均衡 #agent1.sinkgroups.g1.processor.priority.k1 = 10 #agent1.sinkgroups.g1.processor.priority.k2 = 1 #agent1.sinkgroups.g1.processor.maxpenalty = 10000 2. 从监听文件中接收,存入Kafka\n从命令结果到kafka(启动kafka消费者来查看结果)\n#agent 客户端 agent.sources = origin agent.channels = memorychannel agent.sinks = target agent.sources.origin.type = exec agent.sources.origin.command = tail -F /home/hadoop/orderslog/app.log agent.sources.origin.channels = memorychannel agent.sources.origin.interceptors = i1 agent.sources.origin.interceptors.i1.type = static agent.sources.origin.interceptors.i1.key = topic agent.sources.origin.interceptors.i1.value = ordersInfo agent.sinks.loggerSink.type = logger agent.sinks.loggerSink.channel = memorychannel agent.channels.memorychannel.type = memory agent.channels.memorychannel.capacity = 10000 agent.sinks.target.type = avro agent.sinks.target.channel = memorychannel agent.sinks.target.hostname =master agent.sinks.target.port = 4545 #collect 服务器 agent.sources = origin agent.channels = memorychannel agent.sinks = target agent.sources.origin.type = avro agent.sources.origin.channels = memorychannel agent.sources.origin.bind = master agent.sources.origin.port = 4545 agent.sinks.loggerSink.type = logger agent.sinks.loggerSink.channel = memorychannel agent.channels.memorychannel.type = memory agent.channels.memorychannel.capacity = 5000000 agent.channels.memorychannel.transactionCapacity = 1000000 agent.sinks.target.type = org.apache.flume.sink.kafka.KafkaSink agent.sinks.target.kafka.topic = ordersInfo #数据的主题(kafka) agent.sinks.target.kafka.bootstrap.servers=master:9092,slave01:9092,slave02:9092,slave03:9092 agent.sinks.target.metadata.broker.list=master:9092,slave01:9092,slave02:9092,slave03:9092 agent.sinks.target.producer.type=sync #agent.sinks.target.kafka.producer.value.serializer=kafka.serializer.DefaultEncoder agent.sinks.target.kafka.producer.acks=1 agent.sinks.target.flumeBatchSize=100 agent.sinks.target.channel = memorychannel ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E5%91%BD%E4%BB%A4.html","summary":"注:Flume框架对hadoop和zookeeper的依赖只是在jar包上，并不要求flume启动时必须将hadoop和zookeeper服","title":"命令"},{"content":" hdfs namenode -format #第一次启动需要格式化namenode\njps 可以查看java进程,以此来确定hadoop是否启动成功\nsbin/start-dfs.sh # 启动hadoop分布式存储进程 启动了: namenode(主节点) SecondaryNameNode(备份节点) datenode(从节点)\nsbin/start-yarn.sh # 启动hadoop资源管理进程\n// (可以使用start-all.sh来启动hdfs和yarn,但是已被抛弃,因为这个命令启动resourceManager有问题)\nmr-jobhistory-daemon.sh start historyserver # 启动历史服务器\nhdfs dfsadmin -safemode leave # 离开安全模式 统计test.txt文件中的单词数\nyarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /test.txt /wc_out\nscp a.txt hadoop@slave01:~/data 将a.txt文件传输到slave01的~/data文件夹下\nscp -r aa hadoop@slave01:~/data 将aa文件夹传输到slave01的~/data文件夹下\nHDFS常用命令 (大多命令基本和linux命令一样) :\n// 全部路径是 绝对路径 hdfs dfs -put *.txt //上传文件到hdfs hdfs dfs -rm /hdfs001.iml // 删除文件,支持正则 hdfs dfs -rm -r /hq // 删除文件夹 https://www.jianshu.com/p/27c1da28c8fb ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E5%91%BD%E4%BB%A4.html","summary":"hdfs namenode -format #第一次启动需要格式化namenode jps 可以查看java进程,以此来确定hadoop是否启动成功 sbin/start-dfs.sh # 启动hadoop分布式存储进程 启动","title":"命令"},{"content":"[toc]\n1. 大纲 启动hbase服务: start-hbase.sh\n停止hbase服务: stop-hbase.sh\n启动shelll:\thbase shell\nhbase shell命令 描述 alter 修改列族（column family）模式 count 统计表中行的数量 create 创建表 describe 显示表相关的详细信息 delete 删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值） deleteall 删除指定行的所有元素值 disable 使表无效 drop 删除表 enable 使表有效 exists 测试表是否存在 exit 退出hbase shell get 获取行或单元（cell）的值 incr 增加指定表，行或列的值 list 列出hbase中存在的所有表 put 向指向的表单元添加值 tools 列出hbase所支持的工具 scan 通过对表的扫描来获取对用的值 status 返回hbase集群的状态信息 shutdown 关闭hbase集群（与exit不同） truncate 重新创建指定表 version 返回hbase版本信息 来自 \u0026lt;https://www.cnblogs.com/cxzdy/p/5583239.html\u0026gt; 2. ddl命令 2.1. 创建表create 注意：创建表时只需要指定列族名称，不需要指定列名。\n# 语法 create \u0026#39;表名\u0026#39;, {NAME =\u0026gt; \u0026#39;列族名1\u0026#39;}, {NAME =\u0026gt; \u0026#39;列族名2\u0026#39;}, {NAME =\u0026gt; \u0026#39;列族名3\u0026#39;} # 此种方式是上上面的简写方式，使用上面方式可以为列族指定更多的属性，如VERSIONS、TTL、BLOCKCACHE、CONFIGURATION等属性 create \u0026#39;表名\u0026#39;, \u0026#39;列族名1\u0026#39;, \u0026#39;列族名2\u0026#39;, \u0026#39;列族名3\u0026#39; create \u0026#39;表名\u0026#39;, {NAME =\u0026gt; \u0026#39;列族名1\u0026#39;, VERSIONS =\u0026gt; 版本号, TTL =\u0026gt; 过期时间, BLOCKCACHE =\u0026gt; true} # 示例 create \u0026#39;tbl_user\u0026#39;, \u0026#39;info\u0026#39;, \u0026#39;detail\u0026#39; create \u0026#39;t1\u0026#39;, {NAME =\u0026gt; \u0026#39;f1\u0026#39;, VERSIONS =\u0026gt; 1, TTL =\u0026gt; 2592000, BLOCKCACHE =\u0026gt; true} 2.2 修改(添加、删除)表结构Schema alter 添加一个列族\n# 语法 alter \u0026#39;表名\u0026#39;, \u0026#39;列族名\u0026#39; # 示例 alter \u0026#39;tbl_user\u0026#39;, \u0026#39;address\u0026#39; 删除一个列族\n# 语法 alter \u0026#39;表名\u0026#39;, {NAME=\u0026gt; \u0026#39;列族名\u0026#39;, METHOD=\u0026gt; \u0026#39;delete\u0026#39;} # 示例 alter \u0026#39;tbl_user\u0026#39;, {NAME=\u0026gt; \u0026#39;address\u0026#39;, METHOD=\u0026gt; \u0026#39;delete\u0026#39;} 修改列族信息\n# 修改f1列族的版本为5 alter \u0026#39;t1\u0026#39;, NAME =\u0026gt; \u0026#39;f1\u0026#39;, VERSIONS =\u0026gt; 5 # 修改多个列族，修改f2为内存，版本号为5 alter \u0026#39;t1\u0026#39;, \u0026#39;f1\u0026#39;, {NAME =\u0026gt; \u0026#39;f2\u0026#39;, IN_MEMORY =\u0026gt; true}, {NAME =\u0026gt; \u0026#39;f3\u0026#39;, VERSIONS =\u0026gt; 5} # 也可以修改table-scope属性，例如MAX_FILESIZE, READONLY,MEMSTORE_FLUSHSIZE, DEFERRED_LOG_FLUSH等。 # 例如，修改region的最大大小为128MB： alter \u0026#39;t1\u0026#39;, MAX_FILESIZE =\u0026gt; \u0026#39;134217728\u0026#39; 2.3 删除表drop 需要先禁用表，然后再删除表，启用的表是不允许删除的\n# 语法 disable \u0026#39;表名\u0026#39; drop \u0026#39;表名\u0026#39; # 示例 disable \u0026#39;tbl_user\u0026#39; drop \u0026#39;tbl_user\u0026#39; 2.4 显示hbase所支持的所有过滤器 show_filters\n过滤器用于get和scan命令中作为筛选数据的条件，类型关系型数据库中的where的作用\n3. dml命令 3.1. 插入或者修改数据put # 语法 # 当列族中只有一个列时\u0026#39;列族名:列名\u0026#39;使用\u0026#39;列族名\u0026#39; put \u0026#39;表名\u0026#39;, \u0026#39;行键\u0026#39;, \u0026#39;列族名\u0026#39;, \u0026#39;列值\u0026#39; put \u0026#39;表名\u0026#39;, \u0026#39;行键\u0026#39;, \u0026#39;列族名:列名\u0026#39;, \u0026#39;列值\u0026#39; # 示例 # 创建表 create \u0026#39;tbl_user\u0026#39;, \u0026#39;info\u0026#39;, \u0026#39;detail\u0026#39;, \u0026#39;address\u0026#39; # 第一行数据 put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;info:id\u0026#39;, \u0026#39;1\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;info:name\u0026#39;, \u0026#39;张三\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;info:age\u0026#39;, \u0026#39;28\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;detail:birthday\u0026#39;, \u0026#39;1990-06-26\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;detail:email\u0026#39;, \u0026#39;abc@163.com\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;detail:create_time\u0026#39;, \u0026#39;2019-03-04 14:26:10\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;address\u0026#39;, \u0026#39;上海市\u0026#39; # 第二行数据 put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;info:id\u0026#39;, \u0026#39;2\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;info:name\u0026#39;, \u0026#39;李四\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;info:age\u0026#39;, \u0026#39;27\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;detail:birthday\u0026#39;, \u0026#39;1990-06-27\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;detail:email\u0026#39;, \u0026#39;xxx@gmail.com\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;detail:create_time\u0026#39;, \u0026#39;2019-03-05 14:26:10\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;vbirdbest\u0026#39;, \u0026#39;address\u0026#39;, \u0026#39;北京市\u0026#39; # 第一行数据 put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;info:id\u0026#39;, \u0026#39;3\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;info:name\u0026#39;, \u0026#39;王五\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;info:age\u0026#39;, \u0026#39;26\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;detail:birthday\u0026#39;, \u0026#39;1990-06-28\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;detail:email\u0026#39;, \u0026#39;xyz@qq.com\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;detail:create_time\u0026#39;, \u0026#39;2019-03-06 14:26:10\u0026#39; put \u0026#39;tbl_user\u0026#39;, \u0026#39;xiaoming\u0026#39;, \u0026#39;address\u0026#39;, \u0026#39;杭州市\u0026#39; 3.2 全表扫描scan # 获取表的所有数据 # 语法 scan \u0026#39;表名\u0026#39; # 示例 scan \u0026#39;tbl_user\u0026#39; # 扫描整个列簇 # 语法 scan \u0026#39;表名\u0026#39;, {COLUMN=\u0026gt;\u0026#39;列族名\u0026#39;} # 示例 scan \u0026#39;tbl_user\u0026#39;, {COLUMN=\u0026gt;\u0026#39;info\u0026#39;} # 扫描整个列簇的某个列 # 语法 scan \u0026#39;表名\u0026#39;, {COLUMN=\u0026gt;\u0026#39;列族名:列名\u0026#39;} # 示例 scan \u0026#39;tbl_user\u0026#39;, {COLUMN=\u0026gt;\u0026#39;info:age\u0026#39;} 3.3 获取数据get # 语法 get \u0026#39;表名\u0026#39;, \u0026#39;行键\u0026#39; # 示例 get \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39; # 根据某一行某列族的数据 # 语法 get \u0026#39;表名\u0026#39;, \u0026#39;行键\u0026#39;, \u0026#39;列族名\u0026#39; # 示例 get \u0026#39;tbl_user\u0026#39;, \u0026#39;mengday\u0026#39;, \u0026#39;info\u0026#39; # 获取rowKey=r1并且 1552819392398 \u0026lt;= 时间戳范围 \u0026lt; 1552819398244 get \u0026#39;t1\u0026#39;, \u0026#39;r1\u0026#39;, {TIMERANGE =\u0026gt; [1552819392398, 1552819398244]} # 获取指定列的值，多个值使用数组表示 get \u0026#39;t1\u0026#39;, \u0026#39;r1\u0026#39;, {COLUMN =\u0026gt; [\u0026#39;c1\u0026#39;, \u0026#39;c2\u0026#39;, \u0026#39;c3\u0026#39;]} 3.4 删除某个列族中的某个列delete # 语法 delete \u0026#39;表名\u0026#39;, \u0026#39;行键\u0026#39;, \u0026#39;列族名:列名\u0026#39; create \u0026#39;tbl_test\u0026#39;, \u0026#39;columnFamily1\u0026#39; put \u0026#39;tbl_test\u0026#39;, \u0026#39;rowKey1\u0026#39;, \u0026#39;columnFamily1:column1\u0026#39;, \u0026#39;value1\u0026#39; put \u0026#39;tbl_test\u0026#39;, \u0026#39;rowKey1\u0026#39;, \u0026#39;columnFamily1:column2\u0026#39;, \u0026#39;value2\u0026#39; delete \u0026#39;tbl_test\u0026#39;, \u0026#39;rowKey1\u0026#39;, \u0026#39;columnFamily1:column1\u0026#39; 3.5 删除某行数据deleteall # 语法 deleteall \u0026#39;表名\u0026#39;, \u0026#39;行键\u0026#39; # 示例 deleteall \u0026#39;tbl_test\u0026#39;, \u0026#39;rowKey1\u0026#39; 3.6 清空整个表的数据truncate truncate '表名'\n3.7 LIMIT 返回的行数 # 语法 scan \u0026#39;表名\u0026#39;, { LIMIT =\u0026gt; 行数} # 示例 scan \u0026#39;tbl_user\u0026#39;, { LIMIT =\u0026gt; 2 } 3.8 FILTER条件过滤器 过滤器之间可以使用AND、OR连接多个过滤器。\nValueFilter 值过滤器\n# 语法：binary 等于某个值 scan \u0026#39;表名\u0026#39;, FILTER=\u0026gt;\u0026#34;ValueFilter(=,\u0026#39;binary:列值\u0026#39;)\u0026#34; # 语法 substring:包含某个值 scan \u0026#39;表名\u0026#39;, FILTER=\u0026gt;\u0026#34;ValueFilter(=,\u0026#39;substring:列值\u0026#39;)\u0026#34; # 示例 scan \u0026#39;tbl_user\u0026#39;, FILTER=\u0026gt;\u0026#34;ValueFilter(=, \u0026#39;binary:26\u0026#39;)\u0026#34; scan \u0026#39;tbl_user\u0026#39;, FILTER=\u0026gt;\u0026#34;ValueFilter(=, \u0026#39;substring:6\u0026#39;)\u0026#34; ColumnPrefixFilter 列名前缀过滤器\n# 语法 substring:包含某个值 scan \u0026#39;表名\u0026#39;, FILTER=\u0026gt;\u0026#34;ColumnPrefixFilter(\u0026#39;列名前缀\u0026#39;)\u0026#34; # 示例 scan \u0026#39;tbl_user\u0026#39;, FILTER=\u0026gt;\u0026#34;ColumnPrefixFilter(\u0026#39;birth\u0026#39;)\u0026#34; # 通过括号、AND和OR的条件组合多个过滤器 scan \u0026#39;tbl_user\u0026#39;, FILTER=\u0026gt;\u0026#34;ColumnPrefixFilter(\u0026#39;birth\u0026#39;) AND ValueFilter(=,\u0026#39;substring:26\u0026#39;)\u0026#34; https://blog.csdn.net/u013980127/article/details/52443155 https://blog.csdn.net/vbirdbest/article/details/88236575 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/%E5%91%BD%E4%BB%A4.html","summary":"[toc] 1. 大纲 启动hbase服务: start-hbase.sh 停止hbase服务: stop-hbase.sh 启动shelll: hbase shell hbase shell命令 描述 alter 修改列族（column family）模式 count 统","title":"命令"},{"content":"create database [IF NOT EXISTS] userdb; 创建数据库 user userdb; 使用userdb数据库 客户端的链接:\nCli的方式\nhive(jdbc方式)\nweb方式\njdbc方式\nhive --service hiveserver2 \u0026amp; //先后台运行hive beeline -u jdbc:hive2:// -n hive -p a web方式\nhive --service hwi\n建表语法\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 说明：\n1、 CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。\n2、 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。\n3、 LIKE 允许用户复制现有的表结构，但是不复制数据。\n4、 ROW FORMAT 列格式\nDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, \u0026hellip;)]\n用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。\n5、 STORED AS 存储格式\nSEQUENCEFILE|TEXTFILE|RCFILE\n如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。\n6、CLUSTERED BY\n对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。\n把表（或者分区）组织成桶（Bucket）有两个理由：\n（1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。\n（2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。\n原文链接：https://blog.csdn.net/l1212xiao/article/details/80432759\n一些建表例子:\ncreate table student( stuno int comment \u0026#39;xuehao\u0026#39;, name string comment \u0026#39;student name\u0026#39;, course array\u0026lt;string\u0026gt;, score map\u0026lt;string,int\u0026gt;, address struct\u0026lt;province:string,city:string,zip:string\u0026gt; ) row format delimited fields terminated by \u0026#39;\t\u0026#39; collection items terminated by \u0026#39;,\u0026#39; map keys terminated by \u0026#39;:\u0026#39; lines terminated by \u0026#39; \u0026#39; stored as textfile location \u0026#39;/data/\u0026#39;; create table checking( id int , term string, uid int, cid int, checkDate date, checkTime string, result map\u0026lt;string,int\u0026gt; ) partitioned by (year int,month int) clustered by (cid) sorted by (id) into 3 buckets row format delimited fields terminated by \u0026#39;\t\u0026#39; collection items terminated by \u0026#39;,\u0026#39; map keys terminated by \u0026#39;:\u0026#39; lines terminated by \u0026#39; \u0026#39;; 插入数据:\ninsert into table checking partition(yeat,month) select id ,term,uid,cid,checkDate,checkTime,result,year(checkDate),month(checkDate) from checking_temp; // load 命令本质上是把数据上传到表所属的hdfs路径下,也就是说,直接操作hdfs下的数据也能操作表,例如删全表数据,直接把hdfs上对应的文件删除,新增同理 // 这是加载本地文件(不删除原文件), 不要local关键字则加载hdfs上的文件(但此时是移动数据了)(不区分外部表和内部表) load data local inpath \u0026#39;/home/jht/t.txt\u0026#39; into table student partition(age=24); 删除数据:\ninsert overwrite table dw_200_rst_advert_park_idea_place_stat_day PARTITION (dt=\u0026#39;2017-12-20\u0026#39;,game_id = \u0026#39;id\u0026#39;) select * from dw_200_rst_advert_park_idea_place_stat_day where id IS NOT NULL ; -- 按分区删除: ALTER TABLE test1 DROP PARTITION (dt=\u0026#39;2016-04-29\u0026#39;); -- 清空表 truncate table employee; https://blog.csdn.net/zimou5581/article/details/82383906 https://www.cnblogs.com/linn/p/6196293.html 添加分区:\nalter table table_name add partition(age=8); 分隔符在HIVE中的用途\n分隔符 描述 对于文本文件来说，每行都是一条记录，因此换行符可以分隔记录 ^A(Ctrl+A) 用于分隔字段(列)。在CREATE TABLE语句中可以使用八进制编码\u0001表示 ^B(Ctrl+B) 用于分隔ARRAY或者STRUCT中的元素，或用于MAP中键-值对之间的分隔。在CREATE TABLE语句中可以使用八进制编码\u0002表示 ^C(Ctrl+C) 用于MAP中键和值之间的分隔。在CREATE TABLE语句中可以使用八进制编码\u0003表示 Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”1″）、行分隔符（” ”）以及读取文件数据的方法。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。\n我们可以在create表格的时候，选择如下，表格加载input的文件的时候就会按照下面格式匹配\nrow format delimited fields terminated by \u0026#39;\u0001\u0026#39; collection items terminated by \u0026#39;\u0002\u0026#39; map keys terminated by \u0026#39;\u0003\u0026#39; lines terminated by \u0026#39; \u0026#39; stored as textfile; https://www.cnblogs.com/kouryoushine/p/7805597.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%91%BD%E4%BB%A4.html","summary":"create database [IF NOT EXISTS] userdb; 创建数据库 user userdb; 使用userdb数据库 客户端的链接: Cli的方式 hive(jdbc方式) web方式 jdbc方式 hive --service hiveserver2 \u0026amp; //先后台运","title":"命令"},{"content":"注:使用spark时,需要开启HDFS,(如果运行在yarn上还需开YARN)\n启动Spark: (hadoop这个命令不起作用了) start-all.sh\n启动后主机有Master进程, 从机有Worker进程\n停止Spark: stop-all.sh\n进入Spark-Shell:(进入scala环境) spark-shell\n上传jar包:(这里是运行在Standalone上,\u0026ndash;master后是指定资源管理器)\nspark-submit --master spark://master:7077 --class com.yc.hello hello.jar\n更多命令:http://dataunion.org/10345.html\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%91%BD%E4%BB%A4.html","summary":"注:使用spark时,需要开启HDFS,(如果运行在yarn上还需开YARN) 启动Spark: (hadoop这个命令不起作用了) start-all.sh 启动后主机","title":"命令"},{"content":"启动服务 : zkServer.sh start 打开客户端: zkCli.sh -server slave01:2181\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E5%91%BD%E4%BB%A4.html","summary":"启动服务 : zkServer.sh start 打开客户端: zkCli.sh -server slave01:2181","title":"命令"},{"content":" 创建一个Django项目 django-admin.py startproject HelloWorld\nHelloWorld: 项目的容器。\nmanage.py: 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。\nHelloWorld/init.py: 一个空文件，告诉 Python 该目录是一个 Python 包。\nHelloWorld/settings.py: 该 Django 项目的设置/配置。\nHelloWorld/urls.py: 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站\u0026quot;目录\u0026quot;。\nHelloWorld/wsgi.py: 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。\n启动服务 cd HelloWorld python manage.py runserver 0.0.0.0:8000\n#0.0.0.0 让其它电脑可连接到开发服务器\n创建一个app,名为sales (terminal中): django-admin[.py] startapp sales\n建表: python manage.py makemigrations sale 让 Django 知道我们在我们的模型有一些变更\npython manage.py migrate sale 创建表结构\n收集静态资源 (css,js,image): python manage.py collectstatic\n模糊查询: sciencenews = models.Sciencenews.objects.filter(author__icontains=keyword)\nauthor:字段名\nicontains/(contains): 大小写(不)敏感;\nkeyword : 查询的内容\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E5%91%BD%E4%BB%A4.html","summary":"创建一个Django项目 django-admin.py startproject HelloWorld HelloWorld: 项目的容器。 manage.py: 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。 HelloWorld/init.py: 一个空文件，告诉 Python 该目录是一个 Python 包","title":"命令"},{"content":"安装:\n安装selenium: pip install selenium 安装phantomjs: 下载包,将bin/phantomjs.exe文件加入到环境变量path中即可 请求头设置:\ndcap = dict(DesiredCapabilities.PHANTOMJS) headers = {\u0026#39;Accept\u0026#39;: \u0026#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\u0026#39;, \u0026#39;Accept-Language\u0026#39;: \u0026#39;zh-CN,zh;q=0.8\u0026#39;, \u0026#39;Cache-Control\u0026#39;: \u0026#39;max-age=0\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\u0026#39;, \u0026#39;Connection\u0026#39;: \u0026#39;keep-alive\u0026#39;, \u0026#39;Host\u0026#39;: \u0026#39;club.autohome.com.cn\u0026#39;, \\# \u0026#39;Upgrade - Insecure - Requests\u0026#39;:1, \u0026#39;Referer\u0026#39;: \u0026#39;https://www.autohome.com.cn/shanghai/\u0026#39;, } for key, value in headers.iteritems(): dcap[\u0026#39;phantomjs.page.customHeaders.{}\u0026#39;.format(key)] = value driver = webdriver.PhantomJS(desired_capabilities=dcap) 添加代理:\nfrom selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities def get_cookie(url,source=\u0026#39;xcar\u0026#39;): \u0026#34;\u0026#34;\u0026#34; 爱卡出现安全页面,模拟输入验证码获得cookie \u0026#34;\u0026#34;\u0026#34; cookies={} dcap = dict(DesiredCapabilities.PHANTOMJS) dcap[\u0026#34;phantomjs.page.settings.loadImages\u0026#34;] = False check_ip_list(source=source) proxy = ipObjectsToList(source) proxy_random = random.choice(proxy) service_args = [ \u0026#39;--proxy={}\u0026#39;.format(proxy_random), #\u0026#39;http://192.168.1.2:80\u0026#39; \u0026#39;--proxy-type=http\u0026#39;, ] driver = webdriver.PhantomJS(executable_path=phantomjs_path, desired_capabilities=dcap, service_args=service_args) \\#url=\u0026#39;http://www.xcar.com.cn/favicon.ico\u0026#39; driver.get(url) driver.refresh() cookies_temp=driver.get_cookies() cookies_temp={ x[\u0026#39;name\u0026#39;]:x[\u0026#39;value\u0026#39;] for x in cookies_temp } cookies.update(cookies_temp) driver.close() return cookies ","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/phantomjs%E5%92%8Cselenium/%E5%91%BD%E4%BB%A4.html","summary":"安装: 安装selenium: pip install selenium 安装phantomjs: 下载包,将bin/phantomjs.exe文件加入到环境变量path中即可 请求头","title":"命令"},{"content":"[toc]\n1. 按时间查询: db.getCollection('sale_mongo').find({'sellTime':{$lt:new Date('2014-02-01')}})\n2. 查询三种价格: {$group : {_id : \u0026#34;$id\u0026#34;, maxPrice : {$max : \u0026#34;$sellPrice\u0026#34;} , minPrice : {$min : \u0026#34;$sellPrice\u0026#34;} , avgPrice : {$avg : \u0026#34;$sellPrice\u0026#34;}, } } ]) 3. 查询时间: db.getCollection('sale_mongo').find({'sellTime':{$lt:new Date('2014-02-01')}})\n4. 插入时间: timeStr = \u0026#34;2009-08-02 05:00:09\u0026#34; db.insert_many([ {\u0026#39;id\u0026#39;: 0,\u0026#39;sellTime\u0026#39;:datetime.datetime.strptime(timeStr, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;),\u0026#39;sellPrice\u0026#39;:1000.0 }, {\u0026#39;id\u0026#39;: 0,\u0026#39;sellTime\u0026#39;:datetime.datetime.strptime(timeStr, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;),\u0026#39;sellPrice\u0026#39;:1030.0 }, ]) 5. 创建索引: db.COLLECTION_NAME.ensureIndex({KEY:1})\n1/-1 : 升/降序\n1.8版本之前创建索引使用createIndex() db.col.ensureIndex({\u0026ldquo;title\u0026rdquo;:1})\ngetIndexes():方法可以用来查看集合的所有索引\ntotalIndexSize() : 查看集合索引的总大小\ndropIndex(key) : 删除指定的索引\n6. 查询 and : db.mycol.find({\u0026quot;by\u0026quot;:\u0026quot;yibai\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;MongoDB\u0026quot;})\n或者:\ndb.mycol.find({$and:[{\u0026quot;by\u0026quot;:\u0026quot;yiibai tutorials\u0026quot;},{\u0026quot;title\u0026quot;: \u0026quot;MongoDB Overview\u0026quot;}]})\n查询 OR , 将and关键词变成or即可\n7. 批量导出: mongoexport -d social_survey -c public_praise -f k_url,k_content --type=csv -o tempData.csv\n导出csv时必须指定字段\n8. 导出csv mongoexport --host dds-bp1dfb31d5fbb4741.mongodb.rds.aliyuncs.com:3717 -d crawler -u ugc -p a1b2c3d4 -c autohome_tanjiezhe -q '{\u0026quot;crawl_date\u0026quot;:{$gte:ISODate(\u0026quot;2018-04-24T10:34:40.000Z\u0026quot;)}}' --type=csv -o tanjiezhe.csv -f k_id,type,source,series_name\n9. 批量导入数据: 1.mongoimport -d DJangoLearn -c sale_sale -f id,sellTime,sellPrice --file ~/dataOfDjango/data.csv --type csv 2.mongoimport -h dds-bp1dfb31d5fbb4741.mongodb.rds.aliyuncs.com --port 3717 -u ugc -p a1b2c3d4 -d crawler -c autohome_praise_url -f _id,pp_id,series_id,series_name,brand_name,source,k_type --upsert --file pp_url.csv --type csv 10. 批量导入数据(bson/json文件): mongorestore -d db_name 文件夹目录\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E5%91%BD%E4%BB%A4.html","summary":"[toc] 1. 按时间查询: db.getCollection('sale_mongo').find({'sellTime':{$lt:new Date('2014-02-01')}}) 2. 查询三种价格: {$group : {_id : \u0026#34;$id\u0026#34;, maxPrice : {$max : \u0026#34;$sellPrice\u0026#34;} , minPrice : {$min : \u0026#34;$sellPrice\u0026#34;} , avgPrice : {$avg : \u0026#34;$sellPrice\u0026#34;}, } } ]) 3. 查询时间: db.getCollection('sale_mongo').find({'sellTime':{$lt:new Date('2014-02-01')}}) 4. 插入时间: timeStr = \u0026#34;2009-08-02 05:00:09\u0026#34; db.insert_many([ {\u0026#39;id\u0026#39;: 0,\u0026#39;sellTime\u0026#39;:datetime.datetime.strptime(timeStr, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;),\u0026#39;sellPrice\u0026#39;:1000.0 }, {\u0026#39;id\u0026#39;: 0,\u0026#39;sellTime\u0026#39;:datetime.datetime.strptime(timeStr, \u0026#34;%Y-%m-%d","title":"命令"},{"content":" 下载mysql源安装包 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm\n安装mysql源 yum localinstall mysql57-community-release-el7-8.noarch.rpm\n检查mysql源是否安装成功 yum repolist enabled | grep \u0026quot;mysql.*-community.*\u0026quot; 安装MySQL yum install mysql-community-server\n启动MySQL服务 systemctl start mysqld\n查看MySQL的启动状态 systemctl status mysqld\n查看root用户的密码，（冒号后面得就是密码） grep 'temporary password' /var/log/mysqld.log\n登录root mysql -uroot -p\n修改密码 ALTER USER 'root'@'localhost' IDENTIFIED BY 'newPassword';\n添加远程用户 GRANT ALL PRIVILEGES ON *.* TO 'xkj'@'%' IDENTIFIED BY 'a' WITH GRANT OPTION;\n修改编码集 vim /etc/my.cnf (在 [mysqld] 下加) character_set_server=utf8 init_connect=\u0026#39;SET NAMES utf8\u0026#39; 查看编码集 show variables like '%character%' 给用户添加某个database权限 grant all on dbName.* to 'xkj'@'localhost'; 查看所有用户及权限 SELECT DISTINCT CONCAT('User: ''',user,'''@''',host,''';') AS query FROM mysql.user; 修改字段类型: alter table address modify column city varchar(50) 修改表编码集: ALTER TABLE tableName DEFAULT CHARACTER SET utf8; 设置编码集: set character_set_database=utf8; 查看表的编码集: show create table tableName ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%91%BD%E4%BB%A4.html","summary":"下载mysql源安装包 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm 安装mysql源 yum localinstall mysql57-community-release-el7-8.noarch.rpm 检查mysql源是否安装成功 yum repolist enabled | grep \u0026quot;mysql.*-community.*\u0026quot; 安装MySQL yum install mysql-community-server 启动MySQL服务 systemctl start mysqld 查看My","title":"命令"},{"content":"安装:\nyum -y install docker\n启动服务:\nservice docker start (最好用root用户启动,不然启动不了)\n拉取一个 Docker 镜像：\ndocker pull centos:latest\n注: cento：lastest 是镜像的名称以及版本(默认为最新)，Docker Daemon 发现本地没有我们需要的镜像，会自动去 Docker Hub 上去下载镜像，下载完成后，该镜像被默认保存到 /var/lib/docker 目录下。\n来自* \u0026lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg \u0026gt;\n测试运行:\ndocker run hello-world\ndocker run ubuntu:15.10 /bin/echo \u0026quot;Hello world\u0026quot;\ndocker: Docker 的二进制执行文件。 **run:**与前面的 docker 组合来运行一个容器。 ubuntu:15.10指定要运行的镜像，Docker首先从本地主机上查找镜像是否存在，如果不存在，Docker 就会从镜像仓库 Docker Hub 下载公共镜像。 /bin/echo \u0026ldquo;Hello world\u0026rdquo;: 在启动的容器里执行的命令 以上命令完整的意思可以解释为：Docker 以 ubuntu15.10 镜像创建一个新容器，然后在容器里执行 bin/echo \u0026ldquo;Hello world\u0026rdquo;，然后输出结果。\n运行一个新的mysql容器:\ndocker run -p 3307:3306 --name mysql3 -e MYSQL_ROOT_PASSWORD=123456 -d mysql\n运行一个已有的容器\ndocker start 容器名/容器id\n查看主机下存在多少镜像:\ndocker images\n来自* \u0026lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg \u0026gt;\n查看当前有哪些容器在运行 :\ndocker ps -a\n来自* \u0026lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg \u0026gt;\n制作镜像:\n创建文件 Dockerfile,并写入, Dockerfile 中每一条指令都创建镜像的一层(最多127层) 来自* \u0026lt;https://www.cnblogs.com/lsgxeva/p/8746644.html \u0026gt;\n# Docker image for springboot file run # 基础镜像使用java FROM java:jdk8 # VOLUME 指定了临时文件目录为/tmp。 # 其效果是在主机 /var/lib/docker 目录下创建了一个临时文件，并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名为app.jar ADD onlineSpider-0.0.1-SNAPSHOT.jar app.jar # 运行jar包 RUN bash -c \u0026#39;touch /app.jar\u0026#39; ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] 执行命令 (注意,有个 . 这个点指当前目录 ) docker build -t springboot-demo . 来自* \u0026lt;https://blog.csdn.net/junmoxi/article/details/80861199 \u0026gt;\n-t 参数是指定此镜像的tag名\n删除镜像(如果该镜像正在被容器使用):\n1.先停止容器:\ndocker ps docker stop container_name/container_id\n2.删除容器:\ndocker rm container_name/container_id\n来自* \u0026lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg \u0026gt;\n3.删除镜像;\ndocker rmi image_name\n来自* \u0026lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg \u0026gt;\n删除悬挂镜像\n(那些 \u0026lt; none\u0026gt;:\u0026lt; none\u0026gt;样子的镜像,这个样子的有两种,一种的镜像的父层,(镜像是有多层结构的),还有一个种就是无用的(多次使用docker build和docker pull 创建统一个镜像造成) 来自* \u0026lt;https://www.jb51.net/article/124549.htm \u0026gt;\ndocker rmi $(docker images -f \u0026quot;dangling=true\u0026quot; -q) 构建本地仓库并使用\n下载并启动注册器(本地仓库) docker run -d -p 5000:5000 --restart=always --name local_registry registry:latest\n-d 后台运行\n-p 端口映射, 宿主机80端口映射给容器的5000端口\n\u0026ndash;restart=always 容器意外关闭后, 自动重启 (如果重启docker服务, 带这个参数的, 能自动启动为Up状态, 不带这个的,不会自动启动)\n\u0026ndash;name 给容器起个名字, 可以根据这个名字去停止/启动/删除容器\n来自 \u0026lt;https://www.cnblogs.com/zhouyalei/p/6411614.html \u0026gt;\n-p 端口映射, 宿主机80端口映射给容器的5000端口\n\u0026ndash;restart=always 容器意外关闭后, 自动重启 (如果重启docker服务, 带这个参数的, 能自动启动为Up状态, 不带这个的,不会自动启动)\n\u0026ndash;name 给容器起个名字, 可以根据这个名字去停止/启动/删除容器\n来自 \u0026lt;https://www.cnblogs.com/zhouyalei/p/6411614.html \u0026gt;\n这就创建了一个本地仓库,以后就用它了\n开放端口 firewall-cmd --zone=public --add-port=5000/tcp --permanent; firewall-cmd --reload; firewall-cmd --list-all;\n重命名镜像 # docker tag 原镜像名:tag 新镜像名:tag docker tag sprintbootdemo:latest 192.168.142.128/sprintbootdemo:spd1.0.0 相当于拉个版本,既然要上传,就要版本控制一下,tag命令不会删除原镜像(更像是复制并重命名,但是镜像id不变)\n推送镜像 docker push 192.168.142.128/sprintbootdemo:spd1.0.0\n如果报错了,则执行如下: vi /etc/sysconfig/docker 来自 \u0026lt;https://www.cnblogs.com/zhouyalei/p/6411614.html\u0026gt; 添加 --insecure-registry 192.168.142.128:5000 然后重启: systemctl restart docker.service 再推送即可; 完整文件: # Modify these options if you want to change the way the docker daemon runs OPTIONS=\u0026#39;--selinux-enabled --log-driver=journald --signature-verification=false --insecure-registry 192.168.142.128:5000\u0026#39; if [ -z \u0026#34;${DOCKER_CERT_PATH}\u0026#34; ]; then DOCKER_CERT_PATH=/etc/docker fi 拉取镜像 (最好是换一个台机器)\ndocker pull 192.168.142.128/sprintbootdemo:spd1.0.0\n如果不成功,也在这个服务器上增加 \u0026ndash;insecure-registry 192.168.142.128:5000 (如上)\n(这仅仅是项目,还没有数据库)\ndocker run -p 8998:8998 192.168.142.128/sprintbootdemo:spd1.0.0 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/docker/%E5%91%BD%E4%BB%A4.html","summary":"安装: yum -y install docker 启动服务: service docker start (最好用root用户启动,不然启动不了) 拉取一个 Docker 镜像： docker pull centos:latest 注: cento：lastest 是镜像的名称以及版","title":"命令"},{"content":"[toc]\n前言 本文以命令行为主\n其本质还是发送http请求去操作数据\n1. 基本格式 es是以RESTFul风格来命名API的，其API的基本格式如下：\nhttp://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;索引\u0026gt;/\u0026lt;类型\u0026gt;/\u0026lt;文档id\u0026gt;\n这里需要注意的是，该格式从es7.0.0开始，移除Type（类型）这个概念，新的基本格式如下：\nhttp://\u0026lt;ip\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;索引\u0026gt;/_doc/\u0026lt;文档id\u0026gt;\nType（类型）字段那里变为固定值 _doc\nes的动作是以http方法来决定的: 常用的http方法: GET/PUT/POST/DELETE\npost 主做修改, put主做创建\npost也有创建的功能,特别是插入数据生成随机id时\n2. 命令 2.1 创建索引 curl -XPUT http://localhost:9200/xkj_test 没有指定mappings,故为非结构化索引,也没有字段\n// 结构化索引: 指定了mappings curl -X PUT \u0026#39;localhost:9200/accounts\u0026#39; -d \u0026#39; { \u0026#34;mappings\u0026#34;: { // mappings 映射 \u0026#34;man\u0026#34;: { // type \u0026#34;properties\u0026#34;: { // 具体属性 \u0026#34;name\u0026#34;: { // 字段名 \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; // 字段类型 // ....还有其他可以选,比如拆分规则,权重值 }, \u0026#34;country\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;keyword\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;:\u0026#34;integer\u0026#34; }, \u0026#34;date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; } } }, // \u0026#34;worman\u0026#34;: {} // 6.x版本只支持一个type } } }\u0026#39; 其核心是里面的json字符串\n2.2 插入数据 curl -X POST \u0026#39;localhost:9200/accounts/person/1000\u0026#39; -d \u0026#39; // 这个1000表示id,如果不写es会自动生成一个,如果连索引和类型也没有也会一并创建,如果id已存在则修改 { \u0026#34;user\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;工程师\u0026#34;, \u0026#34;desc\u0026#34;: \u0026#34;数据库管理\u0026#34; }\u0026#39; 格式是 ip:port/索引/类型/id, 参数是各属性的json字符串\n2.3 修改数据 post http://localhost:9200/test/_doc/1/_update { \u0026#34;script\u0026#34;: { \u0026#34;lang\u0026#34;: \u0026#34;painless\u0026#34;, \u0026#34;inline\u0026#34;: \u0026#34;ctx._source.age += 30\u0026#34; } } 关键字”script”: 标志以脚本的方式修改文档\n“lang”：表示以何种脚本语言进行修改，“painless”表示以es内置的脚本语言进行修改。此外es还支持多种脚本语言，如Python，js等等\n“inline”：指定脚本内容 “ctx”代表es上下文，_source 代表文档\n根据条件修改信息:\nPOST my_index/_update_by_query { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source[\u0026#39;ipsub\u0026#39;]=0\u0026#34; }, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must_not\u0026#34;: [ { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;ipsub\u0026#34; } } ] } } 上面语句是根据是否存在\u0026quot;ipsub\u0026quot;字段，如果不存在，给信息增加字段\u0026quot;ipsub\u0026quot;，并且赋值0\n2.4 查看 查看xkj_test索引数据格式 curl -X GET http://localhost:9200/xkj_test/ 查看xkj_test索引下指定id查看 curl -X GET \u0026#39;localhost:9200/xkj_test/person/1000?pretty=true\u0026#39; 根据条件查询 curl -X POST \u0026#39;localhost:9200/accounts/person/_search\u0026#39; -d \u0026#39; // 要带上 _search 关键字 { \u0026#34;query\u0026#34; : { \u0026#34;match\u0026#34; : [{ \u0026#34;desc\u0026#34; : \u0026#34;软件\u0026#34; },{ \u0026#34;desc\u0026#34; : \u0026#34;软件 工程\u0026#34; }]} }\u0026#39; and 查询: match写多个条件\nor 查询: 利用空格的分词,达到or的效果(我猜的)\n_score ：匹配度\n_source：文档的字段\n来源: https://www.jianshu.com/p/083d99a1db6e es的查询五花八门,最好的技术书籍就是官网,以后好好看,如果要查数据,借助可视化界面就够用了\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 2.5 删除 删除xkj_test索引\ndelete http://localhost:9200/xkj_test/ 删除xkj_test索引下的数据(指定id)\ndelete http://localhost:9200/xkj_test/_doc/1 根据条件删除xkj_test索引下test1类型的数据\npost http://localhost:9200/xkj_test/test1/_delete_by_query { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;createDate\u0026#34;: { \u0026#34;lt\u0026#34;: \u0026#34;now-31d\u0026#34; } } } } 解释: 删除 createDate 字段下31天前的数据\n利用_delete_by_query插件去批量删除\nhttps://blog.csdn.net/weixin_44034192/article/details/89372934 2.6 简单排序 按照指定字段排序,(默认按照分数排序)\nGET books/_search { \u0026#34;sort\u0026#34;: [ \u0026#34;price\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } ] } // 按照价格降序排,再按年龄排 在 Elasticsearch 中，默认排序是按照相关性的评分（_score）进行降序排序，也可以按照字段的值排序、多级排序、多值字段排序、基于 geo（地理位置）排序以及自定义脚本排序，除此之外，对于相关性的评分也可以用 rescore 二次、三次打分，它可以限定重新打分的窗口大小（window size），并针对作用范围内的文档修改其得分，从而达到精细化控制结果相关性的目的。\n3. 复杂查询 3.1 Query and filter context 查询子句的行为取决于它是用在查询上下文（query context）还是用在过滤器上下文（filter context）：\n3.1.1 Query context 在查询上下文中的查询子句回答了“这个文档与这个查询子句的匹配程度是怎样的？”问题。除了决定文档是否匹配以外，查询子句还会计算一个“_score”，它表示文档与其他文档的相关程度(分数)。\n3.1.2 Filter context 在过滤器上下文中，一个查询子句回答了“这个文档与查询子句匹配吗？”的问题。这个答案是简单的Yes或者No，也不会计算分数。过滤上下文主要用于过滤结构化数据\nPS：Query VS Filter\n查询反应的是文档与查询子句的匹配程度，而过滤反应的是文档是否匹配查询子句 一个是筛选是否满足条件，情况无非两种：是或不是；一个是看满足条件的记录与查询条件的匹配程度 哪些满足条件，这是过滤；满足条件的这些记录与条件的匹配程度，这是查询 过滤不会计算评分，查询会计算评分 3.2 全文查询( Full text) 3.1.1 match query match查询接受文本/数值/日期类型的数据，分析它们，并构造一个查询。\n相当于模糊查询\nmatch是一种布尔类型的查询。这意味着它对提供的文本进行分析，并在分析的过程中为提供的文本构造一个布尔查询。operator 选项可以设置为 or 或者 and 以此来控制布尔子句（默认是 or ）。例如\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;skuName\u0026#34; : \u0026#34;空调\u0026#34; } } }\u0026#39; 3.1.2 Match Phrase Query match_phrase 查询与 match类似，但是它是用于精确匹配或单词接近匹配的。例如：\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;match_phrase\u0026#34; : { \u0026#34;skuName\u0026#34; : \u0026#34;this is a test\u0026#34; } } }\u0026#39; 3.1.3 Multi Match Query multi_match 相当于 match 的多字段版本, multi_match可以指定多个字段，而match只能针对一个字段\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34; : { \u0026#34;query\u0026#34;: \u0026#34;this is a test\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;subject\u0026#34;, \u0026#34;skuName\u0026#34; ] } } } \u0026#39; fields 里的字段用 * 支持模糊匹配, 写成这种 [ \u0026#34;subject^3\u0026#34;, \u0026#34;skuName\u0026#34; ] 表示subject比skuName重要3倍 3.1.4 Query String Query 支持Lucene查询字符串语法，允许指定 AND | OR | NOT ，并且在单个查询字符串中进行多字段查询, 分词后对每个词都查询\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34; : { \u0026#34;default_field\u0026#34; : \u0026#34;skuName\u0026#34;, \u0026#34;query\u0026#34; : \u0026#34;this that thus\u0026#34; } } } \u0026#39; // 在skuName中查询 this , that , thus 三个词 , 其实质解析成: \u0026#34; this OR that OR thus\u0026#34;, 你可以替换OR为AND/NOT // 还可以指定快 例如: \u0026#34;query\u0026#34; : \u0026#34;(new york city) OR (big apple)\u0026#34; 这样将被拆分成 “new york city” 和 “big apple” 两部分，并且每一部分都被分析器独立分析 curl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34; : { \u0026#34;fields\u0026#34; : [\u0026#34;content\u0026#34;, \u0026#34;name\u0026#34;], \u0026#34;query\u0026#34; : \u0026#34;this AND that\u0026#34; } } } \u0026#39; // 等价于 curl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;query_string\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;(content:this OR name:this) AND (content:that OR name:that)\u0026#34; } } } \u0026#39; 注意，按操作符拆分\nquery_string的参数包括：\nquery　实例被解析的查询文本\ndefault_field　如果没有指定前缀字段的话，这是默认的查询字段。（默认查询所有字段）\ndefault_operator　如果没有明确指定操作符的话，那么这是默认的操作符。例如，如果默认操作符是OR的话，那么“my name is jack”将被翻译成“my OR name OR is OR jack”，同理，如果是AND，则被翻译成“my AND name AND is AND jack”\nanalyzer　用来解析查询字符串的解析器的名字\nallow_leading_wildcard　如果设置了，那么 * 或 ? 允许作为第一个字符。默认是true\nlenient　如果设置为true，则格式失败将被忽略\n3.2 单词级查询(term text) 全文本查询会在执行之前对查询字符串进行分析，而单词级别查询会对存储在反向索引中的精确的term进行操作。\n这些查询通常用于结构化的数据，比如：numbers ， dates ，enums 等，而不是对全文本字段。\n（PS：也就是说，全文本查询之前要先对文本内容进行分词，而单词级别的查询直接在相应字段的反向索引中精确查找，单词级别的查询一般用于数值、日期等类型的字段上）\n3.2.1 Term Query 在指定的字段中查找包含指定的精确的term的文档\nterm查询将在反向索引（或者叫倒排索引）中查找包含特定的精确的term的文档。例如：\ncurl -X POST \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34; : { \u0026#34;user\u0026#34; : \u0026#34;Kimchy\u0026#34; } } } \u0026#39; // 查询倒排索引(切词后)中user包含\u0026#34;kimchy\u0026#34;字符串的值, 3.2.2 Terms Query 查找包含指定字段中指定的任何确切term的文档\n筛选出与所提供的terms中任何一个匹配的文档\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;terms\u0026#34; : { \u0026#34;user\u0026#34; : [\u0026#34;kimchy\u0026#34;, \u0026#34;elasticsearch\u0026#34;]} } } \u0026#39; // 查询 切词后包含\u0026#34;kimchy\u0026#34;或者 \u0026#34;elasticsearch\u0026#34;文档 3.2.3 Range Query 查找指定字段在指定范围内包含值（日期、数字或字符串）的文档。\n下面的例子返回age字段的值在10到20之间的文档：\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34; : { \u0026#34;age\u0026#34; : { \u0026#34;gte\u0026#34; : 10, \u0026#34;lte\u0026#34; : 20, \u0026#34;boost\u0026#34; : 2.0 } } } } \u0026#39; range查询可以接受下列参数：\ngte　大于或等于\ngt　大于\nlte　小于或等于\nlt　小于\nboost　设置boost值，默认是1.0\n在日期范围查询的时候，我们可以指定日期格式。例如：(时间格式es会自动转化,用字符串去查时间格式也能行)\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34; : { \u0026#34;born\u0026#34; : { \u0026#34;gte\u0026#34;: \u0026#34;01/01/2012\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;2013\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;dd/MM/yyyy||yyyy\u0026#34; } } } } \u0026#39; // 这个例子是查询在2012-01-01到2013-12-31之间出生的人 curl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34; : { \u0026#34;timestamp\u0026#34; : { \u0026#34;gte\u0026#34;: \u0026#34;2015-01-01 00:00:00\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;now\u0026#34;, \u0026#34;time_zone\u0026#34;: \u0026#34;+01:00\u0026#34; } } } } \u0026#39; 3.2.4 Exsit Query 在特定的字段中查找非空值的文档\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;exists\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;user\u0026#34; } } } \u0026#39; 3.2.5 Prefix Query 查找包含带有指定前缀的term的文档\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;prefix\u0026#34; : { \u0026#34;user\u0026#34; : \u0026#34;ki\u0026#34; } } } \u0026#39; 3.2.6 Wildcard Query 支持通配符查询，*表示任意字符，?表示任意单个字符\n这个查询效率比较慢\ncurl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;wildcard\u0026#34; : { \u0026#34;user\u0026#34; : \u0026#34;ki*y\u0026#34; } } } \u0026#39; 3.2.7 Regexp Query curl -X GET \u0026#34;localhost:9200/_search\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;query\u0026#34;: { \u0026#34;regexp\u0026#34;:{ \u0026#34;name.first\u0026#34;: \u0026#34;s.*y\u0026#34; } } } \u0026#39; prefix 和 wildcard 和 regexp 三者工作方式是一样的 ,需要扫描倒排索引中的词列表才能找到所有匹配的词，然后依次获取每个词相关的文档 ID\n性能对比: prefix \u0026gt; wildcard \u0026gt; regexp\nwildcard 和 regexp要避免左模糊查询\n官网 3.3 聚合查询 Elasticsearch 的聚合（Aggregations）功能十分强大，允许在数据上做复杂的分析统计。Elasticsearch 提供的聚合分析功能主要有指标聚合（metrics aggregations）、桶聚合（bucket aggregations）、管道聚合（pipeline aggregations） 三大类，\n所有的聚合，无论它们是什么类型，都遵从以下的规则。\n使用查询中同样的 JSON 请求来定义它们，而且你是使用键 aggregations 或者是 aggs 来进行标记。需要给每个聚合起一个名字，指定它的类型以及和该类型相关的选项。 它们运行在查询的结果之上。和查询不匹配的文档不会计算在内，除非你使用 global 聚集将不匹配的文档囊括其中。 可以进一步过滤查询的结果，而不影响聚集。 以下是聚合的基本结构：\n\u0026#34;aggregations\u0026#34; : { \u0026lt;!-- 最外层的聚合键，也可以缩写为 aggs --\u0026gt; \u0026#34;\u0026lt;aggregation_name\u0026gt;\u0026#34; : { \u0026lt;!-- 聚合的自定义名字 --\u0026gt; \u0026#34;\u0026lt;aggregation_type\u0026gt;\u0026#34; : { \u0026lt;!-- 聚合的类型，指标相关的，如 max、min、avg、sum，桶相关的 terms、filter 等 --\u0026gt; \u0026lt;aggregation_body\u0026gt; \u0026lt;!-- 聚合体：对哪些字段进行聚合，可以取字段的值，也可以是脚本计算的结果 --\u0026gt; } [,\u0026#34;meta\u0026#34; : { [\u0026lt;meta_data_body\u0026gt;] } ]? \u0026lt;!-- 元 --\u0026gt; [,\u0026#34;aggregations\u0026#34; : { [\u0026lt;sub_aggregation\u0026gt;]+ } ]? \u0026lt;!-- 在聚合里面在定义子聚合 --\u0026gt; } [,\u0026#34;\u0026lt;aggregation_name_2\u0026gt;\u0026#34; : { ... } ]* \u0026lt;!-- 聚合的自定义名字 2 --\u0026gt; } 在最上层有一个 aggregations 的键，可以缩写为 aggs。 在下面一层，需要为聚合指定一个名字。可以在请求的返回中看到这个名字。在同一个请求中使用多个聚合时，这一点非常有用，它让你可以很容易地理解每组结果的含义。 最后，必须要指定聚合的类型。 关于聚合分析的值来源，可以取字段的值，也可以是脚本计算的结果。\n但是用脚本计算的结果时，需要注意脚本的性能和安全性；尽管多数聚集类型允许使用脚本，但是脚本使得聚集变得缓慢，因为脚本必须在每篇文档上运行。为了避免脚本的运行，可以在索引阶段进行计算。\n此外，脚本也可以被人可能利用进行恶意代码攻击，尽量使用沙盒（sandbox）内的脚本语言。\ntext类型默认不支持聚合\n查询所有球员的平均年龄是多少，并对球员的平均薪水加 188（也可以理解为每名球员加 188 后的平均薪水）。\nPOST /player/_search?size=0 { \u0026#34;aggs\u0026#34;: { \u0026#34;avg_age\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34; } }, \u0026#34;avg_salary_188\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;doc.salary.value + 188\u0026#34; } } } } } // 默认情况下，包含聚合的搜索会返回搜索命中和聚合结果, size 表示返回命中的文档数量,设为0表示只返回聚合结果 3.3.1 Metrics aggregations 指标聚合（又称度量聚合）主要从不同文档的分组中提取统计数据，或者，从来自其他聚合的文档桶来提取统计数据。\n这些统计数据通常来自数值型字段，如最小或者平均价格。用户可以单独获取每项统计数据，或者也可以使用 stats 聚合来同时获取它们。更高级的统计数据，如平方和或者是标准差，可以通过 extended stats 聚合来获取。\n3.3.1.1 Min Aggregation Min Aggregation 用于最小值统计。例如，统计 sales 索引中价格最低的是哪本书，查询语句如下：\nGET /sales/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;min_price\u0026#34; : { \u0026#34;min\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;price\u0026#34; } } } } 聚合结果如下：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;min_price\u0026#34;: { \u0026#34;value\u0026#34;: 18.0 } } } 3.3.1.2 Max Aggregation Max Aggregation 用于最大值统计。例如，统计 sales 索引中type是hat的且价格最高的是哪本书，并且计算出对应的价格的 2 倍值，查询语句如下：\nGET /sales/_search?size=0 { \u0026#34;query\u0026#34; : { \u0026#34;constant_score\u0026#34; : { \u0026#34;filter\u0026#34; : { \u0026#34;match\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;hat\u0026#34; } } } }, \u0026#34;aggs\u0026#34; : { \u0026#34;max_price\u0026#34; : { \u0026#34;max\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;price\u0026#34; , \u0026#34;missing\u0026#34;: 60 } }, \u0026#34;max_price_2\u0026#34; : { \u0026#34;max\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;price\u0026#34;, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;_value * 2.0\u0026#34; } } } } } 指定的 field，在脚本中可以用 _value 取字段的值。\n如果指定字段没有值，可以通过 missing 指定默认值；若未指定默认值，缺失该字段值的文档将被忽略（计算）。\n聚合结果如下：\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;max_price\u0026#34;: { \u0026#34;value\u0026#34;: 188.0 }, \u0026#34;max_price_2\u0026#34;: { \u0026#34;value\u0026#34;: 376.0 } } } 3.3.1.3 Avg Aggregation Avg Aggregation 用于计算平均值。例如，统计 exams 索引中考试的平均分数，\nGET /exams/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;avg_grade\u0026#34; : { \u0026#34;avg\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;grade\u0026#34; } } } } { ... \u0026#34;aggregations\u0026#34;: { \u0026#34;avg_grade\u0026#34;: { \u0026#34;value\u0026#34;: 78.0 } } } 除了常规的平均值聚合计算外，elasticsearch 还提供了加权平均值的聚合计算， 详情参见 Elasticsearch 指标聚合之 Weighted Avg Aggregation 。\n用加权平均值代替原本值\n3.3.1.4 Sum Aggregation Sum Aggregation 用于计算总和。\nGET /exams/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;hat_prices\u0026#34; : { \u0026#34;sum\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;price\u0026#34; } } } } { ... \u0026#34;aggregations\u0026#34;: { \u0026#34;hat_prices\u0026#34;: { \u0026#34;value\u0026#34;: 567.0 } } } 3.3.1.5 Count Aggregation Count Aggregation 可按字段统计文档数量。例如，统计 books 索引中包含 author 字段的文档数量，查询语句如下：\nGET /books/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;doc_count\u0026#34; : { \u0026#34;value_count\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;author\u0026#34; } } } } { ... \u0026#34;aggregations\u0026#34;: { \u0026#34;doc_count\u0026#34;: { \u0026#34;value\u0026#34;: 5 } } } 3.3.1.6 Cardinality Aggregation Cardinality Aggregation 用于基数统计，其作用是先执行类似 SQL 中的 distinct 操作，去掉集合中的重复项，然后统计排重后的集合长度。\n例如，在 books 索引中对 language 字段进行 cardinality 操作可以统计出编程语言的种类数，查询语句如下：\nGET /books/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;all_lan\u0026#34; : { \u0026#34;cardinality\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;language\u0026#34; } }, \u0026#34;title_cnt\u0026#34; : { \u0026#34;cardinality\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;title.keyword\u0026#34; } } } } 假设 title 字段为文本类型（text），去重时需要指定 keyword，表示把 title 作为整体去重，即不分词统计。\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;all_lan\u0026#34;: { \u0026#34;value\u0026#34;: 8 }, \u0026#34;title_cnt\u0026#34;: { \u0026#34;value\u0026#34;: 18 } } } 3.3.1.7 Stats Aggregation Stats Aggregation 用于基本统计，会一次返回 count、max、min、avg 和 sum 这 5 个指标。例如，在 exams 索引中对 grade 字段进行分数相关的基本统计，查询语句如下：\nGET /exams/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;grades_stats\u0026#34; : { \u0026#34;stats\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;grade\u0026#34; } } } } 3.3.1.8 Extended Stats Aggregation Extended Stats Aggregation 用于高级统计，和基本统计功能类似，但是会比基本统计多出以下几个统计结果，sum_of_squares（平方和）、variance（方差）、std_deviation（标准差）、std_deviation_bounds（平均值加/减两个标准差的区间）。在 exams 索引中对 grade 字段进行分数相关的高级统计，查询语句如下：\nGET /exams/_search?size=0 { \u0026#34;aggs\u0026#34; : { \u0026#34;grades_stats\u0026#34; : { \u0026#34;extended_stats\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;grade\u0026#34; } } } } 3.3.1.9 Percentiles Aggregation Percentiles Aggregation 用于百分位统计。百分位数是一个统计学术语，如果将一组数据从大到小排序，并计算相应的累计百分位，某一百分位所对应数据的值就称为这一百分位的百分位数。默认情况下，累计百分位为 [ 1, 5, 25, 50, 75, 95, 99 ],也可以用percents指定范围。以下例子给出了在 latency 索引中对 load_time 字段进行加载时间的百分位统计，查询语句如下：\nGET latency/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34; : { \u0026#34;load_time_outlier\u0026#34; : { \u0026#34;percentiles\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;load_time\u0026#34; , \u0026#34;percents\u0026#34;:[30,45] //查询第30%和45%的数据 } } } } 需要注意的是，如上的 load_time 字段必须是数字类型。\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;load_time_outlier\u0026#34;: { \u0026#34;values\u0026#34; : { \u0026#34;1.0\u0026#34;: 5.0, \u0026#34;5.0\u0026#34;: 25.0, \u0026#34;25.0\u0026#34;: 165.0, \u0026#34;50.0\u0026#34;: 445.0, \u0026#34;75.0\u0026#34;: 725.0, \u0026#34;95.0\u0026#34;: 945.0, \u0026#34;99.0\u0026#34;: 985.0 } } } } 第1%位的数据是5\n第5%位的数据是25\n\u0026hellip;\u0026hellip;\n第99%位的数据是99\n3.3.1.10 Percentiles Ranks Aggregation Percentiles Ranks Aggregation 与 Percentiles Aggregation 统计恰恰相反，就是想看当前数值处在什么范围内（百分位）， 假如你查一下当前值 500 和 600 所处的百分位，发现是 90.01 和 100，那么说明有 90.01 % 的数值都在 500 以内，100 % 的数值在 600 以内。\nGET latency/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34; : { \u0026#34;load_time_ranks\u0026#34; : { \u0026#34;percentile_ranks\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;load_time\u0026#34;, \u0026#34;values\u0026#34; : [500, 600] } } } } 同样 load_time 字段必须是数字类型。\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;load_time_ranks\u0026#34;: { \u0026#34;values\u0026#34; : { \u0026#34;500.0\u0026#34;: 90.01, \u0026#34;600.0\u0026#34;: 100.0 } } } } 可以设置 keyed 参数为 true，将对应的 values 作为桶 key 一起返回，默认是 false。\nGET latency/_search { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;load_time_ranks\u0026#34;: { \u0026#34;percentile_ranks\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;load_time\u0026#34;, \u0026#34;values\u0026#34;: [500, 600], \u0026#34;keyed\u0026#34;: true } } } } { ... \u0026#34;aggregations\u0026#34;: { \u0026#34;load_time_ranks\u0026#34;: { \u0026#34;values\u0026#34;: [ { \u0026#34;key\u0026#34;: 500.0, \u0026#34;value\u0026#34;: 90.01 }, { \u0026#34;key\u0026#34;: 600.0, \u0026#34;value\u0026#34;: 100.0 } ] } } } 3.3.2 bucket aggregations bucket 可以理解为一个桶，它会遍历文档中的内容，凡是符合某一要求的就放入一个桶中，分桶相当于 SQL 中的 group by。从另外一个角度，可以将指标聚合看成单桶聚合，即把所有文档放到一个桶中，而桶聚合是多桶型聚合，它根据相应的条件进行分组。\n桶数最多 65536 个,可修改\n3.3.2.1 Terms Aggregation Terms Aggregation 用于词项的分组聚合。最为经典的用例是获取 X 中最频繁（top frequent）的项目，其中 X 是文档中的某个字段，如用户的名称、标签或分类。由于 terms 聚集统计的是每个词条，而不是整个字段值，因此通常需要在一个非分析型的字段上运行这种聚集。原因是, 你期望“big data”作为词组统计，而不是“big”单独统计一次，“data”再单独统计一次。\n支持字段类型 Keyword, Numeric, ip, boolean, binary.\n用户可以使用 terms 聚集，从分析型字段（如内容）中抽取最为频繁的词条。还可以使用这种信息来生成一个单词云。\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;profit_terms\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;skuName\u0026#34; } } } } 结果:\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;genres\u0026#34;: { \u0026#34;doc_count_error_upper_bound\u0026#34;: 0, // 错误文档的个数 \u0026#34;sum_other_doc_count\u0026#34;: 0, // 未统计的文档个数 \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;electronic\u0026#34;, \u0026#34;doc_count\u0026#34;: 6 }, { \u0026#34;key\u0026#34;: \u0026#34;rock\u0026#34;, \u0026#34;doc_count\u0026#34;: 3 }, { \u0026#34;key\u0026#34;: \u0026#34;jazz\u0026#34;, \u0026#34;doc_count\u0026#34;: 2 } ] } } } 返回值中,默认按doc_count数量降序排序, 可以用order字段指定排序规则\n可以用size字段控制返回的桶数\n3.2.2.2 Filter Aggregation Filter Aggregation 是过滤器聚合，可以把符合过滤器中的条件的文档分到一个桶中，即是单分组聚合。\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;age_terms\u0026#34;: { \u0026#34;filter\u0026#34;: {\u0026#34;term\u0026#34;:{\u0026#34;gender\u0026#34;:\u0026#34;F\u0026#34;}}, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_age\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34; } } } } } } 在聚合操作中加入过滤,比 过滤后再聚合要差, 上面的效率比下面的效率要差\nPOST /sales/_search?size=0\u0026amp;filter_path=aggregations { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;t-shirt\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_price\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34; } } } } 3.2.2.3 Filters Aggregation Filters Aggregation 是多过滤器聚合，可以把符合多个过滤条件的文档分到不同的桶中，即每个分组关联一个过滤条件，并收集所有满足自身过滤条件的文档。\n{ \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;messages\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;filters\u0026#34;: { \u0026#34;errors\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;body\u0026#34;: \u0026#34;error\u0026#34; } }, \u0026#34;warnings\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;body\u0026#34;: \u0026#34;warning\u0026#34; } } } } } } } 在这个例子里，我们分析日志信息。聚合会创建两个关于日志数据的分组，一个收集包含错误信息的文档，另一个收集包含告警信息的文档。而且每个分组会按月份划分。\nfilters 写多个过滤器比 写过filter效率好\n3.2.2.4 Range Aggregation Range Aggregation 范围聚合是一个基于多组值来源的聚合，可以让用户定义一系列范围，每个范围代表一个分组。在聚合执行的过程中，从每个文档提取出来的值都会检查每个分组的范围，并且使相关的文档落入分组中。注意，范围聚合的每个范围内包含 from 值但是排除 to 值。\n{ \u0026#34;aggs\u0026#34;: { \u0026#34;age_range\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;ranges\u0026#34;: [{ \u0026#34;to\u0026#34;: 25 }, { \u0026#34;from\u0026#34;: 25, \u0026#34;to\u0026#34;: 35 }, { \u0026#34;from\u0026#34;: 35 }] }, \u0026#34;aggs\u0026#34;: { \u0026#34;bmax\u0026#34;: { \u0026#34;max\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;balance\u0026#34; } } } } } } } 3.2.2.5 Date Range Aggr 日期范围聚合是专用于日期值的范围聚合。该聚合和正常的 范围 聚合的区别主要在于：该聚合可以用 日期数学 表达式表示 from 值 和 to 值，还可以指定 返回 from 和 to 响应字段的日期格式。注意，该聚合包含 from 值，但不包含 to 值。(左闭右开的区间)\nPOST /sales/_search?size=0 { \u0026#34;aggs\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;date_range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;MM-yyyy\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: \u0026#34;now-10M/M\u0026#34; }, { \u0026#34;from\u0026#34;: \u0026#34;now-10M/M\u0026#34; } ] } } } } to: \u0026lt; 现在减去 10 个月，向下舍入到月初\nfrom: \u0026gt;= 现在减去 10 个月，向下舍入到月初\n在上面的例子中，我们创建了两个范围桶，第一个桶会将早于 10 个月之前的所有文档存储，第二个桶会将从 10 月之前开始的文档存储。\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;to\u0026#34;: 1.4436576E12, \u0026#34;to_as_string\u0026#34;: \u0026#34;10-2015\u0026#34;, \u0026#34;doc_count\u0026#34;: 7, \u0026#34;key\u0026#34;: \u0026#34;*-10-2015\u0026#34; }, { \u0026#34;from\u0026#34;: 1.4436576E12, \u0026#34;from_as_string\u0026#34;: \u0026#34;10-2015\u0026#34;, \u0026#34;doc_count\u0026#34;: 0, \u0026#34;key\u0026#34;: \u0026#34;10-2015-*\u0026#34; } ] } } } 时间格式和java的一样\nes-中文文档 3.2.2.6 Range Aggr 基于多桶值源的聚合，使用户能够定义一组范围 - 每个范围代表一个桶。 在聚合过程中，将从每个文档中提取的值根据每个存储区范围进行检查，并将相关/匹配文档“存储”到“存储区”中。 请注意，该聚合包含 from 值，但不包含 to 值\nGET /_search { \u0026#34;aggs\u0026#34;: { \u0026#34;price_ranges\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;price\u0026#34;, \u0026#34;ranges\u0026#34;: [ { \u0026#34;to\u0026#34;: 100.0 }, { \u0026#34;from\u0026#34;: 100.0, \u0026#34;to\u0026#34;: 200.0 }, { \u0026#34;from\u0026#34;: 200.0 } ] } } } } 结果:\n{ ... \u0026#34;aggregations\u0026#34;: { \u0026#34;price_ranges\u0026#34;: { \u0026#34;buckets\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;*-100.0\u0026#34;, \u0026#34;to\u0026#34;: 100.0, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;100.0-200.0\u0026#34;, \u0026#34;from\u0026#34;: 100.0, \u0026#34;to\u0026#34;: 200.0, \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;200.0-*\u0026#34;, \u0026#34;from\u0026#34;: 200.0, \u0026#34;doc_count\u0026#34;: 3 } ] } } } 3.4 复合查询 复合查询包装其他复合查询或叶查询，以组合它们的结果和分数，改变它们的行为，或者从查询切换到过滤上下文。\n简单的说是 将多种查询条件组合到一起\n3.4.1 Bool query filter\n子句在过滤器上下文中执行，这意味着计分被忽略，并且子句被视为用于缓存。(过滤掉不要的数据)\nmust\n子句（查询）必须出现在匹配的文档中，并将有助于得分。(用来做匹配的,表示需要出现在文档里)\nmust_not\n子句（查询）不得出现在匹配的文档中。子句在过滤器上下文中执行，这意味着计分被忽略，并且子句被视为用于缓存。\nshould\n子句（查询）应出现在匹配的文档中。【注意should的最小匹配数】\n关于should子句，特别要注意：\n如果这个布尔查询位于query context，并且有must或者filter子句，那么即使should子句没有匹配任何文档，也没关系\n如果是位于filter context，或者既没有must也没有filter，那么至少有\u0026quot;指定\u0026quot;个should查询必须匹配文档。这个行为可以通过设置minimum_should_match参数来显式地控制。(至少满足几个should子句), 可以是正负整数/正负百分比等各种比例 (默认值是0)\n参数详情 GET product/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;sellPrice\u0026#34;: { \u0026#34;gte\u0026#34;: 1 } } } ], \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;skuName\u0026#34;: \u0026#34;空调\u0026#34; } } ] } } , \u0026#34;_source\u0026#34;: \u0026#34;skuName\u0026#34; } // 查询skuName中\u0026#34;空调\u0026#34;的文档,并且过滤出sellPrice大于等于1的文档 filter 子句类可包含 bool query，实现更复杂的逻辑, (俄罗斯套娃之深圳分套)\n// 多个条件 GET service-java-logs-2021.07.16/_search { \u0026#34;_source\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match_phrase\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;ERROR\u0026#34; } } ], \u0026#34;must_not\u0026#34;: [ { \u0026#34;wildcard\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;*navigation*\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;defaultParameterMap\u0026#34; } }, { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;parameters\u0026#34; } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;log.file.path.keyword\u0026#34;: \u0026#34;/opt/online-shop-manage/logs/online-shop-manage-provider.log\u0026#34; } } ] } } } 3.4.2 boosting query 来源:\nhttps://www.cnblogs.com/hong-fithing/p/11221020.html http://www.ruanyifeng.com/blog/2017/08/elasticsearch.html https://www.cnblogs.com/hirampeng/p/10035858.html https://www.cnblogs.com/cjsblog/p/9910788.html 微信es系列 https://www.knowledgedict.com/tutorial/elasticsearch-sort.html ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%91%BD%E4%BB%A4.html","summary":"[toc] 前言 本文以命令行为主 其本质还是发送http请求去操作数据 1. 基本格式 es是以RESTFul风格来命名API的，其API的基本格式如下： htt","title":"命令"},{"content":"[toc]\n1. kubectl 使用指南 kubectl 是 Kubernetes 自带的客户端，可以用它来直接操作 Kubernetes集群。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数\u0026ndash;kubeconfig 来指定其他位置的 kubeconfig 文件。\nkubectl 并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。\n从用户角度来说，kubectl 就是控制 Kubernetes 的驾驶舱，它允许你执行所有可能的 Kubernetes 操作；从技术角度来看，kubectl 就是 Kubernetes API 的一个客户端而已。\n使用以下语法 kubectl 从终端窗口运行命令： kubectl [command] [TYPE] [NAME] [flags]\ncommand：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 TYPE：指定**资源类型 **。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果: kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAME：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息 kubectl get pods。\nflags: 指定可选的参数。例如，可以使用 -s 或 -server 参数指定 Kubernetes API 服务器的地址和端口。\n在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：\n要按类型和名称指定资源： kubectl get pod pod1 要对所有类型相同的资源进行分组，请执行以下操作：TYPE name1 name2 。 例子：kubectl get pod example-pod1 example-pod2 分别指定多个资源类型：TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE4/name4。 例子：kubectl get pod/example-pod1 replicationcontroller/example-rc1 用一个或多个文件指定资源：-f file1 -f file2 -f file3 使用 YAML 而不是 JSON 因为 YAML 更容易使用，特别是用于配置文件时。 例子：kubectl get -f pod.yaml 2. 简单部署 2.1 部署 Pod 通过 kubectl 部署 Pod 的办法分为两步：\n准备 Pod 的 yaml 文件；\n执行 kubectl 命令部署\n2.1.1 pod 的yaml文件 apiVersion: v1 kind: Pod metadata: name: memory-demo namespace: mem-example spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: \u0026#34;200Mi\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] volumeMounts: - name: redis-storage mountPath: /data/redis volumes: - name: redis-storage emptyDir: {} apiVersion记录 K8S 的 API Server 版本，现在看到的都是v1，用户不用管。\nkind记录该 yaml 的对象，比如这是一份 Pod 的 yaml 配置文件，那么值内容就是Pod。\nmetadata记录了 Pod 自身的元数据，比如这个 Pod 的名字、这个 Pod 属于哪个 namespace\nspec记录了 Pod 内部所有的资源的详细信息，看懂这个很重要：\ncontainers记录了 Pod 内的容器信息，\ncontainers包括了：name容器名，image容器的镜像地址，resources容器需要的 CPU、内存、GPU 等资源，command容器的入口命令，args容器的入口参数，volumeMounts容器要挂载的 Pod 数据卷等。limits是 K8S 为该容器至多分配的资源配额；而requests则是 K8S 为该容器至少分配的资源配额\nvolumes记录了 Pod 内的数据卷信息，后文会详细介绍 Pod 的数据卷。\n2.1.2 执行 kubectl 命令部署 有了 Pod 的 yaml 文件之后，就可以用 kubectl 部署了，命令非常简单：kubectl create -f ${POD_YAML}。\n2.2 部署 Deployment 2.2.1 Deployment 的 yaml 文件 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: rss-site namespace: mem-example spec: replicas: 2 template: metadata: labels: app: web spec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: emory: \u0026#34;200Mi\u0026#34; requests: memory: \u0026#34;100Mi\u0026#34; command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] volumeMounts: - name: redis-storage mountPath: /data/redis volumes: - name: redis-storage emptyDir: {} metadata同 Pod 的 yaml，这里提一点：如果没有指明 namespace，那么就是用 kubectl 默认的 namespace（如果 kubectl 配置文件中没有指明 namespace，那么就是 default 空间）。\nspec，可以看到 Deployment 的spec字段是在 Pod 的spec内容外“包了一层”，那就来看 Deployment 有哪些需要注意的：\nreplicas。副本个数。也就是该 Deployment 需要起多少个相同的 Pod，如果用户成功在 K8S 中配置了 n（n\u0026gt;1）个，那么 Deployment 会确保在集群中始终有 n 个服务在运行。\ntemplate。\nmetadata，新手同学先不管这边的信息。 spec，会发现这完完全全是上文提到的 Pod 的spec内容，在这里写明了 Deployment 下属管理的每个 Pod 的具体内容。 2.2.2 执行 kubectl 命令部署 Deployment 的部署办法同 Pod：kubectl create -f ${DEPLOYMENT_YAML}。由此可见，K8S 会根据配置文件中的kind字段来判断具体要创建的是什么资源。\n部署完 deployment 之后，可以查看到自动创建了 ReplicaSet 和 Pod，\n如下图所示：\n从名字上可以看出,deployment和 replicaSet还有Pod三者的名字是一个累加的方式, 所以看pod的名称, 可以看到deployment和replicaSet的名字是什么\n命令行工具 (kubectl) | Kubernetes Kubernetes 入门\u0026amp;进阶实战 - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/kubernetes/%E5%91%BD%E4%BB%A4.html","summary":"[toc] 1. kubectl 使用指南 kubectl 是 Kubernetes 自带的客户端，可以用它来直接操作 Kubernetes集群。Kubectl 的配置文件在$HOME/.kube 目录。我们可以","title":"命令"},{"content":"[toc]\n1. 大纲 1、根据Java虚拟机规范，Java虚拟机所管理的内存包括方法区、虚拟机栈、本地方法栈、堆、程序计数器等。\n2、我们通常认为JVM中运行时数据存储包括堆和栈。这里所提到的栈其实指的是虚拟机栈，或者说是虚拟栈中的局部变量表。\n3、栈中存放一些基本类型的变量数据（int/short/long/byte/float/double/Boolean/char）和对象引用。\n4、堆中主要存放对象，即通过new关键字创建的对象。\n5、数组引用变量是存放在栈内存中，数组元素是存放在堆内存中。\nhotspot jdk8中移除了永久代以后的内存结构\nJVM结构及堆的划分 - 光何 - 博客园 (cnblogs.com) 2. 方法区(元空间/永久代) 怎么回收对象? 《Java虚拟机规范》 中提到过可以不要求虚拟机在方法区中实现垃圾收集， 事实上也确实有未实现或未能完整实现方法区类型卸载的收集器存在（如JDK 11时期的ZGC收集器就不支持类卸载）\n方法区的垃圾收集主要回收两部分内容： 废弃的常量和不再使用的类型\n回收废弃常量与回收Java堆中的对象非常类似。 举个常量池中字面量回收的例子， 假如一个字符串“java”曾经进入常量池中， 但是当前系统又没有任何一个字符串对象的值是“java”， 换句话说， 已经没有任何字符串对象引用常量池中的“java”常量， 且虚拟机中也没有其他地方引用这个字面量。 如果在这时发生内存回收， 而且垃圾收集器判断确有必要的话， 这个“java”常量就将会被系统清理出常量池。 常量池中其他类（接口） 、 方法、 字段的符号引用也与此类似\n判定一个类型是否属于“不再被使用的类”的条件需要同时满足下面三个条件：\n该类所有的实例都已经被回收， 也就是Java堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收， 这个条件除非是经过精心设计的可替换类加载器的场景， 如OSGi、 JSP的重加载等， 否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用， 无法在任何地方通过反射访问该类的方法 Java虚拟机被允许对满足上述三个条件的无用类进行回收， 这里说的仅仅是“被允许”， 而并不是和对象一样， 没有引用了就必然会回收 ,关于是否要对类型进行回收 , HotSpot虚拟机提供了-Xnoclassgc参数进行控制\n在大量使用反射、 动态代理、 CGLib等字节码框架， 动态生成JSP以及OSGi这类频繁自定义类加载器的场景中， 通常都需要Java虚拟机具备类型卸载的能力， 以保证不会对方法区造成过大的内存压力。\n3. 堆的分代 Java虚拟机根据对象存活的周期不同，把堆内存划分为几块，一般分为新生代、老年代和永久代（对HotSpot虚拟机而言），这就是JVM的内存分代策略。\nJava虚拟机将堆内存划分为新生代、老年代和永久代，永久代是HotSpot虚拟机特有的概念（JDK1.8之后为metaspace替代永久代），它采用永久代的方式来实现方法区，其他的虚拟机实现没有这一概念，永久代主要存放常量、类信息、静态变量等数据，与垃圾回收关系不大，新生代和老年代是垃圾回收的主要区域。\n3.1 新生代（Young Generation） 新生成的对象优先存放在新生代中，新生代对象朝生夕死，存活率很低，在新生代中，常规应用进行一次垃圾收集一般可以回收70% ~ 95% 的空间，回收效率很高。 HotSpot将新生代划分为三块，一块较大的Eden（伊甸）空间和两块较小的Survivor（幸存者）空间，默认比例为8：1：1。划分的目的是因为HotSpot采用复制算法来回收新生代，设置这个比例是为了充分利用内存空间，减少浪费。新生成的对象在Eden区分配（大对象除外，大对象直接进入老年代），当Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。 GC开始时，对象只会存在于Eden区和From Survivor区，To Survivor区是空的（作为保留区域）。GC进行时，Eden区中所有存活的对象都会被复制到To Survivor区，而在From Survivor区中，仍存活的对象会根据它们的年龄值决定去向，要么移到老年代要么复制到To survivor区\n年龄值达到年龄阀值（默认为15，新生代中的对象每熬过一轮垃圾回收，年龄值就加1，GC分代年龄存储在对象的header中）的对象会被移到老年代中，没有达到阀值的对象会被复制到To Survivor区。接着清空Eden区和From Survivor区，新生代中存活的对象都在To Survivor区。\n接着， From Survivor区和To Survivor区会交换它们的角色，也就是说新的To Survivor区就是上次GC清空的From Survivor区，新的From Survivor区就是上次GC的To Survivor区，总之，不管怎样都会保证To Survivor区在一轮GC后是空的。GC时当To Survivor区没有足够的空间存放上一次新生代收集下来的存活对象时，需要依赖老年代进行分配担保，将这些对象存放在老年代中。\n“对象从新生代进入到老年代”的四种情况\nMinor GC/Young GC 时，To Survivor 区不足以存放存活的对象，对象会直接进入到老年代。 经过多次 Minor GC/Young GC 后，如果存活对象的年龄达到了设定阈值，则会晋升到老年代中。 动态年龄判定规则，To Survivor 区中相同年龄的对象，如果其大小之和占到了 To Survivor 区一半以上的空间，那么大于此年龄的对象会直接进入老年代，而不需要达到默认的分代年龄。 大对象：由-XX:PretenureSizeThreshold启动参数控制，若对象大小大于此值，就会绕过新生代，直接在老年代中分配。 (默认值是0 , 表示所有对象都放eden区, 超过eden区剩余大小就放老年代; 这个参数只对 Serial 和ParNew的单线程版收集器有效) -XX:PretenureSizeThreshold 的默认值和作用 - 简书 (jianshu.com) JVM的垃圾回收 - 简书 (jianshu.com) 3.2 老年代（Old Generationn） 在新生代中经历了多次（具体看虚拟机配置的阀值）GC后仍然存活下来的对象会进入老年代中。老年代中的对象生命周期较长，存活率比较高，在老年代中进行GC的频率相对而言较低，而且回收的速度也比较慢。\n这里会发生Full GC , 产生STW(stop the world)\n新生代和老年代的空间配比是 1 : 2\n什么时候会触发Full GC（建议收藏） - 知乎 (zhihu.com) 3.3 永久代（Permanent Generationn） 永久代存储类信息、常量、静态变量、即时编译器编译后的代码等数据，对这一区域而言，Java虚拟机规范指出可以不进行垃圾收集，一般而言不会进行垃圾回收。\nJVM结构及堆的划分 - 光何 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F.html","summary":"[toc] 1. 大纲 1、根据Java虚拟机规范，Java虚拟机所管理的内存包括方法区、虚拟机栈、本地方法栈、堆、程序计数器等。 2、我们通常认为JVM中运","title":"内存区域"},{"content":"[toc]\n快速排序 排序流程:\n快速排序算法通过多次比较和交换来实现排序，其排序流程如下：\n首先设定一个分界值，通过该分界值将数组分成左右两部分。\n将大于或等于分界值的数据集中到数组右边，小于分界值的数据集中到数组的左边。此时，左边部分中各元素都小于或等于分界值，而右边部分中各元素都大于或等于分界值。\n然后，左边和右边的数据可以独立排序。对于左侧的数组数据，又可以取一个分界值，将该部分数据分成左右两部分，同样在左边放置较小值，右边放置较大值。右侧的数组数据也可以做类似处理。\n重复上述过程，可以看出，这是一个递归定义。通过递归将左侧部分排好序后，再递归排好右侧部分的顺序。当左、右两个部分各数据排序完成后，整个数组的排序也就完成了。\n时间复杂度和空间复杂度平均值都是O(log2n)\n计数排序 它的优势在于在对一定范围内的整数排序时，它的复杂度为Ο(n+k)（其中k是整数的范围）,此时快于任何比较排序算法,计数排序算法是一个稳定的排序算法,快排不是\n排序过程(十分巧妙运用数组):\n假设输入的线性表L的长度为n，L=L1,L2,..,Ln；线性表的元素属于有限偏序集S，|S|=k且k=O(n)，S={S1,S2,..Sk}；则计数排序可以描述如下：\n1、扫描整个集合S，对每一个Si∈S，找到在线性表L中小于等于Si的元素的个数T(Si)；\n2、扫描整个线性表L，对L中的每一个元素Li，将Li放在输出线性表的第T(Li)个位置上，并将T(Li)减1。\n举例:\n​\t假设要排序的数组为 A = {1,0,3,1,0,1,1}\n这里最大值为3，最小值为0，那么我们创建一个数组C，长度为4.\n然后一趟扫描数组A，得到A中各个元素的总数，并保持到数组C的对应单元中。\n比如0 的出现次数为2次，则 C[0] = 2;1 的出现次数为4次，则C[1] = 4, (求出现次数也不麻烦,因为值成为了下标,那对下标对应的值累计就行了) 最终c数组如图:\n由于C 是以A的元素为下标的，所以这样一做，A中的元素在C中自然就成为有序的了，这里我们可以知道 顺序为 0,1,3 (2 的计数为0),\n然后我们把这个在C中的记录按每个元素的计数展开到输出数组B中，排序就完成了。\n也就是 B[0] 到 B[1] 为0 B[2] 到 B[5] 为1 这样依此类推。(累加就行)\n时间复杂度为: O(nlogn)\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95.html","summary":"[toc] 快速排序 排序流程: 快速排序算法通过多次比较和交换来实现排序，其排序流程如下： 首先设定一个分界值，通过该分界值将数组分成左右两部分。 将大于或","title":"排序算法"},{"content":"**1.**为什么说比特币是高度透明和隐秘的?使用比特币交易后可不可以查看交易双方?通过比特币地址为什么不能查找到人?\n答:(1) 透明是说所有的交易信息每人都能查看,隐秘是说:交易双方的信息都是加密的,\n​ (2)可以查看,只能查看最近的交易记录,双方的标识id会变化(加密)\n(3)区块链技术作为比特币的底层支撑，可以清晰的记录并查到所有比特币地址、支付以及交易路径，通过分析交易模式，追查到资金的走向以及公钥背后实际的当事人是很有可能的。黑客如果不会洗钱(币),钱就等于是死钱了\n来自 http://www.wanbizu.com/news/201705229806.html ,\n**2.**每产生新的比特币,新的比特币会记录以前的记录吗?如果记录,随着记录的增多,以后产生的比特币会不会疲于更新?如果不同步,也就说明区块链的内容不一样?\n答:(1)当前块只会记录(从生产开始算)10分钟内的交易内容\n(2)内容的确不一样,但是都是承接的\n\u0026ldquo;可以这么理解，比特币系统是一个巨大的、不断更新的账本。每一页都叫做一个区块，按照时间顺序连起来，就叫做比特币的区块链。每10分钟新增一个区块，里面的内容是过去10分钟系统内发生的一些交易。每一笔交易都会完完整整记录在这个账本里，比特币就是账本里记录的钱\u0026rdquo;\n来自* \u0026lt;http://tieba.baidu.com/p/5606568292 \u0026gt;\n3.51%算力攻击\n答:所谓51%攻击，就是利用比特币使用算力作为竞争条件的特点，使用算力优势撤销自己已经发生的付款交易。如果有人掌握了50%以上的算力，他能够比其他人更快地找到开采区块需要的那个随机数，因此他实际上拥有了绝对哪个一区块的有效权利\n来自* \u0026lt;http://8btc.com/article-1949-1.html \u0026gt;\n交易生效是需要下一个块生成才确定的,如果有人掌握50%算力,就有能力算出另一个(并列)块,因为算力够强,继续生成下一块,而区块链会选择最长的链作为主链,从而虚假块成为主链,真正的交易块被遗弃\n但是基本拥有51%算力基本不可能,目前算力是 236万万亿次哈希碰撞每秒 ,而世界第一的神威太湖之光 仅9.3亿亿次每秒,\n**4.**比特币同时多处交易,怎么保证数据的同步?(等6个块)\n答:每次交易只部分块记录(块会记录块产生后的10分钟内的所有交易记录),发生一次就在块中记录一次,然后hash不停在计算,更新后面的块,等6个块就完全确定了交易\n**5.**如果比特币只记录10分钟内的记录,那就意味着有些块记录特别少,有些块记录特别多?\n答: 是先有交易才会有块,等达到一定的交易量时才会被矿工拿着这些交易记录去挖矿,所以交易记录不会有重复\n一次交易流程大致如此:\n1.产生新交易: 我产生一个交易A\n2.签名加密: 验证这个交易是不是我发起的,钱是不是我的\n3.交易在比特币网络中传播: 验证完成后,要别人验证,是不是我的钱,钱够不够之类的\n4.整合交易\u0026amp;构建新区块: 验证交易后，每个比特币网络节点会将这些交易添加到自己的内存池中，内存池也称作交易池，用来暂存尚未被加入到区块的交易记录。而挖矿节点除了收集和验证交易以外，还会将这些交易打包到一个候选的区块中,会把交易A连同其它一些近期被创建的交易整合,打包.挖矿节点需要为内存池中的每笔交易分配一个优先级，并选择较高优先级的交易记录来构建候选区块，在区块被填满后，内存池中的剩余交易会成为下一个区块的候选交易。然后挖矿节点就准备拿候选区块来挖矿\n5.挖矿: 猜测一个数值(nonce),进行计算(相当复杂的),猜出一个小于nonce值就是正确的hash,即正确的块,获得\u0026quot;记账权\u0026quot;,那些交易记录将存在这个块中,节点(矿工)将获得奖励(网络中还有比特币时奖励比特币和交易费,没有比特币就奖励交易费)\n6.新区块连接到区块链:比特币交易生命周期的最后一步是将新区块连接至有最大工作量证明的链中。一个节点一旦验证了一个新的区块，它将尝试将新的区块连接到到现存的区块链组装起来。\n我有一个交易记录A,交易 A 会被一个或者多个签名加密(这些签名用来说明交易 A 是我发起的,不是别人)。而后，交易 A 被广播到比特币网络中，最快收到广播信息的是相邻的2- 3 个节点，这些节点都会参与验证这笔交易，于此同时将交易在网络中再次进行广播，直到这笔交易 A 被网络中大多数节点(所有下载比特币客户端的设备都有可能成为这样的节点)接收,最终，交易 A 被一个正在参与挖矿的节点验证，交易 A 连同其它一些近期被创建的交易一起被打包到一个区块 B ()中，并被添加到区块链上，这时整个区块链就被延长并新增了一个区块 B 。区块 B 获得 6 次以上的“确认”时就被认为是不可撤销的，\n来自* \u0026lt;http://www.yixieshi.com/110742.html \u0026gt;\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8C%BA%E5%9D%97%E9%93%BE.html","summary":"**1.**为什么说比特币是高度透明和隐秘的?使用比特币交易后可不可以查看交易双方?通过比特币地址为什么不能查找到人? 答:(1) 透明是说所有","title":"区块链"},{"content":"Spring Boot 默认使用了logback日志,里面引用的==SLF4J==,默认是带颜色的哦(老帅老帅了),甚至都不需要主动引包, …web等等包中有引用\n可以使用properties和logback.xml两种方式\nproperties logging.path=F:\\demo logging.file=demo.log logging.level.root=info # mybatis 打印返回值 mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl yml logging: level: io: netty: warn org: apache: error logback.xml:\n默认在resource文件夹中找 logback_spring.xml \u0026gt; logback.xml\n(推荐使用前者,springboot会提供更多的功能,例如:springProfile 节点,这个节点可以多环境输出:\n\u0026lt;!--测试环境+开发环境,多个使用逗号隔开--\u0026gt; \u0026lt;springProfile name=\u0026#34;test,dev\u0026#34;\u0026gt; \u0026lt;logger name=\u0026#34;com.dudu.controller\u0026#34; level=\u0026#34;info\u0026#34;/\u0026gt; \u0026lt;/springProfile\u0026gt; \u0026lt;!--生产环境--\u0026gt; \u0026lt;springProfile name=\u0026#34;prod\u0026#34;\u0026gt; \u0026lt;logger name=\u0026#34;com.dudu.controller\u0026#34; level = \u0026#34;error\u0026#34;/\u0026gt; \u0026lt;/springProfile\u0026gt; 可以启动服务的时候指定 profile （如不指定使用默认），如指定prod 的方式为： java -jar xxx.jar –spring.profiles.active=prod 或者 在配置文件中 spring.profiles.active=dev\n如果你的logback.xml有特殊的名字,可以指定(在application.properties中) logging.config=classpath:logging-config.xml\n使用:\nLogger logger = LoggerFactory.getLogger(this.getClass()); logger.info(\u0026#34;nihao\u0026#34;); 案例:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34;60 seconds\u0026#34; debug=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;substitutionProperty name=\u0026#34;log.proj\u0026#34; value=\u0026#34;business-server\u0026#34;/\u0026gt; \u0026lt;substitutionProperty name=\u0026#34;log.base\u0026#34; value=\u0026#34;../logs/${log.proj}\u0026#34; /\u0026gt; \u0026lt;substitutionProperty name=\u0026#34;max.size\u0026#34; value=\u0026#34;200MB\u0026#34; /\u0026gt; \u0026lt;jmxConfigurator /\u0026gt; \u0026lt;appender name=\u0026#34;stdout\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;ch.qos.logback.classic.PatternLayout\u0026#34;\u0026gt; \u0026lt;pattern\u0026gt;%date [%thread] [%X{uid} - %X{url}] %-5level %logger{80}:%line - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;file.debug\u0026#34; class=\u0026#34;ch.qos.logback.core.rolling.RollingFileAppender\u0026#34;\u0026gt; \u0026lt;File\u0026gt;${log.base}/${log.proj}.log\u0026lt;/File\u0026gt; \u0026lt;rollingPolicy class=\u0026#34;ch.qos.logback.core.rolling.TimeBasedRollingPolicy\u0026#34;\u0026gt; \u0026lt;FileNamePattern\u0026gt;${log.base}/${log.proj}_%d{yyyy-MM-dd}.%i.log\u0026lt;/FileNamePattern\u0026gt; \u0026lt;timeBasedFileNamingAndTriggeringPolicy class=\u0026#34;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\u0026#34;\u0026gt; \u0026lt;maxFileSize\u0026gt;${max.size}\u0026lt;/maxFileSize\u0026gt; \u0026lt;/timeBasedFileNamingAndTriggeringPolicy\u0026gt; \u0026lt;/rollingPolicy\u0026gt; \u0026lt;layout class=\u0026#34;ch.qos.logback.classic.PatternLayout\u0026#34;\u0026gt; \u0026lt;pattern\u0026gt;%date [%thread] [%X{uid} - %X{url}] %-5level %logger{80}:%line - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;file.error\u0026#34; class=\u0026#34;ch.qos.logback.core.rolling.RollingFileAppender\u0026#34;\u0026gt; \u0026lt;File\u0026gt;${log.base}/${log.proj}_error.log\u0026lt;/File\u0026gt; \u0026lt;rollingPolicy class=\u0026#34;ch.qos.logback.core.rolling.TimeBasedRollingPolicy\u0026#34;\u0026gt; \u0026lt;FileNamePattern\u0026gt;${log.base}/${log.proj}_error_%d{yyyy-MM-dd}.%i.log\u0026lt;/FileNamePattern\u0026gt; \u0026lt;timeBasedFileNamingAndTriggeringPolicy class=\u0026#34;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\u0026#34;\u0026gt; \u0026lt;maxFileSize\u0026gt;${max.size}\u0026lt;/maxFileSize\u0026gt; \u0026lt;/timeBasedFileNamingAndTriggeringPolicy\u0026gt; \u0026lt;/rollingPolicy\u0026gt; \u0026lt;layout class=\u0026#34;ch.qos.logback.classic.PatternLayout\u0026#34;\u0026gt; \u0026lt;pattern\u0026gt;%date [%thread] [%X{uid} - %X{url}] %-5level %logger{80}:%line - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;filter class=\u0026#34;ch.qos.logback.classic.filter.ThresholdFilter\u0026#34;\u0026gt;\u0026lt;!-- 临界值过滤器,过滤掉低于指定临界值的日志 --\u0026gt; \u0026lt;level\u0026gt;ERROR\u0026lt;/level\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;!-- \u0026lt;logger name=\u0026#34;com.netflix.discovery\u0026#34;\u0026gt; \u0026lt;level value=\u0026#34;info\u0026#34; /\u0026gt; \u0026lt;/logger\u0026gt; \u0026lt;logger name=\u0026#34;org.apache.http\u0026#34;\u0026gt; \u0026lt;level value=\u0026#34;info\u0026#34; /\u0026gt; \u0026lt;/logger\u0026gt; --\u0026gt; \u0026lt;logger name=\u0026#34;org.springframework\u0026#34;\u0026gt; \u0026lt;level value=\u0026#34;warn\u0026#34; /\u0026gt; \u0026lt;/logger\u0026gt; \u0026lt;logger name=\u0026#34;org.apache\u0026#34; level=\u0026#34;warn\u0026#34; /\u0026gt; \u0026lt;logger name=\u0026#34;com.netflix\u0026#34; level=\u0026#34;warn\u0026#34; /\u0026gt; \u0026lt;logger name=\u0026#34;org.hibernate\u0026#34; level=\u0026#34;warn\u0026#34; /\u0026gt; \u0026lt;logger name=\u0026#34;org.mybatis\u0026#34; level=\u0026#34;warn\u0026#34; /\u0026gt; \u0026lt;root level=\u0026#34;debug\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;stdout\u0026#34; /\u0026gt; \u0026lt;appender-ref ref=\u0026#34;file.debug\u0026#34; /\u0026gt; \u0026lt;appender-ref ref=\u0026#34;file.error\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt; 彩色配置:\n\u0026lt;!-- 彩色日志 --\u0026gt; \u0026lt;!-- 彩色日志依赖的渲染类 --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;clr\u0026#34; converterClass=\u0026#34;org.springframework.boot.logging.logback.ColorConverter\u0026#34; /\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;wex\u0026#34; converterClass=\u0026#34;org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter\u0026#34; /\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;wEx\u0026#34; converterClass=\u0026#34;org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter\u0026#34; /\u0026gt; \u0026lt;!-- 彩色日志格式 --\u0026gt; \u0026lt;property name=\u0026#34;CONSOLE_LOG_PATTERN\u0026#34; value=\u0026#34;${CONSOLE_LOG_PATTERN:-%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}\u0026#34; /\u0026gt; \u0026lt;!-- Console 输出设置 --\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;pattern\u0026gt;${CONSOLE_LOG_PATTERN}\u0026lt;/pattern\u0026gt; \u0026lt;charset\u0026gt;utf8\u0026lt;/charset\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; 节点详解: 根节点 包含的属性\nscan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 根节点的子节点：\n下面一共有2个属性，3个子节点， 分别是：\n属性一： 设置上下文名称 ====\n每个logger都关联到logger上下文，默认上下文名称为“default”。但可以使用设置成其他名字，用于区分不同应用程序的记录。一旦设置，不能修改,可以通过%contextName来打印日志上下文名称。 \u0026lt;contextName\u0026gt;logback\u0026lt;/contextName\u0026gt;\n属性二： 设置变量 ====\n用来定义变量值的标签， 有两个属性，name和value；\n其中name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义变量后，可以使“${}”来使用变量。 \u0026lt;property name=\u0026quot;log.path\u0026quot; value=\u0026quot;E:\\logback.log\u0026quot; /\u0026gt;\n子节点一 appender用来格式化日志输出节点，有俩个属性name和class，class用来指定哪种输出策略，常用就是控制台输出策略和文件输出策略。\n子节点二 root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性。\nlevel:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，不能设置为INHERITED或者同义词NULL，默认是DEBUG。 可以包含零个或多个元素，标识这个appender将会添加到这个loger。\n子节点三 logger用来设置某一个包或者具体的某一个类的日志打印级别、以及指定。仅有一个name属性，一个可选的level和一个可选的addtivity属性。\nname:用来指定受此logger约束的某一个包或者具体的某一个类(这个炒鸡好用!)。 level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。如果未设置此属性，那么当前logger将会继承上级的级别。 addtivity:是否向上级logger传递打印信息。默认是true。 来自 http://www.cnblogs.com/zheting/p/6707041.html ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E6%97%A5%E5%BF%97.html","summary":"Spring Boot 默认使用了logback日志,里面引用的==SLF4J==,默认是带颜色的哦(老帅老帅了),甚至都不需要主动引包, …web等等包中有引用","title":"日志"},{"content":"[toc]\n主从复制 复制过程:\n从服务器连接主服务器，发送SYNC（同步）命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令；（从服务器初始化完成） 主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令（从服务器初始化完成后的操作） 优点:\n为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成 Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。 Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据 缺点\nRedis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。 链接：https://www.jianshu.com/p/7967f95655b2\n主从复制是基础,后面的哨兵和Cluster都是基于这种方式,但是这种方式太垃圾了,master死了就不能写操作了,所以一般会用后面两种 使用主从复制很简单,在配置文件中指定master/slave,并设置密码(必须使用)\nfork耗时严重问题 我们可能会开启后台定时 RDB 和 AOF rewrite 功能。但如果你发现，操作 Redis 延迟变大，都发生在 Redis 后台 RDB 和 AOF rewrite 期间，在这期间有可能导致变慢的情况。\n当 Redis 开启了后台 RDB 和 AOF rewrite 后，在执行时，它们都需要主进程创建出一个子进程进行数据的持久化。\n主进程创建子进程，会调用操作系统提供的 fork 函数。\n而 fork 在执行过程中，主进程需要拷贝自己的内存页表给子进程，如果这个实例很大，那么这个拷贝的过程也会比较耗时。\n而且这个 fork 过程会消耗大量的 CPU 资源，在完成 fork 之前，整个 Redis 实例会被阻塞住，无法处理任何客户端请求。\n如果此时你的 CPU 资源本来就很紧张，那么 fork 的耗时会更长，甚至达到秒级，这会严重影响 Redis 的性能。\n也就是说 主从同步和AOF重写操作 时都是用fork函数来同步的\n要想避免这种情况，你可以采取以下方案进行优化：\n控制 Redis 实例的内存：尽量在 10G 以下，执行 fork 的耗时与实例大小有关，实例越大，耗时越久 合理配置数据持久化策略：在 slave 节点执行 RDB 备份，推荐在低峰期执行，而对于丢失数据不敏感的业务（例如把 Redis 当做纯缓存使用），可以关闭 AOF 和 AOF rewrite Redis 实例不要部署在虚拟机上：fork 的耗时也与系统也有关，虚拟机比物理机耗时更久 降低主从库全量同步的概率：适当调大 repl-backlog-size 参数，避免主从全量同步 Redis进阶 - 性能调优：Redis性能调优详解 | Java 全栈知识体系 (pdai.tech) 哨兵模式 哨兵的作用就是监控Redis系统的运行状况。它的功能包括以下两个。\n监控主服务器和从服务器是否正常运行。 主服务器出现故障时自动将从服务器转换为主服务器。 弥补了主从模式下的master死亡的情况\n哨兵是集群,有主节点和从节点,也会产生选举(基于Raft算法(但不一样),和kafka一样),会选举哨兵主节点,redis的master,( 每个redis的bin下都有一个哨兵启动文件),在sentinel模式下,客户端就不用直接连接Redis，而是连接sentinel的ip和port\n哨兵是一个独立的进程，作为进程，它会独立运行\n每个哨兵通过主节点去发现其他从节点和其他哨兵的\n工作方式\n每个sentinel以每秒钟一次的频率向它所知的master，slave以及其他sentinel实例发送一个 PING 命令 如果一个实例距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被sentinel标记为主观下线。 如果一个master被标记为主观下线，则正在监视这个master的所有sentinel要以每秒一次的频率(发送info命令)确认master的确进入了主观下线状态 当有足够数量(quorum)的sentinel在30s（down-after-milliseconds）内确认master的确进入了主观下线状态， 则master会被标记为客观下线 在一般情况下， 每个sentinel会以每 10 秒一次的频率向它已知的所有master，slave发送 INFO 命令 当master被sentinel标记为客观下线时，sentinel向下线的master的所有slave发送 INFO 命令的频率会从 10 秒一次改为 1 秒一次 若没有足够数量的sentinel同意master已经下线，master的客观下线状态就会被移除； 若master重新向sentinel的 PING 命令返回有效回复，master的主观下线状态就会被移除 redis自带哨兵,不需要借助第三方,只需要简单配置即可 ,生产环境建议让redis Sentinel部署到不同的物理机上。\n原文链接：https://blog.csdn.net/miss1181248983/article/details/90056960\n哨兵可以解决大量读的问题,如果有大量写,哨兵也扛不住,这需要Twemproxy,做数据的分片处理,再配合 Lvs 和 Keepalived 解决Twemproxy的单点故障\nhttps://www.jianshu.com/p/84dbb25cc8dc 哨兵与各节点\n哨兵和master通过配置文件直连\nSentinel 自动发现 Slave\n每 10 秒 Sentinel 向 master 节点发送 INFO 命令后获取到所有 slave 的信息 Sentinel 与 slave 建立命令连接和订阅连接 sentinel 自动发现机制\nSentinel 利用 pub/sub（发布/订阅）机制，订阅了每个 master 和 slave 数据节点的 __sentinel__:hello 频道，去自动发现其它也监控了统一 master 的 sentinel 节点 Sentinel 向每 1s 向 __sentinel__:hello 中发送一条消息，包含了其当前维护的最新的 master 配置。如果某个sentinel发现自己的配置版本低于接收到的配置版本，则会用新的配置更新自己的 master 配置 与发现的 Sentinel 之间相互建立命令连接，之后会通过这个命令连接来交换对于 master 数据节点的看法 建立两个异步网络连接：\n命令连接：用于向 Redis master 数据节点发送命令，例如通过 INFO 命令了解：\nmaster 本身运行信息，用于更新本地的 master 字典（Redis Hash 的实现中用到字典使用的的也是这个数据结构） slaves 信息（角色、IP、Port、连接状态、优先级、复制偏移量），用于更新本地的 slave 字典 订阅连接：订阅 __sentinel__:hello 频道，用于发现其他 Sentinel，频道中信息包括：\nSentinel 自身信息（IP、Port、RunID、Epoch） 监视的 Master 节点的信息（Name、IP、Port、Epoch） Redis 哨兵模式(Sentinel) 原理 - 知乎 (zhihu.com) 选举过程\n先选举出sentinel中的leader\n用的过半选举那套,最终决定leader时是 leader最低票数( quorum和Sentinel节点数/2+1的最大值), 所以推荐sentinel要部署奇数台\n然后由sentinel的leader选举slave为master\n过滤故障的节点 选择优先级slave-priority最大的从节点作为主节点，如不存在则继续 选择复制偏移量（数据写入量的字节，记录写了多少数据。主服务器会把偏移量同步给从服务器，当主从的偏移量一致，则数据是完全同步）最大的从节点作为主节点，如不存在则继续 选择runid（redis每次启动的时候生成随机的runid作为redis的标识）最小的从节点作为主节点 redis哨兵模式选举机制_LiaoHongHB的博客-CSDN博客_redis哨兵模式选举机制 Raft协议实战之Redis Sentinel的选举Leader源码解析_xuhao_xuhao的专栏-CSDN博客 Cluster模式 redis的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台redis服务器都存储相同的数据，很浪费内存，所以在redis3.0上加入了cluster模式，实现的redis的分布式存储，也就是说每台redis节点上存储不同的内容。(取数据的时候直接找到对应地址去取)\n默认情况下，redis集群的读和写都是到master上去执行的，不支持slave节点读和写，跟Redis主从复制下读写分离不一样，因为redis集群的核心的理念，主要是使用slave做数据的热备，以及master故障时的主备切换，实现高可用的。\nRedis-Cluster采用无中心结构,它的特点如下\n所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。 工作方式\n在redis的每一个节点上，都有这么两个东西，一个是插槽（slot），它的的取值范围是：0-16383。还有一个就是cluster，可以理解为是一个集群管理的插件。当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。每个节点负责其中一部分槽位。槽位的信息存储于每个节点中。只有master节点会被分配槽位，slave节点不会分配槽位。\n当Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息，并将其缓存在客户端本地。这样当客户端要查找某个 key 时，可以直接定位到目标节点。同时因为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实现槽位信息的校验调整。\n客户端会存一份槽位的分布信息 , 如果槽位信息不对(例如槽位发生了迁移),客户端会更新为最新的槽位信息.\n也就意味着, 客户端一般能直接命中key,不会发生二次访问\n这个客户端是 smart客户端，就是指客户端本地维护一份hashslot =\u0026gt; node的映射表缓存，大部分情况下，直接走本地缓存就可以找到hashslot =\u0026gt; node，不需要通过节点进行moved重定向，(jedisCluster就集成了它)\n槽位定位算法\n【原创】为什么Redis集群有16384个槽 - 孤独烟 - 博客园 (cnblogs.com) 16384 / 8= 2048 (B)\n因为集群节点通信需要带上当前节点槽位信息, 槽位信息存于一个char数组就是集群槽位数/8, 数组中的每一位表示一个槽, 其值为1表示拥有该槽\n所以槽位不宜过大, 不然节点间通信心跳包过大\n总结:\n如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。 65536÷8÷1024=8kb redis的集群主节点数量基本不可能超过1000个。槽位越小，节点少的情况下，压缩比高 跳转重定位\n当客户端向一个节点发出了指令，首先当前节点会计算指令的 key 得到槽位信息，判断计算的槽位是否归当前节点所管理；若槽位不归当前节点管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告诉客户端去连这个节点去获取数据。客户端收到指令后除了跳转到正确的节点上去操作，还会同步更新纠正本地的槽位映射表缓存，后续所有 key 将使用新的槽位映射表。\n直接用控制台发送命令 就会返回真实的key所在ip\n为了保证高可用，redis-cluster集群引入了主从模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点。当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点A1都宕机了，那么该集群就无法再提供服务了\n当一个主节点和其从节点都宕机时, cluster-require-full-coverage配置解决该集群是否可用, 默认为 yes, 表示需要主节点必须全存活\nRedis集群所有节点之间的通信机制\n在Redis集群中，不同的节点之间采用gossip协议进行通信，节点之间通讯的目的是为了维护节点之间的元数据信息。这些元数据就是每个节点包含哪些数据，是否出现故障，通过gossip协议，达到最终数据的一致性。\ngossip协议，是基于流行病传播方式的节点或者进程之间信息交换的协议。原理就是在不同的节点间不断地通信交换信息，一段时间后，所有的节点就都有了整个集群的完整信息，并且所有节点的状态都会达成一致。每个节点会携带集群节点总数的1/10(至少3个)的信息，但只要这些节可以通过网络连通，最终他们的状态就会是一致的。Gossip协议最大的好处在于，即使集群节点的数量增加，每个节点的负载也不会增加很多，几乎是恒定的。\n也意味着元数据的更新有延时，可能导致集群中的一些操作会有一些滞后, 同时结点数太多，意味着达到最终一致性的时间也相对变长，因此官方推荐最大节点数为1000左右。\nredis cluster架构下的每个redis都要开放两个端口号，比如一个是6379，另一个就是加10000的端口号16379。\n6379端口号就是redis服务器入口。 16379端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用的是一种叫gossip 协议的二进制协议 更多知识详见: Redis集群原理详解_张维鹏的博客-CSDN博客_redis集群原理 维护集群的元数据有集中式和 gossip两种方式, 像zookeeper就是集中式\nRedis集群搭建及原理，肝了！ - 知乎 (zhihu.com) gossip协议的常见类型：\ngossip协议常见的消息类型包含： ping、pong、meet、fail等等。\nmeet：主要用于通知新节点加入到集群中\nping：用于交换节点的元数据\npong：ping和meet消息的响应，同样包含了自身节点的状态和集群元数据信息。\nfail：某个节点判断另一个节点 fail 之后，向集群所有节点广播该节点挂掉的消息，其他节点收到消息后标记已下线。\n​\t由于Redis集群的去中心化以及gossip通信机制，Redis集群中的节点只能保证最终一致性。例如当加入新节点时(meet)，只有邀请节点和被邀请节点知道这件事，其余节点要等待 ping 消息一层一层扩散。除了 Fail 是立即全网通知的，其他诸如新节点、节点重上线、从节点选举成为主节点、槽变化等，都需要等待被通知到，也就是Gossip协议是最终一致性的协议。\n集群的扩容与收缩：\n扩容\n（1）启动新节点 （2）使用cluster meet命令将新节点加入到集群 （3）迁移槽和数据：添加新节点后，需要将一些槽和数据从旧节点迁移到新节点\n新节点加入到集群的时候，作为孤儿节点是没有和其他节点进行通讯的。因此需要在集群中任意节点执行 cluster meet 命令让新节点加入进来。假设新节点是 192.168.1.1 5002，老节点是 192.168.1.1 5003，那么运行以下命令将新节点加入到集群中。\n192.168.1.1 5003\u0026gt; cluster meet 192.168.1.1 5002\n这个是由老节点发起的，有点老成员欢迎新成员加入的意思。新节点刚刚建立没有建立槽对应的数据，也就是说没有缓存任何数据。如果这个节点是主节点，需要对其进行槽数据的扩容；如果这个节点是从节点，就需要同步主节点上的数据。总之就是要同步数据。\n实际操作中, 添加主节点命令如下:\n./src/redis-cli --cluster add-node 172.26.237.83:7002 172.26.237.83:7000 -a 0123456789\n在命令中(给这个新节点)分配槽点数,\n第一个ip:port 为需要添加的节点ip和端口，第二个ip:port为当前集群中的节点和端口；\n添加从节点\n./src/redis-cli --cluster add-node --cluster-slave --cluster-master-id db10a9d5c1662d9e3cee21c5776f2e9709f76619 127.0.0.1:7008 127.0.0.1:7007\n节点ID是主节点的ID\n127.0.0.1:7008 是新加的从节点\n127.0.0.1:7007 作为从节点的主节点\nredis集群扩容（添加新节点） - 北向。 - 博客园 (cnblogs.com) Redis集群增加节点和删除节点 - 全me村的希望 - 博客园 (cnblogs.com) 收缩\n（1）迁移槽 （2）忘记节点。通过命令 cluster forget {downNodeId} 通知其他的节点\n为了安全删除节点，Redis集群只能下线没有负责槽的节点。因此如果要下线有负责槽的master节点，则需要先将它负责的槽迁移到其他节点。迁移的过程也与上线操作类似，不同的是下线的时候需要通知全网的其他节点忘记自己，此时通过命令 cluster forget {downNodeId} 通知其他的节点。\n集群的故障检测与故障转恢复机制：\n故障检测\n和哨兵一样, 都存在主观下线和客观下线\n节点A访问节点B超时了,则会认为主观下线,当超过半数的主节点都认为节点B都是客观下线了\n接着向集群广播一条主节点B的Fail 消息，所有收到消息的节点都会标记节点B为客观下线。\n故障恢复\n​\t当故障节点下线后，如果是持有槽的主节点则需要在其从节点中找出一个替换它，从而保证高可用。此时下线主节点的所有从节点都担负着恢复义务，这些从节点会定时监测主节点是否进入客观下线状态，如果是，则触发故障恢复流程。故障恢复也就是选举一个节点充当新的master，选举的过程是基于Raft协议选举方式来实现的。\n从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一定延迟，一定的延迟确保我们等待FAIL状态在集群中传播\n延迟计算公式：DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms\n也就是说, 持有最新数据的slave将会首先发起选举（理论上）, 则更大可能成为主节点\n2.1、从节点过滤：\n检查每个slave节点与master节点断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成master\n就是说, slave和master节点太久没有连接了,就没有资格成为slave\n2.2、投票选举：\n（1）节点排序：\n对通过过滤条件的所有从节点进行排序，按照priority、offset、run id排序，排序越靠前的节点，越优先进行选举。\nreplica-priority 的值越低，优先级越高 (默认值是100) offset越大，表示从master节点复制的数据越多，选举时间越靠前，优先进行选举 如果offset相同，run id越小，优先级越高 （2）更新配置纪元：\n​\t每个主节点会去更新配置纪元（clusterNode.configEpoch），这个值是不断增加的整数。这个值记录了每个节点的版本和整个集群的版本\n（3）发起选举：\n​\t更新完配置纪元以后，从节点会向集群发起广播选举的消息（CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST），要求所有收到这条消息，并且具有投票权的主节点进行投票。每个从节点在一个纪元中只能发起一次选举。\n（4）选举投票：\n​\t如果一个主节点具有投票权，并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。\n​\t每个参与选举的从节点都会接收CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。\n如果超过(N/2 + 1)数量的master节点都投票给了某个从节点，那么选举通过，这个从节点可以切换成master，如果在 cluster-node-timeout*2 的时间内从节点没有获得足够数量的票数，本次选举作废，更新配置纪元，并进行第二轮选举，直到选出新的主节点为止\n2.3、替换主节点：\n当满足投票条件的从节点被选出来以后，会触发替换主节点的操作。删除原主节点负责的槽数据，把这些槽数据添加到自己节点上，并且广播让其他的节点都知道这件事情，新的主节点诞生了。\n（1）被选中的从节点执行SLAVEOF NO ONE命令，使其成为新的主节点\n（2）新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己\n（3）新的主节点对集群进行广播PONG消息，告知其他节点已经成为新的主节点\n（4）新的主节点开始接收和处理槽相关的请求\nmget问题\n现象: 使用mget时性能出现明显下降\n原因: 要查的key分布在多个节点上,所以需要给多个节点发送请求且为串行, 我们使用了一个mget,实际上访问了很多次\n浅谈Redis集群下mget的性能问题 - SegmentFault 思否 Redis集群的运维：\n数据迁移问题：\nRedis集群可以进行节点的动态扩容缩容，这一过程目前还处于半自动状态，需要人工介入。\n带宽消耗问题：\nRedis集群是无中心节点的集群架构，依靠Gossip协议协同自动化修复集群的状态，但goosip有消息延时和消息冗余的问题，在集群节点数量过多的时候，goosip协议通信会消耗大量的带宽\nPub/Sub广播问题：\n集群模式下内部对所有publish命令都会向所有节点进行广播，加重带宽负担，所以集群应该避免频繁使用Pub/sub功能\n集群倾斜：\n集群倾斜是指不同节点之间数据量和请求量出现明显差异，这种情况将加大负载均衡和开发运维的难度。因此需要理解集群倾斜的原因\n（1）数据倾斜：\n​\t节点和槽分配不均 ​\t不同槽对应键数量差异过大 ​\t集合对象包含大量元素 ​\t内存相关配置不一致 （2）请求倾斜：\n​\t合理设计键，热点大集合对象做拆分或者使用hmget代替hgetall避免整体读取\n【原创】谈谈redis的热key问题如何解决 - 孤独烟 - 博客园 (cnblogs.com) 使用二级缓存(@enableCaching) 将key多备份几个,加个后缀名区分,这样就会随机的分布到其他机器 启用集群模式也是通过配置即可,这里只是简述,分布数据以及后续内容远不止这些\n链接：https://www.jianshu.com/p/7967f95655b2\nhttps://blog.csdn.net/miss1181248983/article/details/90056960 https://www.cnblogs.com/williamjie/p/11132211.html Redis集群原理详解_张维鹏的博客-CSDN博客_redis集群原理 Redis集群搭建及原理，肝了！ - 知乎 (zhihu.com) 扩展阅读:\nRedis哈希槽，对于哈希槽的理解，以及高并发情况下哈希槽不够的情况讲解，热点缓存的解决思路_ck784101777的博客-CSDN博客_redis哈希槽 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/%E4%B8%89%E7%A7%8D%E9%9B%86%E7%BE%A4%E6%96%B9%E5%BC%8F.html","summary":"[toc] 主从复制 复制过程: 从服务器连接主服务器，发送SYNC（同步）命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使","title":"三种集群方式"},{"content":"Java日期格式化参数对照表\n常用格式: yyyy-MM-dd HH:mm:ss\n纪元标记 AD 备注 y/u 年份 2001\tu 在使用jdk8格式化使用过 Y 周年年份 2001\t当天所在的周属于的年份, (一年中的最后一周存在跨年的情况) 来自 http://www.cnblogs.com/zheting/p/7702470.html M 月份 July or 07 d 一个月的第几天 10 h A.M./P.M. (1~12)格式小时 12 H 一天中的小时 (0~23) 22 m 分钟数 30 s 秒数 55 S 毫秒数 234 E 星期几 Tuesday D 一年中的第几天 360 F 一个月中第几周的周几 2 (second Wed. in July) w 一年中第几周 40 W 一个月中第几周 1 a A.M./P.M. 标记 PM k 一天中的小时(1~24) 24 K A.M./P.M. (0~11)格式小时 10 z 时区 Eastern Standard Time ' 文字定界符 Delimiter \u0026quot; 单引号\t` // 严格判断时间是否合法 DateTimeFormatter ldt = DateTimeFormatter.ofPattern(\u0026#34;uuuuMMdd\u0026#34;).withResolverStyle(ResolverStyle.STRICT); try { LocalDate localDate = LocalDate.parse(\u0026#34;20190229\u0026#34;, ldt); System.out.println(localDate); } catch (Exception e) { e.printStackTrace(); } // 获取两个日期间隔的所有月份 public static List\u0026lt;String\u0026gt; getBetweenMonths(String start, String end) { LocalDate startDate = LocalDate.parse(start); LocalDate endDate = LocalDate.parse(end); return Stream.iterate(startDate, localDate -\u0026gt; localDate.plusMonths(1)) // 截断无限流，长度为起始时间和结束时间的差+1个 .limit(ChronoUnit.MONTHS.between(startDate, endDate) + 1) // 由于最后要的是字符串，所以map转换一下 .map(LocalDate::toString) // 把流收集为List .collect(Collectors.toList()); } /** LocalDateTime 转化成 Date*/ Function\u0026lt;LocalDateTime, Date\u0026gt; dateTime2Date=l-\u0026gt;Date.from(l.toInstant(ZoneOffset.of(\u0026#34;+8\u0026#34;))); /** Date 转化成 LocalDateTime*/ Function\u0026lt;Date, LocalDateTime\u0026gt; date2DateTime=d-\u0026gt;d.toInstant().atOffset(ZoneOffset.of(\u0026#34;+8\u0026#34;)).toLocalDateTime(); ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%97%B6%E9%97%B4.html","summary":"Java日期格式化参数对照表 常用格式: yyyy-MM-dd HH:mm:ss 纪元标记 AD 备注 y/u 年份 2001 u 在使用jdk8格式化使用过 Y 周年年份 2001 当天所在的周属于的年份, (一年中的最","title":"时间"},{"content":"设计模式二三事 设计模式是众多软件开发人员经过长时间的试错和应用总结出来的，解决特定问题的一系列方案。现行的部分教材在介绍设计模式时，有些会因为案例脱离实际应用场景而令人费解，有些又会因为场景简单而显得有些小题大做。\n本文会根据在美团金融服务平台设计开发时的经验，结合实际的案例，并采用“师生对话”这种相对诙谐的形式去讲解几类常用设计模式的应用。希望能对想提升系统设计能力的同学有所帮助或启发。\n本篇涉及\n策略模式 适配器模式 单例模式 状态模式 观察者模式 建造者模式 装饰器模式 引言 话说这是在程序员世界里一对师徒的对话：\n“老师，我最近在写代码时总感觉自己的代码很不优雅，有什么办法能优化吗？”\n“嗯，可以考虑通过教材系统学习，从注释、命名、方法和异常等多方面实现整洁代码。”\n“然而，我想说的是，我的代码是符合各种编码规范的，但是从实现上却总是感觉不够简洁，而且总是需要反复修改！”学生小明叹气道。\n老师看了看小明的代码说：“我明白了，这是系统设计上的缺陷。总结就是抽象不够、可读性低、不够健壮。”\n“对对对，那怎么能迅速提高代码的可读性、健壮性、扩展性呢？”小明急不可耐地问道。\n老师敲了敲小明的头：“不要太浮躁，没有什么方法能让你立刻成为系统设计专家。但是对于你的问题，我想设计模式可以帮到你。”\n“设计模式？”小明不解。\n“是的。”老师点了点头，“世上本没有路，走的人多了，便变成了路。在程序员的世界中，本没有设计模式，写代码是人多了，他们便总结出了一套能提高开发和维护效率的套路，这就是设计模式。设计模式不是什么教条或者范式，它可以说是一种在特定场景下普适且可复用的解决方案，是一种可以用于提高代码可读性、可扩展性、可维护性和可测性的最佳实践。”\n“哦哦，我懂了，那我应该如何去学习呢？”\n“不急，接下来我来带你慢慢了解设计模式。”\n奖励的发放策略 第一天，老师问小明：“你知道活动营销吗？”\n“这我知道，活动营销是指企业通过参与社会关注度高的已有活动，或整合有效的资源自主策划大型活动，从而迅速提高企业及其品牌的知名度、美誉度和影响力，常见的比如有抽奖、红包等。”\n老师点点头：“是的。我们假设现在就要做一个营销，需要用户参与一个活动，然后完成一系列的任务，最后可以得到一些奖励作为回报。活动的奖励包含美团外卖、酒旅和美食等多种品类券，现在需要你帮忙设计一套奖励发放方案。”\n因为之前有过类似的开发经验，拿到需求的小明二话不说开始了编写起了代码：\n// 奖励服务 class RewardService { // 外部服务 private WaimaiService waimaiService; private HotelService hotelService; private FoodService foodService; // 使用对入参的条件判断进行发奖 public void issueReward(String rewardType, Object ... params) { if (\u0026#34;Waimai\u0026#34;.equals(rewardType)) { WaimaiRequest request = new WaimaiRequest(); // 构建入参 request.setWaimaiReq(params); waimaiService.issueWaimai(request); } else if (\u0026#34;Hotel\u0026#34;.equals(rewardType)) { HotelRequest request = new HotelRequest(); request.addHotelReq(params); hotelService.sendPrize(request); } else if (\u0026#34;Food\u0026#34;.equals(rewardType)) { FoodRequest request = new FoodRequest(params); foodService.getCoupon(request); } else { throw new IllegalArgumentException(\u0026#34;rewardType error!\u0026#34;); } } } 小明很快写好了Demo，然后发给老师看。\n“假如我们即将接入新的打车券，这是否意味着你必须要修改这部分代码？”老师问道。\n小明愣了一愣，没等反应过来老师又问：”假如后面美团外卖的发券接口发生了改变或者替换，这段逻辑是否必须要同步进行修改？”\n小明陷入了思考之中，一时间没法回答。\n经验丰富的老师一针见血地指出了这段设计的问题：“你这段代码有两个主要问题，一是不符合开闭原则，可以预见，如果后续新增品类券的话，需要直接修改主干代码，而我们提倡代码应该是对修改封闭的；二是不符合迪米特法则，发奖逻辑和各个下游接口高度耦合，这导致接口的改变将直接影响到代码的组织，使得代码的可维护性降低。”\n小明恍然大悟：“那我将各个同下游接口交互的功能抽象成单独的服务，封装其参数组装及异常处理，使得发奖主逻辑与其解耦，是否就能更具备扩展性和可维护性？”\n“这是个不错的思路。之前跟你介绍过设计模式，这个案例就可以使用策略模式和适配器模式来优化。”\n小明借此机会学习了这两个设计模式。首先是策略模式：\n策略模式定义了一系列的算法，并将每一个算法封装起来，使它们可以相互替换。策略模式通常包含以下角色：\n抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。 然后是适配器模式：\n适配器模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式包含以下主要角色：\n目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 结合优化思路，小明首先设计出了策略接口，并通过适配器的思想将各个下游接口类适配成策略类：\n// 策略接口 interface Strategy { void issue(Object ... params); } // 外卖策略 class Waimai implements Strategy { private WaimaiService waimaiService; @Override public void issue(Object... params) { WaimaiRequest request = new WaimaiRequest(); // 构建入参 request.setWaimaiReq(params); waimaiService.issueWaimai(request); } } // 酒旅策略 class Hotel implements Strategy { private HotelService hotelService; @Override public void issue(Object... params) { HotelRequest request = new HotelRequest(); request.addHotelReq(params); hotelService.sendPrize(request); } } // 美食策略 class Food implements Strategy { private FoodService foodService; @Override public void issue(Object... params) { FoodRequest request = new FoodRequest(params); foodService.payCoupon(request); } } 然后，小明创建策略模式的环境类，并供奖励服务调用：\n// 使用分支判断获取的策略上下文 class StrategyContext { public static Strategy getStrategy(String rewardType) { switch (rewardType) { case \u0026#34;Waimai\u0026#34;: return new Waimai(); case \u0026#34;Hotel\u0026#34;: return new Hotel(); case \u0026#34;Food\u0026#34;: return new Food(); default: throw new IllegalArgumentException(\u0026#34;rewardType error!\u0026#34;); } } } // 优化后的策略服务 class RewardService { public void issueReward(String rewardType, Object ... params) { Strategy strategy = StrategyContext.getStrategy(rewardType); strategy.issue(params); } } 小明的代码经过优化后，虽然结构和设计上比之前要复杂不少，但考虑到健壮性和拓展性，还是非常值得的。\n“看，我这次优化后的版本是不是很完美？”小明洋洋得意地说。\n“耦合度确实降低了，但还能做的更好。”\n“怎么做？”小明有点疑惑。\n“我问你，策略类是有状态的模型吗？如果不是是否可以考虑做成单例的？”\n“的确如此。”小明似乎明白了。\n“还有一点，环境类的获取策略方法职责很明确，但是你依然没有做到完全对修改封闭。”\n经过老师的点拨，小明很快也领悟到了要点：“那我可以将策略类单例化以减少开销，并实现自注册的功能彻底解决分支判断。”\n小明列出单例模式的要点：\n单例模式设计模式属于创建型模式，它提供了一种创建对象的最佳方式。\n这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。\n最终，小明在策略环境类中使用一个注册表来记录各个策略类的注册信息，并提供接口供策略类调用进行注册。同时使用饿汉式单例模式去优化策略类的设计：\n// 策略上下文，用于管理策略的注册和获取 class StrategyContext { private static final Map\u0026lt;String, Strategy\u0026gt; registerMap = new HashMap\u0026lt;\u0026gt;(); // 注册策略 public static void registerStrategy(String rewardType, Strategy strategy) { registerMap.putIfAbsent(rewardType, strategy); } // 获取策略 public static Strategy getStrategy(String rewardType) { return registerMap.get(rewardType); } } // 抽象策略类 abstract class AbstractStrategy implements Strategy { // 类注册方法 public void register() { StrategyContext.registerStrategy(getClass().getSimpleName(), this); } } // 单例外卖策略 class Waimai extends AbstractStrategy implements Strategy { private static final Waimai instance = new Waimai(); private WaimaiService waimaiService; private Waimai() { register(); } public static Waimai getInstance() { return instance; } @Override public void issue(Object... params) { WaimaiRequest request = new WaimaiRequest(); // 构建入参 request.setWaimaiReq(params); waimaiService.issueWaimai(request); } } // 单例酒旅策略 class Hotel extends AbstractStrategy implements Strategy { private static final Hotel instance = new Hotel(); private HotelService hotelService; private Hotel() { register(); } public static Hotel getInstance() { return instance; } @Override public void issue(Object... params) { HotelRequest request = new HotelRequest(); request.addHotelReq(params); hotelService.sendPrize(request); } } // 单例美食策略 class Food extends AbstractStrategy implements Strategy { private static final Food instance = new Food(); private FoodService foodService; private Food() { register(); } public static Food getInstance() { return instance; } @Override public void issue(Object... params) { FoodRequest request = new FoodRequest(params); foodService.payCoupon(request); } } 最终，小明设计完成的结构类图如下：\n奖励发放策略_类图\n如果使用了Spring框架，还可以利用Spring的Bean机制来代替上述的部分设计，直接使用@Component和@PostConstruct注解即可完成单例的创建和注册，代码会更加简洁。\n至此，经过了多次讨论、反思和优化，小明终于得到了一套低耦合高内聚，同时符合开闭原则的设计。\n“老师，我开始学会利用设计模式去解决已发现的问题。这次我做得怎么样？”\n“合格。但是，依然要戒骄戒躁。”\n任务模型的设计 “之前让你设计奖励发放策略你还记得吗？”老师忽然问道。\n“当然记得。一个好的设计模式，能让工作事半功倍。”小明答道。\n“嗯，那会提到了活动营销的组成部分，除了奖励之外，貌似还有任务吧。”\n小明点了点头，老师接着说：“现在，我想让你去完成任务模型的设计。你需要重点关注状态的流转变更，以及状态变更后的消息通知。”\n小明欣然接下了老师给的难题。他首先定义了一套任务状态的枚举和行为的枚举：\n// 任务状态枚举 @AllArgsConstructor @Getter enum TaskState { INIT(\u0026#34;初始化\u0026#34;), ONGOING( \u0026#34;进行中\u0026#34;), PAUSED(\u0026#34;暂停中\u0026#34;), FINISHED(\u0026#34;已完成\u0026#34;), EXPIRED(\u0026#34;已过期\u0026#34;) ; private final String message; } // 行为枚举 @AllArgsConstructor @Getter enum ActionType { START(1, \u0026#34;开始\u0026#34;), STOP(2, \u0026#34;暂停\u0026#34;), ACHIEVE(3, \u0026#34;完成\u0026#34;), EXPIRE(4, \u0026#34;过期\u0026#34;) ; private final int code; private final String message; } 然后，小明对开始编写状态变更功能：\nclass Task { private Long taskId; // 任务的默认状态为初始化 private TaskState state = TaskState.INIT; // 活动服务 private ActivityService activityService; // 任务管理器 private TaskManager taskManager; // 使用条件分支进行任务更新 public void updateState(ActionType actionType) { if (state == TaskState.INIT) { if (actionType == ActionType.START) { state = TaskState.ONGOING; } } else if (state == TaskState.ONGOING) { if (actionType == ActionType.ACHIEVE) { state = TaskState.FINISHED; // 任务完成后进对外部服务进行通知 activityService.notifyFinished(taskId); taskManager.release(taskId); } else if (actionType == ActionType.STOP) { state = TaskState.PAUSED; } else if (actionType == ActionType.EXPIRE) { state = TaskState.EXPIRED; } } else if (state == TaskState.PAUSED) { if (actionType == ActionType.START) { state = TaskState.ONGOING; } else if (actionType == ActionType.EXPIRE) { state = TaskState.EXPIRED; } } } } 在上述的实现中，小明在updateState方法中完成了2个重要的功能：\n接收不同的行为，然后更新当前任务的状态； 当任务过期时，通知任务所属的活动和任务管理器。 诚然，随着小明的系统开发能力和代码质量意识的提升，他能够认识到这种功能设计存在缺陷。\n“老师，我的代码还是和之前说的那样，不够优雅。”\n“哦，你自己说说看有什么问题？”\n“第一，方法中使用条件判断来控制语句，但是当条件复杂或者状态太多时，条件判断语句会过于臃肿，可读性差，且不具备扩展性，维护难度也大。且增加新的状态时要添加新的if-else语句，这违背了开闭原则，不利于程序的扩展。”\n老师表示同意，小明接着说：“第二，任务类不够高内聚，它在通知实现中感知了其他领域或模块的模型，如活动和任务管理器，这样代码的耦合度太高，不利于扩展。”\n老师赞赏地说道：“很好，你有意识能够自主发现代码问题所在，已经是很大的进步了。”\n“那这个问题应该怎么去解决呢？”小明继续发问。\n“这个同样可以通过设计模式去优化。首先是状态流转的控制可以使用状态模式，其次，任务完成时的通知可以用到观察者模式。”\n收到指示后，小明马上去学习了状态模式的结构：\n状态模式：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。状态模式包含以下主要角色：\n环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。 根据状态模式的定义，小明将TaskState枚举类扩展成多个状态类，并具备完成状态的流转的能力；然后优化了任务类的实现：\n// 任务状态抽象接口 interface State { // 默认实现，不做任何处理 default void update(Task task, ActionType actionType) { // do nothing } } // 任务初始状态 class TaskInit implements State { @Override public void update(Task task, ActionType actionType) { if (actionType == ActionType.START) { task.setState(new TaskOngoing()); } } } // 任务进行状态 class TaskOngoing implements State { private ActivityService activityService; private TaskManager taskManager; @Override public void update(Task task, ActionType actionType) { if (actionType == ActionType.ACHIEVE) { task.setState(new TaskFinished()); // 通知 activityService.notifyFinished(taskId); taskManager.release(taskId); } else if (actionType == ActionType.STOP) { task.setState(new TaskPaused()); } else if (actionType == ActionType.EXPIRE) { task.setState(new TaskExpired()); } } } // 任务暂停状态 class TaskPaused implements State { @Override public void update(Task task, ActionType actionType) { if (actionType == ActionType.START) { task.setState(new TaskOngoing()); } else if (actionType == ActionType.EXPIRE) { task.setState(new TaskExpired()); } } } // 任务完成状态 class TaskFinished implements State { } // 任务过期状态 class TaskExpired implements State { } @Data class Task { private Long taskId; // 初始化为初始态 private State state = new TaskInit(); // 更新状态 public void updateState(ActionType actionType) { state.update(this, actionType); } } 小明欣喜地看到，经过状态模式处理后的任务类的耦合度得到降低，符合开闭原则。状态模式的优点在于符合单一职责原则，状态类职责明确，有利于程序的扩展。但是这样设计的代价是状态类的数目增加了，因此状态流转逻辑越复杂、需要处理的动作越多，越有利于状态模式的应用。除此之外，状态类的自身对于开闭原则的支持并没有足够好，如果状态流转逻辑变化频繁，那么可能要慎重使用。\n处理完状态后，小明又根据老师的指导使用观察者模式去优化任务完成时的通知：\n观察者模式：指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。观察者模式的主要角色如下。\n抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 小明首先设计好抽象目标和抽象观察者，然后将活动和任务管理器的接收通知功能定制成具体观察者：\n// 抽象观察者 interface Observer { void response(Long taskId); // 反应 } // 抽象目标 abstract class Subject { protected List\u0026lt;Observer\u0026gt; observers = new ArrayList\u0026lt;Observer\u0026gt;(); // 增加观察者方法 public void add(Observer observer) { observers.add(observer); } // 删除观察者方法 public void remove(Observer observer) { observers.remove(observer); } // 通知观察者方法 public void notifyObserver(Long taskId) { for (Observer observer : observers) { observer.response(taskId); } } } // 活动观察者 class ActivityObserver implements Observer { private ActivityService activityService; @Override public void response(Long taskId) { activityService.notifyFinished(taskId); } } // 任务管理观察者 class TaskManageObserver implements Observer { private TaskManager taskManager; @Override public void response(Long taskId) { taskManager.release(taskId); } } 最后，小明将任务进行状态类优化成使用通用的通知方法，并在任务初始态执行状态流转时定义任务进行态所需的观察者：\n// 任务进行状态 class TaskOngoing extends Subject implements State { @Override public void update(Task task, ActionType actionType) { if (actionType == ActionType.ACHIEVE) { task.setState(new TaskFinished()); // 通知 notifyObserver(task.getTaskId()); } else if (actionType == ActionType.STOP) { task.setState(new TaskPaused()); } else if (actionType == ActionType.EXPIRE) { task.setState(new TaskExpired()); } } } // 任务初始状态 class TaskInit implements State { @Override public void update(Task task, ActionType actionType) { if (actionType == ActionType.START) { TaskOngoing taskOngoing = new TaskOngoing(); taskOngoing.add(new ActivityObserver()); taskOngoing.add(new TaskManageObserver()); task.setState(taskOngoing); } } } 最终，小明设计完成的结构类图如下：\n任务模型设计_类图\n通过观察者模式，小明让任务状态和通知方实现松耦合（实际上观察者模式还没能做到完全的解耦，如果要做进一步的解耦可以考虑学习并使用发布-订阅模式，这里也不再赘述）。\n至此，小明成功使用状态模式设计出了高内聚、高扩展性、单一职责的任务的整个状态机实现，以及做到松耦合的、符合依赖倒置原则的任务状态变更通知方式。\n“老师，我逐渐能意识到代码的设计缺陷，并学会利用较为复杂的设计模式做优化。”\n“不错，再接再厉！”\n活动的迭代重构 “小明，这次又有一个新的任务。”老师出现在正在认真阅读《设计模式》的小明的面前。\n“好的。刚好我已经学习了设计模式的原理，终于可以派上用场了。”\n“之前你设计开发了活动模型，现在我们需要在任务型活动的参与方法上增加一层风险控制。”\n“OK。借此机会，我也想重构一下之前的设计。”\n活动模型的特点在于其组成部分较多，小明原先的活动模型的构建方式是这样的：\n// 抽象活动接口 interface ActivityInterface { void participate(Long userId); } // 活动类 class Activity implements ActivityInterface { private String type; private Long id; private String name; private Integer scene; private String material; public Activity(String type) { this.type = type; // id的构建部分依赖于活动的type if (\u0026#34;period\u0026#34;.equals(type)) { id = 0L; } } public Activity(String type, Long id) { this.type = type; this.id = id; } public Activity(String type, Long id, Integer scene) { this.type = type; this.id = id; this.scene = scene; } public Activity(String type, String name, Integer scene, String material) { this.type = type; this.scene = scene; this.material = material; // name的构建完全依赖于活动的type if (\u0026#34;period\u0026#34;.equals(type)) { this.id = 0L; this.name = \u0026#34;period\u0026#34; + name; } else { this.name = \u0026#34;normal\u0026#34; + name; } } // 参与活动 @Override public void participate(Long userId) { // do nothing } } // 任务型活动 class TaskActivity extends Activity { private Task task; public TaskActivity(String type, String name, Integer scene, String material, Task task) { super(type, name, scene, material); this.task = task; } // 参与任务型活动 @Override public void participate(Long userId) { // 更新任务状态为进行中 task.getState().update(task, ActionType.START); } } 经过自主分析，小明发现活动的构造不够合理，主要问题表现在：\n活动的构造组件较多，导致可以组合的构造函数太多，尤其是在模型增加字段时还需要去修改构造函数； 部分组件的构造存在一定的顺序关系，但是当前的实现没有体现顺序，导致构造逻辑比较混乱，并且存在部分重复的代码。 发现问题后，小明回忆自己的学习成果，马上想到可以使用创建型模式中的建造者模式去做重构：\n建造者模式：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示。它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。建造者模式的主要角色如下:\n产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 根据建造者模式的定义，上述活动的每个字段都是一个产品。于是，小明可以通过在活动里面实现静态的建造者类来简易地实现：\n// 活动类 class Activity implements ActivityInterface { protected String type; protected Long id; protected String name; protected Integer scene; protected String material; // 全参构造函数 public Activity(String type, Long id, String name, Integer scene, String material) { this.type = type; this.id = id; this.name = name; this.scene = scene; this.material = material; } @Override public void participate(Long userId) { // do nothing } // 静态建造器类，使用奇异递归模板模式允许继承并返回继承建造器类 public static class Builder\u0026lt;T extends Builder\u0026lt;T\u0026gt;\u0026gt; { protected String type; protected Long id; protected String name; protected Integer scene; protected String material; public T setType(String type) { this.type = type; return (T) this; } public T setId(Long id) { this.id = id; return (T) this; } public T setId() { if (\u0026#34;period\u0026#34;.equals(this.type)) { this.id = 0L; } return (T) this; } public T setScene(Integer scene) { this.scene = scene; return (T) this; } public T setMaterial(String material) { this.material = material; return (T) this; } public T setName(String name) { if (\u0026#34;period\u0026#34;.equals(this.type)) { this.name = \u0026#34;period\u0026#34; + name; } else { this.name = \u0026#34;normal\u0026#34; + name; } return (T) this; } public Activity build(){ return new Activity(type, id, name, scene, material); } } } // 任务型活动 class TaskActivity extends Activity { protected Task task; // 全参构造函数 public TaskActivity(String type, Long id, String name, Integer scene, String material, Task task) { super(type, id, name, scene, material); this.task = task; } // 参与任务型活动 @Override public void participate(Long userId) { // 更新任务状态为进行中 task.getState().update(task, ActionType.START); } // 继承建造器类 public static class Builder extends Activity.Builder\u0026lt;Builder\u0026gt; { private Task task; public Builder setTask(Task task) { this.task = task; return this; } public TaskActivity build(){ return new TaskActivity(type, id, name, scene, material, task); } } } 小明发现，上面的建造器没有使用诸如抽象建造器类等完整的实现，但是基本是完成了活动各个组件的建造流程。使用建造器的模式下，可以先按顺序构建字段type，然后依次构建其他组件，最后使用build方法获取建造完成的活动。这种设计一方面封装性好，构建和表示分离；另一方面扩展性好，各个具体的建造者相互独立，有利于系统的解耦。可以说是一次比较有价值的重构。在实际的应用中，如果字段类型多，同时各个字段只需要简单的赋值，可以直接引用Lombok的@Builder注解来实现轻量的建造者。\n重构完活动构建的设计后，小明开始对参加活动方法增加风控。最简单的方式肯定是直接修改目标方法：\npublic void participate(Long userId) { // 对目标用户做风险控制，失败则抛出异常 Risk.doControl(userId); // 更新任务状态为进行中 task.state.update(task, ActionType.START); } 但是考虑到，最好能尽可能避免对旧方法的直接修改，同时为方法增加风控，也是一类比较常见的功能新增，可能会在多处使用。\n“老师，风险控制会出现在多种活动的参与方法中吗？”\n“有这个可能性。有的活动需要风险控制，有的不需要。风控像是在适当的时候对参与这个方法的装饰。”\n“对了，装饰器模式！”\n小明马上想到用装饰器模式来完成设计：\n装饰器模式的定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。装饰器模式主要包含以下角色：\n抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 小明使用了装饰器模式后，新的代码就变成了这样：\n// 抽象装饰角色 abstract class ActivityDecorator implements ActivityInterface { protected ActivityInterface activity; public ActivityDecorator(ActivityInterface activity) { this.activity = activity; } public abstract void participate(Long userId); } // 能够对活动做风险控制的包装类 class RiskControlDecorator extends ActivityDecorator { public RiskControlDecorator(ActivityInterface activity) { super(activity); } @Override public void participate(Long userId) { // 对目标用户做风险控制，失败则抛出异常 Risk.doControl(userId); // 更新任务状态为进行中 activity.participate(userId); } } 最终，小明设计完成的结构类图如下：\n活动迭代重构_类图\n最终，小明通过自己的思考分析，结合学习的设计模式知识，完成了活动模型的重构和迭代。\n“老师，我已经能做到自主分析功能特点，并合理应用设计模式去完成程序设计和代码重构了，实在太感谢您了。”\n“设计模式作为一种软件设计的最佳实践，你已经很好地理解并应用于实践了，非常不错。但学海无涯，还需持续精进！”\n结语 本文以三个实际场景为出发点，借助小明和老师两个虚拟的人物，试图以一种较为诙谐的“对话”方式来讲述设计模式的应用场景、优点和缺点。如果大家想要去系统性地了解设计模式，也可以通过市面上很多的教材进行学习，都介绍了经典的23种设计模式的结构和实现。不过，很多教材的内容即便配合了大量的示例，但有时也会让人感到费解，主要原因在于：一方面，很多案例比较脱离实际的应用场景；另一方面，部分设计模式显然更适用于大型复杂的结构设计，而当其应用到简单的场景时，仿佛让代码变得更加繁琐、冗余。因此，本文希望通过这种“对话+代码展示+结构类图”的方式，以一种更易懂的方式来介绍设计模式。\n当然，本文只讲述了部分比较常见的设计模式，还有其他的设计模式，仍然需要同学们去研读经典著作，举一反三，学以致用。我们也希望通过学习设计模式能让更多的同学在系统设计能力上得到提升。\n转载于-设计模式二三事 ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%AE%9E%E6%88%98-%E7%BE%8E%E5%9B%A2.html","summary":"设计模式二三事 设计模式是众多软件开发人员经过长时间的试错和应用总结出来的，解决特定问题的一系列方案。现行的部分教材在介绍设计模式时，有些会因","title":"实战-美团"},{"content":"使用框架:\nSpring+Dubbo\n分为 提供者/消费者\nprovider:\n**配置:**新建文件 : beans-dubbo.xml,并写入, 可以把具体值写在properties中\n\u0026lt;!-- 配置系统应用名称 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;${APP_NAME}\u0026#34; /\u0026gt; \u0026lt;!-- 通过注册中心发现监控中心服务 --\u0026gt; \u0026lt;dubbo:monitor protocol=\u0026#34;registry\u0026#34; /\u0026gt; \u0026lt;!-- 配置注册中心地址 --\u0026gt; \u0026lt;dubbo:registry protocol=\u0026#34;zookeeper\u0026#34; address=\u0026#34;${ZOOKEEPER_ADDRESS}\u0026#34; /\u0026gt; \u0026lt;!-- 配置服务发布方式（Dubbo 支持多种协议发布，Hessian只是其中的一种,这里用了原生方式） --\u0026gt; \u0026lt;dubbo:protocol name=\u0026#34;dubbo\u0026#34; port=\u0026#34;${APP_PORT}\u0026#34; /\u0026gt; application.xml: 写入 以下配置, 作用: 读取配置文件\n\u0026lt;bean id=\u0026#34;propertyConfigurer\u0026#34; class=\u0026#34;com.jieshun.jht.framework.config.properties.PropertyPlaceholderConfigurer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;locations\u0026#34;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;value\u0026gt;classpath:${profile.properties}/*.properties\u0026lt;/value\u0026gt; \u0026lt;/list\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; 作用:使beans-dubbo.xml 文件生效(在beans路径下,这应该看得懂吧,傻子)\n\u0026lt;!-- hessian调用配置 --\u0026gt; \u0026lt;import resource=\u0026#34;classpath:beans/beans-*.xml\u0026#34; /\u0026gt; 文件:\n1.写一个接口,写个方法\n2.写个实现类,实现这个接口,这个就是你对外提供的方法,实现类上写个注解 @Service(\u0026ldquo;userIntegralManageService\u0026rdquo;),名字与消费者中配置文件中的订阅服务的id保持一致\n别人就能通过接口来调用这个方法,就达到了远程调用\nconsume:\n\u0026lt;!-- 配置系统应用名称 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;${APP_NAME}\u0026#34; /\u0026gt; \u0026lt;!-- 通过注册中心发现监控中心服务 --\u0026gt; \u0026lt;dubbo:monitor protocol=\u0026#34;registry\u0026#34; /\u0026gt; \u0026lt;!-- 配置注册中心地址 --\u0026gt; \u0026lt;dubbo:registry protocol=\u0026#34;zookeeper\u0026#34; address=\u0026#34;${ZOOKEEPER_ADDRESS}\u0026#34; /\u0026gt; \u0026lt;!-- 订阅服务 ,这个服务具体指的是提供的一个接口类,这id就是你能用的对象, --\u0026gt; \u0026lt;dubbo:reference id=\u0026#34;userIntegralManageService\u0026#34; interface=\u0026#34;com.jieshun.jht.account.service.IUserIntegralManageService\u0026#34; /\u0026gt; 基本就是这样,如果未成功,详情看 \u0026ldquo;会员积分系统\u0026rdquo; 项目\n如果要使用一个公共工程来放在方法,基本思路一样,就是把接口放在公共工程中,在提供者中去实现就行(此时需要将公共工程当成jar包导入,使用pom文件嘛)\n使用框架:\nspringBoot + Dubbo:\nconsume:\n**配置:**新建文件 : dubbo-consume.xml,并写入, 可以把具体值写在properties中\n\u0026lt;!-- 提供方应用信息，用于计算依赖关系 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;con\u0026#34; /\u0026gt; \u0026lt;!-- 注册中心暴露服务地址 --\u0026gt; \u0026lt;dubbo:registry protocol=\u0026#34;zookeeper\u0026#34; address=\u0026#34;127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;dubbo:reference id=\u0026#34;exampleService\u0026#34; interface=\u0026#34;com.xkj.springboot.service.ExampleService\u0026#34; /\u0026gt; 文件:\n随便写个类,在类上加上注释: (也就是说,写哪个类上都可以,只要写了就行), (这是用了xml的方式来配置,SpringBoot肯定有直接写properties的方式来配置)\n@Configuration @PropertySource(\u0026#34;classpath:dubbo/dubbo.properties\u0026#34;) @ImportResource({ \u0026#34;classpath:dubbo/*.xml\u0026#34; }) 可以写个controller,方便验证.\nprovider:\n配置:\n\u0026lt;!-- 提供方应用信息，用于计算依赖关系 --\u0026gt; \u0026lt;dubbo:application name=\u0026#34;pro\u0026#34; /\u0026gt; \u0026lt;!-- 注册中心暴露服务地址 --\u0026gt; \u0026lt;!-- \u0026lt;dubbo:registry address=\u0026#34;multicast://224.5.6.7:1234\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;dubbo:registry protocol=\u0026#34;zookeeper\u0026#34; address=\u0026#34;10.170.219.98:2181,10.173.55.173:2181\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;dubbo:registry protocol=\u0026#34;${dubbo.registry.protocol}\u0026#34; address=\u0026#34;${dubbo.registry.address}\u0026#34; /\u0026gt; \u0026lt;!-- 暴露服务 --\u0026gt; \u0026lt;dubbo:protocol name=\u0026#34;${dubbo.protocol.name}\u0026#34; port=\u0026#34;${dubbo.protocol.port}\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;exampleServiceImpl\u0026#34; class=\u0026#34;com.xkj.springboot.service.impl.ExampleServiceImpl\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;dubbo:service interface=\u0026#34;com.xkj.springboot.service.ExampleService\u0026#34; ref=\u0026#34;exampleServiceImpl\u0026#34; retries=\u0026#34;0\u0026#34; timeout=\u0026#34;6000\u0026#34; /\u0026gt; 文件:\n1.写一个接口,写个方法\n2.写个实现类,实现这个接口,这个就是你对外提供的方法,实现类上写个注解 @Service(\u0026ldquo;exampleServiceImpl\u0026rdquo;),名字与消费者中配置文件中的订阅服务的id保持一致\n别人就能通过接口来调用这个方法,就达到了远程调用\npom.xml:\n\u0026lt;!-- dubbo --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.zookeeper\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;zookeeper\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.sgroschupf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;zkclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-configuration-processor\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E4%BD%BF%E7%94%A8.html","summary":"使用框架: Spring+Dubbo 分为 提供者/消费者 provider: **配置:**新建文件 : beans-dubbo.xml,并写入, 可以把具体值写在properties中 \u0026lt;!-- 配置","title":"使用"},{"content":"[toc]\n该文章包括:\n使用提供的curd 条件构造器 自定义sql 分页插件 sql执行效率插件 配置文件: application.properties\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://10.10.203.10:3306/jplatdvv?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;allowMultiQueries=true\u0026amp;allowPublicKeyRetrieval=true spring.datasource.username=test spring.datasource.password=Jht123456 #spring.datasource.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.initialSize=5 spring.datasource.minIdle=5 spring.datasource.maxActive=20 spring.datasource.maxWait=60000 #mybatisMapper文件配置地址 #mybatis框架的配置不起作用了 mybatis-plus.mapper-locations=classpath*:mapper/*Mapper.xml 启动类: SpringBootTestMybatisPlusApplication.java\npackage com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.transaction.annotation.EnableTransactionManagement; import com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor; import com.baomidou.mybatisplus.extension.plugins.PerformanceInterceptor; @SpringBootApplication @EnableTransactionManagement public class SpringBootTestMybatisPlusApplication { public static void main(String[] args) { SpringApplication.run(SpringBootTestMybatisPlusApplication.class, args); } /** * SQL执行效率插件 */ @Bean // @Profile({\u0026#34;dev\u0026#34;,\u0026#34;test\u0026#34;})// 设置 dev test 环境开启 public PerformanceInterceptor performanceInterceptor() { return new PerformanceInterceptor(); } /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } } 实体类 package com.example.demo.entity; import com.baomidou.mybatisplus.annotation.TableField; import com.baomidou.mybatisplus.annotation.TableName; /** * 使用帮助实体类 * * @author xkj * */ @TableName(value = \u0026#34;jpf_help_info\u0026#34;) // 指定表名,默认使用类名(驼峰转下划线) @Data public class HelpInfo { /** 帮助ID */ private String id; /** 帮助编码 */ private String helpNo; /** 帮助内容 */ private String content; /** 帮助标题 */ private String title; /** 创建人ID */ private String createId; /** 创建人名称 */ @TableField(exist=false) // 标明该字段不是数据库字段,默认所有的属性都是数据库字段 private String createName; /** 修改人ID */ private String updateId; /** 修改人名称 */ @TableField(exist=false) private String updateName; 控制类 package com.example.demo.web; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.util.StringUtils; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import com.baomidou.mybatisplus.core.conditions.query.QueryWrapper; import com.baomidou.mybatisplus.core.metadata.IPage; import com.baomidou.mybatisplus.extension.plugins.pagination.Page; import com.example.demo.dao.HelpDao; import com.example.demo.entity.HelpInfo; @RequestMapping(\u0026#34;test\u0026#34;) @RestController public class TestController { private static Logger logger = LoggerFactory.getLogger(TestController.class); @Autowired private HelpDao helpDao; @RequestMapping(\u0026#34;helpPage\u0026#34;) public IPage\u0026lt;HelpInfo\u0026gt; getHelpPage(String content) { IPage\u0026lt;HelpInfo\u0026gt; page=new Page\u0026lt;\u0026gt;(1,4); QueryWrapper\u0026lt;HelpInfo\u0026gt; wrapper=new QueryWrapper\u0026lt;\u0026gt;(); wrapper.select(\u0026#34;HELP_NO\u0026#34;,\u0026#34;TITLE\u0026#34;,\u0026#34;CONTENT\u0026#34;).eq(\u0026#34;id\u0026#34;, \u0026#34;1\u0026#34;); wrapper.like(!StringUtils.isEmpty(content), \u0026#34;CONTENT\u0026#34;, content); // 默认用 and 连接 IPage\u0026lt;HelpInfo\u0026gt; helpInfos = helpDao.selectPage(page,wrapper); logger.info(\u0026#34;返参:{}\u0026#34;,helpInfos); return helpInfos; } @RequestMapping(\u0026#34;helpOne\u0026#34;) public HelpInfo getHelpOne() { QueryWrapper\u0026lt;HelpInfo\u0026gt; wrapper=new QueryWrapper\u0026lt;\u0026gt;() ; wrapper.select(\u0026#34;HELP_NO\u0026#34;,\u0026#34;TITLE\u0026#34;,\u0026#34;CONTENT\u0026#34;) .eq(\u0026#34;id\u0026#34;, \u0026#34;1\u0026#34;) .or() .eq(\u0026#34;update_Id\u0026#34;, \u0026#34;19\u0026#34;) //\t.apply(\u0026#34;left join jpf_account\u0026#34;) // apply 允许自定义sql .last(\u0026#34;limit 1\u0026#34;);// last : 无视java代码顺序,会直接放到sql最后面 HelpInfo helpInfo = helpDao.selectOne(wrapper); logger.info(\u0026#34;返参:{}\u0026#34;,helpInfo); return helpInfo; } @RequestMapping(\u0026#34;helpQuery\u0026#34;) public HelpInfo getHelpQuery() { QueryWrapper\u0026lt;HelpInfo\u0026gt; wrapper=new QueryWrapper\u0026lt;\u0026gt;() ; wrapper.eq(\u0026#34;id\u0026#34;, \u0026#34;1\u0026#34;).or().eq(\u0026#34;update_Id\u0026#34;, \u0026#34;19\u0026#34;).last(\u0026#34;limit 1\u0026#34;); logger.info(\u0026#34;CustomSqlSegment:{}\u0026#34;,wrapper.getCustomSqlSegment());// sql条件, 带where // CustomSqlSegment:WHERE id = #{ew.paramNameValuePairs.MPGENVAL1} OR update_Id = #{ew.paramNameValuePairs.MPGENVAL2} limit 1 logger.info(\u0026#34;SqlSegment:{}\u0026#34;,wrapper.getSqlSegment());// sql条件, 不带where // SqlSegment:id = #{ew.paramNameValuePairs.MPGENVAL1} OR update_Id = #{ew.paramNameValuePairs.MPGENVAL2} limit 1 HelpInfo helpInfo = helpDao.getHelpQuery(wrapper); logger.info(\u0026#34;返参:{}\u0026#34;,helpInfo); return helpInfo; } @RequestMapping(\u0026#34;helpQueryByMe\u0026#34;) public HelpInfo getHelpQueryByMe() { QueryWrapper\u0026lt;HelpInfo\u0026gt; wrapper=new QueryWrapper\u0026lt;\u0026gt;() ; wrapper.eq(\u0026#34;id\u0026#34;, \u0026#34;1\u0026#34;).or().eq(\u0026#34;update_Id\u0026#34;, \u0026#34;19\u0026#34;).last(\u0026#34;limit 1\u0026#34;).select(\u0026#34;HELP_NO\u0026#34;,\u0026#34;TITLE\u0026#34;,\u0026#34;CONTENT\u0026#34;); String id=\u0026#34;1\u0026#34;; HelpInfo helpInfo = helpDao.getHelpQueryByMe(id); // 允许完全自定义sql,和mybatis一样使用(spring boot的配置不一样) logger.info(\u0026#34;返参:{}\u0026#34;,helpInfo); return helpInfo; } } 业务层 // 可以使用 ServiceImpl类下的方法, 更多的操作数据的操作 @Service public class AccountServiceImpl extends ServiceImpl\u0026lt;AccountMapper, JbpAccount\u0026gt; implements AccountService { } dao层 package com.example.demo.dao; import org.apache.ibatis.annotations.Mapper; import org.apache.ibatis.annotations.Param; import com.baomidou.mybatisplus.core.conditions.Wrapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.example.demo.entity.HelpInfo; @Mapper public interface HelpDao extends BaseMapper\u0026lt;HelpInfo\u0026gt; { public HelpInfo getHelpQuery(@Param(\u0026#34;ew\u0026#34;) Wrapper\u0026lt;HelpInfo\u0026gt; wrapper); public HelpInfo getHelpQueryByMe(@Param(\u0026#34;id\u0026#34;) String id); } Mapper文件 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.example.demo.dao.HelpDao\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;getHelpQuery\u0026#34; resultType=\u0026#34;com.example.demo.entity.HelpInfo\u0026#34;\u0026gt; SELECT * FROM jpf_help_info ${ew.customSqlSegment} \u0026lt;!-- customSqlSegment: mybatis-Plus 框架的一个变量,里面是带where 的 sql, 可看代码 --\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;select id=\u0026#34;getHelpQueryByMe\u0026#34; resultType=\u0026#34;com.example.demo.entity.HelpInfo\u0026#34;\u0026gt; SELECT * FROM jpf_help_info where id = #{id} \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatisplus/%E4%BD%BF%E7%94%A8.html","summary":"[toc] 该文章包括: 使用提供的curd 条件构造器 自定义sql 分页插件 sql执行效率插件 配置文件: application.properties spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://10.10.203.10:3306/jplatdvv?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=false\u0026amp;allowMultiQueries=true\u0026amp;allowPublicKeyRetrieval=true spring.datasource.username=test spring.datasource.password=Jht123456 #spring.datasource.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.initialSize=5 spring.datasource.minIdle=5 spring.datasource.maxActive=20 spring.datasource.maxWait=60000 #mybatisMapper文","title":"使用"},{"content":"[TOC]\n一. 添加反编译插件 关键东西:\njd-eclipse http://jd.benow.ca/jd-eclipse/update 二. 不让控制台总是弹出来 window -\u0026gt; preferences -\u0026gt; run/debug -\u0026gt; console 在右边面板去掉\n\u0026#34;Show when program writest to standard out\u0026#34;和 \u0026#34;Show when program writes to standard error\u0026#34; 两个多选框，然后重启Eclipse。\n或者:\n直接选择下图中的按钮,第一个选中时表示正常输出时展现控制台(看英文), 第二个是错误输出时展示\n来自 https://blog.csdn.net/xingxiupaioxue/article/details/46647433 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/eclipse/%E4%BD%BF%E7%94%A8.html","summary":"[TOC] 一. 添加反编译插件 关键东西: jd-eclipse http://jd.benow.ca/jd-eclipse/update 二. 不让控制台总是弹出来 window -\u0026gt; preferences -\u0026gt; run/debug -\u0026gt; console 在右边面板去掉 \u0026#34;Show when program writest to standard out\u0026#34;和 \u0026#34;Show when program writes to standard error\u0026#34; 两个多选框","title":"使用"},{"content":"[Toc]\n一. 入参是Map,取key和value dao接口:\nint updateByBatch(@Param(\u0026#34;content\u0026#34;) Map\u0026lt;String, Integer\u0026gt; alreadySoldNumMap); mapper文件:\n\u0026lt;update id=\u0026#34;updateByBatch\u0026#34; parameterType=\u0026#34;java.util.Map\u0026#34;\u0026gt; update COUPON_CATEGORY \u0026lt;trim prefix=\u0026#34;set\u0026#34; suffixOverrides=\u0026#34;,\u0026#34;\u0026gt; \u0026lt;trim prefix=\u0026#34;ALREADY_SOLD_NUM = case\u0026#34; suffix=\u0026#34;end,\u0026#34;\u0026gt; \u0026lt;foreach collection=\u0026#34;content.keys\u0026#34; item=\u0026#34;key\u0026#34; index=\u0026#34;index\u0026#34;\u0026gt; when ID=#{key} then ALREADY_SOLD_NUM+#{content[${key}]} \u0026lt;/foreach\u0026gt; \u0026lt;/trim\u0026gt; \u0026lt;/trim\u0026gt; where \u0026lt;!-- 循环key--\u0026gt; \u0026lt;foreach collection=\u0026#34;content.keys\u0026#34; separator=\u0026#34;or\u0026#34; item=\u0026#34;key\u0026#34; index=\u0026#34;index\u0026#34;\u0026gt; ID=#{key} \u0026lt;/foreach\u0026gt; \u0026lt;/update\u0026gt; content.keys 得到所有的key; content.values 得到所有的value; 这种方式#{content[${key}]}获取map中的value，传递的map中的key只能是String类型， 如果是其他类型，得到的value是null。#{content[${key}]}还可以写成${content[key]}方式。 来自https://blog.csdn.net/shiqijiamengjie/article/details/77448829 1. 如果固定key: \u0026lt;select id=\u0026#34;selectRule\u0026#34; parameterType=\u0026#34;Map\u0026#34; resultType=\u0026#34;com.ourangel.weixin.domain.Rule\u0026#34;\u0026gt; SELECT ruleId,msgType,event,respId,reqValue,firstRespId,createDate,yn FROM oal_tb_rule WHERE yn = 1 \u0026lt;if test=\u0026#34;_parameter.containsKey(\u0026#39;msgType\u0026#39;)\u0026#34;\u0026gt; AND msgType = #{msgType,jdbcType=VARCHAR}) \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;_parameter.containsKey(\u0026#39;event\u0026#39;)\u0026#34;\u0026gt; AND event = #{event,jdbcType=VARCHAR}) \u0026lt;/if\u0026gt; \u0026lt;/select\u0026gt; 2. 如果value是list: 入参:\nHashMap\u0026lt;String,Object\u0026gt; map = new HashMap\u0026lt;String, Object\u0026gt;(); map.put(\u0026#34;creator\u0026#34;, \u0026#34;creator\u0026#34;); map.put(\u0026#34;createdate\u0026#34;, \u0026#34;createdate\u0026#34;); String[] ids = {\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;}; map.put(\u0026#34;ids\u0026#34;, ids ); mapper:\n\u0026lt;update id=\u0026#34;publishT00_notice\u0026#34; parameterType=\u0026#34;Map\u0026#34;\u0026gt; update test set createdate = #{createdate}, creator = #{creator} where id in \u0026lt;foreach collection=\u0026#34;ids\u0026#34; item=\u0026#34;id\u0026#34; separator=\u0026#34;,\u0026#34; open=\u0026#34;(\u0026#34; close=\u0026#34;)\u0026#34;\u0026gt; #{id} \u0026lt;/foreach\u0026gt; \u0026lt;/update\u0026gt; 或者: \u0026lt;select id=\u0026#34;getOrgCodeLinkByMultiOrgIds\u0026#34; resultType=\u0026#34;com.jieshun.jht.jportal.organize.entity.OrgInfo\u0026#34; parameterType=\u0026#34;map\u0026#34;\u0026gt; \u0026lt;foreach collection=\u0026#34;content.keys\u0026#34; item=\u0026#34;key\u0026#34; separator=\u0026#34;UNION ALL\u0026#34; \u0026gt; SELECT #{key} AS id, CONCAT(\u0026#39;,\u0026#39;,GROUP_CONCAT(ORG_CODE),\u0026#39;,\u0026#39;) AS orgCodeLink FROM JBP_ORG_INFO WHERE ID IN \u0026lt;foreach collection=\u0026#34;content[key]\u0026#34; item=\u0026#34;id\u0026#34; open=\u0026#34;(\u0026#34; separator=\u0026#34;,\u0026#34; close=\u0026#34;)\u0026#34;\u0026gt; #{id} \u0026lt;/foreach\u0026gt; \u0026lt;/foreach\u0026gt; \u0026lt;/select\u0026gt; 二. collection的使用: 1:\n\u0026lt;resultMap id=\u0026#34;customerInfoMap\u0026#34; type=\u0026#34;com.jieshun.jht.jplatform.entity.CustomerInfo\u0026#34;\u0026gt; \u0026lt;collection property=\u0026#34;customerServ\u0026#34; ofType=\u0026#34;com.jieshun.jht.jplatform.entity.CustomerReference\u0026#34; \u0026gt; // customerServ 是实体类中集合的属性名 \u0026lt;result column=\u0026#34;serviceNo\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; property=\u0026#34;serviceNo\u0026#34; /\u0026gt; // column 是数据库中的字段名 ; property 是实体类的属性名 \u0026lt;result column=\u0026#34;serviceName\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; property=\u0026#34;serviceName\u0026#34; /\u0026gt; \u0026lt;/collection\u0026gt; \u0026lt;/resultMap\u0026gt; 2:\n\u0026lt;resultMap id=\u0026#34;queryDetailMap\u0026#34; type=\u0026#34;com.jieshun.jht.jplatform.entity.Role\u0026#34;\u0026gt; \u0026lt;id column=\u0026#34;ROLE_ID\u0026#34; property=\u0026#34;roleId\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;ROLE_NO\u0026#34; property=\u0026#34;roleNo\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;ROLE_NAME\u0026#34; property=\u0026#34;roleName\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;ROLE_TYPE\u0026#34; property=\u0026#34;roleType\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;ROLE_DESC\u0026#34; property=\u0026#34;roleDesc\u0026#34; /\u0026gt; \u0026lt;collection property=\u0026#34;permissions\u0026#34; column=\u0026#34;ID\u0026#34; ofType=\u0026#34;com.jieshun.jht.jplatform.entity.Function\u0026#34; resultMap=\u0026#34;permissionsMap\u0026#34;/\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;resultMap id=\u0026#34;permissionsMap\u0026#34; type=\u0026#34;com.jieshun.jht.jplatform.entity.Function\u0026#34; \u0026gt; \u0026lt;id column=\u0026#34;ID\u0026#34; property=\u0026#34;id\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;FUNC_NO\u0026#34; property=\u0026#34;funcNo\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;FUNC_NAME\u0026#34; property=\u0026#34;funcName\u0026#34; /\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;select id=\u0026#34;queryRoleByRoleNo\u0026#34; resultMap=\u0026#34;queryDetailMap\u0026#34;\u0026gt; SELECT R.ROLE_ID, R.ROLE_NAME, R.ROLE_DESC, R.ROLE_NO, R.ROLE_TYPE, FC.ID, FC.FUNC_NO, FC.FUNC_NAME FROM JPF_ROLE R LEFT JOIN JPF_ROLE_PERM RP ON R.ROLE_ID = RP.ROLE_ID LEFT JOIN JPF_FUNCTION FC ON RP.FUNC_ID = FC.ID AND FC.FUNC_STATUS = \u0026#39;NORMAL\u0026#39; WHERE R.ROLE_NO = #{roleNo} AND ROLE_STATUS !=\u0026#39;DELETE\u0026#39; \u0026lt;/select\u0026gt; 3:\n\u0026lt;!--传多个参数--\u0026gt; \u0026lt;resultMap id=\u0026#34;customerInfoMap\u0026#34; type=\u0026#34;com.jieshun.jht.jplatform.entity.CustomerInfo\u0026#34;\u0026gt; \u0026lt;result column=\u0026#34;id\u0026#34; property=\u0026#34;id\u0026#34;/\u0026gt; \u0026lt;collection property=\u0026#34;customerServ\u0026#34; column=\u0026#34;{id=id_name,age=age}\u0026#34; select=\u0026#34;querySerByAcct\u0026#34; ofType=\u0026#34;com.jieshun.jht.jplatform.entity.CustomerReference\u0026#34; /\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;select id=\u0026#34;querySerByAcct\u0026#34; resultType=\u0026#34;com.jieshun.jht.jplatform.entity.CustomerReference\u0026#34;\u0026gt; SELECT js.SERVICE_NO AS serviceNo, js.SERVICE_NAME AS serviceName FROM JPF_CUSTOMER_SERVICE jcs INNER JOIN JPF_SERVICE AS js ON jcs.SERVICE_ID = js.ID WHERE jcs.CUSTOMER_ID = #{id_name} \u0026lt;/select\u0026gt; 三. chose when otherwise 四. foreach 的使用 UPDATE JPF_CUSTOMER_SERVICE SET OPEN_ID = #{openId}, OPEN_TIME = SYSDATE() WHERE CUSTOMER_ID = #{customerId} AND SERVICE_ID IN \u0026lt;foreach collection=\u0026#34;serviceIds\u0026#34; item=\u0026#34;serivceId\u0026#34; index=\u0026#34;index\u0026#34; open=\u0026#34;(\u0026#34; separator=\u0026#34;,\u0026#34; close=\u0026#34;)\u0026#34;\u0026gt; #{serivceId} \u0026lt;/foreach\u0026gt; item 相当于foreach中的变量 ; collection : 入参变量名 ; index: 循环的下标 ; separator : 一次循环的结束符合 ; open : 循环的开始符号 ; close : 循环的结束符号\n五. 关于批量插入(返主键) \u0026lt;insert id=\u0026#34;batchCreateGateway\u0026#34; useGeneratedKeys=\u0026#34;true\u0026#34; keyProperty=\u0026#34;gatewayId\u0026#34; parameterType=\u0026#34;java.util.List\u0026#34;\u0026gt; INSERT INTO ITR_BAS_GATEWAY_INFO ( GATEWAY_NO,GATEWAY_NAME,MASTER_CUSTOMER_ID,OPERATE_FLAG,STATUS, PROJECT_NO,CREATE_TIME,CREATE_ID,CREATE_ACC,UPDATE_TIME,UPDATE_ID )VALUES \u0026lt;foreach collection=\u0026#34;list\u0026#34; item=\u0026#34;gateway\u0026#34; index=\u0026#34;index\u0026#34; separator=\u0026#34;,\u0026#34;\u0026gt; (#{gateway.gatewayNo},#{gateway.gatewayName},#{gateway.masterCustomerId},#{gateway.operateFlag},#{gateway.status}, #{gateway.projectNo},SYSDATE(),#{gateway.accountId},#{gateway.accountNo},SYSDATE(),#{gateway.accountId} ) \u0026lt;/foreach\u0026gt; \u0026lt;/insert\u0026gt; dao层:\npublic void batchCreateGateway(List\u0026lt;GatewayOperateInput\u0026gt; gatewayInputs); 解释:\n1. 如果用了@Param注解,则不会返回主键 2. 需要返回主键时,加上 useGeneratedKeys=\u0026ldquo;true\u0026rdquo; keyProperty=\u0026ldquo;gatewayId\u0026rdquo; keyProperty表示list中对象的主键字段,则会将返回的主键值注入到List入参中 3. 如果入参是List,Mybatis会默认自动转化成map,key是\u0026quot;list\u0026quot;,value是你的List入参\n六. 指定表名操作 \u0026lt;select id=\u0026#34;getByCode\u0026#34; resultType=\u0026#34;com.wwy.us.mdm.coredata.entity.BaseDO\u0026#34;\u0026gt; select id, is_latest, status from ${tableName} where system_code = #{sysCode} \u0026lt;/select\u0026gt; \u0026ldquo;$\u0026rdquo; 符号不会预编译, 会直接填值, 如果用了 \u0026ldquo;#\u0026rdquo; 则会当成字符串处理(加上单引号)\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B.html","summary":"[Toc] 一. 入参是Map,取key和value dao接口: int updateByBatch(@Param(\u0026#34;content\u0026#34;) Map\u0026lt;String, Integer\u0026gt; alreadySoldNumMap); mapper文件: \u0026lt;update id=\u0026#34;updateByBatch\u0026#34; parameterType=\u0026#34;java.util.Map\u0026#34;\u0026gt; update COUPON_CATEGORY \u0026lt;trim prefix=\u0026#34;set\u0026#34; suffixOverrides=\u0026#34;,\u0026#34;\u0026gt; \u0026lt;trim prefix=\u0026#34;ALREADY_SOLD_NUM = case\u0026#34; suffix=\u0026#34;end,\u0026#34;\u0026gt; \u0026lt;foreach collection=\u0026#34;content.keys\u0026#34; item=\u0026#34;key\u0026#34; index=\u0026#34;index\u0026#34;\u0026gt; when ID=#{key} then ALREADY_SOLD_NUM+#{content[${key}]} \u0026lt;/foreach\u0026gt; \u0026lt;/trim\u0026gt; \u0026lt;/trim\u0026gt; where \u0026lt;!-- 循环ke","title":"使用案例"},{"content":" 从其他表中查询结果并插入新表 INSERT INTO jpf_project_service (project_no,service_id,customer_id,start_time,end_time,create_id,create_time) SELECT sp.PROJECT_NO , sp.SERVICE_ID , sp.MASTER_CUSTOMER_ID, sc.START_TIME, sc.END_TIME,1,sysdate() FROM jbp_service_project sp INNER JOIN jpf_customer_service sc ON sc.CUSTOMER_ID = sp.MASTER_CUSTOMER_ID AND sc.SERVICE_ID = sp.SERVICE_ID; ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B.html","summary":"从其他表中查询结果并插入新表 INSERT INTO jpf_project_service (project_no,service_id,customer_id,start_time,end_time,create_id,create_time) SELECT sp.PROJECT_NO , sp.SERVICE_ID , sp.MASTER_CUSTOMER_ID, sc.START_TIME, sc.END_TIME,1,sysdate() FROM jbp_service_project sp INNER JOIN jpf_customer_service sc ON sc.CUSTOMER_ID = sp.MASTER_CUSTOMER_ID AND sc.SERVICE_ID = sp.SERVICE_ID;","title":"使用案例"},{"content":"[TOC]\n1. 介绍 事务是一系列的动作，它们综合在一起才是一个完整的工作单元，这些动作必须全部完成，如果有一个失败的话，那么事务就会回滚到最开始的状态，仿佛什么都没发生过一样。 在企业级应用程序开发中，事务管理必不可少的技术，用来确保数据的完整性和一致性。\n2.事务有四个特性：ACID 原子性（Atomicity）：事务是一个原子操作，由一系列动作组成。事务的原子性确保动作要么全部完成，要么完全不起作用。 一致性（Consistency）：一旦事务完成（不管成功还是失败），系统必须确保它所建模的业务处于一致的状态，而不会是部分完成部分失败。在现实中的数据不应该被破坏。 隔离性（Isolation）：可能有许多事务会同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。 持久性（Durability）：一旦事务完成，无论发生什么系统错误，它的结果都不应该受到影响，这样就能从任何系统崩溃中恢复过来。通常情况下，事务的结果被写到持久化存储器中。 原文链接：https://blog.csdn.net/trigl/article/details/50968079\n3. 事务管理器 Spring并不直接管理事务，而是提供了多种事务管理器，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。 Spring事务管理器的接口是org.springframework.transaction.PlatformTransactionManager，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。此接口的内容如下：\nPublic interface PlatformTransactionManager()...{ // 由TransactionDefinition得到TransactionStatus对象 TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // 提交 Void commit(TransactionStatus status) throws TransactionException; // 回滚 Void rollback(TransactionStatus status) throws TransactionException; } JDBC事务 Hibernate事务 Java持久化API事务（JPA） Java原生API事务 (跨越了多个事务管理源) 原文链接：https://blog.csdn.net/trigl/article/details/50968079\n4. 基本事务属性的定义 4.1 传播行为 事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：\n传播行为 含义 PROPAGATION_REQUIRED 表示当前方法必须运行在事务中。如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务(springboot中注解默认的行为) PROPAGATION_SUPPORTS 表示当前方法不需要事务上下文，但是如果存在当前事务的话，那么该方法会在这个事务中运行 PROPAGATION_MANDATORY 表示该方法必须在事务中运行，如果当前事务不存在，则会抛出一个异常 PROPAGATION_REQUIRED_NEW 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NOT_SUPPORTED 表示该方法不应该运行在事务中。如果存在当前事务，在该方法运行期间，当前事务将被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NEVER 表示当前方法不应该运行在事务上下文中。如果当前正有一个事务在运行，则会抛出异常 PROPAGATION_NESTED 表示如果当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。如果当前事务不存在，那么其行为与PROPAGATION_REQUIRED一样。注意各厂商对这种传播行为的支持是有所差异的。可以参考资源管理器的文档来确认它们是否支持嵌套事务 原文链接：https://blog.csdn.net/trigl/article/details/50968079\n4.2 隔离级别 事务的第二个维度就是隔离级别（isolation level）。隔离级别定义了一个事务可能受其他并发事务影响的程度。\n并发事务引起的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务。并发虽然是必须的，但可能会导致一下的问题(严重程度依次递增)。 脏读（Dirty reads）——脏读发生在一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读（Nonrepeatable read）——不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了更新。 幻读（Phantom read）——幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。 原文链接：https://blog.csdn.net/trigl/article/details/50968079\n数据库用的是mvcc+各种锁来解决的三种问题(简单理解为乐观锁)\n简述三种的情况的处理 不可重复读与幻读的解决方案 隔离级别(围绕三种读的隔离) 隔离级别 含义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别 (springboot中注解默认级别) ISOLATION_READ_UNCOMMITTED (读未提交)最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 ISOLATION_READ_COMMITTED (读已提交)允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 ISOLATION_REPEATABLE_READ (可重复读)对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 ISOLATION_SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的 mysql默认的事务处理级别是\u0026rsquo;REPEATABLE-READ\u0026rsquo;,也就是可重复读\noracle默认系统事务隔离级别是READ COMMITTED,也就是读已提交\n原文链接：https://blog.csdn.net/trigl/article/details/50968079\n5. 编程式事务 编程式和声明式事务的区别\n编程式事务允许用户在代码中精确定义事务的边界， 声明式事务（基于AOP）有助于用户将操作与事务规则进行解耦。 (@Transactional) 简单地说，编程式事务侵入到了业务代码里面，但是提供了更加详细的事务管理；而声明式事务由于基于AOP，所以既能起到事务管理的作用，又可以不影响业务代码的具体实现。\n原文链接：https://blog.csdn.net/trigl/article/details/50968079\n进阶版使用:\n@Component public class TransactionHelper { @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public \u0026lt;T\u0026gt; T run(Supplier\u0026lt;T\u0026gt; command) { return command.get(); } @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public void run(Runnable command) { command.run(); } @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class) public \u0026lt;T, R\u0026gt; R run(Function\u0026lt;T, R\u0026gt; command, T param) { return command.apply(param); } } // 使用 @Resource private TransactionHelper transactionHelper; private boolean transactionSaveWebsiteMessageByTemplate(List\u0026lt;BaseWebsiteMessageQuery\u0026gt; websiteMessageQuery) { return transactionHelper.run(this::saveWebsiteMessageByTemplate, websiteMessageQuery); } 6. AOP失效 6.1 自调用 自调用: A方法调用B方法, 把事务注解加到B方法上,这就是自调用, 自调用不会触发事务(官方说的)\n这是由AOP原理决定的,动态代理是类的代理,只有出现类的调用才会触发事务, 而类内部的函数调用就不是类的调用了\n解决方案:\n将注解写到A方法上 将B方法抽离出来,再写一个类, 在A方法最后再调用B方法 不用注解,使用声明书事务(就是手动开启事务) 再次模拟一次编程式调用,让A方法在事务内调回来 原理及解决方案 原理介绍 6.2 private 修饰的方法 Spring 自调用事务失效，你是怎么解决的？ - 程序员小航 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E4%BA%8B%E5%8A%A1.html","summary":"[TOC] 1. 介绍 事务是一系列的动作，它们综合在一起才是一个完整的工作单元，这些动作必须全部完成，如果有一个失败的话，那么事务就会回滚到最开始的状态，","title":"事务"},{"content":"[toc]\n并发事务引起的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务。并发虽然是必须的，但可能会导致一下的问题(严重程度依次递增)。\n脏读（Dirty reads）——脏读发生在一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读（Nonrepeatable read）——不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了更新。(用mvcc解决的, 读取改之前的副本就能保证读的数据一致了) 幻读（Phantom read）——幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。(用mvcc+临键锁实现的,把要查的范围内数据全部锁住就可以了) 原文链接：https://blog.csdn.net/trigl/article/details/50968079\n数据库用的是mvcc+各种锁来解决的三种问题(简单理解为乐观锁)\n简述三种的情况的处理 不可重复读与幻读的解决方案 隔离级别(围绕三种读的隔离) 隔离级别 含义 ISOLATION_READ_UNCOMMITTED (读未提交)最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 ISOLATION_READ_COMMITTED (读已提交)允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 ISOLATION_REPEATABLE_READ (可重复读)对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 ISOLATION_SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的 mysql默认的事务处理级别是REPEATABLE-READ,也就是可重复读\noracle默认系统事务隔离级别是READ COMMITTED,也就是读已提交\n原文链接：https://blog.csdn.net/trigl/article/details/50968079\nMySQL如何解决幻读和不可重复度？_兴趣使然的草帽路飞的博客-CSDN博客 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%BA%8B%E5%8A%A1.html","summary":"[toc] 并发事务引起的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务。并发虽然是必须的，但可能会导致一下的问题(严","title":"事务"},{"content":" 原子性(Atomicity)：事务中的全部操作在数据库中是不可分割的，要么全部完成，要么全部不执行。 [1] 一致性(Consistency)：几个并行执行的事务，其执行结果必须与按某一顺序 串行执行的结果相一致。 [1] 隔离性(Isolation)：事务的执行不受其他事务的干扰，事务执行的中间结果对其他事务必须是透明的。 [1] 持久性(Durability):对于任意已提交事务，系统必须保证该事务对数据库的改变不被丢失，即使数据库出现故障。 [1] 事务的ACID特性是由关系数据库系统 (DBMS)来实现的，DBMS采用日志来保证事务的原子性、一致性和持久性。日志记录了事务对数据库所作的更新，如果某个事务在执行过程中发生错误，就可以根据日志撤销事务对数据库已做的更新，使得数据库回滚到执行事务前的初始状态。\n对于事务的隔离性，DBMS是采用锁机制来实现的。当多个事务同时更新数据库中相同的数据时，只允许持有锁的事务能更新该数据，其他事务必须等待，直到前一个事务释放了锁，其他事务才有机会更新该数据。\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E4%BA%8B%E5%8A%A1.html","summary":"原子性(Atomicity)：事务中的全部操作在数据库中是不可分割的，要么全部完成，要么全部不执行。 [1] 一致性(Consistency)：几个","title":"事务"},{"content":"[toc]\n前言 RocketMQ为我们提供了事务消息的功能，它使得我们投放消息和其他的一些操作保持一个整体的原子性。比如：向数据库中插入数据，再向MQ中投放消息，把这两个动作作为一个原子性的操作。貌似其他的MQ是没有这种功能的。\nRocketMQ系列（七）事务消息-阿里云开发者社区 (aliyun.com) 1. 事务消息实现思想 RocketMQ 事务消息的实现原理基于两阶段提交和定时事务状态回查来决定消息最终是提交还是回滚\nRocketMQ实现事务消息主要分为两个阶段：正常事务的发送及提交、事务信息的补偿流程\n整体流程为：\n正常事务发送与提交阶段\n生产者发送一个半消息给MQServer（半消息是指消费者暂时不能消费的消息）\n服务端响应消息写入结果，半消息发送成功\n开始执行本地事务\n根据本地事务的执行状态执行Commit或者Rollback操作\n总结: 先发送MQ,再操作数据库, MQ发送半消息, 等数据库操作完成后,再决定那条半消息是成功还是失败\n事务信息的补偿流程\n如果MQServer长时间没收到本地事务的执行状态会向生产者发起一个确认回查的操作请求\n生产者收到确认回查请求后，检查本地事务的执行状态\n根据检查后的结果执行Commit或者Rollback操作\n补偿阶段主要是用于解决生产者在发送Commit或者Rollback操作时发生超时或失败的情况。\n2. RocketMQ事务流程关键 事务消息在一阶段对用户不可见 事务消息相对普通消息最大的特点就是一阶段发送的消息对用户是不可见的，也就是说消费者不能直接消费。这里RocketMQ的实现方法是原消息的主题与消息消费队列，然后把主题改成 RMQ_SYS_TRANS_HALF_TOPIC (消息体中存储了原来的主题和队列id)，这样由于消费者没有订阅这个主题，所以不会被消费。\n如何处理第二阶段的失败消息？\n在本地事务执行完成后会向MQServer发送Commit或Rollback操作，此时如果在发送消息的时候生产者出故障了，那么要保证这条消息最终被消费，MQServer会像服务端发送回查请求，确认本地事务的执行状态。当然了rocketmq并不会无休止的的信息事务状态回查，默认回查15次, 每60s发一次，如果15次回查还是无法得知事务状态，RocketMQ默认回滚该消息。\n消息状态 事务消息有三种状态： TransactionStatus.CommitTransaction：提交事务消息，消费者可以消费此消息 TransactionStatus.RollbackTransaction：回滚事务，它代表该消息将被删除，不允许被消费。 TransactionStatus.Unknown ：中间状态，它代表需要检查消息队列来确定状态。\n配置文件 : org.apache.rocketmq.common.BrokerConfig\n/** * The minimum time of the transactional message to be checked firstly, one message only exceed this time interval * that can be checked. */ @ImportantField private long transactionTimeOut = 6 * 1000; /** * The maximum number of times the message was checked, if exceed this value, this message will be discarded. */ @ImportantField private int transactionCheckMax = 15; /** * Transaction message check interval. */ @ImportantField private long transactionCheckInterval = 60 * 1000; 3. 提交或回滚事务 根据消息所属 的消息队列获取 Broker 的 IP 与端口 信息 ，然后发送结束事务命令 ，其关键就是根据本地执行事务的状态分别发送 **提交 、 回滚或 “不作为”**的命令\n如果结束事务动作为提交事务 ，则执行提交事务逻辑，其关键 现如下\n首先从结束事务请求命令中获取消息的物理偏移量（ commitlogOffset ），其实现逻辑TransactionalMessageService#.commitMessage 实现\n然后恢复消息的主题 消费队列，构建新的消息对象，由 TransactionalMessageService#endMessageTransaction 实现\n然后将消息再次存储在 commitlog 文件中，此时的消息主题则为业务方发送的消息，将被转发到对应的消息消费队列，供消息消费者消费，其实现由 TransactionalMessageService#sendFinalMessage 实现\n消息存储后，删除 prepare 消息，其实现方法并不是真正的删除，而是将 prepare消息存储到 RMQ_SYS_TRANS_OP_HALF TOPIC 主题中，表示该事务消息（ prepare 状态的消息）已经处理过（提交或回滚），为未处理的事务进行事务回查提供查找依据\n事务的回滚与提交的唯一差别是无须将消息恢复原主题，直接删除 prepare 消息即可，同样是将预处理消息存储在 RMQ_SYS_TRANS_OP_HALF _TOPIC 主题中，表示已处理过该消息\n4. springCloud 整合 RocketMQ RocketMQ 与 Spring Cloud Stream整合(八、事务消息） - 简书 (jianshu.com) Spring Cloud Stream + RocketMq实现事务性消息_简单，坚持-CSDN博客 因为 Spring Cloud Stream 在设计时，并没有考虑事务消息, 所以还是用springCloud-rocketMQ的方式\nRocketMQ进阶 - 事务消息 - 知乎 (zhihu.com) RocketMQ入门教程(五)：可靠消息最终一致性(事务消息)_monday的博客-CSDN博客 RocketMQ：(6) 事务消息 - 湮天霸神666 - 博客园 (cnblogs.com) RocketMQ事务型消息 (aliyun.com) RocketMQ RMQ_SYS_TRANS_HALF_TOPIC 爆掉的问题 | 赵烧鸡腿饭的个人博客 (jingzhouzhao.github.io) ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/rocketmq/%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF.html","summary":"[toc] 前言 RocketMQ为我们提供了事务消息的功能，它使得我们投放消息和其他的一些操作保持一个整体的原子性。比如：向数据库中插入数据，再向MQ","title":"事务消息"},{"content":"[toc]\n基于ElasticSearch 6.x 1 字段类型概述 一级分类 二级分类 具体类型 核心类型 字符串类型 string,text,keyword 整数类型 integer,long,short,byte 浮点类型 double,float,half_float,scaled_float 逻辑类型 boolean 日期类型 date 范围类型 range 二进制类型 binary 复合类型 数组类型 array 对象类型 object 嵌套类型 nested 地理类型 地理坐标类型 geo_point 地理地图 geo_shape 特殊类型 IP类型 ip 范围类型 completion 令牌计数类型 token_count 附件类型 attachment 抽取类型 percolator 2 字符串类型 （1）string string类型在ElasticSearch 旧版本中使用较多，从ElasticSearch 5.x开始不再支持string，由text和keyword类型替代。\n（2）text 当一个字段是要被全文搜索的，比如Email内容、产品描述，应该使用text类型。设置text类型以后，字段内容会被分析，在生成倒排索引以前，字符串会被分析器分成一个一个词项。text类型的字段不用于排序，很少用于聚合。\n（3）keyword keyword类型适用于索引结构化的字段，比如email地址、主机名、状态码和标签。如果字段需要进行过滤(比如查找已发布博客中status属性为published的文章)、排序、聚合。keyword类型的字段只能通过精确值搜索到。\n以下 基于 ElasticSearch 7.x :\n关键字族包括以下字段类型：\nkeyword，用于结构化内容，例如 ID、电子邮件地址、主机名、状态代码、邮政编码或标签。 constant_keyword 用于始终包含相同值的关键字字段。 wildcard 为非结构化内容提供。wildcard类型针对具有大值或高基数的字段进行了优化。 Keyword字段通常用于排序、聚合和术语级别的查询，例如术语。\n简单的说,wildcard 相比keyword做了优化,不过也牺牲了一些场景和功能\n格力中使用wildcard是为了解决cpu过高的问题\n详细解释-译文 3 整数类型 类型 取值范围 byte -128~127 short -32768~32767 integer -231~231-1 long -263~263-1 在满足需求的情况下，尽可能选择范围小的数据类型。比如，某个字段的取值最大值不会超过100，那么选择byte类型即可。迄今为止吉尼斯记录的人类的年龄的最大值为134岁，对于年龄字段，short足矣。字段的长度越短，索引和搜索的效率越高。\n4 浮点类型 类型 取值范围 doule 64位双精度IEEE 754浮点类型 float 32位单精度IEEE 754浮点类型 half_float 16位半精度IEEE 754浮点类型 scaled_float 缩放类型的的浮点数 对于float、half_float和scaled_float,-0.0和+0.0是不同的值，使用term查询查找-0.0不会匹配+0.0，同样range查询中上边界是-0.0不会匹配+0.0，下边界是+0.0不会匹配-0.0。\n其中scaled_float，比如价格只需要精确到分，price为57.34的字段缩放因子为100，存起来就是5734 优先考虑使用带缩放因子的scaled_float浮点类型。\n5 date类型 我们人类使用的计时系统是相当复杂的：秒是基本单位, 60秒为1分钟, 60分钟为1小时, 24小时是一天……如果计算机也使用相同的方式来计时, 那显然就要用多个变量来分别存放年月日时分秒, 不停的进行进位运算, 而且还要处理偶尔的闰年和闰秒以及协调不同的时区. 基于”追求简单”的设计理念, UNIX在内部采用了一种最简单的计时方式：\n计算从UNIX诞生的UTC时间1970年1月1日0时0分0秒起, 流逝的秒数. UTC时间1970年1月1日0时0分0秒就是UNIX时间0, UTC时间1970年1月2日0时0分0秒就是UNIX时间86400. 这个计时系统被所有的UNIX和类UNIX系统继承了下来, 而且影响了许多非UNIX系统. 123 日期类型表示格式可以是以下几种： （1）日期格式的字符串，比如 “2018-01-13” 或 “2018-01-13 12:10:30” （2）long类型的毫秒数( milliseconds-since-the-epoch，epoch就是指UNIX诞生的UTC时间1970年1月1日0时0分0秒) （3）integer的秒数(seconds-since-the-epoch)\nElasticSearch 内部会将日期数据转换为UTC，并存储为milliseconds-since-the-epoch的long型整数。\n例如: 2018-01-01 00:00:00将被转换为2017-12-31T23:00:00 UTC.\n另外需要注意的是, now是不受time_zone影响的.\nhttps://www.cnblogs.com/shoufeng/p/11266136.html 6 boolean类型 逻辑类型（布尔类型）可以接受true/false/”true”/”false”值\n7 binary类型 二进制字段是指用base64来表示索引中存储的二进制数据，可用来存储二进制形式的数据，例如图像。默认情况下，该类型的字段只存储不索引。二进制类型只支持index_name属性。\n7 array类型 在ElasticSearch中，没有专门的数组（Array）数据类型，但是，在默认情况下，任意一个字段都可以包含0或多个值，这意味着每个字段默认都是数组类型，只不过，数组类型的各个元素值的数据类型必须相同。在ElasticSearch中，数组是开箱即用的（out of box），不需要进行任何配置，就可以直接使用。\n在同一个数组中，数组元素的数据类型是相同的，ElasticSearch不支持元素为多个数据类型：[ 10, “some string” ]，常用的数组类型是：\n（1）字符数组: [ “one”, “two” ]\n（2）整数数组: productid:[ 1, 2 ]\n（3）对象（文档）数组: “user”:[ { “name”: “Mary”, “age”: 12 }, { “name”: “John”, “age”: 10 }]，ElasticSearch内部把对象数组展开为 {“user.name”: [“Mary”, “John”], “user.age”: [12,10]}\n8 object类型 JSON天生具有层级关系，文档会包含嵌套的对象\n9 ip类型 ip类型的字段用于存储IPv4或者IPv6的地址\n来源: https://blog.csdn.net/chengyuqiang/article/details/79048800 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html","summary":"[toc] 基于ElasticSearch 6.x 1 字段类型概述 一级分类 二级分类 具体类型 核心类型 字符串类型 string,text,keyword 整数类型 integer,long,short,byte 浮点类型 double,float,half_float,scaled_float 逻辑类型 boolean 日期类型 date 范围类型","title":"数据类型"},{"content":"[toc]\n1.什么是数据倾斜 简单的讲，数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了集群中的一台或者几台机器上计算，而集群中的其他节点空闲。这些倾斜了的数据的计算速度远远低于平均计算速度，导致整个计算过程过慢。\n不是数据集中导致倾斜了,++而是计算太过集中++导致倾斜\n2. 数据倾斜发生时的现象： 绝大多数task执行得都非常快，但个别task执行的极慢。 原本能正常执行的Spark作业，某天突然爆出OOM（内存溢出）异常。观察异常栈，是我们写的业务代码造成的 3. 数据倾斜发生的原理 : 在进行shuffle的时候，必须将各个节点上相同的Key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或者join操作。如果某个key对应的数据量特别大的话，会发生数据倾斜。比如大部分key对应的10条数据，但个别key却对应了100万条数据，那么大部分task会只分配到10条数据，而个别task可能会分配了100万数据。整个spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致OOM。\n反正就是过多的key分配到同一个reduce去执行了,而那台机器无法处理那么多,解决方案基本往减轻压力方向靠\n原文链接：https://blog.csdn.net/weixin_35353187/article/details/84303518\n4. 产生原因 数据倾斜原因有以下几方面：\nkey分布不均匀 业务数据本身的特性 (比如北京/上海数据量比长沙数据要多) 建表时考虑不周 (用户表和日志表,共同拥有访问ip字段,但是类型却不一样或者默认值不同之类) 某些SQL语句本身就有数据倾斜 (出现shuffle操作) 5. 解决方案 hive和MR/spark的方案不一样,毕竟一个是写sql,一个是写程序,但是方向是一样的\nhive的sql写好一点会有效避免数据倾斜\n1. 空值产生的数据倾斜\n场景说明 在日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和用户表中的 user_id 相关联，就会碰到数据倾斜的问题。\n解决方案 1：user_id 为空的不参与关联 解决方案 2：赋予空值新的 key 值 方法 2 比方法 1 效率更好，不但 IO 少了，而且作业数也少了，方案 1 中，log 表 读了两次，jobs 肯定是 2，而方案 2 是 1。这个优化适合无效 id（比如-99，’’，null）产 生的数据倾斜，把空值的 key 变成一个字符串加上一个随机数，就能把造成数据倾斜的 数据分到不同的 reduce 上解决数据倾斜的问题。\n改变之处：使本身为 null 的所有记录不会拥挤在同一个 reduceTask 了，会由于有替代的 随机字符串值，而分散到了多个 reduceTask 中了，由于 null 值关联不上，处理后并不影响最终结果。\n1.1 特殊值分开处理\n我们已经知道user_id = 0是一个特殊key，那么可以把特殊值隔离开来单独做join，这样特殊值肯定会转化成map join，非特殊值就是没有倾斜的普通join了：\nselect * from ( select * from logs where user_id = 0 ) a join ( select * from users where user_id = 0 ) b on a.user_id = b.user_id union all select * from logs a join users b on a.user_id \u0026lt;\u0026gt; 0 and a.user_id = b.user_id; https://blog.csdn.net/anshuai_aw1/article/details/84033160 2. 不同数据类型关联产生数据倾斜\n场景说明\n用户表中 user_id 字段为 int，log 表中 user_id 为既有 string 也有 int 的类型， 当按照两个表的 user_id 进行 join 操作的时候，默认的 hash 操作会按照 int 类型的 id 进 行分配，这样就会导致所有的 string 类型的 id 就被分到同一个 reducer 当中\n解决方案: 把数字类型 id 转换成 string 类型的 id 3. 大小表关联查询产生数据倾斜\n注意：使用map join解决小表关联大表造成的数据倾斜问题。这个方法使用的频率很高。 map join 概念：将其中做连接的小表（全量数据）分发到所有 MapTask 端进行 Join，从 而避免了 reduceTask(因为没有reduce了)，前提要求是内存足以装下该全量数据\n以大表 a 和小表 b 为例，所有的 maptask 节点都装载小表 b 的所有数据，然后大表 a 的 一个数据块数据比如说是 a1 去跟 b 全量数据做链接，就省去了 reduce 做汇总的过程。 所以相对来说，在内存允许的条件下使用 map join 比直接使用 MapReduce 效率还高些， 当然这只限于做 join 查询的时候。\n在 hive 中，直接提供了能够在 HQL 语句指定该次查询使用 map join，map join 的用法是 在查询/子查询的SELECT关键字后面添加/*+ MAPJOIN(tablelist) */提示优化器转化为map join（早期的 Hive 版本的优化器是不能自动优化 map join 的）。\n具体用法\nselect /* +mapjoin(a) */ a.id aid, name, age from a join b on a.id = b.id; select /* +mapjoin(movies) */ a.title, b.rating from movies a join ratings b on a.movieid = b.movieid; 在 hive0.11 版本以后会自动开启 map join 优化，由两个参数控制：\nset hive.auto.convert.join=true; //设置 MapJoin 优化自动开启 set hive.mapjoin.smalltable.filesize=25000000 //设置小表不超过多大时开启 mapjoin 优化 https://www.cnblogs.com/qingyunzong/p/8847597.html https://blog.csdn.net/baichoufei90/article/details/86554840 https://blog.csdn.net/weixin_35353187/article/details/84303518 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.html","summary":"[toc] 1.什么是数据倾斜 简单的讲，数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了集群中的一台或者几台机器上计算，而集","title":"数据倾斜"},{"content":"[toc]\n1.介绍,原理,原因 见 hive中的数据倾斜\n2. 解决方案 ++自定义分区++,这需要用户自己继承partition类,指定分区策略,这种方式效果比较显著。\n重新设计key,++有一种方案是在MAP阶段时给KEY加上一个随机数++,有了随机数的key就不会被大量的分配到同一节点(小几率),++待到REDUCE后再把随机数去掉++即可。(大表连接大表的情况可以用)\n++使用combinner合并++,combinner是在map阶段,reduce之前的一个中间阶段,在这个阶段可以选择性的把大量的相同key数据先进行一个合并,可以看做是local reduce,然后再交给reduce来处理,这样做的好处很多,即减轻了map端向reduce端发送的数据量(减轻了网络带宽),也减轻了map端和reduce端中间的shuffle阶段的数据拉取数量(本地化磁盘IO速率)\n原文链接：https://blog.csdn.net/weixin_35353187/article/details/84303518\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.html","summary":"[toc] 1.介绍,原理,原因 见 hive中的数据倾斜 2. 解决方案 ++自定义分区++,这需要用户自己继承partition类,指定分区策略,这种方式效果","title":"数据倾斜"},{"content":"为什么 MongoDB 使用 B 树？\n首先要了解Btree和B+tree之间的区别,详情见mysql的索引\n作为 NoSQL 的 MongoDB，其目标场景就与更早的数据库就有着比较大的差异，我们来简单总结一下 MongoDB 最终选择使用 B 树的两个原因：\nMySQL 使用 B+ 树是因为数据的遍历在关系型数据库中非常常见，它经常需要处理各个表之间的关系并通过范围查询一些数据；但是 MongoDB 作为面向文档的数据库，与数据之间的关系相比，它更看重以文档为中心的组织方式，所以选择了查询单个文档性能较好的 B 树，这个选择对遍历数据的查询也可以保证可以接受的时延；(没有那么多的范围查询) LSM 树是一种专门用来优化写入的数据结构，它将随机写变成了顺序写显著地提高了写入性能，但是却牺牲了读的效率，这与大多数场景需要的特点是不匹配的，所以 MongoDB 最终还是选择读取性能更好的 B 树作为默认的数据结构； https://blog.csdn.net/kexuanxiu1163/article/details/106821401 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E7%B4%A2%E5%BC%95.html","summary":"为什么 MongoDB 使用 B 树？ 首先要了解Btree和B+tree之间的区别,详情见mysql的索引 作为 NoSQL 的 MongoDB，其目标场景就与更早的数据库就有","title":"索引"},{"content":"[toc]\n前言 说到索引，很多人都知道“索引是一个排序的列表，在这个列表中存储着索引的值和包含这个值的数据所在行的物理地址，在数据十分庞大的时候，索引可以大大加快查询的速度，这是因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据。”\n但是索引是怎么实现的呢？因为索引并不是关系模型的组成部分，因此不同的DBMS有不同的实现，我们针对MySQL数据库的实现进行说明。\n一、MySQL中索引的语法 创建索引\n在创建表的时候添加索引\nCREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); 在创建表以后添加索引\nALTER TABLE my_table ADD [UNIQUE] INDEX index_name(column_name);\n或者\nCREATE INDEX index_name ON my_table(column_name);\n注意：\n1、索引需要占用磁盘空间，因此在创建索引时要考虑到磁盘空间是否足够\n2、创建索引时需要对表加锁，因此实际操作中需要在业务空闲期间进行\n原文链接：https://blog.csdn.net/tongdanping/article/details/79878302\n删除索引\nDROP INDEX my_index ON tablename；\n或者\nALTER TABLE table_name DROP INDEX index_name;\n查看表中的索引\nSHOW INDEX FROM tablename\n查看查询语句使用索引的情况\nexplain SELECT * FROM table_name WHERE column_1='123'; 原文链接：https://blog.csdn.net/tongdanping/article/details/79878302\n二、索引的优缺点 优势：可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序；\n劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的1.5倍；\n索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；\n构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表；\n原文链接：https://blog.csdn.net/tongdanping/article/details/79878302\n三、索引的分类 常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引\n1、主键索引：即主索引，根据主键pk_clolum（length）建立索引，不允许重复，不允许空值；\nALTER TABLE \u0026rsquo;table_name\u0026rsquo; ADD PRIMARY KEY pk_index(\u0026lsquo;col\u0026rsquo;)；\n2、唯一索引：用来建立索引的列的值必须是唯一的，允许空值\nALTER TABLE \u0026rsquo;table_name\u0026rsquo; ADD UNIQUE index_name(\u0026lsquo;col\u0026rsquo;)；\n3、普通索引：用表中的普通列构建的索引，没有任何限制\nALTER TABLE \u0026rsquo;table_name\u0026rsquo; ADD INDEX index_name(\u0026lsquo;col\u0026rsquo;)；\n4、全文索引：用大文本对象的列构建的索引（下一部分会讲解）\nALTER TABLE \u0026rsquo;table_name\u0026rsquo; ADD FULLTEXT INDEX ft_index(\u0026lsquo;col\u0026rsquo;)；\n5、组合索引：用多个列组合构建的索引，这多个列中的值不允许有空值\nALTER TABLE \u0026rsquo;table_name\u0026rsquo; ADD INDEX index_name(\u0026lsquo;col1\u0026rsquo;,\u0026lsquo;col2\u0026rsquo;,\u0026lsquo;col3\u0026rsquo;)；\n*遵循“最左前缀”原则，把最常用作为检索或排序的列放在最左，依次递减，组合索引相当于建立了col1,col1col2,col1col2col3三个索引，而col2或者col3是不能使用索引的。\n如果设立一个组合索引 (a,b,c), 如何使用才会生效?\n1、组合索引字段无论顺序如何改变都会用到索引，前提是所有字段都在where条件上\n2、如果想要使用一个或者两个字段在where条件上，必须有组合索引里的第一个字段，但是与顺序无关，例如a,c或c,a，这种场景是可以命中索引的。但是，b,c或c,b这种是不会命中索引的。\n3、order by 只能使用a，才能用到索引\n*在使用组合索引的时候可能因为列名长度过长而导致索引的key太大，导致效率降低，在允许的情况下，可以只取col1和col2的前几个字符作为索引\nALTER TABLE 'table_name' ADD INDEX index_name(col1(4),col2（3))；\n表示使用col1的前4个字符和col2的前3个字符作为索引\n6、前缀索引：当索引是很长的字符序列时，这个索引将会很占内存，而且会很慢，这时候就会用到前缀索引了。所谓的前缀索引就是去索引的前面几个字母作为索引，但是要降低索引的重复率，索引我们还必须要判断前缀索引的重复率\nalter table test add key(name(4));\nselect 1.0*count(distinct name)/count(*) from test 这是比较整个name的重复率,选择较好的索引长度\nhttps://blog.csdn.net/ma2595162349/article/details/79449493 原文链接：https://blog.csdn.net/tongdanping/article/details/79878302\n四、索引的实现原理 MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，B+Tree索引，哈希索引，全文索引等等，\n4.1、哈希索引 只有memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的hashCode，然后在hashCode相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个hashCode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能。\n缺点:\n0、因为Hash索引比较的是经过Hash计算的值，所以只能进行等式比较，不能用于范围查询\n1、每次都要全表扫描\n2、由于哈希值是按照顺序排列的，但是哈希值映射的真正数据在哈希表中就不一定按照顺序排列，所以无法利用Hash索引来加速任何排序操作\n3、不能用部分索引键来搜索，因为组合索引在计算哈希值的时候是一起计算的。\n4、当哈希值大量重复且数据量非常大时，其检索效率并没有Btree索引高的。\nhttps://www.cnblogs.com/zhidongjian/p/10414129.html 4.2、全文索引 FULLTEXT（全文）索引，仅可用于MyISAM和InnoDB，针对较大的数据，生成全文索引非常的消耗时间和空间。对于文本的大对象，或者较大的CHAR类型的数据，如果使用普通索引，那么匹配文本前几个字符还是可行的，但是想要匹配文本中间的几个单词，那么就要使用LIKE %word%来匹配，这样需要很长的时间来处理，响应时间会大大增加，这种情况，就可使用时FULLTEXT索引了，在生成FULLTEXT索引时，会为文本生成一份单词的清单，在索引时及根据这个单词的清单来索引。FULLTEXT可以在创建表的时候创建，也可以在需要的时候用ALTER或者CREATE INDEX来添加：\n全文索引的查询也有自己特殊的语法，而不能使用LIKE %查询字符串%的模糊查询语法\nSELECT * FROM table_name MATCH(ft_index) AGAINST('查询字符串');\n注意：\n*对于较大的数据集，把数据添加到一个没有FULLTEXT索引的表，然后添加FULLTEXT索引的速度比把数据添加到一个已经有FULLTEXT索引的表快。\n*5.6版本前的MySQL自带的全文索引只能用于MyISAM存储引擎，如果是其它数据引擎，那么全文索引不会生效。5.6版本之后InnoDB存储引擎开始支持全文索引\n*在MySQL中，全文索引支队英文有用，目前对中文还不支持。5.7版本之后通过使用ngram插件开始支持中文。\n*在MySQL中，如果检索的字符串太短则无法检索得到预期的结果，检索的字符串长度至少为4字节，此外，如果检索的字符包括停止词，那么停止词会被忽略。\n五、索引数据结构 为什么用B/B+树这种结构来实现索引呢？\n答：红黑树等结构也可以用来实现索引，但是文件系统及数据库系统普遍使用B/B+树结构来实现索引。mysql是基于磁盘的数据库，索引是以索引文件的形式存在于磁盘中的，索引的查找过程就会涉及到磁盘IO(为什么涉及到磁盘IO请看文章后面的附加理解部分)消耗，磁盘IO的消耗相比较于内存IO的消耗要高好几个数量级，所以索引的组织结构要设计得在查找关键字时要尽量减少磁盘IO的次数。为什么要使用B/B+树，跟磁盘的存储原理有关。\n局部性原理与磁盘预读\n为了提升效率，要尽量减少磁盘IO的次数。实际过程中，磁盘并不是每次严格按需读取，而是每次都会预读。磁盘读取完需要的数据后，会按顺序再多读一部分数据到内存中，这样做的理论依据是计算机科学中注明的局部性原理：\n当一个数据被用到时，其附近的数据也通常会马上被使用;程序运行期间所需要的数据通常比较集中\n（1）由于磁盘顺序读取的效率很高(不需要寻道时间，只需很少的旋转时间)，\n因此对于具有局部性的程序来说，预读可以提高I/O效率.预读的长度一般为页(page)的整倍数。\n（2）MySQL(默认使用InnoDB引擎),将记录按照页的方式进行管理,每页大小默认为16K(这个值可以修改)。linux 默认页大小为4K。\n为什么不用红黑树?\nB-Tree可以借助计算机磁盘预读的机制，并使用如下技巧：\n每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个结点只需一次I/O。\n假设 B-Tree 的高度为 h,B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3，也即索引的B+树层次一般不超过三层，所以查找效率很高）。\n而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。\n链接：https://www.jianshu.com/p/0371c9569736\nhttps://blog.csdn.net/kongmin_123/article/details/82055901 每次插入和删除都会更新树,树需要重新达到平衡(或者说需要保持定义),btree和b+tree的操作有点不同,\n具体图解:\nhttps://blog.csdn.net/disiwei1012/article/details/78632859 https://www.cnblogs.com/nullzx/p/8729425.html 5.1 BTree索引 也就是B-tree https://ac.nowcoder.com/discuss/299888?type=1\u0026order=0\u0026pos=11\u0026page=1 BTree是平衡多路搜索树，设树的度为2d（d\u0026gt;1），高度为h，那么BTree要满足以下条件：\n每个叶子结点的高度一样，等于h； 每个非叶子结点由n-1个key和n个指针point组成，其中d\u0026lt;=n\u0026lt;=2d,key和point相互间隔，结点两端一定是key； 叶子结点指针都为null； 非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据； 多路搜索树是指每个节点的孩子数可以有多个,每个的节点存储的数据也可以有多个, 普通的二叉树只能一个节点数据+两个孩子,\n一个节点不像图中画的只有几个数值,为了方便理解嘛,可能是成千上万个数值, **每个索引节点一般都是操作系统页的整数倍,**InnoDB默认是16K,到时候查出来的数据,在内存中进行筛选(索引数据都是在磁盘上的)\n一个千万量级，且存储引擎是MyISAM或者InnoDB的表，其索引树的高度在3~5之间\n\u0026gt;来自: https://cloud.tencent.com/developer/news/373193 BTree的结构如下：\n在BTree的机构下，就可以使用二分查找的查找方式，时间复杂度为h*log(n)，一般来说树的高度是很小的，h一般为3左右，因此BTree是一个非常高效的查找结构。\n5.2 B+Tree索引 B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于：\nB+Tree中的非叶子结点不存储数据，只存储键值； B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址； B+Tree的每个非叶子节点由n个键值key和n个指针point组成； B+Tree的结构如下：\n![img](F:\\学习资料\\个人笔记\\MDImages\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E7%B4%A2%E5%BC%95.html","summary":"[toc] 前言 说到索引，很多人都知道“索引是一个排序的列表，在这个列表中存储着索引的值和包含这个值的数据所在行的物理地址，在数据十分庞大的时候，索引","title":"索引"},{"content":"[toc]\n1. 数据结构 索引结构与mysql的一样, 都是使用B+tree\n只是oracle这里叫bTree\n如果存储结构是索引组织表, 才和mysql一样\nOracle中索引的原理 - 想飞_毛毛虫 - 博客园 (cnblogs.com) 2. 失效场景 与mysql的基本一致\n3. 执行计划 获取执行计划的方法\n6种方法：\n1. explain plan for获取（即PL/SQL中的F5）； 2. set autotrace on （跟踪性能统计）； 3. statistics_level=all（获取表访问次数）; 4. 通过dbms_xplan.display_cursor输入sql_id参数直接获取 5. 10046 trace跟踪 6. awrsqrpt.sql 各自的适用场景：\n1.如果某SQL执行非常长时间才会出结果，甚至慢到返回不了结果，这时候看执行计划就只能用方法1，或者方法4调用现成的； 2.跟踪某条SQL最简单的方法是方法1，其次就是方法2； 3.如果想观察到某条SQL有多条执行计划的情况，只能用方法4和方法6； 4.如果SQL中含有多函数，函数中套有SQL等多层递归调用，想准确分析，只能使用方法5； 5.要想确保看到真实的执行计划，不能用方法1和方法2； 6.要想获取表被访问的次数，只能使用方法3； 重点讲一下第一种\nEXPLAIN PLAN FOR命令不会运行 SQL 语句，因此创建的执行计划不一定与执行该语句时的实际计划相同。\n该命令会将生成的执行计划保存到全局的临时表 PLAN_TABLE 中，然后使用系统包 DBMS_XPLAN 中的存储过程格式化显示该表中的执行计划。\n使用步骤\n步骤1：explain plan for \u0026ldquo;你的SQL\u0026rdquo;\n步骤2：select * from table(dbms_xplan.display());\n案例介绍\n接下来，我们同样需要理解执行计划中各种信息的含义：\nPlan hash value 是该语句的哈希值。SQL 语句和执行计划会存储在库缓存中，哈希值相同的语句可以重用已有的执行计划，也就是软解析；\nId 是一个序号，但不代表执行的顺序。执行的顺序按照缩进来判断，缩进越多的越先执行，同样缩进的从上至下执行。Id 前面的星号表示使用了谓词判断，参考下面的 Predicate Information；\nOperation 表示当前的操作，也就是如何访问表的数据、如何实现表的连接、如何进行排序操作等；\nName 显示了访问的表名、索引名或者子查询等，前提是当前操作涉及到了这些对象；\nRows 是 Oracle 估计的当前操作返回的行数，也叫基数（Cardinality）；\nBytes 是 Oracle 估计的当前操作涉及的数据量\nCost (%CPU) 是 Oracle 计算执行该操作所需的代价；\nTime 是 Oracle 估计执行该操作所需的时间；\nPredicate Information 显示与 Id 相关的谓词信息。access 是访问条件，影响到数据的访问方式（扫描表还是通过索引）；filter 是过滤条件，获取数据后根据该条件进行过滤。\n在上面的示例中，Id 的执行顺序依次为 3 -\u0026gt; 2 -\u0026gt; 5 -\u0026gt; 4- \u0026gt;1。首先，Id = 3 扫描主键索引 DEPT_ID_PK，Id = 2 按主键 ROWID 访问表 DEPARTMENTS，结果已经排序；其次，Id = 5 全表扫描访问 EMPLOYEES 并且利用 filter 过滤数据，Id = 4 基于部门编号进行排序和过滤；最后 Id = 1 执行合并连接。显然，此处 Oracle 选择了排序合并连接的方式实现两个表的连接。\n一文搞懂各种数据库SQL执行计划：MySQL、Oracle等 - 知乎 (zhihu.com) 读懂Oracle执行计划（一）_Dongguabai的博客-CSDN博客_看懂oracle执行计划 ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/oracle/%E7%B4%A2%E5%BC%95.html","summary":"[toc] 1. 数据结构 索引结构与mysql的一样, 都是使用B+tree 只是oracle这里叫bTree 如果存储结构是索引组织表, 才和mysql一样 Or","title":"索引"},{"content":"[toc]\n1 . 2. 死锁 2.1 死锁是怎么被发现的？ 2.1.1 死锁成因\u0026amp;\u0026amp;检测方法 与java中死锁的成因一样,都是互相等待资源造成,\ninnodb是怎么探知死锁的？\n直观方法是在两个事务相互等待时，当一个等待时间超过设置的某一阀值时，对其中一个事务进行回滚，另一个事务就能继续执行。这种方法简单有效，在innodb中，参数innodb_lock_wait_timeout用来设置超时时间。\n仅用上述方法来检测死锁太过被动，innodb还提供了wait-for graph算法来主动进行死锁检测，每当加锁请求无法立即满足需要并进入等待时，wait-for graph算法都会被触发\n2.1.2 wait-for graph原理 他们相互等待对方的资源，而且形成环路！我们将每辆车看为一个节点，当节点1需要等待节点2的资源时，就生成一条有向边指向节点2，最后形成一个有向图。我们只要检测这个有向图是否出现环路即可，出现环路就是死锁！这就是wait-for graph算法\n2.2 innodb隔离级别、索引与锁 2.3 死锁成因 2.3.1不同表相同记录行锁冲突 这种情况很好理解，事务A和事务B操作两张表，但出现循环等待锁情况\n2.3.2相同表记录行锁冲突 这种情况比较常见，之前遇到两个job在执行数据批量更新时，jobA处理的的id列表为[1,2,3,4]，而job处理的id列表为[8,9,10,4,2]，这样就造成了死锁。\n2.3.3不同索引锁冲突 这种情况比较隐晦，事务A在执行时，除了在二级索引加锁外，还会在聚簇索引上加锁，在聚簇索引上加锁的顺序是[1,4,2,3,5]，而事务B执行时，只在聚簇索引上加锁，加锁顺序是[1,2,3,4,5]，这样就造成了死锁的可能性\n2.3.4 gap锁(间隙锁)冲突 innodb在RR级别下，如下的情况也会产生死锁，比较隐晦。不清楚的同学可以自行根据上节的gap锁原理分析下。\n2.4 如何尽可能避免死锁 1）以固定的顺序访问表和行。比如对第2节两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；又比如对于3.1节的情形，将两个事务的sql顺序调整为一致，也能避免死锁。\n2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。\n3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。\n4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。\n5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。\n2.5.怎么解决死锁 通过应用业务日志定位到问题代码，找到相应的事务对应的sql； 查看数据库死锁相关日志,确定死锁情况; 找到死锁的进程,手动kill一个或多个进程(破坏环路) 一般来说,数据库会帮我们检测死锁并破坏死锁情况\nshow engine innodb status; 查看引擎信息, 包含了死锁信息\nMySQL死锁日志的查看和分析 3. 意向锁 在Innodb引擎支持表锁和行锁，意向锁将提高锁判断的效率,并可以让行锁和表锁同时存在 .\n行锁分为共享锁，一个事务对一行的共享只读锁。排它锁，一个事务对一行的排他读写锁。\nMyISAM 引擎中只有表锁\n共享锁:加了读锁，只允许别的事务继续加读锁而不能加写锁，也就是只读\n排它锁: 加了写锁，别的事务不允许加任何锁。\n来自: https://www.zhihu.com/question/51513268 考虑一个例子:\n事物A获得了一个行锁,事物B想来获得一个表锁,此时表锁应该不能成功,不然就冲突了.但是事物B要去判断是否会冲突,就会判断有没有表锁和行锁,而行锁只能一行行记录去判断,效率低下,所以此时需要意向锁. 当事务A想获得行锁时,会自动先获得一个意向锁,成功后才能获得行锁; 此时事务B判断完表锁后,再判断共享锁,这样就减少了判断次数\n意向锁分为 意向共享锁和意向排它锁, 共享锁可以有多把, 从其特性看,意向锁是表锁,所以它可以让行锁和表锁同时存在\n意向锁 表锁和行锁共存 4. for update for update 仅适用于InnoDB，并且必须开启事务，在begin与commit之间才生效。\n这叫获得写锁或者排它锁 , 获得排他锁后,别人将不能获得排它锁 , 也不能修改该数据,只能读取\nSELECT * FROM user WHERE id = 1 FOR UPDATE;\nInnoDB 默认是行级锁，当有明确指定的主键/索引时候(索引要生效)，是行级锁，否则是表级锁。 如果没有命中数据则不会锁 5.间隙锁Gap Locks 间隙锁基于非唯一索引，它锁定一段范围内的索引记录。间隙锁基于下面将会提到的Next-Key Locking 算法，请务必牢记：使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据, (这个区间内有几条就锁几条, 并且想修改或者插入到这个区间都不允许)。\n加锁规则有以下特性:\n加锁的基本单位是 临键锁（next-key lock）,他是前开后闭原则\n插叙过程中访问的对象会增加锁\n索引上的等值查询\u0026ndash;给唯一索引加锁的时候，next-key lock升级为行锁\n索引上的等值查询\u0026ndash;向右遍历时最后一个值不满足查询需求时，next-key lock 退化为间隙锁\n唯一索引上的范围查询会访问到不满足条件的第一个值为止\n例如:\nSELECT * FROM 表名称 WHERE id BETWEN 1 AND 10 FOR UPDATE;\n即所有在（1，10）区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。\n简单记忆: 就是锁 区间的,但是不包括边界 , 只包括 区间内的值以及它们的\u0026quot;间隙\u0026quot;\n间隙锁存在于非唯一索引中，锁定开区间范围内的一段间隔，它是基于临键锁实现的。在索引记录之间的间隙中加锁，或者是在某一条索引记录之前或者之后加锁，并不包括该索引记录本身。 临键锁存在于非唯一索引中，该类型的每条记录的索引上都存在这种锁，它是一种特殊的间隙锁，锁定一段左开右闭的索引区间。即，除了锁住记录本身，也锁住索引之间的间隙。 MySQL如何解决幻读和不可重复读？ MYSQL（04）-间隙锁详解 - 简书 (jianshu.com) 6. 临键锁Next-Key Locks Next-Key 可以理解为一种特殊的间隙锁，本质是间隙锁和行锁的结合。 通过临建锁和MVCC可以解决幻读的问题。 每个数据行上的非唯一索引列上都会存在一把临键锁，当某个事务持有该数据行的临键锁时，会锁住一段左开右闭区间的数据。需要强调的一点是，InnoDB 中行级锁是基于索引实现的，临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁。\n假设有如下表：\n引擎：InnoDB，隔离级别：Repeatable-Read：table(id PK, age KEY, name )\nid age name 1 10 Lee 3 24 Soraka 5 32 Zed 7 45 Talon 该表中 age 列潜在的临键锁有：\n(-∞, 10], (10, 24], (24, 32], (32, 45], (45, +∞], 执行如下命令：\n-- 根据非唯一索引列 UPDATE 某条记录 UPDATE table SET name = Vladimir WHERE age = 24; 因为24在边界上, 会锁住 上一个区间和下一个区间, 也就是说会锁住 (10,32)\n执行如下命令：\n-- 根据非唯一索引列 UPDATE 某条记录 UPDATE table SET name = Vladimir WHERE age = 22; 22在区间内, 会锁住 其所在区间, 也就是说会锁住 (10,24]\n执行如下命令:\n-- 根据非唯一索引列 delete 不存在的值 DELETE FROM table WHERE age = \u0026#39;100\u0026#39;;// 100是不存在的值 会锁住100到无穷大都会锁住 (100,∞],\n一次MySQL死锁问题的排查与分析(一) | Throwable (throwx.cn) MySQL:insert 加锁 - 简书 (jianshu.com) 面试官：MySQL的UPDATE语句会加哪些锁？ | 毛英东的个人博客 (maoyingdong.com) MDL锁 意向锁 行锁 间隙锁 Next-key Locking ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%94%81.html","summary":"[toc] 1 . 2. 死锁 2.1 死锁是怎么被发现的？ 2.1.1 死锁成因\u0026amp;\u0026amp;检测方法 与java中死锁的成因一样,都是互相等待资源造成, innodb是怎么","title":"锁"},{"content":"[toc]\n1.假如我有 100亿条数据，但是我们的内存只有1M，但是我们磁盘很大 我们现在要对这100亿条数据进行排序，是没法把所有的数据一次性的load进行内存进行排序的，这就涉及到一个外部排序的问题，我们的1M内存只能装进1亿条数据，每次都只能对这 1亿条数据进行排序，排好序后输出到磁盘，总共输出100个文件，最后怎么把这100个文件进行merge成一个全局有序的大文件。\n我们可以每个文件（有序的）都取一部分头部数据最为一个 buffer， 并且把这 100个 buffer放在一个堆里面，进行堆排序，比较方式就是对所有堆元素（buffer）的head元素进行比较大小， 然后不断的把每个堆顶的 buffer 的head 元素 pop 出来输出到最终文件中，(想象链表的有序合并) 然后继续堆排序，继续输出。如果哪个buffer 空了，就去对应的文件中继续补充一部分数据。最终就得到一个全局有序的大文件。\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/%E9%80%9A%E7%94%A8.html","summary":"[toc] 1.假如我有 100亿条数据，但是我们的内存只有1M，但是我们磁盘很大 我们现在要对这100亿条数据进行排序，是没法把所有的数据一次性的loa","title":"通用"},{"content":"1.RPC: RPC（Remote Procedure Call）—\n远程过程调用 ，它是一种通过网络 从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。\n客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息 ，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。\n运行时,一次客户机对服务器的RPC调用,其内部操作大致有如下十步：\n1.调用客户端句柄；执行传送参数\n2.调用本地系统内核 发送网络消息\n3.消息传送 到远程主机 4.服务器句柄得到消息并取得参数\n5.执行远程过程\n6.执行的过程将结果返回服务器句柄\n7.服务器句柄返回结果，调用远程系统内核 8.消息传回本地主机 9.客户句柄由内核接收消息\n10.客户接收句柄返回的数据\n来自链接 \u0026gt;\n2.Hessian:\nHessian是一个轻量级的remoting onhttp工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。\nDubbo大多使用的是Hessian协议\n来自* \u0026lt;https://baike.baidu.com/item/Hessian \u0026gt;\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E5%BE%AE%E6%9C%8D%E5%8A%A1.html","summary":"1.RPC: RPC（Remote Procedure Call）— 远程过程调用 ，它是一种通过网络 从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。 客户机调用进程","title":"微服务"},{"content":"安装spynner:\n①. 需先安装pyqt4,\n下载地址:https://www.riverbankcomputing.com/software/pyqt/download/https://www.riverbankcomputing.com/software/pyqt/download\n②. 安装 autopy:\n下载地址: https://pypi.python.org/pypi/autopy/0.51 注: 安装时会自动安装在python下,我用的是Anaconda中的python,所有不成功,下载二进制文件安装autopy-0.51-cp27-cp27m-win_amd64.whl\n安装命令: pip install autopy-0.51-cp27-cp27m-win_amd64.whl\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/%E9%97%AE%E9%A2%98.html","summary":"安装spynner: ①. 需先安装pyqt4, 下载地址:https://www.riverbankcomputing.com/software","title":"问题"},{"content":"centos\n配置ip地址 vim /etc/sysconfig/network-scripts/ifcfg-eno* 配置静态IP\nHWADDR=00:0C:29:8D:24:73 TYPE=Ethernet BOOTPROTO=static #启用静态IP地址 DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_PEERDNS=yes IPV6_PEERROUTES=yes IPV6_FAILURE_FATAL=no NAME=eno16777736 UUID=ae0965e7-22b9-45aa-8ec9-3f0a20a85d11 ONBOOT=yes #开启自动启用网络连接 IPADDR=192.168.21.128 #设置IP地址 GATEWAY=192.168.21.2 #设置网关 DNS1=8.8.8.8 #设置主DNS DNS2=8.8.4.4 #设置备DNS 重启网卡 service network restart 或者 systemctl restart network.service\n注意网卡物理地址要正确, 使用ip addr 命令查看的地址和要HWADDR一样,不然会启动失败\n开启/关闭网卡 ip link set eth0 up/down\n开启网络 vim /etc/sysconfig/network NETWORKING=yes\n表示系统是否使用网络，一般设置为yes。如果设为no，则不能使用网络。\n建议关闭防火墙 systemctl stop firewalld.service 查看防火墙状态 systemctl status firewalld.service 禁止firewall开机启动 systemctl disable firewalld.service service iptables stop\n更换pip源 阿里云 http://mirrors.aliyun.com/pypi/simple/\n豆瓣(douban) http://pypi.douban.com/simple/\n清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/\n中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/\nlinux: 修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下： [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple windows: 直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini，内容如下 [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple 注: 如果地址不是http的,例如阿里,则还需添加 trusted-host = mirrors.aliyun.com\n文件物理属性(另一种权限) chattr: 修改属性\nlsattr: 查看属性\neg. sudo chattr +i a.txt\neg. lsattr a.txt\n服务器间传递文件(需要知道密码,或者有密钥支持) scp local_file remote_username@remote_ip:remote_folder 来自 http://www.runoob.com/linux/linux-comm-scp.html eg:\n​\tscp a.txt hadoop@slave01:~/data ​\t将a.txt文件传输到slave01的~/data文件夹下\n​ scp -r aa hadoop@slave01:~/data ​ 将aa文件夹传输到slave01的~/data文件夹下 ​ ​ ​ ​ ​ ​ ​\n","permalink":"https://xiaokunji.com/zh/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86/%E7%B3%BB%E7%BB%9F%E7%BA%A7%E5%91%BD%E4%BB%A4.html","summary":"centos 配置ip地址 vim /etc/sysconfig/network-scripts/ifcfg-eno* 配置静态IP HWADDR=00:0C:29:8D:24:73 TYPE=Ethernet BOOTPROTO=static #启用静态IP地址 DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_PEERDNS=yes IPV6_PEERROUTES=yes IPV6_FAILURE_FATAL=no NAME=eno16777736 UUID=ae0965e7-22b9-45aa-8ec9-3f0a20a85d11 ONBOOT=yes #开启自动启用网络连接 IPADDR=192.168.21.128 #设置IP地址 GATEWAY=192.168.21.2 #设置网关 DNS1=8.8.8.8 #设置主","title":"系统级命令"},{"content":"在项目中,下载普通图片(png,jpg等)都正常,但是svg图片不能下载,主要问题是返回的请求头不对,要写成如下:\n\u0026#34;Content-Type\u0026#34;, \u0026#34;image/svg+xml;charset=UTF-8\u0026#34; 完整代码:\n@RequestMapping(\u0026#34;/downLoadImage\u0026#34;) public BaseResponse downLoadImage(@Param(\u0026#34;imageName\u0026#34;) String imageName, HttpServletResponse resp) { logger.info(\u0026#34;下载图片开始-入参：{}\u0026#34;, imageName); BaseResponse response = new BaseResponse(); FileImageInputStream fs = null; ServletOutputStream os = null; try { if (StringUtil.isNullOrEmpty(imageName)) { logger.info(\u0026#34;图片名称为空\u0026#34;); setResponse(response, ResponseCode.JIESHUN0003.getCode()); return response; } File file = ImageUtil.downLoadImage(imageName, response); if (file == null) { logger.info(\u0026#34;没有找到图片..\u0026#34;); return response; } // 如果是svg格式后缀的头信息不一样 String suffix = imageName.substring(imageName.lastIndexOf(\u0026#34;.\u0026#34;)); fs = new FileImageInputStream(file); int len = (int) fs.length(); byte[] bs = new byte[len]; fs.read(bs, 0, len); if (\u0026#34;.svg\u0026#34;.equals(suffix)) { resp.setHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;image/svg+xml;charset=UTF-8\u0026#34;); resp.setHeader(\u0026#34;Accept-Ranges\u0026#34;, \u0026#34;bytes\u0026#34;); } else { resp.setHeader(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/octet-stream\u0026#34;); resp.setHeader(\u0026#34;Content-Disposition\u0026#34;, \u0026#34;attachment;filename=\u0026#34; + file.getName()); } os = resp.getOutputStream(); os.write(bs); os.flush(); } catch (Exception e) { logger.error(\u0026#34;下载图片失败，入参：{}\u0026#34;, imageName, e); setSysErrorResponse(response); return response; } finally { logger.info(\u0026#34;下载图片结束。 返参：{}\u0026#34;, response); try { if (fs != null) { fs.close(); } } catch (IOException e) { logger.error(\u0026#34;流关闭异常！\u0026#34;, e); } try { if (os != null) { os.close(); } } catch (IOException e) { logger.error(\u0026#34;流关闭异常！\u0026#34;, e); } } return null; } ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E4%B8%8B%E8%BD%BDsvg%E5%9B%BE%E7%89%87.html","summary":"在项目中,下载普通图片(png,jpg等)都正常,但是svg图片不能下载,主要问题是返回的请求头不对,要写成如下: \u0026#34;Content-Type\u0026#34;, \u0026#34;image/svg+xml;charset=UTF-8\u0026#34; 完整代码: @RequestMapping(\u0026#34;/downLoadImage\u0026#34;) public BaseResponse downLoadImage(@Param(\u0026#34;imageName\u0026#34;) String imageName,","title":"下载svg图片"},{"content":"[TOC]\n1. 前言 线程池解决的核心问题就是资源管理问题。在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。\n为解决资源分配这个问题，线程池采用了“池化”（Pooling）思想。池化，顾名思义，是为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。\n“池化”思想不仅仅能应用在计算机领域，在金融、设备、人员管理、工作管理等领域也有相关的应用。\n在计算机领域中的表现为：统一管理IT资源，包括服务器、存储、和网络资源等等。通过共享资源，使用户在低投入中获益。除去线程池，还有其他比较典型的几种使用策略包括：\n内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。 连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。 实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。 jdk 提供了快捷创建线程池的方式\nExecutors 类下提供了定长/延时/单一等线程池,但由于上线使用系统资源,一般不建议使用\n手动创建ThreadPoolExecutor对象\n其核心参数:\n/** * corePoolSize – 要保留在池中的线程数，即使它们处于空闲状态，除非设置了allowCoreThreadTimeOut maximumPoolSize – 池中允许的最大线程数 keepAliveTime – 当线程数大于核心数时，这是多余空闲线程在终止前等待新任务的最长时间。 unit – keepAliveTime参数的时间单位 workQueue – 用于在执行任务之前保存任务的队列。 这个队列将只保存execute方法提交的Runnable任务 */ public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } 其执行流程\n为什么一般不建议使用Executors创建线程池？_damokelisijian866的博客-CSDN博客 2、线程池核心设计与实现 2.1 总体设计 Java中的线程池核心实现类是ThreadPoolExecutor，本章基于JDK 1.8的源码来分析Java线程池的核心设计与实现。我们首先来看一下ThreadPoolExecutor的UML类图，了解下ThreadPoolExecutor的继承关系。\nThreadPoolExecutor实现的顶层接口是Executor，顶层接口Executor提供了一种思想：将任务提交和任务执行进行解耦。用户无需关注如何创建线程，如何调度线程来执行任务，用户只需提供Runnable对象，将任务的运行逻辑提交到执行器(Executor)中，由Executor框架完成线程的调配和任务的执行部分。ExecutorService接口增加了一些能力：\n（1）扩充执行任务的能力，补充可以为一个或一批异步任务生成Future的方法；\n（2）提供了管控线程池的方法，比如停止线程池的运行。\nAbstractExecutorService则是上层的抽象类，将执行任务的流程串联了起来，保证下层的实现只需关注一个执行任务的方法即可。\n最下层的实现类ThreadPoolExecutor实现最复杂的运行部分，ThreadPoolExecutor将会一方面维护自身的生命周期，另一方面同时管理线程和任务，使两者良好的结合从而执行并行任务。\nThreadPoolExecutor是如何运行，如何同时维护线程和执行任务的呢？其运行机制如下图所示：\n线程池在内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。\n线程池的运行主要分成两部分：任务管理、线程管理。\n任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：\n直接申请线程执行该任务；\n缓冲到队列中等待线程执行；\n拒绝该任务。\n线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。\n2.2 生命周期管理 线程池运行的状态，并不是用户显式设置的，而是伴随着线程池的运行，由内部来维护。线程池内部使用一个变量维护两个值：运行状态(runState)和线程数量 (workerCount)。在具体实现中，线程池将运行状态(runState)、线程数量 (workerCount)两个关键参数的维护放在了一起，如下代码所示：\nprivate final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); ctl这个AtomicInteger类型，是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段， 它同时包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，高3位保存runState，低29位保存workerCount，两个变量之间互不干扰。用一个变量去存储两个值，可避免在做相关决策时，出现不一致的情况，不必为了维护两者的一致，而占用锁资源。通过阅读线程池源代码也可以发现，经常出现要同时判断线程池运行状态和线程数量的情况。线程池也提供了若干方法去供用户获得线程池当前的运行状态、线程个数。这里都使用的是位运算的方式，相比于基本运算，速度也会快很多。\n关于内部封装的获取生命周期状态、获取线程池线程数量的计算方法如以下代码所示：\nprivate static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } //计算当前运行状态 private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } //计算当前线程数量 private static int ctlOf(int rs, int wc) { return rs | wc; } //通过状态和线程数生成ctl ThreadPoolExecutor的运行状态有5种，分别为：\n2.3 任务执行机制 2.3.1 任务调度 任务调度是线程池的主要入口，当用户提交了一个任务，接下来这个任务将如何执行都是由这个阶段决定的。了解这部分就相当于了解了线程池的核心运行机制。\n首先，所有任务的调度都是由execute方法完成的，这部分完成的工作是：检查现在线程池的运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务。其执行过程如下：\n首先检测线程池运行状态，如果不是RUNNING，则直接拒绝，线程池要保证在RUNNING的状态下执行任务。 如果workerCount \u0026lt; corePoolSize，则创建并启动一个线程来执行新提交的任务。 如果workerCount \u0026gt;= corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中。 如果workerCount \u0026gt;= corePoolSize \u0026amp;\u0026amp; workerCount \u0026lt; maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务。 如果workerCount \u0026gt;= maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。 其执行流程如下图所示：\n2.3.2 任务缓冲 任务缓冲模块是线程池能够管理任务的核心部分。线程池的本质是对任务和线程的管理，而做到这一点最关键的思想就是将任务和线程两者解耦，不让两者直接关联，才可以做后续的分配工作。线程池中是以生产者消费者模式，通过一个阻塞队列来实现的。阻塞队列缓存任务，工作线程从阻塞队列中获取任务。\n阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。\n下图中展示了线程1往阻塞队列中添加元素，而线程2从阻塞队列中移除元素：\n使用不同的队列可以实现不一样的任务存取策略。在这里，我们可以再介绍下阻塞队列的成员：\n2.3.3 任务申请 由上文的任务分配部分可知，任务的执行有两种可能：一种是任务直接由新创建的线程执行。另一种是线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行。第一种情况仅出现在线程初始创建的时候，第二种是线程获取任务绝大多数的情况。\n线程需要从任务缓存模块中不断地取任务执行，帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块之间的通信。这部分策略由getTask方法实现，其执行流程如下图所示：\njava.util.concurrent.ThreadPoolExecutor#getTask\ngetTask这部分进行了多次判断，为的是控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。\n2.3.4 任务拒绝 任务拒绝模块是线程池的保护部分，线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝掉该任务，采取任务拒绝策略，保护线程池。\n拒绝策略是一个接口，其设计如下：\npublic interface RejectedExecutionHandler { void rejectedExecution(Runnable r, ThreadPoolExecutor executor); } 用户可以通过实现这个接口去定制拒绝策略，也可以选择JDK提供的四种已有拒绝策略，其特点如下：\n2.4 Worker线程管理 2.4.1 Worker线程 线程池为了掌握线程的状态并维护线程的生命周期，设计了线程池内的工作线程Worker。我们来看一下它的部分代码：\nprivate final class Worker extends AbstractQueuedSynchronizer implements Runnable{ final Thread thread;//Worker持有的线程 Runnable firstTask;//初始化的任务，可以为null } Worker这个工作线程，实现了Runnable接口，并持有一个线程thread，一个初始化的任务firstTask。thread是在调用构造方法时通过ThreadFactory来创建的线程，可以用来执行任务；firstTask用它来保存传入的第一个任务，这个任务可以有也可以为null。如果这个值是非空的，那么线程就会在启动初期立即执行这个任务，也就对应核心线程创建时的情况；如果这个值是null，那么就需要创建一个线程去执行任务列表（workQueue）中的任务，也就是非核心线程的创建。\nWorker执行任务的模型如下图所示：\n线程池需要管理线程的生命周期，需要在线程长时间不运行的时候进行回收。线程池使用一张Hash表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期。这个时候重要的就是如何判断线程是否在运行。\nWorker是通过继承AQS，使用AQS来实现独占锁这个功能。没有使用可重入锁ReentrantLock，而是使用AQS，为的就是实现不可重入的特性去反应线程现在的执行状态。\n1.lock方法一旦获取了独占锁，表示当前线程正在执行任务中。 2.如果正在执行任务，则不应该中断线程。 3.如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断。 4.线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态；如果线程是空闲状态则可以安全回收。\n在线程回收过程中就使用到了这种特性，回收过程如下图所示：\n2.4.2 Worker线程增加 增加线程是通过线程池中的addWorker方法，该方法的功能就是增加一个线程，该方法不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅完成增加线程，并使它运行，最后返回是否成功这个结果。addWorker方法有两个参数：firstTask、core。firstTask参数用于指定新增的线程执行的第一个任务，该参数可以为空；core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize，其执行流程如下图所示：\n2.4.3 Worker线程回收\n线程池中线程的销毁依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。Worker被创建出来后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务。当Worker无法获取到任务，也就是获取的任务为空时，循环会结束，Worker会主动消除自身在线程池内的引用。\ntry { while (task != null || (task = getTask()) != null) { //执行任务 } } finally { processWorkerExit(w, completedAbruptly);//获取不到任务时，主动回收自己 } 线程回收的工作是在processWorkerExit方法完成的。 java.util.concurrent.ThreadPoolExecutor#processWorkerExit\n事实上，在这个方法中，将线程引用移出线程池就已经结束了线程销毁的部分。但由于引起线程销毁的可能性有很多，线程池还要判断是什么引发了这次销毁，是否要改变线程池的现阶段状态，是否要根据新状态，重新分配线程。\n2.4.4 Worker线程执行任务 在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的执行过程如下：\nwhile循环不断地通过getTask()方法获取任务。 getTask()方法从阻塞队列中取任务。 如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。 执行任务。 如果getTask结果为null则跳出循环，执行processWorkerExit()方法，销毁线程。 3 动态化线程池 线程池使用面临的核心的问题在于：线程池的参数并不好配置\n不同场景不同业务不同时间段等 对线程池的使用不一样,所以需要动态参数, 可以借助配置中心等方式实现\n还可以监控线程池的使用, 任务监控, 负载告警 等等\nJava线程池实现原理及其在美团业务中的实践 - 美团技术团队 (meituan.com) 4. 使用 常用的几个阻塞队列：\nLinkedBlockingQueue\n链式阻塞队列，底层数据结构是链表，默认大小是Integer.MAX_VALUE，也可以指定大小。\nArrayBlockingQueue\n数组阻塞队列，底层数据结构是数组，需要指定队列的大小。\nSynchronousQueue\n同步队列，内部容量为0，每个put操作必须等待一个take操作，反之亦然。\nDelayQueue\n延迟队列，该队列中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素 。\nbean 注入\npackage com.gree.ecommerce.config; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Primary; import org.springframework.scheduling.annotation.EnableAsync; import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor; import java.util.concurrent.RejectedExecutionHandler; import java.util.concurrent.ThreadPoolExecutor; @Configuration @EnableAsync public class ThreadPoolConfig { /** * */ private static final int MULTIPLE = 5; /** * */ private static final int SECKILL_MULTIPLE = 10000; /** * */ private static final int ALIVE_TIMEOUT = 30; /** * 核心线程数：线程池创建时候初始化的线程数 */ private final int corePoolSize = Runtime.getRuntime().availableProcessors(); /** * 最大线程数：线程池最大的线程数，只有在缓冲队列满了之后才会申请超过核心线程数的线程 */ private final int maxPoolSize = Runtime.getRuntime().availableProcessors() * MULTIPLE; /** * 缓冲队列：用来缓冲执行任务的队列 */ private final int queueCapacity = maxPoolSize * MULTIPLE; /** * 缓冲队列：用来缓冲执行任务的队列 */ private final int seckillQueueCapacity = maxPoolSize * SECKILL_MULTIPLE; /** * 允许线程的空闲时间(单位：秒)：当超过了核心线程出之外的线程在空闲时间到达之后会被销毁 */ private final int keepAliveSeconds = ALIVE_TIMEOUT; /** * 线程池名的前缀：设置好了之后可以方便我们定位处理任务所在的线程池 */ private String seckillThreadNamePrefix = \u0026#34;seckill-\u0026#34;; @Bean(name = \u0026#34;managePoolTaskExecutor\u0026#34;) @Primary public ThreadPoolTaskExecutor getManagePoolTaskExecutor() { return generateThreadPoolTaskExecutor(corePoolSize, maxPoolSize, queueCapacity, keepAliveSeconds, threadNamePrefix, new ThreadPoolExecutor.CallerRunsPolicy()); } /** * 秒杀线程池 */ @Bean(name = \u0026#34;seckillPoolTaskExecutor\u0026#34;) public ThreadPoolTaskExecutor getSeckillPoolTaskExecutor() { return generateThreadPoolTaskExecutor(corePoolSize, maxPoolSize, seckillQueueCapacity, keepAliveSeconds, seckillThreadNamePrefix, new ThreadPoolExecutor.DiscardPolicy()); } /** * * @param threadNamePrefix 异步方法内部线程名称 * @param corePoolSize 核心线程数 * @param maxPoolSize 线程池维护线程的最大数量,只有在缓冲队列满了之后才会申请超过核心线程数的线程 * @param queueCapacity 缓存队列 * @param keepAliveSeconds 允许的空闲时间,当超过了核心线程数之外的线程在空闲时间到达之后会被销毁 * @return ThreadPoolTaskExecutor */ private ThreadPoolTaskExecutor generateThreadPoolTaskExecutor(int corePoolSize, int maxPoolSize, int queueCapacity, int keepAliveSeconds, String threadNamePrefix, RejectedExecutionHandler rejectedExecutionHandler){ ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor(); taskExecutor.setCorePoolSize(corePoolSize); taskExecutor.setMaxPoolSize(maxPoolSize); taskExecutor.setQueueCapacity(queueCapacity); taskExecutor.setKeepAliveSeconds(keepAliveSeconds); taskExecutor.setThreadNamePrefix(threadNamePrefix); taskExecutor.setRejectedExecutionHandler(rejectedExecutionHandler); //当调度器shutdown被调用时等待当前被调度的任务完成 taskExecutor.setWaitForTasksToCompleteOnShutdown(true); taskExecutor.initialize(); return taskExecutor; } } ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E6%B1%A0.html","summary":"[TOC] 1. 前言 线程池解决的核心问题就是资源管理问题。在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。 为解决资源分","title":"线程池"},{"content":"哪里会产生小文件 ?\n源数据本身有很多小文件 动态分区会产生大量小文件 reduce个数越多, 小文件越多 按分区插入数据的时候会产生大量的小文件, 文件个数 = maptask个数 * 分区数 小文件太多造成的影响 ?\n从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。 HDFS存储太多小文件, 会导致namenode元数据特别大, 占用太多内存, 制约了集群的扩展 小文件解决方法\n方法一: 通过调整参数进行合并\n#每个Map最大输入大小(这个值决定了合并后文件的数量) set mapred.max.split.size=256000000; #一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并) set mapred.min.split.size.per.node=100000000; #一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并) set mapred.min.split.size.per.rack=100000000; #执行Map前进行小文件合并 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; #===设置map输出和reduce输出进行合并的相关参数： #设置map端输出进行合并，默认为true set hive.merge.mapfiles = true #设置reduce端输出进行合并，默认为false set hive.merge.mapredfiles = true #设置合并文件的大小 set hive.merge.size.per.task = 256*1000*1000 #当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。 set hive.merge.smallfiles.avgsize=16000000 **方法二: **\n针对按分区插入数据的时候产生大量的小文件的问题, 可以使用DISTRIBUTE BY rand() 将数据随机分配给Reduce，这样可以使得每个Reduce处理的数据大体一致.\n# 设置每个reducer处理的大小为5个G set hive.exec.reducers.bytes.per.reducer=5120000000; # 使用distribute by rand()将数据随机分配给reduce, 避免出现有的文件特别大, 有的文件特别小 insert overwrite table test partition(dt) select * from iteblog_tmp DISTRIBUTE BY rand(); 方法三: 使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件\n压缩和输出小文件合并是无法并存的，两者都有时，输出小文件合并会失效。除非，表的存储方式是SequenceFile\nhttps://blog.csdn.net/djd1234567/article/details/51581201 方法四: 使用hadoop的archive归档\n#用来控制归档是否可用 set hive.archive.enabled=true; #通知Hive在创建归档时是否可以设置父目录 set hive.archive.har.parentdir.settable=true; #控制需要归档文件的大小 set har.partfile.size=1099511627776; #使用以下命令进行归档 ALTER TABLE srcpart ARCHIVE PARTITION(ds=\u0026#39;2008-04-08\u0026#39;, hr=\u0026#39;12\u0026#39;); #对已归档的分区恢复为原文件 ALTER TABLE srcpart UNARCHIVE PARTITION(ds=\u0026#39;2008-04-08\u0026#39;, hr=\u0026#39;12\u0026#39;); #::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive https://blog.csdn.net/weixin_42582592/article/details/85084575 捷顺是采用手动的方法,文件个数和大小达标后把所有数据写到一起(通过写hive命令),当然也通过配置配合使用(本质是通过写一次操作,使配置生效)\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%B0%8F%E6%96%87%E4%BB%B6.html","summary":"哪里会产生小文件 ? 源数据本身有很多小文件 动态分区会产生大量小文件 reduce个数越多, 小文件越多 按分区插入数据的时候会产生大量的小文件, 文件","title":"小文件"},{"content":"[toc]\n1. 配置多个properties文件: 在application.properties文件中添加\nspring.profiles.active=jdbc,constants,resultCode\n表示,使用application-jdbc.properties 和 application-constants.properties 和 application-resultCode.properties\n命名方式:application-xxx.properties\n2.属性变量引用方式@@ 这种属性应用方式是field_name=@field_value@。\n两个@符号是springboot为替代${}属性占位符产生，原因是${}会被maven处理，所以应该是起不到引用变量的作用。 @@方式可以引用springboot 非默认配置文件（即其他配置文件) 中的变量； springboot默认配置文件是src/main/resources/application.properties 原文：https://blog.csdn.net/u011672034/article/details/79130001\n你可以使用Maven的资源过滤（resource filter）自动暴露来自Maven项目的属性，如果使用spring-boot-starter-parent，你可以通过@..@占位符引用Maven项目的属性，例如：\napp.encoding=@project.build.sourceEncoding@\napp.java.version=@java.version@\napp.project.version=@project.version@\n本地环境可以用,但是打包后,不一定能用,这种方式下springboot读取了classpath中的配置,但是现在配置一般外置,所以要用这种方式,得把配置文件打包至jar中.\n还有一种方式是读取jar中的pom.properties文件,打成jar包后,会生成一个pom.properties文件,里面包含版本信息,这种方式暂未实现\n链接：https://www.jianshu.com/p/8c281e822c4a\n3. application.yml 和bootstrap.yml 区别 加载顺序 若application.yml 和bootstrap.yml 在同一目录下：bootstrap.yml 先加载 application.yml后加载\n配置区别 bootstrap.yml 和 application.yml 都可以用来配置参数。 bootstrap.yml 用来程序引导时执行，应用于更加早期配置信息读取。可以理解成系统级别的一些参数配置，这些参数一般是不会变动的。一旦bootStrap.yml 被加载，则内容不会被覆盖。 application.yml 可以用来定义应用级别的， 应用程序特有配置信息，可以用来配置后续各个模块中需使用的公共参数等。\n3.属性覆盖问题\n启动上下文时，Spring Cloud 会创建一个 Bootstrap Context，作为 Spring 应用的 Application Context 的父上下文。\n初始化的时候，Bootstrap Context 负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的 Environment。Bootstrap 属性有高优先级，默认情况下，它们不会被本地配置覆盖。\n也就是说如果加载的 application.yml 的内容标签与 bootstrap 的标签一致，application 也不会覆盖 bootstrap，而 application.yml 里面的内容可以动态替换。\n典型的应用场景如下：\n当使用 Spring Cloud Config Server 的时候，你应该在 bootstrap.yml 里面指定 spring.application.name 和 spring.cloud.config.server.git.uri 一些第三方服务的链接信息等 和一些加密/解密的信息 application.yml与bootstrap.yml的区别 - Chen洋 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/springboot/%E5%B0%8F%E7%9F%A5%E8%AF%86.html","summary":"[toc] 1. 配置多个properties文件: 在application.properties文件中添加 spring.profiles.active=jdbc,constants,resultCode 表示,使用application-jdbc.","title":"小知识"},{"content":"我建议 Elasticsearch 为第一优先级。需要掌握的内容如下。\n（1）掌握 Elasticsearch 的基本概念，主要包括：\n索引（index） 类型（type） 映射（mapping） 文档（document） 倒排索引原理 文档打分机制 集群（cluster）——单节点、集群安装与部署 健康状态（red/yellow/green） 数据存储 数据类型（long/date/text、keyword/nested等） 数据展示（结合Head插件的基础可视化） …… （2）掌握 Elasitcsearch 的基本操作，主要包括：\n新增（insert） 删除（delete/deletebyquery） 修改（update/updatebyquery） 查找（search） 精确匹配检索（term、terms、range、exists） 模糊匹配检索（wildcard、prefix、negix正则） 分词全文检索（match/match_phrase等） 多条件 bool 检索（must/must_not/should多重组合） 分词（英文分词、拼音分词、中文分词） 高亮 分页查询 指定关键词返回 批量操作 bulk scroll 查询 reindex 操作 …… （3）掌握 Elasticsearch 高级操作，主要包括：\n聚合统计（数量聚合、最大值、最小值、平均值、求和等聚合操作） 图像化展示（hisgram 按照日期等聚合） 聚合后分页 父子文档 数组类型 nested 嵌套类型 ES 插件错误排查（集群问题、检索问题、性能问题） ES 性能调优（配置调优、集群调优等） …… （4）掌握 Elasticsearch Java/Python 等API，主要包括：\nElasticsearch 原生自带 API、JEST、Springboot 等 API 选型 Elasticsearch 多条件 bool 复杂检索 API Elasticsearch 分页 API Elasticsearch 高亮 API Elasticsearch 聚合 API Elasticsearch 相关 JSON 数据解析 …… （5）Elasticsearch 结合场景开发实战，主要包括：\n数据可视化（Kibana、Grafana 等 其中 Grafana 比较适合监控类场景） 通过 logstash/beats 等导入数据 Elasticsearch 和 Kafka 结合的应用场景 Elasticsearch 和 Mongo 结合的应用场景 Elasticsearch 和 Hadoop 结合的应用场景 结合业务需求的定制化应用场景（日志分析、文档检索、全文检索、金融等各行业检索） …… 建议的第二学习优先级为 Kibana。需要掌握的内容如下。\nKibana 安装与部署 ES 节点数据同步到 Kibana Kibana Dev Tools 开发工具熟练使用 Kibana 图像化组合展示 将 Kibana 图像化展示效果图应用到自己的开发环境中 …… 第三学习优先级为 Logstash。需要掌握的内容如下。\nLogstash 的安装与部署 Logstash 将本地文件导入 ES logstashinputjdbc 插件（5.X后无需安装）将 MySQL/Oracle 等关系型数据库数据导入 ES，全量导入和增量导入实现。 logstashinputmongo插件将 Mongo 数据导入 ES logstashinputkafaka 插件将 Kafak 数据导入 ES logstashoutput* 插件将 ES 数据导入不同的数据库和实时数据流中 …… 第四学习优先级为 Beats。需要掌握的内容如下。\n不同类型的 Beats 安装与部署 将业务数据通过 Beats 导入 ES …… ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/elk/%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%85%88%E7%BA%A7%E5%AF%BC%E8%AE%BA.html","summary":"我建议 Elasticsearch 为第一优先级。需要掌握的内容如下。 （1）掌握 Elasticsearch 的基本概念，主要包括： 索引（index） 类型（type） 映射（mapping） 文档（d","title":"学习优先级(导论)"},{"content":"[toc]\n缓存穿透 缓存穿透，是指频繁查询一个数据库一定不存在的数据,这种redis一般不会保存,这样redis被穿过了,直接去访问了数据库,这种穿透多了,数据库就压力大。正常的使用缓存流程大致是，数据查询先进行缓存查询，如果key不存在或者key已经过期，再对数据库进行查询，并把查询到的对象\n缓存被穿透了,因为总是查询一些在redis中查不到的值\n解决方案:\n使用锁,将这次请求锁住,等数据库返回后才释放(分布式环境用分布式锁,单机用普通锁(synchronized等)) 接口限流与熔断、降级 布隆过滤器(java的guava包中默认误判率3%(最合适)) (将全部\u0026quot;可能\u0026quot;数据存到布隆过滤器中,当redis中找不到时,先经过布隆,如果过滤器说不存在,就不用访问数据库了,直接返回) 将这种key存到redis,设置较短的过期时间,比如1分钟/5分钟 缓存雪崩 缓存雪崩，是指在某一个时间段，缓存集中过期失效,大批量key失效,命中redis的概率就小了,只能去查数据库了.\n其实集中过期，倒不是非常致命，比较致命的缓存雪崩，是缓存服务器某个节点宕机或断网。因为自然形成的缓存雪崩，一定是在某个时间段集中创建缓存，那么那个时候数据库能顶住压力，这个时候，数据库也是可以顶住压力的。无非就是对数据库产生周期性的压力而已。\n解决方案:\n也是像解决缓存穿透一样加锁排队\n建立备份缓存, 采用哨兵或者集群 ,多起几个节点\n给同时添加key时设置不同过期时间\n事前：确保Redis本身的高可用性，数据恢复备份、主从架构+哨兵，Redis cluster，一旦主节点挂了，从节点跟上\n事中：当redis不可用时，少量请求可以走缓存生产服务的本地缓存ehcache获取数据，基于hystrix对商品服务和redis操作做限流保护 配置降级、超时、熔断策略；从而保证发生缓存雪崩时缓存生产服务不会被拖死\n事后：基于redis的数据备份，快速将redis重新跑起来对外提供服务\n缓存击穿 缓存击穿，热点key在请求高峰期失效了，瞬间大量请求落到数据库，就像在一个屏障上凿开了一个洞。\n解决方案:\n本地缓存 + redis缓存的多级缓存 也是像解决缓存穿透一样加锁排队 链接:https://baijiahao.baidu.com/s?id=1619572269435584821\u0026amp;wfr=spider\u0026amp;for=pc\nhttps://blog.csdn.net/fanrenxiang/article/details/80542580 如何解决redis缓存击穿问题_Sebastian Xia的博客-CSDN博客_redis缓存击穿怎么解决 ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/redis/%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%A9%BF%E9%80%8F%E5%92%8C%E5%87%BB%E7%A9%BF.html","summary":"[toc] 缓存穿透 缓存穿透，是指频繁查询一个数据库一定不存在的数据,这种redis一般不会保存,这样redis被穿过了,直接去访问了数据库,这种穿透","title":"雪崩和穿透和击穿"},{"content":" 链接：https://www.zhihu.com/question/26568496/answer/224439650\n**a.蓝色部分:**是Hadoop生态系统组件，黄色部分是Spark生态组件，虽然他们是两种不同的大数据处理框架，但它们不是互斥的，Spark与hadoop 中的MapReduce是一种相互共生的关系。Hadoop提供了Spark许多没有的功能，比如分布式文件系统，而Spark 提供了实时内存计算，速度非常快。有一点大家要注意，Spark并不是一定要依附于Hadoop才能生存，除了Hadoop的HDFS，还可以基于其他的云平台，当然啦，大家一致认为Spark与Hadoop配合默契最好摆了。\n**b.技术趋势：**Spark在崛起，hadoop和Storm中的一些组件在消退。大家在学习使用相关技术的时候，记得与时俱进掌握好新的趋势、新的替代技术，以保持自己的职业竞争力。\nHSQL未来可能会被Spark SQL替代，现在很多企业都是HIVE SQL和Spark SQL两种工具共存，当Spark SQL逐步成熟的时候，就有可能替换HSQL；\nMapReduce也有可能被Spark 替换，趋势是这样，但目前Spark还不够成熟稳定，还有比较长的路要走；\nHadoop中的算法库Mahout正被Spark中的算法库MLib所替代，为了不落后，大家注意去学习Mlib算法库；\nStorm会被Spark Streaming替换吗?在这里，Storm虽然不是hadoop生态中的一员，但我仍然想把它放在一起做过比较。由于Spark和hadoop天衣无缝的结合，Spark在逐步的走向成熟和稳定，其生态组件也在逐步的完善，是冉冉升起的新星，我相信Storm会逐步被挤压而走向衰退。\n","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E5%88%9D%E5%A7%8B%E7%94%9F%E6%80%81%E5%9C%88/%E4%B8%80%E5%BC%A0%E5%BE%88%E7%89%9B%E9%80%BC%E7%9A%84%E5%9B%BE.html","summary":"链接：https://www.zhihu.com/question/26568496/answer/224439650 **a.蓝色部分:**","title":"一张很牛逼的图"},{"content":"error 是不可以被捕获的\n","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%BC%82%E5%B8%B8.html","summary":"error 是不可以被捕获的","title":"异常"},{"content":"MySQL引擎:\n一.主要是以下两种(不止这两种)\nInnodb(5.5版本以后默认引擎):\nInnodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。++它的设计的目标就是处理大数据容量的数据库系统++。它本身实际上是基于Mysql后台的完整的系统。Mysql运行的时候，Innodb会在内存中建立缓冲池，用于缓冲数据和索引。但是，该引擎是不支持全文搜索的。同时，启动也比较的慢，它是不会保存表的行数的。当进行Select count(*) from table指令的时候，需要进行扫描全表。所以当需要使用数据库的事务时，该引擎就是首选。由于锁的粒度小，写操作是不会锁定全表的。所以在并发度较高的场景下使用会提升效率的。主键索引上就存储业务数据,二级索引存储的主键索引的位置 (擅长写)\nMyIASM:\n它是MySql的默认引擎，但不提供事务的支持，也不支持行级锁和外键。因此当执行Insert插入和Update更新语句时，即执行写操作的时候需要锁定这个表。所以会导致效率会降低。不过和Innodb不同的是，MyIASM引擎是保存了表的行数，于是当进行Select count(*) from table语句时，可以直接的读取已经保存的值而不需要进行扫描全表。所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的。可以将MyIASM作为数据库引擎的首选。\n在索引树上仅存储数据的物理地址,再根据地址再去找到业务数据.(擅长读)\n二. 查看MySQL提供哪些引擎:\nshow engines ","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%BC%95%E6%93%8E.html","summary":"MySQL引擎: 一.主要是以下两种(不止这两种) Innodb(5.5版本以后默认引擎): Innodb引擎提供了对数据库ACID事务的支持。并","title":"引擎"},{"content":"[toc]\n1. 游标 相当于指针,通过游标PL/SQL程序可以一次处理查询结果集中的一行,并可以对该行数据执行特定操作\n显示游标: 显式游标用于处理返回多行的查询。 步骤如下:\n声明游标: cursor cur_name [ (in_param [ , in_param]…) ] [ return return_type ] is select_sentence; 打开游标: open cur_name [ (in_param [ , in_param]…) ]; 读取游标: fetch cur_name into (variable); 关闭游标: close cur_name 隐式游标: 在 PL/SQL 程序中执行DML SQL 语句时自动创建隐式游标，名字默认叫sql,感觉隐式就是普通的PL/SQL(特别是配合for使用),无形中使用,因为打开,读取等操作Oracle系统自动完成 REF 游标：REF 游标用于处理运行时才能确定的动态 SQL 查询的结果 https://www.2cto.com/database/201501/371435.html http://www.cnblogs.com/sc-xx/archive/2011/12/03/2275084.html 2. 存储过程 存储过程相当于java中函数,将一些操作集合起来.存储过程报错在数据库中,可以被重复使用,(也正因为如此,存储过程不便迁移),存储过程是已经编译好的代码,所有被引用时,效率非常高(这也是项目中喜欢用存储过程的原因)\n语法:\ncreate [ or replace ] procedure pro_name [ (in_param in/out param_type [ , in_param in/out param_type ]…) ] is|as begin plsql_sentences; [ exception ] [ do_something_sentences; ] end [ pro_name ] 调用:\nbegin pro_name [ (in_param [ , in_param]…) ]; end; 函数必须有返回值,这是与存储过程的最大的不同,也说明了函数不如存储过程灵活\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/oracle/%E6%B8%B8%E6%A0%87%E5%92%8C%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B.html","summary":"[toc] 1. 游标 相当于指针,通过游标PL/SQL程序可以一次处理查询结果集中的一行,并可以对该行数据执行特定操作 显示游标: 显式游标用于处理返回多行的","title":"游标和存储过程"},{"content":"[toc]\n1.什么是hive 1. Hive 由 Facebook 实现并开源\n2. 是基于 Hadoop 的一个数据仓库工具\n3. 可以将结构化的数据映射为一张数据库表\n4. 并提供 HQL(Hive SQL)查询功能\n5. 底层数据是存储在 HDFS 上\n6. Hive的本质是将 SQL 语句转换为 MapReduce 任务运行\n7. 使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，适用于离线的批量数据计算。\n数据仓库之父比尔·恩门（Bill Inmon）在 1991 年出版的“Building the Data Warehouse”（《建 立数据仓库》）一书中所提出的定义被广泛接受——数据仓库（Data Warehouse）是一个面 向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史 变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。\nHive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行，所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理\n1.1 Hive 特点 优点：\n1、可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器cpu i7-6700k 4核心8线程，8核心16线程，内存64G =\u0026gt; 128G\n2、延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数\n3、良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行\n缺点：\n1、Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作）\n2、Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。\n3、Hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别）。\n2. hive架构 2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行\nHive 的核心是驱动引擎， 驱动引擎由四部分组成(执行顺序亦如此)：\n(1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST）\n(2) 编译器：编译器是将语法树编译为逻辑执行计划(就是翻译成MapReduce)\n(3) 优化器：优化器是对逻辑执行计划进行优化\n(4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划\n来源 https://www.bilibili.com/video/BV1W4411B7cN?p=4 2.2 元数据存储系统 ： RDBMS MySQL 元数据 :通俗的讲，就是存储在 Hive 中的数据的描述信息。\nHive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录\nMetastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理\n解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程）\nHive 和 MySQL 之间通过 MetaStore 服务交互\n2.3. 表的分类 Hive 中的表分为 内部表、外部表、分区表和 Bucket 表\n内部表和外部表的区别：\n删除内部表，删除表元数据和数据\n删除外部表，删除元数据，不删除数据\n内部表和外部表的使用选择：\n大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。\n使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中\n使用外部表的场景是针对一个数据集有多个不同的 Schema\n通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，都可以对 hive 表的数据存储目录中的数据进行增删操作。\n选择: 另一种说法: 由于外部表不会被直接删除,因此更安全,而内部表可以直接删除,因此更方便,可以用作临时表, 捷顺目前并没有明显的区分\n分区表和分桶表的区别：\nHive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似。\n分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多\n3. 数据类型 3.1 基本类型 描述 示例 boolean true/false TRUE tinyint 1字节的有符号整数 -128~127 1Y smallint 2个字节的有符号整数，-32768~32767 1S int 4个字节的带符号整数 1 bigint 8字节带符号整数 1L float 4字节单精度浮点数 1.0 double 8字节双精度浮点数 1.0 deicimal 任意精度的带符号小数 1.0 String 字符串，变长 “a”,’b’ varchar 变长字符串 “a”,’b’ char 固定长度字符串 “a”,’b’ binary 字节数组 无法表示 timestamp 时间戳，纳秒精度 122327493795 date 日期 ‘2018-04-07’ Hive 支持关系型数据中大多数基本数据类型,和其他的SQL语言一样，这些都是保留字。需要注意的是所有的这些数据类型都是对Java中接口的实现，因此这些类型的具体行为细节和Java中对应的类型是完全一致的。例如，string类型实现的是Java中的String，float实现的是Java中的float，等等。\n3.2 复杂类型 类型 描述 示例 array 有序的的同类型的集合 array(1,2) map key-value,key必须为原始类型，value可以任意类型 map(‘a’,1,’b’,2) struct 字段集合,类型可以不同 struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) 4. 存储格式 1. textfile\ntextfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。\n2. SequenceFile\nSequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。\nSequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。\n3. RCFile\n一种行列存储相结合的存储方式。\n4. ORCFile\n数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。\n5. Parquet\nParquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。\n来自: https://www.cnblogs.com/qingyunzong/p/8733924.html ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D.html","summary":"[toc] 1.什么是hive 1. Hive 由 Facebook 实现并开源 2. 是基于 Hadoop 的一个数据仓库工具 3. 可以将结构化的数据映射为一张数据库表 4. 并提供 HQL(Hive SQL)查询功能 5. 底层数据","title":"原理及介绍"},{"content":"在启动的配置参数上加\n-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=${debug_port}\n其中${debug_port}是用户自定义的，为debug端口,不能是被用的端口(自己用个新的)\n打开idea，在右上角点击edit configurations,进去之后点击+号，选择remote，host处填写远程服务器的iP,端口填写debug 端口，如果包含多个module，可以执行要运行的module的名字，然后点击apply按钮。\n在name那里给配置起一个名字：本地debug\n然后debug模式启动,然后打上断点()\n在远程服务的界面上点击相应操作,就可以在本地debug了\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/idea/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95.html","summary":"在启动的配置参数上加 -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=${debug_port} 其中${debug_port}是用户自定义的，为debug端口,不能是被用的端口(自己用个新的) 打开idea，在右上","title":"远程调试"},{"content":"主流的正则引擎又分为3类：\nDFA， 传统型NFA， POSIX NFA 使用DFA引擎的程序主要有：awk,egrep,flex,lex,MySQL,Procmail等；\n使用传统型NFA引擎的程序主要有：GNU Emacs,Java,ergp,less,more,.NET语言,PCRE library,Perl,PHP,Python,Ruby,sed,vi；\n使用POSIX NFA引擎的程序主要有：mawk,Mortice Kern Systems’ utilities,GNU Emacs(使用时可以明确指定)；\n也有使用DFA/NFA混合的引擎：GNU awk,GNU grep/egrep,Tcl。\nDFA\n引擎在线性时状态下执行，因为它们不要求回溯（并因此它们永远不测试相同的字符两次）。DFA 引擎还可以确保匹配最长的可能的字符串。但是，因为 DFA 引擎只包含有限的状态，所以它不能匹配具有反向引用的模式；并且因为它不构造显示扩展，所以它不可以捕获子表达式。\nNFA\n传统的 NFA 引擎运行所谓的“贪婪的”匹配回溯算法，以指定顺序测试正则表达式的所有可能的扩展并接受第一个匹配项。因为传统的 NFA 构造正则表达式的特定扩展以获得成功的匹配，所以它可以捕获子表达式匹配和匹配的反向引用。但是，因为传统的 NFA 回溯，所以它可以访问完全相同的状态多次（如果通过不同的路径到达该状态）。因此，在最坏情况下，它的执行速度可能非常慢。因为传统的 NFA 接受它找到的第一个匹配，所以它还可能会导致其他（可能更长）匹配未被发现。\n举例简单说明NFA与DFA工作的区别：\n比如有字符串this is yansen’s blog，正则表达式为 /ya(msen|nsen|nsem)/ (不要在乎表达式怎么样，这里只是为了说明引擎间的工作区别)。\nNFA工作方式如下，先在字符串中查找 y 然后匹配其后是否为 a ，如果是 a 则继续，查找其后是否为 m 如果不是则匹配其后是否为 n (此时淘汰msen选择支)。然后继续看其后是否依次为 s,e，接着测试是否为 n ，是 n 则匹配成功，不是则测试是否为 m 。为什么是 m ？因为 NFA 工作方式是以正则表达式为标准，反复测试字符串，这样同样一个字符串有可能被反复测试了很多次！\n而DFA则不是如此，DFA会从 this 中 t 开始依次查找 y，定位到 y ，已知其后为 a ，则查看表达式是否有 a ，此处正好有 a 。然后字符串 a 后为 n ，DFA依次测试表达式，此时 msen 不符合要求淘汰。nsen 和 nsem 符合要求，然后DFA依次检查字符串，检测到sen 中的 n 时只有nsen 分支符合，则匹配成功！\n由此可以看出来，两种引擎的工作方式完全不同，++一个(NFA)以表达式为主导，一个(DFA)以文本为主导++！一般而论，DFA引擎则搜索更快一些！但是NFA以表达式为主导，反而更容易操纵，\n","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E6%AD%A3%E5%88%99%E5%BC%95%E6%93%8E.html","summary":"主流的正则引擎又分为3类： DFA， 传统型NFA， POSIX NFA 使用DFA引擎的程序主要有：awk,egrep,flex,lex,MySQL,Procm","title":"正则引擎"},{"content":"[toc]\nJava堆是被所有线程共享的一块内存区域，主要用于存放对象实例，在堆上为对象分配内存就是把一块大小确定的内存从堆内存中划分出来，将对象放进去。常用的分配方法有指针碰撞和空闲列表以及TLAB\n指针碰撞 适用于堆内存完整的情况，已分配的内存和空闲内存分表在不同的一侧，通过一个指针指向分界点，当需要分配内存时，把指针往空闲的一端移动与对象大小相等的距离即可，用于Serial和ParNew等不会产生内存碎片的垃圾收集器。\n空闲列表 适用于堆内存不完整的情况，已分配的内存和空闲内存相互交错，JVM通过维护一张内存列表记录可用的内存块信息，当分配内存时，从列表中找到一个足够大的内存块分配给对象实例，并更新列表上的记录，最常见的使用此方案的垃圾收集器就是CMS。\nTLAB 全称: （Thread Local Allocation Buffer 线程本地分配缓存区）\n在使用指针碰撞的场景下, 由于堆内存本身是线程共享的，在多线程场景下，当一个线程需要创建对象，这时指针还没来得及修改（指针是在新对象占完位之后才能进行修改），如果另一个线程也需要分配空间，就会造成两个对象空间冲突，TLAB可以解决这种场景.\nTALB的思路 TALB的解决思路比较简单粗暴，既然是因为堆内存多线程共享引发了问题，那我就直接给每个线程一个私有的区域分配对象不就解决了吗？\nTALB的原理 TALB就是在堆内存上额外为每个线程分配一块线程私有区域，其大小一般比较小，默认占Eden区的1%。其本质就是通过start、top、end三个指针实现，其中start和end分别指向这个TALB的开始和结尾位置，用于确定该TALB在堆上对应区域，避免其他线程再过来分配内存，top实时指向TALB区域内当前可分配的第一个位置，当一个TALB满了或剩余空间不足以存储新申请的对象时，线程会向JVM再申请一块TALB。\n到这里为止，指针碰撞多线程问题似乎已经得到了解决，不过由于TALB空间本身较小（默认只占Eden区1%），所以就很容易出现TALB剩余区域不足以存储新对象的情况，这时线程会把新对象存到新申请的TALB中，这样原有的TALB中剩余区域就会被浪费，造成内存泄漏。那么如何解决内存泄漏呢？\n最大浪费空间 由于TALB内存浪费现象较为严重，所以JVM开发人员提出了一个最大浪费空间对TALB进行约束。 当TALB剩余空间存不下新对象时，会进行一个判断： ① 如果当前TALB剩余空间小于最大浪费空间，则TALB所属线程会向JVM申请一个新的TALB区域存储新对象，如果依旧存储不下，则对象会放在Eden区创建。 ② 如果当前TALB剩余空间大于最大浪费空间，则对象直接去Eden区创建。\nTALB的局限性 虽然TALB解决了指针碰撞在多线程场景下的问题，并且通过最大浪费空间可以减少内存泄漏，但其本身依旧有一些缺点： ① GC更频繁： 由于每个TALB所占用的空间都要比线程实际需要的空间大小大一些（因为不可能每个TALB都刚好存满，也就是TALB空间浪费更严重），所以一批对象直接存储在Eden区会比存储在TALB区占用更少的空间，进而容易引发Minor GC。 ② TALB允许内存浪费，会导致Eden区内存不连续。\nJVM指针碰撞和空闲列表 Java基础知识点总结系列 指针碰撞、空闲列表、TLAB是什么关系？ - 知乎 (zhihu.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E6%8C%87%E9%92%88%E7%A2%B0%E6%92%9E%E7%A9%BA%E9%97%B2%E5%88%97%E8%A1%A8tlab.html","summary":"[toc] Java堆是被所有线程共享的一块内存区域，主要用于存放对象实例，在堆上为对象分配内存就是把一块大小确定的内存从堆内存中划分出来，将对象放进","title":"指针碰撞、空闲列表、TLAB"},{"content":"原因: 使用modleresource时 返回注入的值是模型,当你放回的值不是模型对应的字段则会报错,比如进行了聚合操作\n解决: 重写resource,自定义返回的值\nSaleResource\nclass SaleResource(Resource): avgPrice = fields.IntegerField(attribute=\u0026#39;avgPrice\u0026#39;) maxPrice = fields.DecimalField(attribute=\u0026#39;maxPrice\u0026#39;) minPrice = fields.DecimalField(attribute=\u0026#39;minPrice\u0026#39;) class Meta: resource_name = \u0026#39;sale\u0026#39; authorization = Authorization() filtering = { \u0026#39;id\u0026#39;: ALL_WITH_RELATIONS, \u0026#39;pub_date\u0026#39;: [\u0026#39;exact\u0026#39;, \u0026#39;lt\u0026#39;, \u0026#39;lte\u0026#39;, \u0026#39;gte\u0026#39;, \u0026#39;gt\u0026#39;], } def get_object_list(self, request): reID = request.GET[\u0026#39;id\u0026#39;] logger.debug(\u0026#34;需要查询三种价格的ID:\u0026#34; + reID) results = [] queryset = Sale.objects.filter(id=reID) \\ .annotate(avgPrice=Avg(\u0026#34;sellPrice\u0026#34;), maxPrice=Max(\u0026#34;sellPrice\u0026#34;), minPrice=Min(\u0026#34;sellPrice\u0026#34;)) \\ .values(\u0026#39;avgPrice\u0026#39;, \u0026#39;maxPrice\u0026#39;, \u0026#39;minPrice\u0026#39;) for item in queryset: \\# logging.warn(item) new_obj = BaseJsonModel() new_obj.minPrice = item[\u0026#39;minPrice\u0026#39;] new_obj.maxPrice = item[\u0026#39;maxPrice\u0026#39;] new_obj.avgPrice = item[\u0026#39;avgPrice\u0026#39;] results.append(new_obj) return results def obj_get_list(self, bundle, **kwargs): return self.get_object_list(bundle.request) BaseJsonModel\nclass BaseJsonModel(object): def __init__(self, initial=None): self.__dict__[\u0026#39;_data\u0026#39;] = {} if hasattr(initial, \u0026#39;items\u0026#39;): self.__dict__[\u0026#39;_data\u0026#39;] = initial def __getattr__(self, name): return self._data.get(name, None) def __setattr__(self, name, value): self.__dict__[\u0026#39;_data\u0026#39;][name] = value def to_dict(self): return self._data ","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E9%87%8D%E5%86%99resource.html","summary":"原因: 使用modleresource时 返回注入的值是模型,当你放回的值不是模型对应的字段则会报错,比如进行了聚合操作 解决: 重写resourc","title":"重写resource"},{"content":"[toc]\n1.介绍 利用主从数据库来实现读写分离，从而分担主数据库的压力。在多个服务器上部署mysql，将其中一台认为主数据库，而其他为从数据库，实现主从同步。其中主数据库负责主动写的操作，而从数据库则只负责主动读的操作（slave从数据库仍然会被动的进行写操作，为了保持数据一致性），这样就可以很大程度上的避免数据丢失的问题，同时也可减少数据库的连接，减轻主数据库的负载。\n原文：https://blog.csdn.net/qq_15092079/article/details/81672920\n在上面的模型中，Mysql-A就是主服务器，即master，Mysql-B就是从服务器，即slave。\n在Mysql-A的数据库事件（例如修改数据库的sql操作语句），都会存储到日志系统A中，在相应的端口（默认3306）通过网络发送给Mysql-B。Mysql-B收到后，写入本地日志系统B，然后一条条的将数据库事件在数据库Mysql-B中完成。\n日志系统A，是MYSQL的日志类型中的二进制日志，也就是专门用来保存修改数据库表的所有动作，即bin log，注意MYSQL会在执行语句之后，释放锁之前，写入二进制日志，确保事务安全。\n日志系统B，不是二进制日志，由于它是从MYSQL-A的二进制日志复制过来的，并不是自己的数据库变化产生的，有点接力的感觉，称为中继日志，即relay log。\n通过上面的机制，可以保证Mysql-A和Mysql-B的数据库数据一致，但是时间上肯定有延迟，即Mysql-B的数据是滞后的。因此，会出现这样的问题，Mysql-A的数据库操作是可以并发的执行的，但是Mysql-B只能从relay log中一条一条的读取执行。若Mysql-A的写操作很频繁，Mysql-B很可能就跟不上了。\n原文：https://blog.csdn.net/qq_15092079/article/details/81672920\n大致流程: 主数据库 负责写,会把所有的写操作记录到日志文件(二进制文件)中, 然后 从服务器来 同步这个日志文件,然后以此更新 从服务器上 的日志\n主从同步复制有以下几种方式：\n（1）同步复制，master的变化，必须等待slave-1,slave-2,\u0026hellip;,slave-n完成后才能返回。\n（2）异步复制，master只需要完成自己的数据库操作即可，至于slaves是否收到二进制日志，是否完成操作，不用关心。MYSQL的默认设置。\n（3）半同步复制，master只保证slaves中的一个操作成功，就返回，其他slave不管。这个功能，是由google为MYSQL引入的。\n本文说的是在centos 7系统上，实现的mysql5.7数据库的主从同步配置，从而实现读写分离操作。\n原文：https://blog.csdn.net/qq_15092079/article/details/81672920\n是不是我无限制地增加从库的数量就可以抵抗大量的并发呢？\n实际上并不是的。因为随着从库数量增加，从库连接上来的 IO 线程比较多，主库也需要创建同样多的log dump 线程来处理复制的请求，对于主库资源消耗比较高，同时受限于主库的网络带宽，所以在实际使用中，一般一个主库最多挂 3～5 个从库。\n2.实践 以下操作未经自己实践!!\n原文：https://blog.csdn.net/qq_15092079/article/details/81672920\n2.1 分别在两台centos 7系统上安装mysql 5.7\n具体的安装步骤可以见此链接，https://blog.csdn.net/qq_15092079/article/details/81629238。\n本文中的两台服务器的IP地址分别为主服务器（192.168.17.130）和从服务器（192.168.17.132）。\n分别在这两个服务器上创建test数据库，以备后面测试。\n2.2 master主服务器的配置 2.2.1 配置文件my.cnf的修改 #根据上一篇文章，编辑my.cnf文件 [root@localhost mysql]# vim /etc/my.cnf #在[mysqld]中添加： server-id=1 log_bin=master-bin log_bin_index=master-bin.index binlog_do_db=test #备注： #server-id 服务器唯一标识。 #log_bin 启动MySQL二进制日志，即数据同步语句，从数据库会一条一条的执行这些语句。 #binlog_do_db 指定记录二进制日志的数据库，即需要复制的数据库名，如果复制多个数据库，重复设置这个选项即可。 #binlog_ignore_db 指定不记录二进制日志的数据库，即不需要复制的数据库名，如果有多个数据库，重复设置这个选项即可。 #其中需要注意的是，binlog_do_db和binlog_ignore_db为互斥选项，一般只需要一个即可。 2.2.2 创建从服务器的用户和权限 #进入mysql数据库 [root@localhost mysql]# mysql -uroot -p Enter password: #创建从数据库的masterbackup用户和权限 mysql\u0026gt; grant replication slave on *.* to masterbackup@\u0026#39;192.168.17.%\u0026#39; identified by \u0026#39;123456\u0026#39;; #备注 #192.168.17.%通配符，表示0-255的IP都可访问主服务器，正式环境请配置指定从服务器IP #若将 192.168.17.% 改为 %，则任何ip均可作为其从数据库来访问主服务器 #退出mysql mysql\u0026gt; exit; 重启mysql服务\nservice mysql restart\n查看主服务器状态\nshow master status; +-------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +-------------------+----------+--------------+------------------+-------------------+ | master-bin.000001 | 154 | test | | | +-------------------+----------+--------------+------------------+-------------------+ 2.3 slave从服务器的配置 2.3.1 配置文件my.cnf的修改 #根据上一篇文章，编辑my.cnf文件 [root@localhost mysql]# vim /etc/my.cnf #在[mysqld]中添加： server-id=2 relay-log=slave-relay-bin relay-log-index=slave-relay-bin.index #replicate-do-db=test #备注： #server-id 服务器唯一标识，如果有多个从服务器，每个服务器的server-id不能重复，跟IP一样是唯一标识，如果你没设置server-id或者设置为0，则从服务器不会连接到主服务器。 #relay-log 启动MySQL二进制日志，可以用来做数据备份和崩溃恢复，或主服务器挂掉了，将此从服务器作为其他从服务器的主服务器。 #replicate-do-db 指定同步的数据库，如果复制多个数据库，重复设置这个选项即可。若在master端不指定binlog-do-db，则在slave端可用replication-do-db来过滤。 #replicate-ignore-db 不需要同步的数据库，如果有多个数据库，重复设置这个选项即可。 #其中需要注意的是，replicate-do-db和replicate-ignore-db为互斥选项，一般只需要一个即可 重启mysql服务\nservice mysql restart\n2.3.2 连接master主服务器 change master to master_host=\u0026#39;192.168.17.130\u0026#39;,master_port=3306,master_user=\u0026#39;masterbackup\u0026#39;,master_password=\u0026#39;123456\u0026#39;,master_log_file=\u0026#39;master-bin.000001\u0026#39;,master_log_pos=154; #备注：\n#master_host对应主服务器的IP地址。\n#master_port对应主服务器的端口。\n#master_log_file对应show master status显示的File列：master-bin.000001。\n#master_log_pos对应show master status显示的Position列：154。\n2.3.3 启动slave数据同步 #启动slave数据同步 mysql\u0026gt; start slave; #停止slave数据同步（若有需要） mysql\u0026gt; stop slave; 查看slave信息\nshow slave status\\G; # 好像不要 \\G Slave_IO_Running和Slave_SQL_Running都为yes，则表示同步成功。\n至此已完成!!!!,如果对 主数据库 进行写操作, 在 从数据库 则会看到\nwindows 下 https://blog.csdn.net/u010509052/article/details/80449134 3.问题 3.1 读写分离的延迟问题 使用场景: 数据量大的情况下使用的技术不是读写分离，是分表和分库，或者使用分布式存储引擎，读写分离不能解决数据量大的问题。\n来自链接 如果对数据库进行了读写分离,那可能存在查不到刚刚写入的数据,因为同步数据需要时间,(不是高并发的情况下,一般不会出现这种问题,因为同步数据是从节点IO读取master的binlog日志)\n方案大致有以下几种\n一个是半同步复制，用来解决主库数据丢失问题； semi-sync复制，指的就是主库写入binlog日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了\n一个是并行复制，用来解决主从同步延时问题。 对于这种需要立即读的场景,指定(写库)数据库去查询 借助第三方组件, 贡献数据, 例如 存入redis, 查询数据时先查redis, (这种方式推荐在新增场景, 修改场景会有并发问题) 原文链接：https://blog.csdn.net/wolf_love666/article/details/90444154\n","permalink":"https://xiaokunji.com/zh/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.html","summary":"[toc] 1.介绍 利用主从数据库来实现读写分离，从而分担主数据库的压力。在多个服务器上部署mysql，将其中一台认为主数据库，而其他为从数据库，实现","title":"主从复制"},{"content":"[TOC]\n一 @Autowired 通过 @Autowired 的使用来消除 set ，get方法 注:区别于Lombok,lombok是生成set和get方法,你能直接使用 ;\n@Autowired 是用来注值的,spring注值本质上是用了set方法来找到对象并初始化的,在这个注解没出来前,需要写get,set方法\n通过(1),可以看到,@Autowired 本质上是用来初始化对象的,(源城时好像是用来初始化实体类,捷顺用来初始化Dao层对象) @Autowired默认按类型装配\n注:捷顺用来初始化Dao层对象,关键是怎么和Mapper连接呢,有两种方案:\n在Dao层接口类上 写 @Mapper 注解,表示这个接口要对接mapper文件 在application.xml中 添加配置: \u0026lt;!-- 自动扫描了所有的XxxxMapper.java，这样就不用一个一个手动配置Mpper的映射了，只要Mapper接口类和Mapper映射文件对应起来就可以了 --\u0026gt; \u0026lt;bean class=\u0026#34;org.mybatis.spring.mapper.MapperScannerConfigurer\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;basePackage\u0026#34; value=\u0026#34;com.jieshun.jht.integral.dao\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 来自 http://www.cnblogs.com/zghull/archive/2012/06/27/2565480.html 二 @Resource 这个是用来初始化对象的,这和@Autowired注解不一样,\n@Autowired默认按类型装配 , @Resource按名称装配, 这是使用注解是要自己写名称的,捷顺在Controller里用初始化对象,使用这个注解,在接口实现类(impl)上使用注解 @Service(\u0026ldquo;mebershipService\u0026rdquo;)来表示这个类要放入IOC中,并指定名字,然后使用这个指定的名字就能获得对象,\n如果没有指定名字就会按类型装配\n来自 http://www.cnblogs.com/zghull/archive/2012/06/27/2565480.html 三 @Service 当你需要定义某个类为一个bean，则在这个类的类名前一行使用@Service(\u0026ldquo;XXX\u0026rdquo;),就相当于讲这个类定义为一个bean，bean名称为XXX;\n会配合@Resource来使用\n来自 http://www.cnblogs.com/Struts-pring/p/4951661.html ​\n四 @Async 用法: 在方法上或者类上标注, 这样调用方法时就会异步执行, 它需要搭配 @EnableAsync注解使用\n它还有一个value值,表示指定使用哪个线程池,不指定就用叫\u0026rsquo;taskExecutor\u0026rsquo;的线程池, 连默认线程池都没有就注解自己的线程池(有内存泄露风险,而且不重用线程)\n原理: Spring容器启动初始化bean时，判断类中是否使用了@Async注解，创建切入点和切入点处理器，根据切入点创建代理，在调用@Async注解标注的方法时，会调用代理，执行切入点处理器invoke方法，将方法的执行提交给线程池，实现异步执行。\n需要注意的一个错误用法是，如果A类的a方法(没有标注@Async)调用它自己的b方法(标注@Async)是不会异步执行的，因为从a方法进入调用的都是它本身，不会进入代理。(称为本地调用)\nmode=AdviceMode.ASPECTJ 指定切面模式就可以生效\nSpring @Async之四：Aysnc的异步执行的线程池实现原理 - duanxz - 博客园 (cnblogs.com) Spring @Async 注解的使用以及原理（一）灵颖桥人的博客-CSDN博客@async注解 @Import @Import注解可以用于导入第三方包 ，当然@Bean注解也可以，但是@Import注解快速导入的方式更加便捷.Import注解本身在springboot中用的很多，特别是ImportSelector方式在springboot中使用的特别多\n@Import的三种用法 @Import的三种用法主要包括：\n直接填class数组方式 ImportSelector方式【重点】 ImportBeanDefinitionRegistrar方式 第一种用法：直接填class数组 直接填对应的class数组，class数组可以有0到多个。\n语法如下：\n@Import({ 类名.class , 类名.class... }) public class TestDemo { } 对应的import的bean都将加入到spring容器中，这些在容器中bean名称是该类的全类名 ，比如com.yc.类名\n第二种用法：ImportSelector方式【重点】 这种方式的前提就是一个类要实现ImportSelector接口，假如我要用这种方法，目标对象是Myclass这个类，分析具体如下：\n创建Myclass类并实现ImportSelector接口\npublic class Myclass implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { // 特意换个bean名字 return new String[]{\u0026#34;com.yc.Test.TestDemo3\u0026#34;}; } } 分析实现接口的selectImports方法中的：\n1、返回值： 就是我们实际上要导入到容器中的组件全类名【重点 】 2、参数： AnnotationMetadata表示当前被@Import注解给标注的所有注解信息【不是重点】 需要注意的是selectImports方法可以返回空数组但是不能返回null，否则会报空指针异常！\n案例:\n@Import({TestDemo2.class,Myclass.class}) public class TestDemo { @Bean public AccountDao2 accountDao2(){ return new AccountDao2(); } } /** * 打印容器中的组件测试 */ public class AnnotationTestDemo { public static void main(String[] args) { AnnotationConfigApplicationContext applicationContext=new AnnotationConfigApplicationContext(TestDemo.class); //这里的参数代表要做操作的类 String[] beanDefinitionNames = applicationContext.getBeanDefinitionNames(); for (String name : beanDefinitionNames){ System.out.println(name); } } } 所以初始化TestDemo类时就会有三个对象被初始化\n第三种用法：ImportBeanDefinitionRegistrar方式 同样是一个接口，类似于第二种ImportSelector用法，相似度80%，只不过这种用法比较自定义化注册，具体如下：\npublic class Myclass2 implements ImportBeanDefinitionRegistrar { @Override public void registerBeanDefinitions(AnnotationMetadata annotationMetadata, BeanDefinitionRegistry beanDefinitionRegistry) { //指定bean定义信息（包括bean的类型、作用域...） RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(TestDemo4.class); //注册一个bean指定bean名字（id） beanDefinitionRegistry.registerBeanDefinition(\u0026#34;TestDemo4444\u0026#34;,rootBeanDefinition); } } spring注解之@Import注解的三种使用方式 - 宜春 - 博客园 (cnblogs.com) ","permalink":"https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E6%B3%A8%E8%A7%A3.html","summary":"[TOC] 一 @Autowired 通过 @Autowired 的使用来消除 set ，get方法 注:区别于Lombok,lombok是生成set和get方法,你能直接使用 ; @Autowired 是用来注值的,sprin","title":"注解"},{"content":" 存款准备金率: 存款准备金利率（英文：Deposit Reserve Rate）是央行支付给金融机构缴存的 存款准备金 （法定存款准备金 和超额存款准备金 ）所支付的利率。\n来自:链接 \u0026gt;\n解释: 我们会往银行存钱,为了保证你来取钱时银行仓库里还有钱,银行要给中央银行一些钱,作为最低存款,这些钱不能用于贷款,\n如果比率下降了,则上交国家的钱就减少了(存的钱基本不会变),银行就有更多的钱用来贷款(赚钱),流通在市面上的钱就多了(王健林就能贷到更多的钱(反正你贷不到))\nhttps://baijiahao.baidu.com/s?id=1613737512299536546\u0026wfr=spider\u0026for=pc ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E9%87%91%E8%9E%8D/%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87.html","summary":"存款准备金率: 存款准备金利率（英文：Deposit Reserve Rate）是央行支付给金融机构缴存的 存款准备金 （法定存款准备金 和超额存款准备金 ）所支付的","title":"专业词汇"},{"content":"[toc]\n1. 介绍 Hive自定义函数包括三种UDF、UDAF、UDTF\nUDF(User-Defined-Function) 一进一出 UDAF(User- Defined Aggregation Funcation) 聚集函数，多进一出。Count/max/min UDTF(User-Defined Table-Generating Functions) 一进多出，如lateral view explore) 2. 函数类型 2.1. UDF(一进一出) 继承UDF类\n重写evaluate方法\n将该java文件编译成jar\n2.2. UDAF(多进一出) 实现方法:\n用户的UDAF必须继承了org.apache.hadoop.hive.ql.exec.UDAF；\n用户的UDAF必须包含至少一个实现了org.apache.hadoop.hive.ql.exec的静态类，诸如实现了 UDAFEvaluator\n一个计算函数必须实现的5个方法的具体含义如下：\ninit()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。\niterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则 就返回true。\nterminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。\nmerge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。\nterminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。\n部分聚集结果的数据类型和最终结果的数据类型可以不同。\n2.3. UDTF(一进多出) 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF\ninitialize()：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）\nprocess：初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列 可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数\n最后close()方法调用，对需要清理的方法进行清理\n来自: https://blog.csdn.net/weixin_42181917/article/details/82865140 https://www.cnblogs.com/lrxvx/p/10974341.html 3. 加载函数的方式 3.1 使用add jar [classPath]语句(临时加载) #加载jar add jar [classPath] #创建函数 create temporary function [functionName] as [calssPath] 这种方式不建议在生产环境中使用，通过该方式添加的jar文件只存在于当前会话中，当会话关闭后不能够继续使用该jar文件，最常见的问题是创建了永久函数到metastore中，再次使用该函数时却提示ClassNotFoundException。所以使用该方式每次都要使用add jar [classPath]语句添加相关的jar文件到classPath中。倒是可以用在临时使用函数的情况\n3.2 修改hive-site.xml文件 修改参数hive.aux.jars.path的值指向UDF文件所在的路径。该参数需要手动添加到hive-site.xml文件中。\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.aux.jars.path\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file:///[path],file:///[path]\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 3.3 将jar包放入hive的jar目录(永久加载) 是在${HIVE_HOME}下创建auxlib目录，将UDF文件放到该目录中，这样hive在启动时会将其中的jar文件加载到classpath中。\n或者 指定jar所在目录(这种反而更管理一点),实现:\n可以拷贝${HIVE_HOME}/conf中的hive-env.sh.template 为 hive-env.sh 文件，并修改最后一行：\nexport HIVE_AUX_JARS_PATH=[classPath]\n或者在系统中直接添加HIVE_AUX_JARS_PATH环境变量。\n来自: https://blog.csdn.net/snail_bing/article/details/82869435 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0.html","summary":"[toc] 1. 介绍 Hive自定义函数包括三种UDF、UDAF、UDTF UDF(User-Defined-Function) 一进一出 UDAF(User- Defined Aggregation Funcation) 聚集函数，多进一出。Count/max/min UDTF(User-Defined Table-Generating Functions) 一进多出，","title":"自定义函数"},{"content":"[toc]\n1. Exactly-once 字面意思是\u0026quot;精确一次\u0026quot;;\n只要在flink和kafka中出现,描述一下kafka中的含义:\nkafka有生产和消费,同一个消息有生产的次数和消费的次数,正确来说,应该都只有一次才对,但是在某些情况下由于各种原因导致出现不止一次(因为网络问题导致同一消息生产多次或者消费多次),这就出现了解决++幂等性++的问题,在kafka的高端应用中就变得格外重要(所以总有文章提到)\n2.Parquet列式存储格式 parquet只是一种存储格式，与上层语言无关(数据不是完整可看性)\n把IO只给查询需要用到的数据，只加载需要被计算的列 列式的压缩效果更好，节省空间 适配通用性 存储空间优化 计算时间优化 一个Parquet文件是由一个header以及一个或多个block块组成，以一个footer结尾。\nheader中只包含一个4个字节的数字PAR1用来识别整个Parquet文件格式。\n文件中所有的metadata都存在于footer中。footer中的metadata包含了格式的版本信息，schema信息、key-value paris以及所有block中的metadata信息。\nfooter中最后两个字段为一个以4个字节长度的footer的metadata,以及同header中包含的一样的PAR1。\n在Parquet文件中，每一个block都具有一组Row group,它们是由一组Column chunk组成的列数据。继续往下，每一个column chunk中又包含了它具有的pages。每个page就包含了来自于相同列的值\n常用parquet文件读写的几种方式:\n用spark的hadoopFile api读取hive中的parquet格式 用sparkSql读写hive中的parquet 用新旧MapReduce读写parquet格式文件 https://www.cnblogs.com/tonglin0325/p/10244676.html https://parquet.apache.org/documentation/latest/ https://www.cnblogs.com/windliu/p/10942252.html ","permalink":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%BB%BC%E5%90%88.html","summary":"[toc] 1. Exactly-once 字面意思是\u0026quot;精确一次\u0026quot;; 只要在flink和kafka中出现,描述一下kafka中的含义: kafka有生产和消费,同","title":"综合"},{"content":"变量的作用域\n在Python程序中创建、改变、查找变量名时，都是在一个保存变量名的空间中进行，我们称之为命名空间，也被称之为作用域。python的作用域是静态的，在源代码中变量名被赋值的位置决定了该变量能被访问的范围。即Python变量的作用域由变量所在源代码中的位置决定。\n只有当变量在Module(模块)、def(函数)中定义的时候，才会有作用域的概念\n在作用域中定义的变量，一般只在作用域中有效。 需要注意的是：在if-elif-else、for-else、while、try-except\try-finally等关键字的语句块中并不会产成作用域\n注:\n在Python中，scope是由namespace按特定的层级结构组合起来的。 scope一定是namespace，但namespace不一定是scope.\n来自* \u0026lt;http://python.jobbole.com/81367/ \u0026gt;\nclass没有作用域(scope)，但有一个局部的名空间(namespace)，它并不构成一个作用域。 这意味着在类定义中的表达式可以访问该名空间。\n搜索变量名的优先级：局部作用域 \u0026gt; 嵌套作用域 \u0026gt; 全局作用域 \u0026gt; 内置作用域\nLEGB****法则： 当在函数中使用未确定的变量名时，Python会按照优先级依次搜索4个作用域，以此来确定该变量名的意义。首先搜索局部作用域(L)，之后是上一层嵌套结构中def或lambda函数的嵌套作用域(E)，之后是全局作用域(G)，最后是内置作用域(B)。按这个查找原则，在第一处找到的地方停止。如果没有找到，则会出发NameError错误。\n**L(local)**局部作用域 局部变量：包含在def关键字定义的语句块中，即在函数中定义的变量。每当函数被调用时都会创建一个新的局部作用域。Python中也有递归，即自己调用自己，每次调用都会创建一个新的局部命名空间。在函数内部的变量声明，除非特别的声明为全局变量，否则均默认为局部变量。有些情况需要在函数内部定义全局变量，这时可以使用global关键字来声明变量的作用域为全局。局部变量域就像一个 栈，仅仅是暂时的存在，依赖创建该局部作用域的函数是否处于活动的状态。所以，一般建议尽量少定义全局变量，因为全局变量在模块文件运行的过程中会一直存在，占用内存空间。\n注意：如果需要在函数内部对全局变量赋值，需要在函数内部通过global语句声明该变量为全局变量。\n**E(enclosing)**嵌套作用域 E也包含在def关键字中，E和L是相对的，E相对于更上层的函数而言也是L。与L的区别在于，对一个函数而言，L是定义在此函数内部的局部作用域，而E是定义在此函数的上一层父级函数的局部作用域。主要是为了实现Python的闭包，而增加的实现。\n**G(global)**全局作用域 即在模块层次中定义的变量，每一个模块都是一个全局作用域。也就是说，在模块文件顶层声明的变量具有全局作用域，从外部开来，模块的全局变量就是一个模块对象的属性。\n注意：全局作用域的作用范围仅限于单个模块文件内\n**B(built-in)**内置作用域 系统内固定模块里定义的变量，如预定义在builtin 模块内的变量。\n不要使用 from xx import * , 会把xx模块中所有东西(变量,函数,类,等等)( 以 _ 开头的变量,函数不会引入)引入变为全局变量,就不知道引入了什么,怕 会引起作用域的冲突\n","permalink":"https://xiaokunji.com/zh/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/%E4%BD%9C%E7%94%A8%E5%9F%9F.html","summary":"变量的作用域 在Python程序中创建、改变、查找变量名时，都是在一个保存变量名的空间中进行，我们称之为命名空间，也被称之为作用域。pytho","title":"作用域"},{"content":"[toc]\n1. 简介 Airflow是一个可编程，调度和监控的工作流平台，基于有向无环图(DAG)，airflow可以定义一组有依赖的任务，按照依赖依次执行。airflow提供了丰富的命令行工具用于系统管控，而其web管理界面同样也可以方便的管控调度任务，并且对任务运行状态进行实时监控，方便了系统的运维和管理。\nAirflow 中常见的名词概念：\nDAG\nDAG 意为有向无循环图，在 Airflow 中则定义了整个完整的作业。同一个 DAG 中的所有 Task 拥有相同的调度时间。\nTask\nTask 为 DAG 中具体的作业任务，它必须存在于某一个 DAG 之中。Task 在 DAG 中配置依赖关系，跨 DAG 的依赖是可行的，但是并不推荐。跨 DAG 依赖会导致 DAG 图的直观性降低，并给依赖管理带来麻烦。\nDAG Run\n当一个 DAG 满足它的调度时间，或者被外部触发时，就会产生一个 DAG Run。可以理解为由 DAG 实例化的实例。\nTask Instance\n当一个 Task 被调度启动时，就会产生一个 Task Instance。可以理解为由 Task 实例化的实例。\n2、Airflow 的服务构成 一个正常运行的 Airflow 系统一般由以下几个服务构成\nWebServer\n​\tAirflow 提供了一个可视化的 Web 界面。启动 WebServer 后，就可以在 Web 界面上查看定义好的 DAG 并监控及改变运行状况。也可以在 Web 界面中对一些变量进行配置。\nWorker\n​\t一般来说我们用 Celery Worker 来执行具体的作业。Worker 可以部署在多台机器上，并可以分别设置接收的队列。当接收的队列中有作业任务时，Worker 就会接收这个作业任务，并开始执行。Airflow 会自动在每个部署 Worker 的机器上同时部署一个 Serve Logs 服务，这样我们就可以在 Web 界面上方便的浏览分散在不同机器上的作业日志了。\nScheduler\n​\t整个 Airflow 的调度由 Scheduler 负责发起，每隔一段时间 Scheduler 就会检查所有定义完成的 DAG 和定义在其中的作业，如果有符合运行条件的作业，Scheduler 就会发起相应的作业任务以供 Worker 接收。\nFlower\n​\tFlower 提供了一个可视化界面以监控所有 Celery Worker 的运行状况。这个服务并不是必要的。\n3. Airflow 的 Web 界面 1、DAG 列表 DAG 列表\n左侧 On/Off 按钮控制 DAG 的运行状态，Off 为暂停状态，On 为运行状态。注意：所有 DAG 脚本初次部署完成时均为 Off 状态。 若 DAG 名称处于不可点击状态，可能为 DAG 被删除或未载入。若 DAG 未载入，可点击右侧刷新按钮进行刷新。注意：由于可以部署若干 WebServer，所以单次刷新可能无法刷新所有 WebServer 缓存，可以尝试多次刷新。 Recent Tasks 会显示最近一次 DAG Run（可以理解为 DAG 的执行记录）中 Task Instances（可以理解为作业的执行记录）的运行状态，如果 DAG Run 的状态为 running，此时显示最近完成的一次以及正在运行的 DAG Run 中所有 Task Instances 的状态。 Last Run 显示最近一次的 execution date。注意：execution date 并不是真实执行时间，具体细节在下文 DAG 配置中详述。将鼠标移至 execution date 右侧 info 标记上，会显示 start date，start date 为真实运行时间。start date 一般为 execution date 所对应的下次执行时间。 2、作业操作框 在 DAG 的树状图和 DAG 图中都可以点击对应的 Task Instance 以弹出 Task Instance 模态框，以进行 Task Instance 的相关操作。注意：选择的 Task Instance 为对应 DAG Run 中的 Task Instance。\n作业操作框\n在作业名字的右边有一个漏斗符号，点击后整个 DAG 的界面将只显示该作业及该作业的依赖作业。当该作业所处的 DAG 较大时，此功能有较大的帮助。 Task Instance Details 显示该 Task Instance 的详情，可以从中得知该 Task Instance 的当前状态，以及处于当前状态的原因。例如，若该 Task Instance 为 no status 状态，迟迟不进入 queued 及 running 状态，此时就可通过 Task Instance Details 中的 Dependency 及 Reason 得知原因。 Rendered 显示该 Task Instance 被渲染后的命令。 Run 指令可以直接执行当前作业。 Clear 指令为清除当前 Task Instance 状态，清除任意一个 Task Instance 都会使当前 DAG Run 的状态变更为 running。注意：如果被清除的 Task Instance 的状态为 running，则会尝试 kill 该 Task Instance 所执行指令，并进入 shutdown 状态，并在 kill 完成后将此次执行标记为 failed（如果 retry 次数没有用完，将标记为 up_for_retry）。Clear 有额外的5个选项，均为多选，这些选项从左到右依次为： Past: 同时清除所有过去的 DAG Run 中此 Task Instance 所对应的 Task Instance。 Future: 同时清除所有未来的 DAG Run 中此 Task Instance 所对应的 Task Instance。注意：仅清除已生成的 DAG Run 中的 Task Instance。 Upstream: 同时清除该 DAG Run 中所有此 Task Instance 上游的 Task Instance。 Downstream: 同时清除该 DAG Run 中所有此 Task Instance 下游的 Task Instance。 Recursive: 当此 Task Instance 为 sub DAG 时，循环清除所有该 sub DAG 中的 Task Instance。注意：若当此 Task Instance 不是 sub DAG 则忽略此选项。 Mark Success 指令为讲当前 Task Instance 状态标记为 success。注意：如果该 Task Instance 的状态为 running，则会尝试 kill 该 Task Instance 所执行指令，并进入 shutdown 状态，并在 kill 完成后将此次执行标记为 failed（如果 retry 次数没有用完，将标记为 up_for_retry）。 4. DAG 配置 Airflow 中的 DAG 是由 Python 脚本来配置的，因而可扩展性非常强。Airflow 提供了一些 DAG 例子，\n# -*- coding: utf-8 -*- import airflow from airflow.operators.bash_operator import BashOperator from airflow.operators.dummy_operator import DummyOperator from airflow.models import DAG args = { \u0026#39;owner\u0026#39;: \u0026#39;airflow\u0026#39;, \u0026#39;start_date\u0026#39;: airflow.utils.dates.days_ago(2) # 作业的开始时间，即作业将在这个时间点以后开始调度。 } dag = DAG( dag_id=\u0026#39;example_bash_operator\u0026#39;, # 给 DAG 取一个名字,不能重复 default_args=args, schedule_interval=\u0026#39;0 0 * * *\u0026#39; # 配置 DAG 的执行周期，语法和 crontab 的一致 ) cmd = \u0026#39;ls -l\u0026#39; run_this_last = DummyOperator(task_id=\u0026#39;run_this_last\u0026#39;, dag=dag) run_this = BashOperator( task_id=\u0026#39;run_after_loop\u0026#39;, bash_command=\u0026#39;echo 1\u0026#39;, dag=dag) run_this.set_downstream(run_this_last) for i in range(3): i = str(i) task = BashOperator( task_id=\u0026#39;runme_\u0026#39;+i, bash_command=\u0026#39;echo \u0026#34;{{ task_instance_key_str }}\u0026#34; \u0026amp;\u0026amp; sleep 1\u0026#39;, dag=dag) task.set_downstream(run_this) task = BashOperator( task_id=\u0026#39;also_run_this\u0026#39;, bash_command=\u0026#39;echo \u0026#34;run_id={{ run_id }} | dag_run={{ dag_run }}\u0026#34;\u0026#39;, dag=dag) # 用[]可以并行执行多个job, task \u0026gt;\u0026gt; [job1,job2] # 等同于 task.set_downstream(run_this_last) task \u0026gt;\u0026gt; run_this_last 那么现在，让我们看一下当一个新配置的 DAG 生效后第一次调度会在什么时候。其实第一次调度时间是在作业中配置的 start date 的第二个满足 schedule interval 的时间点，并且记录的 execution date 为作业中配置的 start date 的第一个满足 schedule interval 的时间点.\n假设我们配置了一个作业的 start date 为 2017年10月1日，配置的 schedule interval 为 **00 12 * * *** 那么第一次执行的时间将是 2017年10月2日 12点 而此时记录的 execution date 为 2017年10月1日 12点。因此 execution date 并不是如其字面说的表示执行时间，真正的执行时间是 execution date 所显示的时间的下一个满足 schedule interval 的时间点。\n浅谈 实战 使用 搭建及问题 官网 ","permalink":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/airflow/airflow.html","summary":"[toc] 1. 简介 Airflow是一个可编程，调度和监控的工作流平台，基于有向无环图(DAG)，airflow可以定义一组有依赖的任务，按照依赖依次执","title":"airflow"},{"content":" 神代綺凛 ななひら天下第一！(侵删) Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 / / 名称： 米二 网址： https://xiaokunji.com 图标： https://xiaokunji.com/img/Q.svg 描述： 一个记录技术积累的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n联系邮箱: mess@xiaokunji.com ","permalink":"https://xiaokunji.com/zh/links.html","summary":"神代綺凛 ななひら天下第一！(侵删) Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 / / 名称： 米二 网址： https://xiaokunji.com 图标： https://xiaokunji.com/img/Q.svg 描述： 一个记录技术积累的博客 👉友","title":"🤝友链"}]