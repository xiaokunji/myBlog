<!DOCTYPE html>
<html dir="auto" lang="zh"><head><meta charset="utf-8"/><meta content="IE=edge" http-equiv="x-ua-compatible"/><meta content="width=device-width,initial-scale=1,shrink-to-fit=no" name="viewport"/><meta content="index, follow" name="robots"/><title>安装与配置 | 米二</title><meta content=" **一、安装Spark集群**, **1. 解压安装包**, **2. 配置spark**, shuffled以及RDD的数据存放目录, worker端进程的工作目录, **3. 复制到其他节点**, **5. 启动验证**, **6. 打包运行**, **二,高可用(热备)**" name="keywords"/><meta content="     " name="description"/><meta content="xkj" name="author"/><link href="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html" rel="canonical"/><link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.dc6bd2c841ee388e899e13872ae986c0e847e8b957d6714ad6ddc628ef2d17ff.css" integrity="sha256-3GvSyEHuOI6JnhOHKumGwOhH6LlX1nFK1t3GKO8tF/8=" rel="preload stylesheet"/><script crossorigin="anonymous" defer="" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload="hljs.initHighlightingOnLoad()" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js"></script>
<link href="https://xiaokunji.com/img/Q.svg" rel="icon"/><link href="https://xiaokunji.com/img/Q.svg" rel="icon" sizes="16x16" type="image/png"/><link href="https://xiaokunji.com/img/Q.svg" rel="icon" sizes="32x32" type="image/png"/><link href="https://xiaokunji.com/Q.svg" rel="apple-touch-icon"/><link href="https://xiaokunji.com/Q.svg" rel="mask-icon"/><meta content="#2e2e33" name="theme-color"/><meta content="#2e2e33" name="msapplication-TileColor"/><link href="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html" hreflang="zh" rel="alternate"/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta content="安装与配置" property="og:title"/><meta content="     " property="og:description"/><meta content="article" property="og:type"/><meta content="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html" property="og:url"/><meta content="hadoop生态圈" property="article:section"/><meta content="2023-08-22T00:00:00+00:00" property="article:published_time"/><meta content="2023-08-29T00:59:17+08:00" property="article:modified_time"/><meta content="summary" name="twitter:card"/><meta content="安装与配置" name="twitter:title"/><meta content="     " name="twitter:description"/><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"安装与配置","item":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html"}]}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"安装与配置","name":"安装与配置","description":"     ","keywords":[" **一、安装Spark集群**"," **1. 解压安装包**"," **2. 配置spark**"," shuffled以及RDD的数据存放目录"," worker端进程的工作目录"," **3. 复制到其他节点**"," **5. 启动验证**"," **6. 打包运行**"," **二,高可用(热备)**"],"articleBody":"[toc]\n注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和stop-all.sh,而spark启动/关闭用到了这两个脚本,而hadoop中这么命令已经遗弃了\n源笔记:\n1.修改slave文件,配置从机\n如下:\nslave01\nslave02\nslave03\n2.修改spark.env.sh,如下:\nexport JAVA_HOME=/mysoftware/jdk1.8.0_101 export SPARK_MASTER_IP=master export SCALA_HOME=/mysoftware/scala-2.12.3 export HADOOP_HOME=/mysoftware/hadoop-2.7.3 export HADOOP_CONF_DIR=/mysoftware/hadoop-2.7.3/etc/hadoop/ 3.发送到从机即可,集群环境搭建完毕\n一、安装Spark集群 1. 解压安装包 tar -zxvf ~/jar/spark-1.6.3-bin-hadoop2.6.tgz -C /data\n2. 配置spark 涉及到的配置文件有以下几个：\n${SPARK_HOME}/conf/spark-env.sh\n${SPARK_HOME}/conf/slaves\n${SPARK_HOME}/conf/spark-defaults.conf\n这三个文件都是由原始的template文件复制过来的，比如cp spark-env.sh.template spark-env.sh\n配置文件1：spark-env.sh\nJAVA_HOME=/data/jdk1.8.0_111 SCALA_HOME=/data/scala-2.11.8 SPARK_MASTER_HOST=master SPARK_MASTER_PORT=7077 HADOOP_CONF_DIR=/data/hadoop-2.6.5/etc/hadoop # shuffled以及RDD的数据存放目录 SPARK_LOCAL_DIRS=/data/spark_data # worker端进程的工作目录 SPARK_WORKER_DIR=/data/spark_data/spark_works 注意：需要在本地创建/data/spark_data/spark_works目录\n配置文件2：slaves\nmaster slave1 slave2 配置文件3：spark-defaults.conf\nspark.master spark://master:7077 spark.serializer org.apache.spark.serializer.KryoSerializer spark.eventLog.enabled true spark.eventLog.dir file:///data/spark_data/history/event-log spark.history.fs.logDirectory file:///data/spark_data/history/spark-events spark.eventLog.compress true 注意：需要在本地创建/data/spark_data/history/event-log、/data/spark_data/history/spark-events\n3. 复制到其他节点 在master上：\nscp -r /data/spark* aboutyun@slave1:~/ scp -r /data/spark* aboutyun@slave2:~/ 在slave1和slave2上：\nmv ~/spark* /data\n4. 设置环境变量\n将以下内容加入到~/.bashrc文件中，\nexport SPARK_HOME=/data/spark-1.6.3-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH 然后执行以下命令：\nsource ~/.bashrc\n5. 启动验证 在master机器上进行如下操作\n可直接使用 start-all.sh , 但是要注意hadoop的命令, hadoop也有一个一样的命令,但是已经遗弃\n1）启动master\nstart-master.sh\n在master机器上执行jps命令\n上图说明在master节点上成功启动Master进程\n2）启动slave\n在master和slave机器上执行jps命令\n上面的图片说明在每台机器上都成功启动了Worker进程。\n3）访问WebUI\n在master、slave1和slave2这三台中任意一台机器上的浏览器中输入：http://master:8080/，看到如下图片，就说明我们的spark集群安装成功了。\n趟过的坑\n配置core-site.xml和hdfs-site.xml文件时所指定的本地目录一定要自己创建，否则在执行玩格式化hdfs后，启动hdfs会丢失进程。 6. 打包运行 https://www.cnblogs.com/654wangzai321/p/9513488.html 二,高可用(热备) ","wordCount":"961","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-08-29T00:59:17+08:00","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class="dark" id="top"><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class="header"><nav class="nav"><div class="logo"><a accesskey="h" href="https://xiaokunji.com/zh/" title="米二 (Alt + H)"><img alt="" aria-label="logo" height="35" src="https://xiaokunji.com/img/Q.svg"/>米二</a><div class="logo-switches"><button accesskey="t" id="theme-toggle" title="(Alt + T)"><svg fill="none" height="18" id="moon" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"></path></svg><svg fill="none" height="18" id="sun" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="5"></circle><line x1="12" x2="12" y1="1" y2="3"></line><line x1="12" x2="12" y1="21" y2="23"></line><line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line><line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line><line x1="1" x2="3" y1="12" y2="12"></line><line x1="21" x2="23" y1="12" y2="12"></line><line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line><line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line></svg></button><ul class="lang-switch"><li>|</li><li><a aria-label="English" href="https://xiaokunji.com/en/" title="English">English</a></li></ul></div></div><ul id="menu"><li><a href="https://xiaokunji.com/zh/" title="🏠主页"><span>🏠主页</span></a></li><li><a accesskey="/" href="https://xiaokunji.com/zh/search" title="🔍搜索 (Alt + /)"><span>🔍搜索</span></a></li><li><a href="https://xiaokunji.com/zh/post.html" title="📚文章"><span>📚文章</span></a></li><li><a href="https://xiaokunji.com/zh/archives.html" title="⏱时间轴"><span>⏱时间轴</span></a></li><li><a href="https://xiaokunji.com/zh/tags.html" title="🔖标签"><span>🔖标签</span></a></li><li><a href="https://xiaokunji.com/zh/categories.html" title="📖分类"><span>📖分类</span></a></li><li><a href="https://xiaokunji.com/zh/links.html" title="🤝友链"><span>🤝友链</span></a></li></ul></nav></header><main class="main"><article class="post-single"><header class="post-header"><nav aria-label="breadcrumb"><ul><a href="https://xiaokunji.com/zh/">🏠</a> <span>&gt;</span>
<a href="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html">hadoop生态圈</a> <span>&gt;</span>
<a href="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Spark.html">Spark</a> <span>&gt;</span></ul></nav><h1 class="post-title">安装与配置</h1><div class="post-description"></div><div class="post-meta">创建: <span title="2023-08-22 00:00:00 +0000 UTC">2023-08-22</span> · 更新: 2023-08-29 · xkj
 | 分类:  <ul class="post-categories-meta" style="display:inline"><a href="https://xiaokunji.com/zh/categories/Hadoop%E7%94%9F%E6%80%81%E5%9C%88.html">Hadoop生态圈</a></ul><span id="busuanzi_container_page_pv"> | 访问: <span id="busuanzi_value_page_pv">1</span></span></div></header><aside class="toc-container wide" id="toc-container"><div class="toc"><details open=""><summary accesskey="c" title="(Alt + C)"><span class="details">目录</span></summary><div class="inner"><ul><li><a aria-label="一、安装Spark集群" href="#%e4%b8%80%e5%ae%89%e8%a3%85spark%e9%9b%86%e7%be%a4"><strong>一、安装Spark集群</strong></a><ul><li><a aria-label="1. 解压安装包" href="#1-%e8%a7%a3%e5%8e%8b%e5%ae%89%e8%a3%85%e5%8c%85"><strong>1. 解压安装包</strong></a></li><li><a aria-label="2. 配置spark" href="#2-%e9%85%8d%e7%bd%aespark"><strong>2. 配置spark</strong></a></li><li><a aria-label="3. 复制到其他节点" href="#3-%e5%a4%8d%e5%88%b6%e5%88%b0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9"><strong>3. 复制到其他节点</strong></a></li><li><a aria-label="5. 启动验证" href="#5-%e5%90%af%e5%8a%a8%e9%aa%8c%e8%af%81"><strong>5. 启动验证</strong></a></li><li><a aria-label="6. 打包运行" href="#6-%e6%89%93%e5%8c%85%e8%bf%90%e8%a1%8c"><strong>6. 打包运行</strong></a></li></ul></li><li><a aria-label="二,高可用(热备)" href="#%e4%ba%8c%e9%ab%98%e5%8f%af%e7%94%a8%e7%83%ad%e5%a4%87"><strong>二,高可用(热备)</strong></a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class="post-content"><p>[toc]</p><p>注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和stop-all.sh,而spark启动/关闭用到了这两个脚本,而hadoop中这么命令已经遗弃了</p><blockquote><p>源笔记:</p><p><strong>1.修改slave文件,配置从机</strong></p><p>如下:</p><p>slave01</p><p>slave02</p><p>slave03</p><p><strong>2.修改spark.env.sh,如下:</strong></p><div class="highlight"><pre style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4" tabindex="0"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>export <span style="color:#79c0ff">JAVA_HOME</span><span style="color:#ff7b72;font-weight:700">=</span>/mysoftware/jdk1.8.0_101
</span></span><span style="display:flex"><span>export <span style="color:#79c0ff">SPARK_MASTER_IP</span><span style="color:#ff7b72;font-weight:700">=</span>master
</span></span><span style="display:flex"><span>export <span style="color:#79c0ff">SCALA_HOME</span><span style="color:#ff7b72;font-weight:700">=</span>/mysoftware/scala-2.12.3
</span></span><span style="display:flex"><span>export <span style="color:#79c0ff">HADOOP_HOME</span><span style="color:#ff7b72;font-weight:700">=</span>/mysoftware/hadoop-2.7.3
</span></span><span style="display:flex"><span>export <span style="color:#79c0ff">HADOOP_CONF_DIR</span><span style="color:#ff7b72;font-weight:700">=</span>/mysoftware/hadoop-2.7.3/etc/hadoop/
</span></span></code></pre></div><p><strong>3.发送到从机即可,集群环境搭建完毕</strong></p></blockquote><h1 id="一安装spark集群"><strong>一、安装Spark集群</strong><a aria-hidden="true" class="anchor" hidden="" href="#一安装spark集群">#</a></h1><h2 id="1-解压安装包"><strong>1. 解压安装包</strong><a aria-hidden="true" class="anchor" hidden="" href="#1-解压安装包">#</a></h2><p>tar -zxvf ~/jar/spark-1.6.3-bin-hadoop2.6.tgz -C /data</p><h2 id="2-配置spark"><strong>2. 配置spark</strong><a aria-hidden="true" class="anchor" hidden="" href="#2-配置spark">#</a></h2><p>涉及到的配置文件有以下几个：</p><p>${SPARK_HOME}/conf/spark-env.sh</p><p>${SPARK_HOME}/conf/slaves</p><p>${SPARK_HOME}/conf/spark-defaults.conf</p><p>这三个文件都是由原始的template文件复制过来的，比如cp spark-env.sh.template spark-env.sh</p><p><strong>配置文件1：spark-env.sh</strong></p><div class="highlight"><pre style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4" tabindex="0"><code class="language-sh" data-lang="sh"><span style="display:flex"><span><span style="color:#79c0ff">JAVA_HOME</span><span style="color:#ff7b72;font-weight:700">=</span>/data/jdk1.8.0_111
</span></span><span style="display:flex"><span><span style="color:#79c0ff">SCALA_HOME</span><span style="color:#ff7b72;font-weight:700">=</span>/data/scala-2.11.8
</span></span><span style="display:flex"><span><span style="color:#79c0ff">SPARK_MASTER_HOST</span><span style="color:#ff7b72;font-weight:700">=</span>master
</span></span><span style="display:flex"><span><span style="color:#79c0ff">SPARK_MASTER_PORT</span><span style="color:#ff7b72;font-weight:700">=</span><span style="color:#a5d6ff">7077</span>
</span></span><span style="display:flex"><span><span style="color:#79c0ff">HADOOP_CONF_DIR</span><span style="color:#ff7b72;font-weight:700">=</span>/data/hadoop-2.6.5/etc/hadoop
</span></span><span style="display:flex"><span><span style="color:#8b949e;font-style:italic"># shuffled以及RDD的数据存放目录</span>
</span></span><span style="display:flex"><span><span style="color:#79c0ff">SPARK_LOCAL_DIRS</span><span style="color:#ff7b72;font-weight:700">=</span>/data/spark_data
</span></span><span style="display:flex"><span><span style="color:#8b949e;font-style:italic"># worker端进程的工作目录</span>
</span></span><span style="display:flex"><span><span style="color:#79c0ff">SPARK_WORKER_DIR</span><span style="color:#ff7b72;font-weight:700">=</span>/data/spark_data/spark_works
</span></span></code></pre></div><blockquote><p>注意：需要在本地创建/data/spark_data/spark_works目录</p></blockquote><p><strong>配置文件2：slaves</strong></p><div class="highlight"><pre style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4" tabindex="0"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>master 
</span></span><span style="display:flex"><span>slave1 
</span></span><span style="display:flex"><span>slave2
</span></span></code></pre></div><p><strong>配置文件3：spark-defaults.conf</strong></p><div class="highlight"><pre style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4" tabindex="0"><code class="language-properties" data-lang="properties"><span style="display:flex"><span>spark.master        <span style="color:#a5d6ff">spark://master:7077</span>
</span></span><span style="display:flex"><span>spark.serializer        <span style="color:#a5d6ff">org.apache.spark.serializer.KryoSerializer</span>
</span></span><span style="display:flex"><span>spark.eventLog.enabled        <span style="color:#a5d6ff">true</span>
</span></span><span style="display:flex"><span>spark.eventLog.dir        <span style="color:#a5d6ff">file:///data/spark_data/history/event-log</span>
</span></span><span style="display:flex"><span>spark.history.fs.logDirectory        <span style="color:#a5d6ff">file:///data/spark_data/history/spark-events</span>
</span></span><span style="display:flex"><span>spark.eventLog.compress        <span style="color:#a5d6ff">true</span>
</span></span></code></pre></div><blockquote><p>注意：需要在本地创建/data/spark_data/history/event-log、/data/spark_data/history/spark-events</p></blockquote><h2 id="3-复制到其他节点"><strong>3. 复制到其他节点</strong><a aria-hidden="true" class="anchor" hidden="" href="#3-复制到其他节点">#</a></h2><p>在master上：</p><div class="highlight"><pre style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4" tabindex="0"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>scp -r /data/spark* aboutyun@slave1:~/
</span></span><span style="display:flex"><span>scp -r /data/spark* aboutyun@slave2:~/
</span></span></code></pre></div><p>在slave1和slave2上：</p><p><code>mv ~/spark* /data</code></p><p><strong>4. 设置环境变量</strong></p><p>将以下内容加入到~/.bashrc文件中，</p><div class="highlight"><pre style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4" tabindex="0"><code class="language-sh" data-lang="sh"><span style="display:flex"><span>export <span style="color:#79c0ff">SPARK_HOME</span><span style="color:#ff7b72;font-weight:700">=</span>/data/spark-1.6.3-bin-hadoop2.6
</span></span><span style="display:flex"><span>export <span style="color:#79c0ff">PATH</span><span style="color:#ff7b72;font-weight:700">=</span><span style="color:#79c0ff">$SPARK_HOME</span>/bin:<span style="color:#79c0ff">$SPARK_HOME</span>/sbin:<span style="color:#79c0ff">$PATH</span>
</span></span></code></pre></div><p>然后执行以下命令：</p><p><code>source ~/.bashrc</code></p><h2 id="5-启动验证"><strong>5. 启动验证</strong><a aria-hidden="true" class="anchor" hidden="" href="#5-启动验证">#</a></h2><p>在master机器上进行如下操作</p><blockquote><p>可直接使用 start-all.sh , 但是要注意hadoop的命令, hadoop也有一个一样的命令,但是已经遗弃</p></blockquote><p><strong>1）启动master</strong></p><p><code>start-master.sh</code></p><p>在master机器上执行jps命令</p><p><img alt="img" loading="lazy" src="https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174246.png"/></p><p>上图说明在master节点上成功启动Master进程</p><p><strong>2）启动slave</strong></p><p>在master和slave机器上执行jps命令</p><p><img alt="img" loading="lazy" src="https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174247.png"/></p><p>上面的图片说明在每台机器上都成功启动了Worker进程。</p><p><strong>3）访问WebUI</strong></p><p>在master、slave1和slave2这三台中任意一台机器上的浏览器中输入：http://master:8080/，看到如下图片，就说明我们的spark集群安装成功了。</p><p><img alt="img" loading="lazy" src="https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174248.png"/></p><p><strong>趟过的坑</strong></p><ol><li>配置core-site.xml和hdfs-site.xml文件时所指定的本地目录一定要自己创建，否则在执行玩格式化hdfs后，启动hdfs会丢失进程。</li></ol><h2 id="6-打包运行"><strong>6. 打包运行</strong><a aria-hidden="true" class="anchor" hidden="" href="#6-打包运行">#</a></h2><blockquote><p><a href="https://www.cnblogs.com/654wangzai321/p/9513488.html" rel="noopener" target="_blank">https://www.cnblogs.com/654wangzai321/p/9513488.html</a></p></blockquote><h1 id="二高可用热备"><strong>二,高可用(热备)</strong><a aria-hidden="true" class="anchor" hidden="" href="#二高可用热备">#</a></h1></div><footer class="post-footer"><ul class="post-tags"><li><a href="https://xiaokunji.com/zh/tags/Spark.html">Spark</a></li><li><a href="https://xiaokunji.com/zh/tags/Hadoop%E7%94%9F%E6%80%81%E5%9C%88.html">Hadoop生态圈</a></li></ul></footer></article></main><script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><footer class="footer"><span>Copyright
©
-2023
<a href="https://xiaokunji.com/zh/" style="color:#939393">米二</a>
All Rights Reserved</span>
<span id="busuanzi_container"><span class="fa fa-user">用户数:</span><span id="busuanzi_value_site_uv"></span>
<span class="fa fa-eye">访问数:</span><span id="busuanzi_value_site_pv"></span></span></footer><a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)"><svg fill="currentcolor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg"><path d="M12 6H0l6-6z"></path></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>