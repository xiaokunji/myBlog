<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>原理及介绍 | 米二</title><meta name=keywords content=" 1.什么是hive, **1.1 Hive 特点**, **2. hive架构**, **2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor**, **2.2 元数据存储系统 ： RDBMS MySQL**, **2.3. 表的分类**, **3. 数据类型**, **3.1 基本类型**, **3.2 复杂类型**, **4. 存储格式**"><meta name=description content="     "><meta name=author content="xkj"><link rel=canonical href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D.html><link crossorigin=anonymous href=/assets/css/stylesheet.b3faf608c544858ba700943ffe182cb647f38432d29a07d73234965beacb26f6.css integrity="sha256-s/r2CMVEhYunAJQ//hgstkfzhDLSmgfXMjSWW+rLJvY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=16x16 href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=32x32 href=https://xiaokunji.com/img/Q.svg><link rel=apple-touch-icon href=https://xiaokunji.com/Q.svg><link rel=mask-icon href=https://xiaokunji.com/Q.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D.html><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="原理及介绍"><meta property="og:description" content="     "><meta property="og:type" content="article"><meta property="og:url" content="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D.html"><meta property="article:section" content="hadoop生态圈"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-04T07:19:38+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="原理及介绍"><meta name=twitter:description content="     "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"原理及介绍","item":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"原理及介绍","name":"原理及介绍","description":"     ","keywords":[" 1.什么是hive"," **1.1 Hive 特点**"," **2. hive架构**"," **2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor**"," **2.2 元数据存储系统 ： RDBMS MySQL**"," **2.3. 表的分类**"," **3. 数据类型**"," **3.1 基本类型**"," **3.2 复杂类型**"," **4. 存储格式**"],"articleBody":"[toc]\n1.什么是hive 1. Hive 由 Facebook 实现并开源\n2. 是基于 Hadoop 的一个数据仓库工具\n3. 可以将结构化的数据映射为一张数据库表\n4. 并提供 HQL(Hive SQL)查询功能\n5. 底层数据是存储在 HDFS 上\n6. Hive的本质是将 SQL 语句转换为 MapReduce 任务运行\n7. 使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，适用于离线的批量数据计算。\n数据仓库之父比尔·恩门（Bill Inmon）在 1991 年出版的“Building the Data Warehouse”（《建 立数据仓库》）一书中所提出的定义被广泛接受——数据仓库（Data Warehouse）是一个面 向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史 变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。\nHive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行，所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理\n1.1 Hive 特点 优点：\n1、可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器cpu i7-6700k 4核心8线程，8核心16线程，内存64G =\u003e 128G\n2、延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数\n3、良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行\n缺点：\n1、Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作）\n2、Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。\n3、Hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别）。\n2. hive架构 2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行\nHive 的核心是驱动引擎， 驱动引擎由四部分组成(执行顺序亦如此)：\n(1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST）\n(2) 编译器：编译器是将语法树编译为逻辑执行计划(就是翻译成MapReduce)\n(3) 优化器：优化器是对逻辑执行计划进行优化\n(4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划\n来源 https://www.bilibili.com/video/BV1W4411B7cN?p=4 2.2 元数据存储系统 ： RDBMS MySQL 元数据 :通俗的讲，就是存储在 Hive 中的数据的描述信息。\nHive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录\nMetastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理\n解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程）\nHive 和 MySQL 之间通过 MetaStore 服务交互\n2.3. 表的分类 Hive 中的表分为 内部表、外部表、分区表和 Bucket 表\n内部表和外部表的区别：\n删除内部表，删除表元数据和数据\n删除外部表，删除元数据，不删除数据\n内部表和外部表的使用选择：\n大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。\n使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中\n使用外部表的场景是针对一个数据集有多个不同的 Schema\n通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，都可以对 hive 表的数据存储目录中的数据进行增删操作。\n选择: 另一种说法: 由于外部表不会被直接删除,因此更安全,而内部表可以直接删除,因此更方便,可以用作临时表, 捷顺目前并没有明显的区分\n分区表和分桶表的区别：\nHive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似。\n分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多\n3. 数据类型 3.1 基本类型 描述 示例 boolean true/false TRUE tinyint 1字节的有符号整数 -128~127 1Y smallint 2个字节的有符号整数，-32768~32767 1S int 4个字节的带符号整数 1 bigint 8字节带符号整数 1L float 4字节单精度浮点数 1.0 double 8字节双精度浮点数 1.0 deicimal 任意精度的带符号小数 1.0 String 字符串，变长 “a”,’b’ varchar 变长字符串 “a”,’b’ char 固定长度字符串 “a”,’b’ binary 字节数组 无法表示 timestamp 时间戳，纳秒精度 122327493795 date 日期 ‘2018-04-07’ Hive 支持关系型数据中大多数基本数据类型,和其他的SQL语言一样，这些都是保留字。需要注意的是所有的这些数据类型都是对Java中接口的实现，因此这些类型的具体行为细节和Java中对应的类型是完全一致的。例如，string类型实现的是Java中的String，float实现的是Java中的float，等等。\n3.2 复杂类型 类型 描述 示例 array 有序的的同类型的集合 array(1,2) map key-value,key必须为原始类型，value可以任意类型 map(‘a’,1,’b’,2) struct 字段集合,类型可以不同 struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) 4. 存储格式 1. textfile\ntextfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。\n2. SequenceFile\nSequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。\nSequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。\n3. RCFile\n一种行列存储相结合的存储方式。\n4. ORCFile\n数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。\n5. Parquet\nParquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。\n来自: https://www.cnblogs.com/qingyunzong/p/8733924.html ","wordCount":"2610","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-09-04T07:19:38.39913048Z","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaokunji.com/zh/ accesskey=h title="米二 (Alt + H)"><img src=https://xiaokunji.com/img/Q.svg alt aria-label=logo height=35>米二</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaokunji.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://xiaokunji.com/zh/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://xiaokunji.com/zh/search title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://xiaokunji.com/zh/post.html title=📚文章><span>📚文章</span></a></li><li><a href=https://xiaokunji.com/zh/archives.html title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://xiaokunji.com/zh/tags.html title=🔖标签><span>🔖标签</span></a></li><li><a href=https://xiaokunji.com/zh/categories.html title=📖分类><span>📖分类</span></a></li><li><a href=https://xiaokunji.com/zh/links.html title=🤝友链><span>🤝友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><nav aria-label=breadcrumb><ul><a href=https://xiaokunji.com/zh/>🏠</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>hadoop生态圈</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive.html>Hive</a> <span>></span></ul></nav><h1 class=post-title>原理及介绍</h1><div class=post-description></div><div class=post-meta>创建:&nbsp;<span title='2023-08-22 00:00:00 +0000 UTC'>2023-08-22</span>&nbsp;·&nbsp;更新:&nbsp;2023-09-04&nbsp;·&nbsp;xkj
&nbsp;|&nbsp;分类: &nbsp;<ul class=post-categories-meta><a href=https://xiaokunji.com/zh/categories/Hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></ul><span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv>1</span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#1%e4%bb%80%e4%b9%88%e6%98%afhive aria-label=1.什么是hive>1.什么是hive</a><ul><li><a href=#11-hive-%e7%89%b9%e7%82%b9 aria-label="1.1 Hive 特点"><strong>1.1 Hive 特点</strong></a></li></ul></li><li><a href=#2-hive%e6%9e%b6%e6%9e%84 aria-label="2. hive架构"><strong>2. hive架构</strong></a><ul><li><a href=#21-%e5%ba%95%e5%b1%82%e7%9a%84driver-%e9%a9%b1%e5%8a%a8%e5%99%a8driver%e7%bc%96%e8%af%91%e5%99%a8compiler%e4%bc%98%e5%8c%96%e5%99%a8optimizer%e6%89%a7%e8%a1%8c%e5%99%a8executor aria-label="2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor"><strong>2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor</strong></a></li><li><a href=#22-%e5%85%83%e6%95%b0%e6%8d%ae%e5%ad%98%e5%82%a8%e7%b3%bb%e7%bb%9f--rdbms-mysql aria-label="2.2 元数据存储系统 ： RDBMS MySQL"><strong>2.2 元数据存储系统 ： RDBMS MySQL</strong></a></li><li><a href=#23-%e8%a1%a8%e7%9a%84%e5%88%86%e7%b1%bb aria-label="2.3. 表的分类"><strong>2.3. 表的分类</strong></a></li><li><a href=#3-%e6%95%b0%e6%8d%ae%e7%b1%bb%e5%9e%8b aria-label="3. 数据类型"><strong>3. 数据类型</strong></a></li><li><a href=#31-%e5%9f%ba%e6%9c%ac%e7%b1%bb%e5%9e%8b aria-label="3.1 基本类型"><strong>3.1 基本类型</strong></a></li><li><a href=#32-%e5%a4%8d%e6%9d%82%e7%b1%bb%e5%9e%8b aria-label="3.2 复杂类型"><strong>3.2 复杂类型</strong></a></li></ul></li><li><a href=#4-%e5%ad%98%e5%82%a8%e6%a0%bc%e5%bc%8f aria-label="4. 存储格式"><strong>4. 存储格式</strong></a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>[toc]</p><h1 id=1什么是hive>1.什么是hive<a hidden class=anchor aria-hidden=true href=#1什么是hive>#</a></h1><p><strong>1. Hive 由 Facebook 实现并开源</strong></p><p><strong>2. 是基于 Hadoop 的一个数据仓库工具</strong></p><p><strong>3. 可以将结构化的数据映射为一张数据库表</strong></p><p><strong>4. 并提供 HQL(Hive SQL)查询功能</strong></p><p><strong>5. 底层数据是存储在 HDFS 上</strong></p><p><strong>6. Hive的本质是将 SQL 语句转换为 MapReduce 任务运行</strong></p><p><strong>7. 使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，适用于离线的批量数据计算。</strong></p><p>　　数据仓库之父比尔·恩门（Bill Inmon）在 1991 年出版的“Building the Data Warehouse”（《建 立数据仓库》）一书中所提出的定义被广泛接受——数据仓库（Data Warehouse）是一个面 向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史 变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。</p><p>　　Hive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行，所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理</p><h2 id=11-hive-特点><strong>1.1 Hive 特点</strong><a hidden class=anchor aria-hidden=true href=#11-hive-特点>#</a></h2><p><strong>优点</strong>：</p><p>　　1、<strong>可扩展性,横向扩展</strong>，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器cpu i7-6700k 4核心8线程，8核心16线程，内存64G => 128G</p><p>　　2、<strong>延展性</strong>，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数</p><p>　　3、<strong>良好的容错性</strong>，可以保障即使有节点出现问题，SQL 语句仍可完成执行</p><p><strong>缺点</strong>：</p><p>　　1、<strong>Hive 不支持记录级别的增删改操作</strong>，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作）</p><p>　　2、<strong>Hive 的查询延时很严重</strong>，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。</p><p>　　3、<strong>Hive 不支持事务</strong>（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别）。</p><h1 id=2-hive架构><strong>2. hive架构</strong><a hidden class=anchor aria-hidden=true href=#2-hive架构>#</a></h1><h2 id=21-底层的driver-驱动器driver编译器compiler优化器optimizer执行器executor><strong>2.1 底层的Driver： 驱动器Driver，编译器Compiler，优化器Optimizer，执行器Executor</strong><a hidden class=anchor aria-hidden=true href=#21-底层的driver-驱动器driver编译器compiler优化器optimizer执行器executor>#</a></h2><p>　　Driver 组件完成 HQL 查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行 计划的生成。生成的逻辑执行计划存储在 HDFS 中，并随后由 MapReduce 调用执行</p><p>　　Hive 的核心是驱动引擎， 驱动引擎由四部分组成(执行顺序亦如此)：</p><p>　　　　(1) 解释器：解释器的作用是将 HiveSQL 语句转换为抽象语法树（AST）</p><p>　　　　(2) 编译器：编译器是将语法树编译为逻辑执行计划(就是翻译成MapReduce)</p><p>　　　　(3) 优化器：优化器是对逻辑执行计划进行优化</p><p>　　　　(4) 执行器：执行器是调用底层的运行框架执行逻辑执行计划</p><p><img loading=lazy src=https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174028.png alt=image-20200919172030960></p><blockquote><p>来源 <a href="https://www.bilibili.com/video/BV1W4411B7cN?p=4" target=_blank rel=noopener>https://www.bilibili.com/video/BV1W4411B7cN?p=4</a></p></blockquote><h2 id=22-元数据存储系统--rdbms-mysql><strong>2.2 元数据存储系统 ： RDBMS MySQL</strong><a hidden class=anchor aria-hidden=true href=#22-元数据存储系统--rdbms-mysql>#</a></h2><p>　　<strong>元数据</strong> :通俗的讲，就是存储在 Hive 中的数据的描述信息。</p><p>　　Hive 中的元数据通常包括：<u>表的名字，表的列和分区及其属性，表的属性（内部表和 外部表），表的数据所在目录</u></p><p>　　Metastore 默认存在自带的 Derby 数据库中。缺点就是不适合多用户操作，并且数据存 储目录不固定。数据库跟着 Hive 走，极度不方便管理</p><p>　　解决方案：通常存我们自己创建的 MySQL 库（本地 或 远程）</p><p>　　Hive 和 MySQL 之间通过 MetaStore 服务交互</p><h2 id=23-表的分类><strong>2.3. 表的分类</strong><a hidden class=anchor aria-hidden=true href=#23-表的分类>#</a></h2><p>Hive 中的表分为 内部表、外部表、分区表和 Bucket 表</p><p><strong>内部表和外部表的区别：</strong></p><p>　　<strong>删除内部表，删除表元数据和数据</strong></p><p>　　<strong>删除外部表，删除元数据，不删除数据</strong></p><p><strong>内部表和外部表的使用选择：</strong></p><p>　　大多数情况，他们的区别不明显，如果数据的所有处理都在 Hive 中进行，那么倾向于 选择内部表，但是如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。</p><p>　　使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中</p><p>　　使用外部表的场景是针对一个数据集有多个不同的 Schema</p><p>　　通过外部表和内部表的区别和使用选择的对比可以看出来，hive 其实仅仅只是对存储在 HDFS 上的数据提供了一种新的抽象。而不是管理存储在 HDFS 上的数据。所以不管创建内部 表还是外部表，<u>都可以对 hive 表的数据存储目录中的数据进行增删操作。</u></p><blockquote><p>选择: 另一种说法: 由于外部表不会被直接删除,因此更安全,而内部表可以直接删除,因此更方便,可以用作临时表, 捷顺目前并没有明显的区分</p></blockquote><p><strong>分区表和分桶表的区别：</strong></p><p>　　Hive 数据表可以根据某些字段进行分区操作，细化数据管理，可以让部分查询更快。同 时表和分区也可以进一步被划分为 Buckets，分桶表的原理和 MapReduce 编程中的 HashPartitioner 的原理类似。</p><p>　　分区和分桶都是细化数据管理，但是分区表是手动添加区分，由于 Hive 是读模式，所 以对添加进分区的数据不做模式校验，分桶表中的数据是按照某些分桶字段进行 hash 散列 形成的多个文件，所以数据的准确性也高很多</p><h2 id=3-数据类型><strong>3. 数据类型</strong><a hidden class=anchor aria-hidden=true href=#3-数据类型>#</a></h2><h2 id=31-基本类型><strong>3.1 基本类型</strong><a hidden class=anchor aria-hidden=true href=#31-基本类型>#</a></h2><table><thead><tr><th>描述</th><th>示例</th><th></th></tr></thead><tbody><tr><td>boolean</td><td>true/false</td><td>TRUE</td></tr><tr><td>tinyint</td><td>1字节的有符号整数</td><td>-128~127 1Y</td></tr><tr><td>smallint</td><td>2个字节的有符号整数，-32768~32767</td><td>1S</td></tr><tr><td>int</td><td>4个字节的带符号整数</td><td>1</td></tr><tr><td>bigint</td><td>8字节带符号整数</td><td>1L</td></tr><tr><td>float</td><td>4字节单精度浮点数</td><td>1.0</td></tr><tr><td>double</td><td>8字节双精度浮点数</td><td>1.0</td></tr><tr><td>deicimal</td><td>任意精度的带符号小数</td><td>1.0</td></tr><tr><td>String</td><td>字符串，变长</td><td>“a”,’b’</td></tr><tr><td>varchar</td><td>变长字符串</td><td>“a”,’b’</td></tr><tr><td>char</td><td>固定长度字符串</td><td>“a”,’b’</td></tr><tr><td>binary</td><td>字节数组</td><td>无法表示</td></tr><tr><td>timestamp</td><td>时间戳，纳秒精度</td><td>122327493795</td></tr><tr><td>date</td><td>日期</td><td>‘2018-04-07’</td></tr></tbody></table><p>Hive 支持关系型数据中大多数基本数据类型,和其他的SQL语言一样，这些都是保留字。需要注意的是所有的这些数据类型都是对Java中接口的实现，因此这些类型的具体行为细节和Java中对应的类型是完全一致的。例如，string类型实现的是Java中的String，float实现的是Java中的float，等等。</p><h2 id=32-复杂类型><strong>3.2 复杂类型</strong><a hidden class=anchor aria-hidden=true href=#32-复杂类型>#</a></h2><table><thead><tr><th>类型</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>array</td><td>有序的的同类型的集合</td><td>array(1,2)</td></tr><tr><td>map</td><td>key-value,key必须为原始类型，value可以任意类型</td><td>map(‘a’,1,’b’,2)</td></tr><tr><td>struct</td><td>字段集合,类型可以不同</td><td>struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td></tr></tbody></table><h1 id=4-存储格式><strong>4. 存储格式</strong><a hidden class=anchor aria-hidden=true href=#4-存储格式>#</a></h1><p><strong>1. textfile</strong></p><p>textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。</p><p><strong>2. SequenceFile</strong></p><p>SequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。</p><p>SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。</p><p><strong>3. RCFile</strong></p><p>一种行列存储相结合的存储方式。</p><p><strong>4. ORCFile</strong></p><p>数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。</p><p><strong>5. Parquet</strong></p><p>Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。</p><blockquote><p>来自: <a href=https://www.cnblogs.com/qingyunzong/p/8733924.html target=_blank rel=noopener>https://www.cnblogs.com/qingyunzong/p/8733924.html</a></p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://xiaokunji.com/zh/tags/Hive.html>Hive</a></li><li><a href=https://xiaokunji.com/zh/tags/Hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></li></ul></footer></article></main><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><footer class=footer><span>Copyright
&copy;
-2023
<a href=https://xiaokunji.com/zh/ style=color:#939393>米二</a>
All Rights Reserved</span>
<span id=busuanzi_container><span class="fa fa-user">用户数:</span><span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye">访问数:</span><span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>