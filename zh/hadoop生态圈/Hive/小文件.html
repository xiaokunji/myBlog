<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>小文件 | 米二</title><meta name=keywords content="每个Map最大输入大小(这个值决定了合并后文件的数量),一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并),一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并),执行Map前进行小文件合并,===设置map输出和reduce输出进行合并的相关参数：,设置map端输出进行合并，默认为true,设置reduce端输出进行合并，默认为false,设置合并文件的大小,当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。, 设置每个reducer处理的大小为5个G, 使用distribute by rand()将数据随机分配给reduce,避免出现有的文件特别大,有的文件特别小,用来控制归档是否可用,通知Hive在创建归档时是否可以设置父目录,控制需要归档文件的大小,使用以下命令进行归档,对已归档的分区恢复为原文件,::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive"><meta name=description content="     "><meta name=author content="xkj"><link rel=canonical href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%B0%8F%E6%96%87%E4%BB%B6.html><link crossorigin=anonymous href=/assets/css/stylesheet.b3faf608c544858ba700943ffe182cb647f38432d29a07d73234965beacb26f6.css integrity="sha256-s/r2CMVEhYunAJQ//hgstkfzhDLSmgfXMjSWW+rLJvY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=16x16 href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=32x32 href=https://xiaokunji.com/img/Q.svg><link rel=apple-touch-icon href=https://xiaokunji.com/Q.svg><link rel=mask-icon href=https://xiaokunji.com/Q.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%B0%8F%E6%96%87%E4%BB%B6.html><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="小文件"><meta property="og:description" content="     "><meta property="og:type" content="article"><meta property="og:url" content="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%B0%8F%E6%96%87%E4%BB%B6.html"><meta property="article:section" content="hadoop生态圈"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-04T05:50:23+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="小文件"><meta name=twitter:description content="     "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"小文件","item":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%B0%8F%E6%96%87%E4%BB%B6.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"小文件","name":"小文件","description":"     ","keywords":["每个Map最大输入大小(这个值决定了合并后文件的数量)","一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)","一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)","执行Map前进行小文件合并","===设置map输出和reduce输出进行合并的相关参数：","设置map端输出进行合并，默认为true","设置reduce端输出进行合并，默认为false","设置合并文件的大小","当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。"," 设置每个reducer处理的大小为5个G"," 使用distribute by rand()将数据随机分配给reduce, 避免出现有的文件特别大, 有的文件特别小","用来控制归档是否可用","通知Hive在创建归档时是否可以设置父目录","控制需要归档文件的大小","使用以下命令进行归档","对已归档的分区恢复为原文件","::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive"],"articleBody":"哪里会产生小文件 ?\n源数据本身有很多小文件 动态分区会产生大量小文件 reduce个数越多, 小文件越多 按分区插入数据的时候会产生大量的小文件, 文件个数 = maptask个数 * 分区数 小文件太多造成的影响 ?\n从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。 HDFS存储太多小文件, 会导致namenode元数据特别大, 占用太多内存, 制约了集群的扩展 小文件解决方法\n方法一: 通过调整参数进行合并\n#每个Map最大输入大小(这个值决定了合并后文件的数量) set mapred.max.split.size=256000000; #一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并) set mapred.min.split.size.per.node=100000000; #一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并) set mapred.min.split.size.per.rack=100000000; #执行Map前进行小文件合并 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; #===设置map输出和reduce输出进行合并的相关参数： #设置map端输出进行合并，默认为true set hive.merge.mapfiles = true #设置reduce端输出进行合并，默认为false set hive.merge.mapredfiles = true #设置合并文件的大小 set hive.merge.size.per.task = 256*1000*1000 #当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。 set hive.merge.smallfiles.avgsize=16000000 **方法二: **\n针对按分区插入数据的时候产生大量的小文件的问题, 可以使用DISTRIBUTE BY rand() 将数据随机分配给Reduce，这样可以使得每个Reduce处理的数据大体一致.\n# 设置每个reducer处理的大小为5个G set hive.exec.reducers.bytes.per.reducer=5120000000; # 使用distribute by rand()将数据随机分配给reduce, 避免出现有的文件特别大, 有的文件特别小 insert overwrite table test partition(dt) select * from iteblog_tmp DISTRIBUTE BY rand(); 方法三: 使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件\n压缩和输出小文件合并是无法并存的，两者都有时，输出小文件合并会失效。除非，表的存储方式是SequenceFile\nhttps://blog.csdn.net/djd1234567/article/details/51581201 方法四: 使用hadoop的archive归档\n#用来控制归档是否可用 set hive.archive.enabled=true; #通知Hive在创建归档时是否可以设置父目录 set hive.archive.har.parentdir.settable=true; #控制需要归档文件的大小 set har.partfile.size=1099511627776; #使用以下命令进行归档 ALTER TABLE srcpart ARCHIVE PARTITION(ds='2008-04-08', hr='12'); #对已归档的分区恢复为原文件 ALTER TABLE srcpart UNARCHIVE PARTITION(ds='2008-04-08', hr='12'); #::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive https://blog.csdn.net/weixin_42582592/article/details/85084575 捷顺是采用手动的方法,文件个数和大小达标后把所有数据写到一起(通过写hive命令),当然也通过配置配合使用(本质是通过写一次操作,使配置生效)\n","wordCount":"1019","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-09-04T05:50:23.633349671Z","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive/%E5%B0%8F%E6%96%87%E4%BB%B6.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaokunji.com/zh/ accesskey=h title="米二 (Alt + H)"><img src=https://xiaokunji.com/img/Q.svg alt aria-label=logo height=35>米二</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaokunji.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://xiaokunji.com/zh/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://xiaokunji.com/zh/search title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://xiaokunji.com/zh/post.html title=📚文章><span>📚文章</span></a></li><li><a href=https://xiaokunji.com/zh/archives.html title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://xiaokunji.com/zh/tags.html title=🔖标签><span>🔖标签</span></a></li><li><a href=https://xiaokunji.com/zh/categories.html title=📖分类><span>📖分类</span></a></li><li><a href=https://xiaokunji.com/zh/links.html title=🤝友链><span>🤝友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><nav aria-label=breadcrumb><ul><a href=https://xiaokunji.com/zh/>🏠</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>hadoop生态圈</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/Hive.html>Hive</a> <span>></span></ul></nav><h1 class=post-title>小文件</h1><div class=post-description></div><div class=post-meta>创建:&nbsp;<span title='2023-08-22 00:00:00 +0000 UTC'>2023-08-22</span>&nbsp;·&nbsp;更新:&nbsp;2023-09-04&nbsp;·&nbsp;xkj
&nbsp;|&nbsp;分类: &nbsp;<ul class=post-categories-meta><a href=https://xiaokunji.com/zh/categories/Hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></ul><span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv>1</span></span></div></header><div class=post-content><p><strong>哪里会产生小文件 ?</strong></p><ul><li>源数据本身有很多小文件</li><li>动态分区会产生大量小文件</li><li>reduce个数越多, 小文件越多</li><li>按分区插入数据的时候会产生大量的小文件, 文件个数 = maptask个数 * 分区数</li></ul><p><strong>小文件太多造成的影响 ?</strong></p><ul><li>从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。</li><li>HDFS存储太多小文件, 会导致namenode元数据特别大, 占用太多内存, 制约了集群的扩展</li></ul><p><strong>小文件解决方法</strong></p><p><strong>方法一: 通过调整参数进行合并</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#每个Map最大输入大小(这个值决定了合并后文件的数量)</span>
</span></span><span style=display:flex><span>set mapred.max.split.size<span style=color:#ff7b72;font-weight:700>=</span>256000000;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span>
</span></span><span style=display:flex><span>set mapred.min.split.size.per.node<span style=color:#ff7b72;font-weight:700>=</span>100000000;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)</span>
</span></span><span style=display:flex><span>set mapred.min.split.size.per.rack<span style=color:#ff7b72;font-weight:700>=</span>100000000;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#执行Map前进行小文件合并</span>
</span></span><span style=display:flex><span>set hive.input.format<span style=color:#ff7b72;font-weight:700>=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#===设置map输出和reduce输出进行合并的相关参数：</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#设置map端输出进行合并，默认为true</span>
</span></span><span style=display:flex><span>set hive.merge.mapfiles <span style=color:#ff7b72;font-weight:700>=</span> true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#设置reduce端输出进行合并，默认为false</span>
</span></span><span style=display:flex><span>set hive.merge.mapredfiles <span style=color:#ff7b72;font-weight:700>=</span> true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#设置合并文件的大小</span>
</span></span><span style=display:flex><span>set hive.merge.size.per.task <span style=color:#ff7b72;font-weight:700>=</span> 256*1000*1000
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span>
</span></span><span style=display:flex><span>set hive.merge.smallfiles.avgsize<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>16000000</span>
</span></span></code></pre></div><p>**方法二: **</p><p><strong>针对按分区插入数据的时候产生大量的小文件的问题, 可以使用DISTRIBUTE BY rand() 将数据随机分配给Reduce，这样可以使得每个Reduce处理的数据大体一致.</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#8b949e;font-style:italic># 设置每个reducer处理的大小为5个G</span>
</span></span><span style=display:flex><span>set hive.exec.reducers.bytes.per.reducer<span style=color:#ff7b72;font-weight:700>=</span>5120000000;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># 使用distribute by rand()将数据随机分配给reduce, 避免出现有的文件特别大, 有的文件特别小</span>
</span></span><span style=display:flex><span>insert overwrite table test partition<span style=color:#ff7b72;font-weight:700>(</span>dt<span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>select</span> * from iteblog_tmp
</span></span><span style=display:flex><span>DISTRIBUTE BY rand<span style=color:#ff7b72;font-weight:700>()</span>;
</span></span></code></pre></div><p><strong>方法三: 使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件</strong></p><blockquote><p>压缩和输出小文件合并是无法并存的，两者都有时，输出小文件合并会失效。除非，表的存储方式是SequenceFile</p><p><a href=https://blog.csdn.net/djd1234567/article/details/51581201 target=_blank rel=noopener>https://blog.csdn.net/djd1234567/article/details/51581201</a></p></blockquote><p><strong>方法四: 使用hadoop的archive归档</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#用来控制归档是否可用</span>
</span></span><span style=display:flex><span>set hive.archive.enabled<span style=color:#ff7b72;font-weight:700>=</span>true;
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#通知Hive在创建归档时是否可以设置父目录</span>
</span></span><span style=display:flex><span>set hive.archive.har.parentdir.settable<span style=color:#ff7b72;font-weight:700>=</span>true;
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#控制需要归档文件的大小</span>
</span></span><span style=display:flex><span>set har.partfile.size<span style=color:#ff7b72;font-weight:700>=</span>1099511627776;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#使用以下命令进行归档</span>
</span></span><span style=display:flex><span>ALTER TABLE srcpart ARCHIVE PARTITION<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#79c0ff>ds</span><span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#39;2008-04-08&#39;</span>, <span style=color:#79c0ff>hr</span><span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#39;12&#39;</span><span style=color:#ff7b72;font-weight:700>)</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#对已归档的分区恢复为原文件</span>
</span></span><span style=display:flex><span>ALTER TABLE srcpart UNARCHIVE PARTITION<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#79c0ff>ds</span><span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#39;2008-04-08&#39;</span>, <span style=color:#79c0ff>hr</span><span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>&#39;12&#39;</span><span style=color:#ff7b72;font-weight:700>)</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>#::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive</span>
</span></span></code></pre></div><blockquote><p><a href=https://blog.csdn.net/weixin_42582592/article/details/85084575 target=_blank rel=noopener>https://blog.csdn.net/weixin_42582592/article/details/85084575</a></p></blockquote><blockquote><p>捷顺是采用手动的方法,文件个数和大小达标后把所有数据写到一起(通过写hive命令),当然也通过配置配合使用(本质是通过写一次操作,使配置生效)</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://xiaokunji.com/zh/tags/Hive.html>Hive</a></li><li><a href=https://xiaokunji.com/zh/tags/Hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></li></ul></footer></article></main><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><footer class=footer><span>Copyright
&copy;
-2023
<a href=https://xiaokunji.com/zh/ style=color:#939393>米二</a>
All Rights Reserved</span>
<span id=busuanzi_container><span class="fa fa-user">用户数:</span><span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye">访问数:</span><span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>