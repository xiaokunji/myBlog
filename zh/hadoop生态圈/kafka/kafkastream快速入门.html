<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>KafkaStream快速入门 | 米二</title><meta name=keywords content=" 1.1.  目标, 1.2.  我们想做什么, 1.3.  启动Kafka 集群, Download and install Confluent Platform 3.0.0 from ZIP archive  , *** IMPORTANT STEP ****  , The subsequent paths and commands used throughout this quickstart assume that  , your are in the following working directory:  , Note: If you want to uninstall the Confluent Platform at the end of this quickstart, , run the following commands.  ,  ,   $ rm -rf confluent-3.0.0/  ,   $ rm -rf /var/lib/kafka           Data files of Kafka  ,   $ rm -rf /var/lib/kafka-streams   Data files of Kafka Streams  ,   $ rm -rf /var/lib/zookeeper       Data files of ZooKeeper  , Start ZooKeeper.  Run this command in its own terminal.  , Start Kafka.  Run this command in its own terminal  , 1.4.  准备输入数据, 1.5.  在KafkaStreams中处理输入数据, Run the WordCount demo application.  There won't be any STDOUT output.  , You can safely ignore any WARN log messages.  , 1.6.  检查输出结果, Why not this,you may ask?  "><meta name=description content="     "><meta name=author content="xkj"><link rel=canonical href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafkastream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html><link crossorigin=anonymous href=/assets/css/stylesheet.b3faf608c544858ba700943ffe182cb647f38432d29a07d73234965beacb26f6.css integrity="sha256-s/r2CMVEhYunAJQ//hgstkfzhDLSmgfXMjSWW+rLJvY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=16x16 href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=32x32 href=https://xiaokunji.com/img/Q.svg><link rel=apple-touch-icon href=https://xiaokunji.com/Q.svg><link rel=mask-icon href=https://xiaokunji.com/Q.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafkastream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="KafkaStream快速入门"><meta property="og:description" content="     "><meta property="og:type" content="article"><meta property="og:url" content="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafkastream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html"><meta property="article:section" content="hadoop生态圈"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-30T16:27:33+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="KafkaStream快速入门"><meta name=twitter:description content="     "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"KafkaStream快速入门","item":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafkastream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"KafkaStream快速入门","name":"KafkaStream快速入门","description":"     ","keywords":[" 1.1.  目标"," 1.2.  我们想做什么"," 1.3.  启动Kafka 集群"," Download and install Confluent Platform 3.0.0 from ZIP archive  "," *** IMPORTANT STEP ****  "," The subsequent paths and commands used throughout this quickstart assume that  "," your are in the following working directory:  "," Note: If you want to uninstall the Confluent Platform at the end of this quickstart,  "," run the following commands.  ","  ","   $ rm -rf confluent-3.0.0/  ","   $ rm -rf /var/lib/kafka           Data files of Kafka  ","   $ rm -rf /var/lib/kafka-streams   Data files of Kafka Streams  ","   $ rm -rf /var/lib/zookeeper       Data files of ZooKeeper  "," Start ZooKeeper.  Run this command in its own terminal.  "," Start Kafka.  Run this command in its own terminal  "," 1.4.  准备输入数据"," 1.5.  在KafkaStreams中处理输入数据"," Run the WordCount demo application.  There won't be any STDOUT output.  "," You can safely ignore any WARN log messages.  "," 1.6.  检查输出结果"," Why not this, you may ask?  "],"articleBody":"[toc]\n1.1. 目标 本快速入门指南的目标是提供与KafkaStreams的第一个应用程序示例。我们将演示在你的第一个示例程序中，如果使用Kafka Streams库和演示一个简单的端到端的数据流。 值得注意的是，这种快速入门只涵盖了KafkaStreams的表面，这篇文档的剩余部分将会提供更多的细节，我们将在快速入门指南中为你指明方向。\n1.2. 我们想做什么 在这个快速入门中，我们将运行包含Apachekafka的一个wordcount演示应用程序。下面代码的关键在于使用Java8的lambda表达式，易于阅读。(摘自WordCountLambdaExample):\n[java] view plain copy //序列化/反序列化Sting和Long类型 final Serde stringSerde = Serdes.String(); final Serde longSerde = Serdes.Long(); //通过指定输入topic “mystream”来构造KStream实例，\n//输入数据就以文本的形式保存在topic “mystream” 中。\n//(在本示例中，我们忽略所有消息的key.)\nKStream textLines = builder.stream(stringSerde, stringSerde, \"mystream\"); KStream wordCounts = textLines\n//以空格为分隔符，将每行文本数据拆分成多个单词。\n//这些文本行就是从输入topic中读到的每行消息的Value。\n//我们使用flatMapValues方法来处理每个消息Value，而不是更通用的flatMap .flatMapValues(value -\u003e Arrays.asList(value.toLowerCase().split(\"\\W+\")))\n//我们随后将调用countByKey来计算每个单词出现的次数\n//所以我们将每个单词作为map的key。\n.map((key, value) -\u003e new KeyValue\u003c\u003e(value, value)) //通过key来统计每个单词的次数\n//\n//这会将流类型从KStream转为KTable (word-count).\n//因此我们必须提供String和long的序列化反序列化方法。\n//\n.countByKey(stringSerde, \"Counts\") //转化KTable到KStream\n.toStream();\n//将KStream写入到输出topic中。\nwordCounts.to(stringSerde, longSerde, \"streams-wordcount-output\"); 在上面的代码执行过程中，我们将执行如下步骤：\n1、 启动一台kafka集群 2、 使用Kafkaconsole producer命令行生产者客户端往Kafka Topic中写入示例输入数据 3、 在Java应用程序中使用kafkaStream库来处理输入数据。这里，我们使用了一个包含kafka的WordCount示例程序。 4、 使用Kafkaconsole consumer命令行消费者客户端检查应用程序的输出。 5、 停止Kafka集群 1.3. 启动Kafka 集群 在本章节中，我们会在一台机器上安装并启动Kafka集群。该集群有一个单节点Kafka(只有一个Broker)外加一个单节点Zookeeper构成。在wordcount演示程序中，这种集群依赖是必须的。我们假定kafka broker运行地址为localhost:9092, Zookeeper本地地址为localhost:2181。 首先，安装Oracle JRE或JDK 1.7及以上版本 然后，下载和安装包含Kafka Streams的新版本Apache Kafka. 为此，我们使用Confluent Platform 3.0.0版本。 (下面操作比较简单，所以不翻译了。)\n[plain] view plain copy # Download and install Confluent Platform 3.0.0 from ZIP archive $ wget http://packages.confluent.io/archive/3.0/confluent-3.0.0-2.11.zip $ unzip confluent-3.0.0-2.11.zip # *** IMPORTANT STEP **** # The subsequent paths and commands used throughout this quickstart assume that # your are in the following working directory: $ cd confluent-3.0.0/ # Note: If you want to uninstall the Confluent Platform at the end of this quickstart, # run the following commands. # # $ rm -rf confluent-3.0.0/ # $ rm -rf /var/lib/kafka # Data files of Kafka # $ rm -rf /var/lib/kafka-streams # Data files of Kafka Streams # $ rm -rf /var/lib/zookeeper # Data files of ZooKeeper 提示：可以通过Installationvia ZIP and TAR archives 和ConfluentPlatform Quickstart 获取更进一步信息。 我们首先启动ZooKeeper实例。该实例将监听本地2181端口。由于这是一个长期运行的服务，你应该在自己的终端中运行。\n[plain] view plain copy # Start ZooKeeper. Run this command in its own terminal. $ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties 接下来，我们启动Kakfa的Broker，这将监听本地9092端口，然后连接到我们刚刚启动的Zookeeper实例。这也是一个长期运行的服务，也应该在终端中运行它。\n[plain] view plain copy # Start Kafka. Run this command in its own terminal $ ./bin/kafka-server-start ./etc/kafka/server.properties 现在，我们的单节点kafka集群已经完全运转起来了，我们就可以着手准备输入数据，运行我们的第一个kafka Streams示例程序。\n1.4. 准备输入数据 提示：在本章节中，我们将使用内置的命令行工具来输入kakfa数据。在实际使用中，你应该通过其他方式将数据写入Kafka中，比如通过你自己应用程序中的Kafka客户端。 现在，我们将一些输入数据发送到Kafka的topic中，然后由Kafka Streams的应用程序做后续处理。 首先，我们要创建名称为mystream的topic：\n[plain] view plain copy $ ./bin/kafka-topics.sh --create --zookeeper slave01:2181,slave02:2181,slave03:2181 --replication-factor 1 --partitions 1 --topic mystream 下一步，我们生成一些输入数据并保存在本地文件/tmp/file-input.txt中。\n[plain] view plain copy $ echo -e \"all streams lead to kafka hello kafka streams join kafka summit\" \u003e /tmp/file-input.txt 生成的文件将包含如下内容：\n[plain] view plain copy all streams lead to kafka hello kafka streams join kafka summit 最后，我们发送这些数据到input topic\n[plain] view plain copy $ cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list master:9092,slave01:9092,slave02:9092,slave03:9092 --topic mystream Kafka console-producer从stdin中读取数据，并将每一行作为单独的消息发送到kafka的输入流中。该消息的key是null，消息是每行内容，使用字符串编码。\n注意: 你可能想知道这样一步步的快速启动和真实流处理系统的差异，在大型的实时的流处理系统中，数据总是在移动的，快速入门的目的仅仅是做功能证明。简单来说，一个端到端的数据管道建立在Kafka和Kafka Streams的各个方面。出于说教的原因，我们故意将快速入门清楚地拆分成一系列分开连续的步骤。\n但在实践中，这些步骤通常会看起来有些不同并且会有并发的存在。比如输入数据可能不会来源于本地文件，而是直接从分布式系统中发送的，并且数据将被连续的写入Kafka。类似的，流处理应用程序可能在第一行数据发送之前就已经启动并运行。\n1.5. 在KafkaStreams中处理输入数据 现在，我们已经生成了一些输入数据，我们可以运行我们的第一个基于Kafka Streams的java应用程序。\n我们将运行WordCount演示应用程序，它使用了ApacheKafka。它实现了WordCount算法，从输入文本来计算直方图。然而和其他你之前见过的操作被绑定在数据上的WordCount实例程序不同的是，这个示例程序是数据无界，无限流动的。和有界算法的变体类似，他是一个有状态的算法，跟踪并更新word的计数器。然后因为它必须接受无界的输入数据，它会周期性的输出其当前状态和计算结果，同时继续处理更多的数据，因为它不知道是否已经处理了所有的数据。这就是他和Hadoop 的Mapreduce算法之间的典型差异。一旦我们了解这种差异，检查了实际的输出数据之后，会更容易接受它。 由于wordCount示例程序与kafka打包在一起，已经在Kafka的Broker中集成，这就意味着我们不需要做额外的事情就可以运行它，无需编译任何Java源代码。\n[plain] view plain copy # Run the WordCount demo application. There won't be any STDOUT output. # You can safely ignore any WARN log messages. $ ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo 注意，这里没有魔术式的部署，实际上，使用kafkaStreams库中的任何应用程序，就像启动任何普通的Java应用程序，该脚本kafka-run-class也只是一个简单的java -cp命令的包装。\n该WordCount示例程序将从输入topic中读取数据，然后计算wordCount，将计算结果不断进行输出。演示将运行几秒钟，然后和其他典型流处理应用程序不同的是，它将会自动终止。\n1.6. 检查输出结果 在本章节中，我们将使用内置的命令行工具从kafka中手工读取数据。在实际使用中，你可以通过其他方式，通过Kakfa客户端从Kafka中读取数据。比如，如果你可以在自己的应用程序中使用Kafka客户端将数据从Kakfa中迁移到其它数据系统。 现在，我们可以从kafka输出topic中读取数据并检查wordcount实例运行结果。\n[plain] view plain copy ./bin/kafka-console-consumer --zookeeper slave01:2181 \\ --topic streams-wordcount \\ --from-beginning \\ --formatter kafka.tools.DefaultMessageFormatter \\ --property print.key=true\\ --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \\ --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer kafka-console-consumer.sh --zookeeper slave01:2181 --topic streams-wordcount --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer Wordcount的数据将会被打印在如下的控制台中：\n[plain] view plain copy all 1 streams 1 lead 1 to 1 kafka 1 hello 1 kafka 2 streams 2 join 1 kafka 3 summit 1 这里，第一列是Kafka消息的key的字符串格式，第二列是消息的值，long类型。你可以通过Ctrl+c命令来终止控制台输出。 但是等一下，输出看起来是不是很奇怪？为什么会出现重复的条目？比如streams出现了两次：\n[plain] view plain copy # Why not this, you may ask? all 1 lead 1 to 1 hello 1 streams 2 join 1 kafka 3 summit 1 对于上面的输出的解释是，wordCount应用程序的输出实际上是持续更新的流，其中每行记录是一个单一的word(即Message Key，比如Kafka)的计数。对于同一个Key的多个记录，每个记录之后是前一个的更新。\n当第二个文本航的hello kafkastreams被处理的时候，我们观察到，相对第一次，已经存在的条目KTable被更新了(Kafak和Streams这两个单词). 修改后的记录被在此发送到了KStream。 这就解释了上述KStream第二列中显示的信息，为什么输出的topic上显示的内容，因为它是包含了变化的完整内容\n[plain] view plain copy all 1 streams 1 lead 1 to 1 kafka 1 hello 1 kafka 2 streams 2 join 1 kafka 3 summit 1 更多参看官网:http://kafka.apache.org/documentation/streams/\n","wordCount":"3732","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-08-30T16:27:33.771734927Z","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafkastream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaokunji.com/zh/ accesskey=h title="米二 (Alt + H)"><img src=https://xiaokunji.com/img/Q.svg alt aria-label=logo height=35>米二</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaokunji.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://xiaokunji.com/zh/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://xiaokunji.com/zh/search.html title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://xiaokunji.com/zh/post.html title=📚文章><span>📚文章</span></a></li><li><a href=https://xiaokunji.com/zh/archives.html title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://xiaokunji.com/zh/tags.html title=🔖标签><span>🔖标签</span></a></li><li><a href=https://xiaokunji.com/zh/categories.html title=📖分类><span>📖分类</span></a></li><li><a href=https://xiaokunji.com/zh/links.html title=🤝友链><span>🤝友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><nav aria-label=breadcrumb><ul><a href=https://xiaokunji.com/zh/>🏠</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>hadoop生态圈</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka.html>Kafka</a> <span>></span></ul></nav><h1 class=post-title>KafkaStream快速入门</h1><div class=post-description></div><div class=post-meta>创建:&nbsp;<span title='2023-08-22 00:00:00 +0000 UTC'>2023-08-22</span>&nbsp;·&nbsp;更新:&nbsp;2023-08-30&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;xkj
&nbsp;|&nbsp;分类: &nbsp;<ul class=post-categories-meta><a href=https://xiaokunji.com/zh/categories/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></ul><span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#11--%e7%9b%ae%e6%a0%87 aria-label="1.1.  目标">1.1. 目标</a></li><li><a href=#12--%e6%88%91%e4%bb%ac%e6%83%b3%e5%81%9a%e4%bb%80%e4%b9%88 aria-label="1.2.  我们想做什么">1.2. 我们想做什么</a></li><li><a href=#13--%e5%90%af%e5%8a%a8kafka-%e9%9b%86%e7%be%a4 aria-label="1.3.  启动Kafka 集群">1.3. 启动Kafka 集群</a></li><li><a href=#14--%e5%87%86%e5%a4%87%e8%be%93%e5%85%a5%e6%95%b0%e6%8d%ae aria-label="1.4.  准备输入数据">1.4. 准备输入数据</a></li><li><a href=#15--%e5%9c%a8kafkastreams%e4%b8%ad%e5%a4%84%e7%90%86%e8%be%93%e5%85%a5%e6%95%b0%e6%8d%ae aria-label="1.5.  在KafkaStreams中处理输入数据">1.5. 在KafkaStreams中处理输入数据</a></li><li><a href=#16--%e6%a3%80%e6%9f%a5%e8%be%93%e5%87%ba%e7%bb%93%e6%9e%9c aria-label="1.6.  检查输出结果">1.6. 检查输出结果</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>[toc]</p><h2 id=11--目标>1.1. 目标<a hidden class=anchor aria-hidden=true href=#11--目标>#</a></h2><p>本快速入门指南的目标是提供与KafkaStreams的第一个应用程序示例。我们将演示在你的第一个示例程序中，如果使用Kafka Streams库和演示一个简单的端到端的数据流。
值得注意的是，这种快速入门只涵盖了KafkaStreams的表面，这篇文档的剩余部分将会提供更多的细节，我们将在快速入门指南中为你指明方向。</p><h2 id=12--我们想做什么>1.2. 我们想做什么<a hidden class=anchor aria-hidden=true href=#12--我们想做什么>#</a></h2><p>在这个快速入门中，我们将运行包含Apachekafka的一个wordcount演示应用程序。下面代码的关键在于使用Java8的lambda表达式，易于阅读。(摘自WordCountLambdaExample):</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[java] view plain copy
</span></span><span style=display:flex><span>//序列化/反序列化Sting和Long类型  
</span></span><span style=display:flex><span>final Serde&lt;String&gt; stringSerde = Serdes.String();  
</span></span><span style=display:flex><span>final Serde&lt;Long&gt; longSerde = Serdes.Long();  
</span></span></code></pre></div><p>//通过指定输入topic “mystream”来构造KStream实例，<br>//输入数据就以文本的形式保存在topic “mystream” 中。<br>//(在本示例中，我们忽略所有消息的key.)<br><code>KStream&lt;String, String> textLines = builder.stream(stringSerde, stringSerde, "mystream");</code></p><p><code>KStream&lt;String, Long> wordCounts = textLines</code><br>//以空格为分隔符，将每行文本数据拆分成多个单词。<br>//这些文本行就是从输入topic中读到的每行消息的Value。<br>//我们使用flatMapValues方法来处理每个消息Value，而不是更通用的<code>flatMap .flatMapValues(value -> Arrays.asList(value.toLowerCase().split("\W+")))</code><br>//我们随后将调用countByKey来计算每个单词出现的次数<br>//所以我们将每个单词作为map的key。<br><code>.map((key, value) -> new KeyValue&lt;>(value, value))</code>
//通过key来统计每个单词的次数<br>//<br>//这会将流类型从KStream&lt;String,String>转为KTable&lt;String,Long> (word-count).<br>//因此我们必须提供String和long的序列化反序列化方法。<br>//<br><code>.countByKey(stringSerde, "Counts")</code>
//转化KTable&lt;String,Long>到KStream&lt;String,Long><br>.toStream();</p><p>//将KStream&lt;String,Long>写入到输出topic中。<br><code>wordCounts.to(stringSerde, longSerde, "streams-wordcount-output");</code></p><p>在上面的代码执行过程中，我们将执行如下步骤：</p><ul><li>1、 启动一台kafka集群</li><li>2、 使用Kafkaconsole producer命令行生产者客户端往Kafka Topic中写入示例输入数据</li><li>3、 在Java应用程序中使用kafkaStream库来处理输入数据。这里，我们使用了一个包含kafka的WordCount示例程序。</li><li>4、 使用Kafkaconsole consumer命令行消费者客户端检查应用程序的输出。</li><li>5、 停止Kafka集群</li></ul><h2 id=13--启动kafka-集群>1.3. 启动Kafka 集群<a hidden class=anchor aria-hidden=true href=#13--启动kafka-集群>#</a></h2><p>在本章节中，我们会在一台机器上安装并启动Kafka集群。该集群有一个单节点Kafka(只有一个Broker)外加一个单节点Zookeeper构成。在wordcount演示程序中，这种集群依赖是必须的。我们假定kafka broker运行地址为localhost:9092, Zookeeper本地地址为localhost:2181。
首先，安装Oracle JRE或JDK 1.7及以上版本
然后，下载和安装包含Kafka Streams的新版本Apache Kafka. 为此，我们使用Confluent Platform 3.0.0版本。
(下面操作比较简单，所以不翻译了。)</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span># Download and install Confluent Platform 3.0.0 from ZIP archive  
</span></span><span style=display:flex><span>$ wget http://packages.confluent.io/archive/3.0/confluent-3.0.0-2.11.zip  
</span></span><span style=display:flex><span>$ unzip confluent-3.0.0-2.11.zip  
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span># *** IMPORTANT STEP ****  
</span></span><span style=display:flex><span># The subsequent paths and commands used throughout this quickstart assume that  
</span></span><span style=display:flex><span># your are in the following working directory:  
</span></span><span style=display:flex><span>$ cd confluent-3.0.0/  
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span># Note: If you want to uninstall the Confluent Platform at the end of this quickstart,  
</span></span><span style=display:flex><span># run the following commands.  
</span></span><span style=display:flex><span>#  
</span></span><span style=display:flex><span>#   $ rm -rf confluent-3.0.0/  
</span></span><span style=display:flex><span>#   $ rm -rf /var/lib/kafka          # Data files of Kafka  
</span></span><span style=display:flex><span>#   $ rm -rf /var/lib/kafka-streams  # Data files of Kafka Streams  
</span></span><span style=display:flex><span>#   $ rm -rf /var/lib/zookeeper      # Data files of ZooKeeper  
</span></span></code></pre></div><p>提示：可以通过Installationvia ZIP and TAR archives 和ConfluentPlatform Quickstart 获取更进一步信息。
我们首先启动ZooKeeper实例。该实例将监听本地2181端口。由于这是一个长期运行的服务，你应该在自己的终端中运行。</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span># Start ZooKeeper.  Run this command in its own terminal.  
</span></span><span style=display:flex><span>$ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties  
</span></span></code></pre></div><p>接下来，我们启动Kakfa的Broker，这将监听本地9092端口，然后连接到我们刚刚启动的Zookeeper实例。这也是一个长期运行的服务，也应该在终端中运行它。</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span># Start Kafka.  Run this command in its own terminal  
</span></span><span style=display:flex><span>$ ./bin/kafka-server-start ./etc/kafka/server.properties  
</span></span></code></pre></div><p>现在，我们的单节点kafka集群已经完全运转起来了，我们就可以着手准备输入数据，运行我们的第一个kafka Streams示例程序。</p><h2 id=14--准备输入数据>1.4. 准备输入数据<a hidden class=anchor aria-hidden=true href=#14--准备输入数据>#</a></h2><p>提示：在本章节中，我们将使用内置的命令行工具来输入kakfa数据。在实际使用中，你应该通过其他方式将数据写入Kafka中，比如通过你自己应用程序中的Kafka客户端。
现在，我们将一些输入数据发送到Kafka的topic中，然后由Kafka Streams的应用程序做后续处理。
首先，我们要创建名称为mystream的topic：</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>$ ./bin/kafka-topics.sh --create --zookeeper slave01:2181,slave02:2181,slave03:2181 --replication-factor 1 --partitions 1 --topic mystream  
</span></span></code></pre></div><p>下一步，我们生成一些输入数据并保存在本地文件/tmp/file-input.txt中。</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>$ echo -e &#34;all streams lead to kafka
</span></span><span style=display:flex><span>hello kafka streams
</span></span><span style=display:flex><span>join kafka summit&#34; &gt; /tmp/file-input.txt
</span></span></code></pre></div><p>生成的文件将包含如下内容：</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>all streams lead to kafka  
</span></span><span style=display:flex><span>hello kafka streams  
</span></span><span style=display:flex><span>join kafka summit  
</span></span></code></pre></div><p>最后，我们发送这些数据到input topic</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>$ cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list master:9092,slave01:9092,slave02:9092,slave03:9092 --topic mystream  
</span></span></code></pre></div><p>Kafka console-producer从stdin中读取数据，并将每一行作为单独的消息发送到kafka的输入流中。该消息的key是null，消息是每行内容，使用字符串编码。<br>注意: 你可能想知道这样一步步的快速启动和真实流处理系统的差异，在大型的实时的流处理系统中，数据总是在移动的，快速入门的目的仅仅是做功能证明。简单来说，一个端到端的数据管道建立在Kafka和Kafka Streams的各个方面。出于说教的原因，我们故意将快速入门清楚地拆分成一系列分开连续的步骤。<br>但在实践中，这些步骤通常会看起来有些不同并且会有并发的存在。比如输入数据可能不会来源于本地文件，而是直接从分布式系统中发送的，并且数据将被连续的写入Kafka。类似的，流处理应用程序可能在第一行数据发送之前就已经启动并运行。</p><h2 id=15--在kafkastreams中处理输入数据>1.5. 在KafkaStreams中处理输入数据<a hidden class=anchor aria-hidden=true href=#15--在kafkastreams中处理输入数据>#</a></h2><p>现在，我们已经生成了一些输入数据，我们可以运行我们的第一个基于Kafka Streams的java应用程序。<br>我们将运行WordCount演示应用程序，它使用了ApacheKafka。它实现了WordCount算法，从输入文本来计算直方图。然而和其他你之前见过的操作被绑定在数据上的WordCount实例程序不同的是，这个示例程序是数据无界，无限流动的。和有界算法的变体类似，他是一个有状态的算法，跟踪并更新word的计数器。然后因为它必须接受无界的输入数据，它会周期性的输出其当前状态和计算结果，同时继续处理更多的数据，因为它不知道是否已经处理了所有的数据。这就是他和Hadoop 的Mapreduce算法之间的典型差异。一旦我们了解这种差异，检查了实际的输出数据之后，会更容易接受它。<br>由于wordCount示例程序与kafka打包在一起，已经在Kafka的Broker中集成，这就意味着我们不需要做额外的事情就可以运行它，无需编译任何Java源代码。</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span># Run the WordCount demo application.  There won&#39;t be any STDOUT output.  
</span></span><span style=display:flex><span># You can safely ignore any WARN log messages.  
</span></span><span style=display:flex><span>$ ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo  
</span></span></code></pre></div><p>注意，这里没有魔术式的部署，实际上，使用kafkaStreams库中的任何应用程序，就像启动任何普通的Java应用程序，该脚本kafka-run-class也只是一个简单的java -cp命令的包装。<br>该WordCount示例程序将从输入topic中读取数据，然后计算wordCount，将计算结果不断进行输出。演示将运行几秒钟，然后和其他典型流处理应用程序不同的是，它将会自动终止。</p><h2 id=16--检查输出结果>1.6. 检查输出结果<a hidden class=anchor aria-hidden=true href=#16--检查输出结果>#</a></h2><p>在本章节中，我们将使用内置的命令行工具从kafka中手工读取数据。在实际使用中，你可以通过其他方式，通过Kakfa客户端从Kafka中读取数据。比如，如果你可以在自己的应用程序中使用Kafka客户端将数据从Kakfa中迁移到其它数据系统。<br>现在，我们可以从kafka输出topic中读取数据并检查wordcount实例运行结果。</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>./bin/kafka-console-consumer --zookeeper slave01:2181 \  
</span></span><span style=display:flex><span>          --topic streams-wordcount \  
</span></span><span style=display:flex><span>          --from-beginning \  
</span></span><span style=display:flex><span>          --formatter kafka.tools.DefaultMessageFormatter \  
</span></span><span style=display:flex><span>          --property print.key=true\  
</span></span><span style=display:flex><span>          --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \  
</span></span><span style=display:flex><span>          --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer  
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> kafka-console-consumer.sh --zookeeper slave01:2181 --topic streams-wordcount --property print.key=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer   --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer  
</span></span></code></pre></div><p>Wordcount的数据将会被打印在如下的控制台中：</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>all     1  
</span></span><span style=display:flex><span>streams 1  
</span></span><span style=display:flex><span>lead    1  
</span></span><span style=display:flex><span>to      1  
</span></span><span style=display:flex><span>kafka   1  
</span></span><span style=display:flex><span>hello   1  
</span></span><span style=display:flex><span>kafka   2  
</span></span><span style=display:flex><span>streams 2  
</span></span><span style=display:flex><span>join    1  
</span></span><span style=display:flex><span>kafka   3  
</span></span><span style=display:flex><span>summit  1  
</span></span></code></pre></div><p>这里，第一列是Kafka消息的key的字符串格式，第二列是消息的值，long类型。你可以通过Ctrl+c命令来终止控制台输出。
但是等一下，输出看起来是不是很奇怪？为什么会出现重复的条目？比如streams出现了两次：</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span># Why not this, you may ask?  
</span></span><span style=display:flex><span>all     1  
</span></span><span style=display:flex><span>lead    1  
</span></span><span style=display:flex><span>to      1  
</span></span><span style=display:flex><span>hello   1  
</span></span><span style=display:flex><span>streams 2  
</span></span><span style=display:flex><span>join    1  
</span></span><span style=display:flex><span>kafka   3  
</span></span><span style=display:flex><span>summit  1  
</span></span></code></pre></div><p>对于上面的输出的解释是，wordCount应用程序的输出实际上是持续更新的流，其中每行记录是一个单一的word(即Message Key，比如Kafka)的计数。对于同一个Key的多个记录，每个记录之后是前一个的更新。</p><p>当第二个文本航的hello kafkastreams被处理的时候，我们观察到，相对第一次，已经存在的条目KTable被更新了(Kafak和Streams这两个单词). 修改后的记录被在此发送到了KStream。
这就解释了上述KStream第二列中显示的信息，为什么输出的topic上显示的内容，因为它是包含了变化的完整内容</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>[plain] view plain copy
</span></span><span style=display:flex><span>all     1  
</span></span><span style=display:flex><span>streams 1  
</span></span><span style=display:flex><span>lead    1  
</span></span><span style=display:flex><span>to      1  
</span></span><span style=display:flex><span>kafka   1  
</span></span><span style=display:flex><span>hello   1  
</span></span><span style=display:flex><span>kafka   2  
</span></span><span style=display:flex><span>streams 2  
</span></span><span style=display:flex><span>join    1  
</span></span><span style=display:flex><span>kafka   3  
</span></span><span style=display:flex><span>summit  1  
</span></span></code></pre></div><blockquote><p>更多参看官网:http://kafka.apache.org/documentation/streams/</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://xiaokunji.com/zh/tags/kafka.html>Kafka</a></li><li><a href=https://xiaokunji.com/zh/tags/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></li></ul></footer></article></main><footer class=footer><span>Copyright
&copy;
-2023
<a href=https://xiaokunji.com/zh/ style=color:#939393>米二</a>
All Rights Reserved</span>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<span id=busuanzi_container><span class="fa fa-user">用户数:</span><span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye">访问数:</span><span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>