<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>安装与配置 | 米二</title><meta name=keywords content=" **一、安装Spark集群**, **1. 解压安装包**, **2. 配置spark**, shuffled以及RDD的数据存放目录, worker端进程的工作目录, **3. 复制到其他节点**, **5. 启动验证**, **6. 打包运行**, **二,高可用(热备)**"><meta name=description content="     "><meta name=author content="xkj"><link rel=canonical href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html><link crossorigin=anonymous href=/assets/css/stylesheet.b3faf608c544858ba700943ffe182cb647f38432d29a07d73234965beacb26f6.css integrity="sha256-s/r2CMVEhYunAJQ//hgstkfzhDLSmgfXMjSWW+rLJvY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=16x16 href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=32x32 href=https://xiaokunji.com/img/Q.svg><link rel=apple-touch-icon href=https://xiaokunji.com/Q.svg><link rel=mask-icon href=https://xiaokunji.com/Q.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="安装与配置"><meta property="og:description" content="     "><meta property="og:type" content="article"><meta property="og:url" content="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html"><meta property="article:section" content="hadoop生态圈"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-30T16:26:26+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="安装与配置"><meta name=twitter:description content="     "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"安装与配置","item":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"安装与配置","name":"安装与配置","description":"     ","keywords":[" **一、安装Spark集群**"," **1. 解压安装包**"," **2. 配置spark**"," shuffled以及RDD的数据存放目录"," worker端进程的工作目录"," **3. 复制到其他节点**"," **5. 启动验证**"," **6. 打包运行**"," **二,高可用(热备)**"],"articleBody":"[toc]\n注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和stop-all.sh,而spark启动/关闭用到了这两个脚本,而hadoop中这么命令已经遗弃了\n源笔记:\n1.修改slave文件,配置从机\n如下:\nslave01\nslave02\nslave03\n2.修改spark.env.sh,如下:\nexport JAVA_HOME=/mysoftware/jdk1.8.0_101 export SPARK_MASTER_IP=master export SCALA_HOME=/mysoftware/scala-2.12.3 export HADOOP_HOME=/mysoftware/hadoop-2.7.3 export HADOOP_CONF_DIR=/mysoftware/hadoop-2.7.3/etc/hadoop/ 3.发送到从机即可,集群环境搭建完毕\n一、安装Spark集群 1. 解压安装包 tar -zxvf ~/jar/spark-1.6.3-bin-hadoop2.6.tgz -C /data\n2. 配置spark 涉及到的配置文件有以下几个：\n${SPARK_HOME}/conf/spark-env.sh\n${SPARK_HOME}/conf/slaves\n${SPARK_HOME}/conf/spark-defaults.conf\n这三个文件都是由原始的template文件复制过来的，比如cp spark-env.sh.template spark-env.sh\n配置文件1：spark-env.sh\nJAVA_HOME=/data/jdk1.8.0_111 SCALA_HOME=/data/scala-2.11.8 SPARK_MASTER_HOST=master SPARK_MASTER_PORT=7077 HADOOP_CONF_DIR=/data/hadoop-2.6.5/etc/hadoop # shuffled以及RDD的数据存放目录 SPARK_LOCAL_DIRS=/data/spark_data # worker端进程的工作目录 SPARK_WORKER_DIR=/data/spark_data/spark_works 注意：需要在本地创建/data/spark_data/spark_works目录\n配置文件2：slaves\nmaster slave1 slave2 配置文件3：spark-defaults.conf\nspark.master spark://master:7077 spark.serializer org.apache.spark.serializer.KryoSerializer spark.eventLog.enabled true spark.eventLog.dir file:///data/spark_data/history/event-log spark.history.fs.logDirectory file:///data/spark_data/history/spark-events spark.eventLog.compress true 注意：需要在本地创建/data/spark_data/history/event-log、/data/spark_data/history/spark-events\n3. 复制到其他节点 在master上：\nscp -r /data/spark* aboutyun@slave1:~/ scp -r /data/spark* aboutyun@slave2:~/ 在slave1和slave2上：\nmv ~/spark* /data\n4. 设置环境变量\n将以下内容加入到~/.bashrc文件中，\nexport SPARK_HOME=/data/spark-1.6.3-bin-hadoop2.6 export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH 然后执行以下命令：\nsource ~/.bashrc\n5. 启动验证 在master机器上进行如下操作\n可直接使用 start-all.sh , 但是要注意hadoop的命令, hadoop也有一个一样的命令,但是已经遗弃\n1）启动master\nstart-master.sh\n在master机器上执行jps命令\n上图说明在master节点上成功启动Master进程\n2）启动slave\n在master和slave机器上执行jps命令\n上面的图片说明在每台机器上都成功启动了Worker进程。\n3）访问WebUI\n在master、slave1和slave2这三台中任意一台机器上的浏览器中输入：http://master:8080/，看到如下图片，就说明我们的spark集群安装成功了。\n趟过的坑\n配置core-site.xml和hdfs-site.xml文件时所指定的本地目录一定要自己创建，否则在执行玩格式化hdfs后，启动hdfs会丢失进程。 6. 打包运行 https://www.cnblogs.com/654wangzai321/p/9513488.html 二,高可用(热备) ","wordCount":"961","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-08-30T16:26:26.170963938Z","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaokunji.com/zh/ accesskey=h title="米二 (Alt + H)"><img src=https://xiaokunji.com/img/Q.svg alt aria-label=logo height=35>米二</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaokunji.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://xiaokunji.com/zh/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://xiaokunji.com/zh/search.html title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://xiaokunji.com/zh/post.html title=📚文章><span>📚文章</span></a></li><li><a href=https://xiaokunji.com/zh/archives.html title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://xiaokunji.com/zh/tags.html title=🔖标签><span>🔖标签</span></a></li><li><a href=https://xiaokunji.com/zh/categories.html title=📖分类><span>📖分类</span></a></li><li><a href=https://xiaokunji.com/zh/links.html title=🤝友链><span>🤝友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><nav aria-label=breadcrumb><ul><a href=https://xiaokunji.com/zh/>🏠</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>hadoop生态圈</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark.html>Spark</a> <span>></span></ul></nav><h1 class=post-title>安装与配置</h1><div class=post-description></div><div class=post-meta>创建:&nbsp;<span title='2023-08-22 00:00:00 +0000 UTC'>2023-08-22</span>&nbsp;·&nbsp;更新:&nbsp;2023-08-30&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;xkj
&nbsp;|&nbsp;分类: &nbsp;<ul class=post-categories-meta><a href=https://xiaokunji.com/zh/categories/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></ul><span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e4%b8%80%e5%ae%89%e8%a3%85spark%e9%9b%86%e7%be%a4 aria-label=一、安装Spark集群><strong>一、安装Spark集群</strong></a><ul><li><a href=#1-%e8%a7%a3%e5%8e%8b%e5%ae%89%e8%a3%85%e5%8c%85 aria-label="1. 解压安装包"><strong>1. 解压安装包</strong></a></li><li><a href=#2-%e9%85%8d%e7%bd%aespark aria-label="2. 配置spark"><strong>2. 配置spark</strong></a></li><li><a href=#3-%e5%a4%8d%e5%88%b6%e5%88%b0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9 aria-label="3. 复制到其他节点"><strong>3. 复制到其他节点</strong></a></li><li><a href=#5-%e5%90%af%e5%8a%a8%e9%aa%8c%e8%af%81 aria-label="5. 启动验证"><strong>5. 启动验证</strong></a></li><li><a href=#6-%e6%89%93%e5%8c%85%e8%bf%90%e8%a1%8c aria-label="6. 打包运行"><strong>6. 打包运行</strong></a></li></ul></li><li><a href=#%e4%ba%8c%e9%ab%98%e5%8f%af%e7%94%a8%e7%83%ad%e5%a4%87 aria-label=二,高可用(热备)><strong>二,高可用(热备)</strong></a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>[toc]</p><p>注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和stop-all.sh,而spark启动/关闭用到了这两个脚本,而hadoop中这么命令已经遗弃了</p><blockquote><p>源笔记:</p><p><strong>1.修改slave文件,配置从机</strong></p><p>如下:</p><p>slave01</p><p>slave02</p><p>slave03</p><p><strong>2.修改spark.env.sh,如下:</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>export <span style=color:#79c0ff>JAVA_HOME</span><span style=color:#ff7b72;font-weight:700>=</span>/mysoftware/jdk1.8.0_101
</span></span><span style=display:flex><span>export <span style=color:#79c0ff>SPARK_MASTER_IP</span><span style=color:#ff7b72;font-weight:700>=</span>master
</span></span><span style=display:flex><span>export <span style=color:#79c0ff>SCALA_HOME</span><span style=color:#ff7b72;font-weight:700>=</span>/mysoftware/scala-2.12.3
</span></span><span style=display:flex><span>export <span style=color:#79c0ff>HADOOP_HOME</span><span style=color:#ff7b72;font-weight:700>=</span>/mysoftware/hadoop-2.7.3
</span></span><span style=display:flex><span>export <span style=color:#79c0ff>HADOOP_CONF_DIR</span><span style=color:#ff7b72;font-weight:700>=</span>/mysoftware/hadoop-2.7.3/etc/hadoop/
</span></span></code></pre></div><p><strong>3.发送到从机即可,集群环境搭建完毕</strong></p></blockquote><h1 id=一安装spark集群><strong>一、安装Spark集群</strong><a hidden class=anchor aria-hidden=true href=#一安装spark集群>#</a></h1><h2 id=1-解压安装包><strong>1. 解压安装包</strong><a hidden class=anchor aria-hidden=true href=#1-解压安装包>#</a></h2><p>tar -zxvf ~/jar/spark-1.6.3-bin-hadoop2.6.tgz -C /data</p><h2 id=2-配置spark><strong>2. 配置spark</strong><a hidden class=anchor aria-hidden=true href=#2-配置spark>#</a></h2><p>涉及到的配置文件有以下几个：</p><p>${SPARK_HOME}/conf/spark-env.sh</p><p>${SPARK_HOME}/conf/slaves</p><p>${SPARK_HOME}/conf/spark-defaults.conf</p><p>这三个文件都是由原始的template文件复制过来的，比如cp spark-env.sh.template spark-env.sh</p><p><strong>配置文件1：spark-env.sh</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#79c0ff>JAVA_HOME</span><span style=color:#ff7b72;font-weight:700>=</span>/data/jdk1.8.0_111
</span></span><span style=display:flex><span><span style=color:#79c0ff>SCALA_HOME</span><span style=color:#ff7b72;font-weight:700>=</span>/data/scala-2.11.8
</span></span><span style=display:flex><span><span style=color:#79c0ff>SPARK_MASTER_HOST</span><span style=color:#ff7b72;font-weight:700>=</span>master
</span></span><span style=display:flex><span><span style=color:#79c0ff>SPARK_MASTER_PORT</span><span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>7077</span>
</span></span><span style=display:flex><span><span style=color:#79c0ff>HADOOP_CONF_DIR</span><span style=color:#ff7b72;font-weight:700>=</span>/data/hadoop-2.6.5/etc/hadoop
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># shuffled以及RDD的数据存放目录</span>
</span></span><span style=display:flex><span><span style=color:#79c0ff>SPARK_LOCAL_DIRS</span><span style=color:#ff7b72;font-weight:700>=</span>/data/spark_data
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># worker端进程的工作目录</span>
</span></span><span style=display:flex><span><span style=color:#79c0ff>SPARK_WORKER_DIR</span><span style=color:#ff7b72;font-weight:700>=</span>/data/spark_data/spark_works
</span></span></code></pre></div><blockquote><p>注意：需要在本地创建/data/spark_data/spark_works目录</p></blockquote><p><strong>配置文件2：slaves</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>master 
</span></span><span style=display:flex><span>slave1 
</span></span><span style=display:flex><span>slave2
</span></span></code></pre></div><p><strong>配置文件3：spark-defaults.conf</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-properties data-lang=properties><span style=display:flex><span>spark.master        <span style=color:#a5d6ff>spark://master:7077</span>
</span></span><span style=display:flex><span>spark.serializer        <span style=color:#a5d6ff>org.apache.spark.serializer.KryoSerializer</span>
</span></span><span style=display:flex><span>spark.eventLog.enabled        <span style=color:#a5d6ff>true</span>
</span></span><span style=display:flex><span>spark.eventLog.dir        <span style=color:#a5d6ff>file:///data/spark_data/history/event-log</span>
</span></span><span style=display:flex><span>spark.history.fs.logDirectory        <span style=color:#a5d6ff>file:///data/spark_data/history/spark-events</span>
</span></span><span style=display:flex><span>spark.eventLog.compress        <span style=color:#a5d6ff>true</span>
</span></span></code></pre></div><blockquote><p>注意：需要在本地创建/data/spark_data/history/event-log、/data/spark_data/history/spark-events</p></blockquote><h2 id=3-复制到其他节点><strong>3. 复制到其他节点</strong><a hidden class=anchor aria-hidden=true href=#3-复制到其他节点>#</a></h2><p>在master上：</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>scp -r /data/spark* aboutyun@slave1:~/
</span></span><span style=display:flex><span>scp -r /data/spark* aboutyun@slave2:~/
</span></span></code></pre></div><p>在slave1和slave2上：</p><p><code>mv ~/spark* /data</code></p><p><strong>4. 设置环境变量</strong></p><p>将以下内容加入到~/.bashrc文件中，</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>export <span style=color:#79c0ff>SPARK_HOME</span><span style=color:#ff7b72;font-weight:700>=</span>/data/spark-1.6.3-bin-hadoop2.6
</span></span><span style=display:flex><span>export <span style=color:#79c0ff>PATH</span><span style=color:#ff7b72;font-weight:700>=</span><span style=color:#79c0ff>$SPARK_HOME</span>/bin:<span style=color:#79c0ff>$SPARK_HOME</span>/sbin:<span style=color:#79c0ff>$PATH</span>
</span></span></code></pre></div><p>然后执行以下命令：</p><p><code>source ~/.bashrc</code></p><h2 id=5-启动验证><strong>5. 启动验证</strong><a hidden class=anchor aria-hidden=true href=#5-启动验证>#</a></h2><p>在master机器上进行如下操作</p><blockquote><p>可直接使用 start-all.sh , 但是要注意hadoop的命令, hadoop也有一个一样的命令,但是已经遗弃</p></blockquote><p><strong>1）启动master</strong></p><p><code>start-master.sh</code></p><p>在master机器上执行jps命令</p><p><img loading=lazy src=https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174246.png alt=img></p><p>上图说明在master节点上成功启动Master进程</p><p><strong>2）启动slave</strong></p><p>在master和slave机器上执行jps命令</p><p><img loading=lazy src=https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174247.png alt=img></p><p>上面的图片说明在每台机器上都成功启动了Worker进程。</p><p><strong>3）访问WebUI</strong></p><p>在master、slave1和slave2这三台中任意一台机器上的浏览器中输入：http://master:8080/，看到如下图片，就说明我们的spark集群安装成功了。</p><p><img loading=lazy src=https://gitee.com/xiaokunji/my-images/raw/master/myMD/20210711174248.png alt=img></p><p><strong>趟过的坑</strong></p><ol><li>配置core-site.xml和hdfs-site.xml文件时所指定的本地目录一定要自己创建，否则在执行玩格式化hdfs后，启动hdfs会丢失进程。</li></ol><h2 id=6-打包运行><strong>6. 打包运行</strong><a hidden class=anchor aria-hidden=true href=#6-打包运行>#</a></h2><blockquote><p><a href=https://www.cnblogs.com/654wangzai321/p/9513488.html target=_blank rel=noopener>https://www.cnblogs.com/654wangzai321/p/9513488.html</a></p></blockquote><h1 id=二高可用热备><strong>二,高可用(热备)</strong><a hidden class=anchor aria-hidden=true href=#二高可用热备>#</a></h1></div><footer class=post-footer><ul class=post-tags><li><a href=https://xiaokunji.com/zh/tags/spark.html>Spark</a></li><li><a href=https://xiaokunji.com/zh/tags/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></li></ul></footer></article></main><footer class=footer><span>Copyright
&copy;
-2023
<a href=https://xiaokunji.com/zh/ style=color:#939393>米二</a>
All Rights Reserved</span>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<span id=busuanzi_container><span class="fa fa-user">用户数:</span><span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye">访问数:</span><span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>