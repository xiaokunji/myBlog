<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SparkStreaming | 米二</title><meta name=keywords content=" **1.概述**, **2. Streaming架构**, **2.1 计算流程：**, **2.2 容错性：**, **2.3 实时性：**, **2.4 扩展性与吞吐量：**, **3.使用**, **3.1 基本使用(接收套接字)**, **3.2 HDFS接收**, **3.3 windows窗口函数**, **3.4 连接kafka**"><meta name=description content="     "><meta name=author content="xkj"><link rel=canonical href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparkstreaming.html><link crossorigin=anonymous href=/assets/css/stylesheet.b3faf608c544858ba700943ffe182cb647f38432d29a07d73234965beacb26f6.css integrity="sha256-s/r2CMVEhYunAJQ//hgstkfzhDLSmgfXMjSWW+rLJvY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=16x16 href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=32x32 href=https://xiaokunji.com/img/Q.svg><link rel=apple-touch-icon href=https://xiaokunji.com/Q.svg><link rel=mask-icon href=https://xiaokunji.com/Q.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparkstreaming.html><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="SparkStreaming"><meta property="og:description" content="     "><meta property="og:type" content="article"><meta property="og:url" content="https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparkstreaming.html"><meta property="article:section" content="Hadoop生态圈"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-28T16:39:17+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="SparkStreaming"><meta name=twitter:description content="     "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"SparkStreaming","item":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparkstreaming.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SparkStreaming","name":"SparkStreaming","description":"     ","keywords":[" **1.概述**"," **2. Streaming架构**"," **2.1 计算流程：**"," **2.2 容错性：**"," **2.3 实时性：**"," **2.4 扩展性与吞吐量：**"," **3.使用**"," **3.1 基本使用(接收套接字)**"," **3.2 HDFS接收**"," **3.3 windows窗口函数**"," **3.4 连接kafka**"],"articleBody":"[toc]\n1.概述 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。\n来自* \u003chttp://www.cnblogs.com/shishanyuan/p/4747735.html \u003e\n2. Streaming架构 2.1 计算流程： Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。\n图Spark Streaming构架\n2.2 容错性： 对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。\n对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。\nSpark Streaming中RDD的lineage关系图\n2.3 实时性： 对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右）\n2.4 扩展性与吞吐量： Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍。\n来自* \u003chttp://www.cnblogs.com/shishanyuan/p/4747735.html \u003e\n3.使用 3.1 基本使用(接收套接字) import org.apache.spark._ import org.apache.spark.streaming._ import org.apache.spark.streaming.StreamingContext._ // Create a local StreamingContext with two working thread and batch interval of 1 second. // The master requires 2 cores to prevent from a starvation scenario. val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\") val ssc = new StreamingContext(conf, Seconds(5)) // 后面时间的含义: //1. 每隔5秒切一次RDD(可能包含多个) //2. 每隔5秒提交一个job,或者说每隔5秒执行一次 //3. 这个job需5秒内算完,不然下次就要来了,造成处理积压 // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.socketTextStream(\"localhost\", 9999) // Split each line into words val words = lines.flatMap(_.split(\" \")) import org.apache.spark.streaming.StreamingContext._ // Count each word in each batch val pairs = words.map(word =\u003e (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print() ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate 使用:\n1.在集群中执行命令: nc -lk 9999\n2.然后输入内容,用空格隔开, 3.然后在IDE控制台就能看见单词计数结果 解释: 1.创建StreamingContext对象 同Spark初始化需要创建SparkContext对象一样，使用Spark Streaming就需要创建StreamingContext对象。创建StreamingContext对象所需的参数与SparkContext基本一致，包括指明Master，设定名称(如NetworkWordCount)。需要注意的是参数Seconds(1)，Spark Streaming需要指定处理数据的时间间隔，如上例所示的1s，那么Spark Streaming会以1s为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置； 2.创建InputDStream ​ 如同Storm的Spout，Spark Streaming需要指明数据源。如上例所示的socketTextStream，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括Kafka、 Flume、HDFS/S3、Kinesis和Twitter等数据源； 3.操作Dstream ​ 对于从数据源得到的DStream，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的WordCount执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用Map和ReduceByKey方法进行计算，当然最后还有使用print()方法输出结果； 4.启动Spark Streaming ​ 之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当ssc.start()启动后程序才真正进行所有预期的操作。\n来自 \u003chttp://www.cnblogs.com/shishanyuan/p/4747735.html \u003ehttp://spark.apache.org/docs/2.2.1/streaming-programming-guide.html 3.2 HDFS接收 streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) eg. streamingContext.fileStream(\"hdfs://192.168.2.101:8020/data/\") Spark Streaming将监视目录dataDirectory并处理在该目录中创建的任何文件（不支持在嵌套目录中写入的文件）。注意\n文件必须具有相同的数据格式。 必须dataDirectory通过原子地将文件移动或重命名到数据目录中来创建文件。 移动后，不得更改文件。因此，如果文件被连续追加，则不会读取新数据。 对于简单的文本文件，有一个更简单的方法streamingContext.textFileStream(dataDirectory)。文件流不需要运行接收器，因此不需要分配内核。\n**基于自定义接收器的流：**可以使用通过自定义接收器接收的数据流创建DStream。有关更多详细信息，请参见《定制接收器指南》 。 **RDD队列作为流：**为了使用测试数据测试Spark Streaming应用程序，还可以使用，基于RDD队列创建DStream streamingContext.queueStream(queueOfRDDs)。推送到队列中的每个RDD将被视为DStream中的一批数据，并像流一样进行处理。 3.3 windows窗口函数 （实现一阶段内的累加 ，而不是程序启动时）\n​ 假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。\n​ 窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错。\n​ 优化后的window操作要保存状态所以要设置checkpoint路径，没有优化的window操作可以不设置checkpoint路径。\nval sc = new SparkConf().setMaster(\"local[3]\").setAppName(\"WindowsFunc\") val ssc = new StreamingContext(sc,Seconds(2)) // 每隔5秒接收一次(以前的也保留) ssc.checkpoint(\"./checkPoint\") val line = ssc.socketTextStream(\"192.168.2.101\",9999)// Receiver机制会占用一个线程 val wc = line.flatMap(_.split(\" \")).map((_,1)) wc.reduceByKeyAndWindow(_+_,Seconds(8),Seconds(2)).print()// 每隔四秒算前八秒的数据 reduceByKeyAndWindow 函数\n`reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])``\n前面的func作用和上一个reduceByKeyAndWindow相同，后面的invFunc是用于处理流出rdd的。 因为窗口滑动的原因,就会造成数据重复计算(如上图的time3),使用该函数可以减少计算(会计算出重复量.再计算出增量,再合并)\n在下面这个例子中，如果把3秒的时间窗口当成一个池塘，池塘每一秒都会有鱼游进或者游出，那么第一个函数表示每由进来一条鱼，就在该类鱼的数量上累加。而第二个函数是，每由出去一条鱼，就将该鱼的总数减去一。(简单这么理解,当数据出窗时,就会执行这个反向函数)\n`wc.reduceByKeyAndWindow(+,-,Seconds(8),Seconds(2)).print()// 每隔四秒算前八秒的数据(其结果与上方一直)``\n以后用到了再看吧\nhttps://www.cnblogs.com/yjd_hycf_space/p/7053722.html http://www.freesion.com/article/487428411/ https://www.iteye.com/blog/humingminghz-2308138 https://blog.csdn.net/legotime/article/details/51836040 3.4 连接kafka package xkj.sparkStream import com.alibaba.fastjson.JSON import entity.MyPerson import org.apache.kafka.common.serialization.StringDeserializer import org.apache.log4j.Level import org.apache.log4j.Logger._ import org.apache.spark.rdd.RDD import org.apache.spark.sql.{SaveMode, SparkSession} import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies} import org.apache.spark.streaming.{Seconds, StreamingContext} object KafkaTest { def main(args: Array[String]): Unit = { getLogger(\"org.apache.hadoop\").setLevel(Level.ERROR) getLogger(\"org.apache.zookeeper\").setLevel(Level.WARN) getLogger(\"org.apache.hive\").setLevel(Level.WARN) getLogger(\"org.apache\").setLevel(Level.WARN) val spark = SparkSession.builder().master(\"local[3]\").appName(\"KafkaTest\").getOrCreate() import spark.implicits._ val ssc = new StreamingContext(spark.sparkContext, Seconds(4)) ssc.checkpoint(\"./checkPoint\") val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -\u003e \"192.168.2.101:9092\", \"key.deserializer\" -\u003e classOf[StringDeserializer], \"value.deserializer\" -\u003e classOf[StringDeserializer], \"group.id\" -\u003e \"xkj\" ) val topics: Set[String] = Set(\"mystream\") // val stream = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)) // stream.foreachRDD(rdd =\u003e { // rdd.foreach(line =\u003e { // println(\"key=\" + line.key() + \", value=\" + line.value() + \", offset=\" + line.offset()) // }) // }) val streamPerson = KafkaUtils.createDirectStream(ssc,LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](topics,kafkaParams)) streamPerson.foreachRDD(rdd=\u003e{ val v: RDD[String] = rdd.map(_.value()) val rddP: RDD[MyPerson] = v.map(JSON.parseObject(_, classOf[MyPerson])) val df = rddP.toDF() df.show() df.write.mode(SaveMode.Append).json(\"hdfs://192.168.2.101:9000/user\") }) ssc.start() ssc.awaitTermination() } } 特别注意版本:\npom.xml\n2.3.0 org.apache.spark spark-streaming_2.11 ${spark.version} org.apache.spark spark-streaming-kafka-0-10_2.11 ${spark.version} https://blog.csdn.net/zhaolq1024/article/details/85685189 https://blog.51cto.com/simplelife/2311296 ","wordCount":"3539","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-08-28T16:39:17.896916742Z","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparkstreaming.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaokunji.com/zh/ accesskey=h title="米二 (Alt + H)"><img src=https://xiaokunji.com/img/Q.svg alt aria-label=logo height=35>米二</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaokunji.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://xiaokunji.com/zh/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://xiaokunji.com/zh/search.html title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://xiaokunji.com/zh/post.html title=📚文章><span>📚文章</span></a></li><li><a href=https://xiaokunji.com/zh/archives.html title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://xiaokunji.com/zh/tags.html title=🔖标签><span>🔖标签</span></a></li><li><a href=https://xiaokunji.com/zh/categories.html title=📖分类><span>📖分类</span></a></li><li><a href=https://xiaokunji.com/zh/links.html title=🤝友链><span>🤝友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><nav aria-label=breadcrumb><ul><a href=https://xiaokunji.com/zh/>🏠</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>hadoop生态圈</a> <span>></span>
<a href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark.html>Spark</a> <span>></span></ul></nav><h1 class=post-title>SparkStreaming</h1><div class=post-description></div><div class=post-meta><span title='2023-08-22 00:00:00 +0000 UTC'>2023-08-22</span>&nbsp;·&nbsp;8 分钟&nbsp;·&nbsp;xkj
&nbsp;|&nbsp;分类: &nbsp;<ul class=post-categories-meta><a href=https://xiaokunji.com/zh/categories/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></ul><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv></span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#1%e6%a6%82%e8%bf%b0 aria-label=1.概述><strong>1.概述</strong></a></li><li><a href=#2-streaming%e6%9e%b6%e6%9e%84 aria-label="2. Streaming架构"><strong>2. Streaming架构</strong></a><ul><li><a href=#21-%e8%ae%a1%e7%ae%97%e6%b5%81%e7%a8%8b aria-label="2.1 计算流程："><strong>2.1 计算流程：</strong></a></li><li><a href=#22-%e5%ae%b9%e9%94%99%e6%80%a7 aria-label="2.2 容错性："><strong>2.2 容错性：</strong></a></li><li><a href=#23-%e5%ae%9e%e6%97%b6%e6%80%a7 aria-label="2.3 实时性："><strong>2.3 实时性：</strong></a></li><li><a href=#24-%e6%89%a9%e5%b1%95%e6%80%a7%e4%b8%8e%e5%90%9e%e5%90%90%e9%87%8f aria-label="2.4 扩展性与吞吐量："><strong>2.4 扩展性与吞吐量：</strong></a></li></ul></li><li><a href=#3%e4%bd%bf%e7%94%a8 aria-label=3.使用><strong>3.使用</strong></a><ul><li><a href=#31-%e5%9f%ba%e6%9c%ac%e4%bd%bf%e7%94%a8%e6%8e%a5%e6%94%b6%e5%a5%97%e6%8e%a5%e5%ad%97 aria-label="3.1 基本使用(接收套接字)"><strong>3.1 基本使用(接收套接字)</strong></a></li><li><a href=#32-hdfs%e6%8e%a5%e6%94%b6 aria-label="3.2 HDFS接收"><strong>3.2 HDFS接收</strong></a></li><li><a href=#33-windows%e7%aa%97%e5%8f%a3%e5%87%bd%e6%95%b0 aria-label="3.3 windows窗口函数"><strong>3.3 windows窗口函数</strong></a></li><li><a href=#34-%e8%bf%9e%e6%8e%a5kafka aria-label="3.4 连接kafka"><strong>3.4 连接kafka</strong></a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>[toc]</p><h1 id=1概述><strong>1.概述</strong><a hidden class=anchor aria-hidden=true href=#1概述>#</a></h1><p>Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。</p><p><img loading=lazy src=F:%5c%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99%5c%e4%b8%aa%e4%ba%ba%e7%ac%94%e8%ae%b0%5cMDImages%5cip_image001-1598543590569.jpeg alt=img></p><blockquote><p>来自* <em>&lt;</em><a href=http://www.cnblogs.com/shishanyuan/p/4747735.html target=_blank rel=noopener><em>http://www.cnblogs.com/shishanyuan/p/4747735.html</em></a>
<em>></em></p></blockquote><h1 id=2-streaming架构><strong>2. Streaming架构</strong><a hidden class=anchor aria-hidden=true href=#2-streaming架构>#</a></h1><h2 id=21-计算流程><strong>2.1 计算流程：</strong><a hidden class=anchor aria-hidden=true href=#21-计算流程>#</a></h2><p>Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。</p><p><img loading=lazy src=F:%5c%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99%5c%e4%b8%aa%e4%ba%ba%e7%ac%94%e8%ae%b0%5cMDImages%5cip_image002-1598543590569.jpeg alt=img></p><p>图Spark Streaming构架</p><h2 id=22-容错性><strong>2.2 容错性：</strong><a hidden class=anchor aria-hidden=true href=#22-容错性>#</a></h2><p>对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。</p><p>对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。</p><p><img loading=lazy src=F:%5c%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99%5c%e4%b8%aa%e4%ba%ba%e7%ac%94%e8%ae%b0%5cMDImages%5cip_image003-1598543590569.jpeg alt=img></p><p>Spark Streaming中RDD的lineage关系图</p><h2 id=23-实时性><strong>2.3 实时性：</strong><a hidden class=anchor aria-hidden=true href=#23-实时性>#</a></h2><p>对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右）</p><h2 id=24-扩展性与吞吐量><strong>2.4 扩展性与吞吐量：</strong><a hidden class=anchor aria-hidden=true href=#24-扩展性与吞吐量>#</a></h2><p>Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍。</p><blockquote><p>来自* <em>&lt;</em><a href=http://www.cnblogs.com/shishanyuan/p/4747735.html target=_blank rel=noopener><em>http://www.cnblogs.com/shishanyuan/p/4747735.html</em></a>
<em>></em></p></blockquote><h1 id=3使用><strong>3.使用</strong><a hidden class=anchor aria-hidden=true href=#3使用>#</a></h1><h2 id=31-基本使用接收套接字><strong>3.1 基本使用(接收套接字)</strong><a hidden class=anchor aria-hidden=true href=#31-基本使用接收套接字>#</a></h2><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark._</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.streaming._</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.streaming.StreamingContext._</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Create a local StreamingContext with two working thread and batch interval of 1 second.
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// The master requires 2 cores to prevent from a starvation scenario.
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span><span style=color:#ff7b72>val</span> conf <span style=color:#ff7b72>=</span> <span style=color:#ff7b72>new</span> <span style=color:#f0883e;font-weight:700>SparkConf</span><span style=color:#ff7b72;font-weight:700>().</span>setMaster<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;local[2]&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>setAppName<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;NetworkWordCount&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>val</span> ssc <span style=color:#ff7b72>=</span> <span style=color:#ff7b72>new</span> <span style=color:#f0883e;font-weight:700>StreamingContext</span><span style=color:#ff7b72;font-weight:700>(</span>conf<span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#f0883e;font-weight:700>Seconds</span><span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>5</span><span style=color:#ff7b72;font-weight:700>))</span> 
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// 后面时间的含义: 
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//1. 每隔5秒切一次RDD(可能包含多个)
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//2. 每隔5秒提交一个job,或者说每隔5秒执行一次
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//3. 这个job需5秒内算完,不然下次就要来了,造成处理积压
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Create a DStream that will connect to hostname:port, like localhost:9999
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span><span style=color:#ff7b72>val</span> lines <span style=color:#ff7b72>=</span> ssc<span style=color:#ff7b72;font-weight:700>.</span>socketTextStream<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;localhost&#34;</span><span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#a5d6ff>9999</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Split each line into words
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span><span style=color:#ff7b72>val</span> words <span style=color:#ff7b72>=</span> lines<span style=color:#ff7b72;font-weight:700>.</span>flatMap<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>.</span>split<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34; &#34;</span><span style=color:#ff7b72;font-weight:700>))</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.streaming.StreamingContext._</span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Count each word in each batch
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span><span style=color:#ff7b72>val</span> pairs <span style=color:#ff7b72>=</span> words<span style=color:#ff7b72;font-weight:700>.</span>map<span style=color:#ff7b72;font-weight:700>(</span>word <span style=color:#ff7b72>=&gt;</span> <span style=color:#ff7b72;font-weight:700>(</span>word<span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#a5d6ff>1</span><span style=color:#ff7b72;font-weight:700>))</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>val</span> wordCounts <span style=color:#ff7b72>=</span> pairs<span style=color:#ff7b72;font-weight:700>.</span>reduceByKey<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#ff7b72>_</span> <span style=color:#ff7b72;font-weight:700>+</span> <span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>// Print the first ten elements of each RDD generated in this DStream to the console
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>wordCounts<span style=color:#ff7b72;font-weight:700>.</span>print<span style=color:#ff7b72;font-weight:700>()</span>
</span></span><span style=display:flex><span>ssc<span style=color:#ff7b72;font-weight:700>.</span>start<span style=color:#ff7b72;font-weight:700>()</span>              <span style=color:#8b949e;font-style:italic>// Start the computation
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>ssc<span style=color:#ff7b72;font-weight:700>.</span>awaitTermination<span style=color:#ff7b72;font-weight:700>()</span>  <span style=color:#8b949e;font-style:italic>// Wait for the computation to terminate
</span></span></span></code></pre></div><blockquote><p>使用:</p><p>1.在集群中执行命令: nc -lk 9999<br>2.然后输入内容,用空格隔开,
3.然后在IDE控制台就能看见单词计数结果
解释:
<strong>1.创建StreamingContext对象</strong>
同Spark初始化需要创建SparkContext对象一样，使用Spark Streaming就需要创建StreamingContext对象。创建StreamingContext对象所需的参数与SparkContext基本一致，包括指明Master，设定名称(如NetworkWordCount)。需要注意的是参数Seconds(1)，Spark Streaming需要指定处理数据的时间间隔，如上例所示的1s，那么Spark Streaming会以1s为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置；
<strong>2.创建InputDStream</strong>
​ 如同Storm的Spout，Spark Streaming需要指明数据源。如上例所示的socketTextStream，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括Kafka、 Flume、HDFS/S3、Kinesis和Twitter等数据源；
<strong>3.操作Dstream</strong>
​ 对于从数据源得到的DStream，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的WordCount执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用Map和ReduceByKey方法进行计算，当然最后还有使用print()方法输出结果；
<strong>4.启动Spark Streaming</strong>
​ 之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当ssc.start()启动后程序才真正进行所有预期的操作。</p><p><em>来自</em> <em>&lt;</em><a href=http://www.cnblogs.com/shishanyuan/p/4747735.html target=_blank rel=noopener><em>http://www.cnblogs.com/shishanyuan/p/4747735.html</em></a>
<em>></em><a href=http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html target=_blank rel=noopener><em>http://spark.apache.org/docs/2.2.1/streaming-programming-guide.html</em></a></p></blockquote><h2 id=32-hdfs接收><strong>3.2 HDFS接收</strong><a hidden class=anchor aria-hidden=true href=#32-hdfs接收>#</a></h2><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>streamingContext.fileStream<span style=color:#ff7b72;font-weight:700>[</span>KeyClass, ValueClass, InputFormatClass<span style=color:#ff7b72;font-weight:700>](</span>dataDirectory<span style=color:#ff7b72;font-weight:700>)</span> 
</span></span><span style=display:flex><span>eg.  streamingContext.fileStream<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;hdfs://192.168.2.101:8020/data/&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span></code></pre></div><p>Spark Streaming将监视目录dataDirectory并处理在该目录中创建的任何文件（不支持在嵌套目录中写入的文件）。注意</p><ul><li>文件必须具有相同的数据格式。</li><li>必须dataDirectory通过原子地<em>将</em>文件<em>移动</em>或<em>重命名</em>到数据目录中来创建文件。</li><li>移动后，不得更改文件。因此，如果文件被连续追加，则不会读取新数据。</li></ul><p>对于简单的文本文件，有一个更简单的方法streamingContext.textFileStream(dataDirectory)。文件流不需要运行接收器，因此不需要分配内核。</p><ul><li>**基于自定义接收器的流：**可以使用通过自定义接收器接收的数据流创建DStream。有关更多详细信息，请参见《<a href=http://spark.apache.org/docs/2.2.1/streaming-custom-receivers.html target=_blank rel=noopener>定制接收器指南》</a>
。</li><li>**RDD队列作为流：**为了使用测试数据测试Spark Streaming应用程序，还可以使用，基于RDD队列创建DStream streamingContext.queueStream(queueOfRDDs)。推送到队列中的每个RDD将被视为DStream中的一批数据，并像流一样进行处理。</li></ul><h2 id=33-windows窗口函数><strong>3.3 windows窗口函数</strong><a hidden class=anchor aria-hidden=true href=#33-windows窗口函数>#</a></h2><p><strong>（实现一阶段内的累加 ，而不是程序启动时）</strong></p><p><img loading=lazy src=F:%5c%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99%5c%e4%b8%aa%e4%ba%ba%e7%ac%94%e8%ae%b0%5cMDImages%5c1-1882315162.png alt=img></p><p>​ <strong>假设每隔5s 1个batch,上图中窗口长度为15s，窗口滑动间隔10s。</strong></p><p>​ <strong>窗口长度和滑动间隔必须是batchInterval的整数倍。如果不是整数倍会检测报错</strong>。</p><p>​ <strong>优化后的window操作要保存状态所以要设置checkpoint路径，没有优化的window操作可以不设置checkpoint路径。</strong></p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#ff7b72>val</span> sc <span style=color:#ff7b72>=</span> <span style=color:#ff7b72>new</span> <span style=color:#f0883e;font-weight:700>SparkConf</span><span style=color:#ff7b72;font-weight:700>().</span>setMaster<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;local[3]&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>setAppName<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;WindowsFunc&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>val</span> ssc <span style=color:#ff7b72>=</span> <span style=color:#ff7b72>new</span> <span style=color:#f0883e;font-weight:700>StreamingContext</span><span style=color:#ff7b72;font-weight:700>(</span>sc<span style=color:#ff7b72;font-weight:700>,</span><span style=color:#f0883e;font-weight:700>Seconds</span><span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>))</span> <span style=color:#8b949e;font-style:italic>// 每隔5秒接收一次(以前的也保留)
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>ssc<span style=color:#ff7b72;font-weight:700>.</span>checkpoint<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;./checkPoint&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>val</span> line <span style=color:#ff7b72>=</span> ssc<span style=color:#ff7b72;font-weight:700>.</span>socketTextStream<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;192.168.2.101&#34;</span><span style=color:#ff7b72;font-weight:700>,</span><span style=color:#a5d6ff>9999</span><span style=color:#ff7b72;font-weight:700>)</span><span style=color:#8b949e;font-style:italic>// Receiver机制会占用一个线程
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span><span style=color:#ff7b72>val</span> wc <span style=color:#ff7b72>=</span> line<span style=color:#ff7b72;font-weight:700>.</span>flatMap<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>.</span>split<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34; &#34;</span><span style=color:#ff7b72;font-weight:700>)).</span>map<span style=color:#ff7b72;font-weight:700>((</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>,</span><span style=color:#a5d6ff>1</span><span style=color:#ff7b72;font-weight:700>))</span>
</span></span><span style=display:flex><span>wc<span style=color:#ff7b72;font-weight:700>.</span>reduceByKeyAndWindow<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>+</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>,</span><span style=color:#f0883e;font-weight:700>Seconds</span><span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>8</span><span style=color:#ff7b72;font-weight:700>),</span><span style=color:#f0883e;font-weight:700>Seconds</span><span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>2</span><span style=color:#ff7b72;font-weight:700>)).</span>print<span style=color:#ff7b72;font-weight:700>()</span><span style=color:#8b949e;font-style:italic>// 每隔四秒算前八秒的数据
</span></span></span></code></pre></div><p>reduceByKeyAndWindow 函数</p><p>`reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])``</p><p>　　前面的func作用和上一个reduceByKeyAndWindow相同，后面的invFunc是用于处理流出rdd的。 因为窗口滑动的原因,就会造成数据重复计算(如上图的time3),使用该函数可以减少计算(会计算出重复量.再计算出增量,再合并)</p><p>　　在下面这个例子中，如果把3秒的时间窗口当成一个池塘，池塘每一秒都会有鱼游进或者游出，那么第一个函数表示每由进来一条鱼，就在该类鱼的数量上累加。而第二个函数是，每由出去一条鱼，就将该鱼的总数减去一。(简单这么理解,当数据出窗时,就会执行这个反向函数)</p><p>`wc.reduceByKeyAndWindow(<em>+</em>,<em>-</em>,Seconds(8),Seconds(2)).print()// 每隔四秒算前八秒的数据(其结果与上方一直)``</p><blockquote><p>以后用到了再看吧</p><p><a href=https://www.cnblogs.com/yjd_hycf_space/p/7053722.html target=_blank rel=noopener>https://www.cnblogs.com/yjd_hycf_space/p/7053722.html</a></p><p><a href=http://www.freesion.com/article/487428411/ target=_blank rel=noopener>http://www.freesion.com/article/487428411/</a></p><p><a href=https://www.iteye.com/blog/humingminghz-2308138 target=_blank rel=noopener>https://www.iteye.com/blog/humingminghz-2308138</a></p><p><a href=https://blog.csdn.net/legotime/article/details/51836040 target=_blank rel=noopener>https://blog.csdn.net/legotime/article/details/51836040</a></p></blockquote><h2 id=34-连接kafka><strong>3.4 连接kafka</strong><a hidden class=anchor aria-hidden=true href=#34-连接kafka>#</a></h2><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#ff7b72>package</span> <span style=color:#ff7b72>xkj.sparkStream</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>com.alibaba.fastjson.JSON</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>entity.MyPerson</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.kafka.common.serialization.StringDeserializer</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.log4j.Level</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.log4j.Logger._</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.rdd.RDD</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.sql.</span><span style=color:#ff7b72;font-weight:700>{</span><span style=color:#f0883e;font-weight:700>SaveMode</span><span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#f0883e;font-weight:700>SparkSession</span><span style=color:#ff7b72;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.streaming.kafka010.</span><span style=color:#ff7b72;font-weight:700>{</span><span style=color:#f0883e;font-weight:700>ConsumerStrategies</span><span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#f0883e;font-weight:700>KafkaUtils</span><span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#f0883e;font-weight:700>LocationStrategies</span><span style=color:#ff7b72;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>org.apache.spark.streaming.</span><span style=color:#ff7b72;font-weight:700>{</span><span style=color:#f0883e;font-weight:700>Seconds</span><span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#f0883e;font-weight:700>StreamingContext</span><span style=color:#ff7b72;font-weight:700>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>object</span> <span style=color:#f0883e;font-weight:700>KafkaTest</span> <span style=color:#ff7b72;font-weight:700>{</span>
</span></span><span style=display:flex><span>  <span style=color:#ff7b72>def</span> main<span style=color:#ff7b72;font-weight:700>(</span>args<span style=color:#ff7b72>:</span> <span style=color:#ff7b72>Array</span><span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>String</span><span style=color:#ff7b72;font-weight:700>])</span><span style=color:#ff7b72>:</span> <span style=color:#ff7b72>Unit</span> <span style=color:#ff7b72;font-weight:700>=</span> <span style=color:#ff7b72;font-weight:700>{</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    getLogger<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;org.apache.hadoop&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>setLevel<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#f0883e;font-weight:700>Level</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>ERROR</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>    getLogger<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;org.apache.zookeeper&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>setLevel<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#f0883e;font-weight:700>Level</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>WARN</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>    getLogger<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;org.apache.hive&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>setLevel<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#f0883e;font-weight:700>Level</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>WARN</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>    getLogger<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;org.apache&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>setLevel<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#f0883e;font-weight:700>Level</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>WARN</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>val</span> spark <span style=color:#ff7b72>=</span> <span style=color:#f0883e;font-weight:700>SparkSession</span><span style=color:#ff7b72;font-weight:700>.</span>builder<span style=color:#ff7b72;font-weight:700>().</span>master<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;local[3]&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>appName<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;KafkaTest&#34;</span><span style=color:#ff7b72;font-weight:700>).</span>getOrCreate<span style=color:#ff7b72;font-weight:700>()</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>import</span> <span style=color:#ff7b72>spark.implicits._</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>val</span> ssc <span style=color:#ff7b72>=</span> <span style=color:#ff7b72>new</span> <span style=color:#f0883e;font-weight:700>StreamingContext</span><span style=color:#ff7b72;font-weight:700>(</span>spark<span style=color:#ff7b72;font-weight:700>.</span>sparkContext<span style=color:#ff7b72;font-weight:700>,</span> <span style=color:#f0883e;font-weight:700>Seconds</span><span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>4</span><span style=color:#ff7b72;font-weight:700>))</span>
</span></span><span style=display:flex><span>    ssc<span style=color:#ff7b72;font-weight:700>.</span>checkpoint<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;./checkPoint&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>val</span> kafkaParams <span style=color:#ff7b72>=</span> <span style=color:#f0883e;font-weight:700>Map</span><span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>String</span>, <span style=color:#ff7b72>Object</span><span style=color:#ff7b72;font-weight:700>](</span>
</span></span><span style=display:flex><span>      <span style=color:#a5d6ff>&#34;bootstrap.servers&#34;</span> <span style=color:#ff7b72;font-weight:700>-&gt;</span> <span style=color:#a5d6ff>&#34;192.168.2.101:9092&#34;</span><span style=color:#ff7b72;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#a5d6ff>&#34;key.deserializer&#34;</span> <span style=color:#ff7b72;font-weight:700>-&gt;</span> classOf<span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>StringDeserializer</span><span style=color:#ff7b72;font-weight:700>],</span>
</span></span><span style=display:flex><span>      <span style=color:#a5d6ff>&#34;value.deserializer&#34;</span> <span style=color:#ff7b72;font-weight:700>-&gt;</span> classOf<span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>StringDeserializer</span><span style=color:#ff7b72;font-weight:700>],</span>
</span></span><span style=display:flex><span>      <span style=color:#a5d6ff>&#34;group.id&#34;</span> <span style=color:#ff7b72;font-weight:700>-&gt;</span> <span style=color:#a5d6ff>&#34;xkj&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>val</span> topics<span style=color:#ff7b72>:</span> <span style=color:#ff7b72>Set</span><span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>String</span><span style=color:#ff7b72;font-weight:700>]</span> <span style=color:#ff7b72>=</span> <span style=color:#f0883e;font-weight:700>Set</span><span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;mystream&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//    val stream = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topics, kafkaParams))
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//    stream.foreachRDD(rdd =&gt; {
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//      rdd.foreach(line =&gt; {
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//        println(&#34;key=&#34; + line.key() + &#34;,  value=&#34; + line.value() + &#34;, offset=&#34; + line.offset())
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//      })
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic>//    })
</span></span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic></span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>val</span> streamPerson <span style=color:#ff7b72>=</span> <span style=color:#f0883e;font-weight:700>KafkaUtils</span><span style=color:#ff7b72;font-weight:700>.</span>createDirectStream<span style=color:#ff7b72;font-weight:700>(</span>ssc<span style=color:#ff7b72;font-weight:700>,</span><span style=color:#f0883e;font-weight:700>LocationStrategies</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>PreferConsistent</span><span style=color:#ff7b72;font-weight:700>,</span><span style=color:#f0883e;font-weight:700>ConsumerStrategies</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>Subscribe</span><span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>String</span>,<span style=color:#ff7b72>String</span><span style=color:#ff7b72;font-weight:700>](</span>topics<span style=color:#ff7b72;font-weight:700>,</span>kafkaParams<span style=color:#ff7b72;font-weight:700>))</span>
</span></span><span style=display:flex><span>    streamPerson<span style=color:#ff7b72;font-weight:700>.</span>foreachRDD<span style=color:#ff7b72;font-weight:700>(</span>rdd<span style=color:#ff7b72;font-weight:700>=&gt;{</span>
</span></span><span style=display:flex><span>      <span style=color:#ff7b72>val</span> v<span style=color:#ff7b72>:</span> <span style=color:#ff7b72>RDD</span><span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>String</span><span style=color:#ff7b72;font-weight:700>]</span> <span style=color:#ff7b72>=</span> rdd<span style=color:#ff7b72;font-weight:700>.</span>map<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>.</span>value<span style=color:#ff7b72;font-weight:700>())</span>
</span></span><span style=display:flex><span>      <span style=color:#ff7b72>val</span> rddP<span style=color:#ff7b72>:</span> <span style=color:#ff7b72>RDD</span><span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>MyPerson</span><span style=color:#ff7b72;font-weight:700>]</span> <span style=color:#ff7b72>=</span> v<span style=color:#ff7b72;font-weight:700>.</span>map<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#f0883e;font-weight:700>JSON</span><span style=color:#ff7b72;font-weight:700>.</span>parseObject<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#ff7b72>_</span><span style=color:#ff7b72;font-weight:700>,</span> classOf<span style=color:#ff7b72;font-weight:700>[</span><span style=color:#ff7b72>MyPerson</span><span style=color:#ff7b72;font-weight:700>]))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#ff7b72>val</span> df <span style=color:#ff7b72>=</span> rddP<span style=color:#ff7b72;font-weight:700>.</span>toDF<span style=color:#ff7b72;font-weight:700>()</span>
</span></span><span style=display:flex><span>      df<span style=color:#ff7b72;font-weight:700>.</span>show<span style=color:#ff7b72;font-weight:700>()</span>
</span></span><span style=display:flex><span>      df<span style=color:#ff7b72;font-weight:700>.</span>write<span style=color:#ff7b72;font-weight:700>.</span>mode<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#f0883e;font-weight:700>SaveMode</span><span style=color:#ff7b72;font-weight:700>.</span><span style=color:#f0883e;font-weight:700>Append</span><span style=color:#ff7b72;font-weight:700>).</span>json<span style=color:#ff7b72;font-weight:700>(</span><span style=color:#a5d6ff>&#34;hdfs://192.168.2.101:9000/user&#34;</span><span style=color:#ff7b72;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72;font-weight:700>})</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ssc<span style=color:#ff7b72;font-weight:700>.</span>start<span style=color:#ff7b72;font-weight:700>()</span>
</span></span><span style=display:flex><span>    ssc<span style=color:#ff7b72;font-weight:700>.</span>awaitTermination<span style=color:#ff7b72;font-weight:700>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#ff7b72;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>}</span>
</span></span></code></pre></div><p>特别注意版本:</p><p>pom.xml</p><div class=highlight><pre tabindex=0 style=color:#c9d1d9;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#7ee787>&lt;properties&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#7ee787>&lt;spark.version&gt;</span>2.3.0<span style=color:#7ee787>&lt;/spark.version&gt;</span>
</span></span><span style=display:flex><span><span style=color:#7ee787>&lt;/properties&gt;</span>
</span></span><span style=display:flex><span><span style=color:#7ee787>&lt;dependencies&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#7ee787>&lt;dependency&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#7ee787>&lt;groupId&gt;</span>org.apache.spark<span style=color:#7ee787>&lt;/groupId&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#7ee787>&lt;artifactId&gt;</span>spark-streaming_2.11<span style=color:#7ee787>&lt;/artifactId&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#7ee787>&lt;version&gt;</span>${spark.version}<span style=color:#7ee787>&lt;/version&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#7ee787>&lt;/dependency&gt;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#7ee787>&lt;dependency&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#7ee787>&lt;groupId&gt;</span>org.apache.spark<span style=color:#7ee787>&lt;/groupId&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#7ee787>&lt;artifactId&gt;</span>spark-streaming-kafka-0-10_2.11<span style=color:#7ee787>&lt;/artifactId&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#7ee787>&lt;version&gt;</span>${spark.version}<span style=color:#7ee787>&lt;/version&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#7ee787>&lt;/dependency&gt;</span>
</span></span><span style=display:flex><span><span style=color:#7ee787>&lt;/dependencies&gt;</span>
</span></span></code></pre></div><blockquote><p><a href=https://blog.csdn.net/zhaolq1024/article/details/85685189 target=_blank rel=noopener>https://blog.csdn.net/zhaolq1024/article/details/85685189</a></p><p><a href=https://blog.51cto.com/simplelife/2311296 target=_blank rel=noopener>https://blog.51cto.com/simplelife/2311296</a></p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://xiaokunji.com/zh/tags/spark.html>Spark</a></li><li><a href=https://xiaokunji.com/zh/tags/hadoop%E7%94%9F%E6%80%81%E5%9C%88.html>Hadoop生态圈</a></li></ul><nav class=paginav><a class=prev href=https://xiaokunji.com/zh/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/sparksql.html><span class=title>« 上一页</span><br><span>SparkSQL</span></a>
<a class=next href=https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/spi.html><span class=title>下一页 »</span><br><span>SPI</span></a></nav></footer></article></main><footer class=footer><span>Copyright
&copy;
-2023
<a href=https://xiaokunji.com/zh/ style=color:#939393>米二</a>
All Rights Reserved</span>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<span id=busuanzi_container><span class="fa fa-user">用户数:</span><span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye">访问数:</span><span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>