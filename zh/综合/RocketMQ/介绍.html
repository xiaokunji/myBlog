<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>介绍 | 米二</title><meta name=keywords content=" 1. 简介, 2. 组成, 3. 生产者, 3.1 三种发送消息方式, 4. 消费者, 5. broker, 5.1 动态、增减 Broker, 5.2 消息存储, 5.2.1 消息存储整体架构, 5.2.2 页缓存与内存映射, 6. topic, 5. RocketMQ 集群部署模式,  A&amp;Q"><meta name=description content="     "><meta name=author content="xkj"><link rel=canonical href=https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/RocketMQ/%E4%BB%8B%E7%BB%8D.html><link crossorigin=anonymous href=/assets/css/stylesheet.b3faf608c544858ba700943ffe182cb647f38432d29a07d73234965beacb26f6.css integrity="sha256-s/r2CMVEhYunAJQ//hgstkfzhDLSmgfXMjSWW+rLJvY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=16x16 href=https://xiaokunji.com/img/Q.svg><link rel=icon type=image/png sizes=32x32 href=https://xiaokunji.com/img/Q.svg><link rel=apple-touch-icon href=https://xiaokunji.com/Q.svg><link rel=mask-icon href=https://xiaokunji.com/Q.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/RocketMQ/%E4%BB%8B%E7%BB%8D.html><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="介绍"><meta property="og:description" content="     "><meta property="og:type" content="article"><meta property="og:url" content="https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/RocketMQ/%E4%BB%8B%E7%BB%8D.html"><meta property="article:section" content="综合"><meta property="article:published_time" content="2023-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-04T06:32:51+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="介绍"><meta name=twitter:description content="     "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"介绍","item":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/RocketMQ/%E4%BB%8B%E7%BB%8D.html"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"介绍","name":"介绍","description":"     ","keywords":[" 1. 简介"," 2. 组成"," 3. 生产者"," 3.1 三种发送消息方式"," 4. 消费者"," 5. broker"," 5.1 动态、增减 Broker"," 5.2 消息存储"," 5.2.1 消息存储整体架构"," 5.2.2 页缓存与内存映射"," 6. topic"," 5. RocketMQ 集群部署模式","  A\u0026Q"],"articleBody":"[toc]\n1. 简介 RocketMQ是一个纯Java、分布式、队列模型的开源消息中间件，前身是MetaQ，是阿里参考Kafka特点研发的一个队列模型的消息中间件，后开源给apache基金会成为了apache的顶级开源项目，具有高性能、高可靠、高实时、分布式特点\n2. 组成 他主要有四大核心组成部分：NameServer、Broker、Producer以及Consumer四部分。\nRocketMQ架构上主要分为四部分，如上图所示:\nProducer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳\nConsumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳. Consumer既可以从Master订阅消息，也可以从Slave订阅消息. Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。\nNameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。\n​\t主要包括两个功能：\nBroker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；\n路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。\nNameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。\nBrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。\nRemoting Module：整个Broker的实体，负责处理来自clients端的请求。\nClient Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息\nStore Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。\nHA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。\nIndex Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。\n集群工作流程：\n启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。\nBroker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。\n收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。\n如果是自动创建主题, 则会沿用默认主题TBW102的配置,它在哪,自动创建的主题就在哪\nRocketMQ自动创建topic - 简书 (jianshu.com) Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。\nConsumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。\nNameServer：\n主要负责对于源数据的管理，包括了对于Topic和路由信息的管理。\nNameServer压力不会太大，平时主要开销是在维持心跳和提供Topic-Broker的关系数据。\n但有一点需要注意，Broker向NameServer发心跳时， 会带上当前自己所负责的所有Topic信息，如果Topic个数太多（万级别），会导致一次心跳中，就Topic的数据就几十M，网络情况差的话， 网络传输失败，心跳失败，导致NameServer误认为Broker心跳失败。\nNameServer 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。\n每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。\nProducer\n消息生产者，负责产生消息，一般由业务系统负责产生消息。\nRocketMQ 提供了三种方式发送消息：同步、异步和单向\n同步发送：同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。\n异步发送：异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。\n单向发送：单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。\nBroker\n消息中转角色，负责存储消息，转发消息。\nBroker是具体提供业务的服务器，单个Broker节点与所有的NameServer节点保持长连接及心跳，并会定时将Topic信息注册到NameServer，顺带一提底层的通信和连接都是基于Netty实现的。\nBroker负责消息存储，以Topic为纬度支持轻量级的队列，单机可以支撑上万队列规模，支持消息推拉模型。\nConsumer\n消息消费者，负责消费消息，一般是后台系统负责异步消费。\nConsumer也由用户部署，支持PUSH和PULL两种消费模式，支持集群消费和广播消息，提供实时的消息订阅机制。\nPull：拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。(默认20s拉取一次)\nPush：推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。\npush 方式其本质是加快版的pull(长轮询), 从结果来看就像是push一样\n消息领域模型\nMessage\nMessage（消息）就是要传输的信息。\n一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题。\nTopic\nTopic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。\n一个 Topic 也可以被 0个、1个、多个消费者订阅。\nTag\nTag（标签）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。\n标签有助于保持您的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。\nGroup\n分组，一个组可以订阅多个Topic。\n分为ProducerGroup，ConsumerGroup，代表某一类的生产者和消费者，一般来说同一个服务可以作为Group，同一个Group一般来说发送和消费的消息都是一样的\nQueue\n在Kafka中叫Partition，每个Queue内部是有序的，在RocketMQ中分为读和写两种队列，一般来说读写队列数量一致，如果不一致就会出现很多问题。\n读写队列，则是在做路由信息时使用。在消息发送时，使用写队列个数返回路由信息，而消息消费时按照读队列个数返回路由信息。在物理文件层面，只有写队列才会创建文件。举个例子：写队列个数是8，设置的读队列个数是4.这个时候，会创建8个文件夹，代表0 1 2 3 4 5 6 7，但在消息消费时，路由信息只返回4，在具体拉取消息时，就只会消费0 1 2 3这4个队列中的消息，4 5 6 7中的信息压根就不会被消费。反过来，如果写队列个数是4，读队列个数是8，在生产消息时只会往0 1 2 3中生产消息，消费消息时则会从0 1 2 3 4 5 6 7所有的队列中消费，当然 4 5 6 7中压根就没有消息 ，假设消费group有两个消费者，事实上只有第一个消费者在真正的消费消息(0 1 2 3)，第二个消费者压根就消费不到消息。由此可见，只有readQueueNums\u003e=writeQueueNums,程序才能正常进行\n简单来说就是, 读队列表示消费的数量, 写队列表示生产的数量\nrocketmq设置读写队列数的目的在于方便队列的缩容和扩容。思考一个问题，一个topic在每个broker上创建了128个队列，现在需要将队列缩容到64个，怎么做才能100%不会丢失消息，并且无需重启应用程序？\n最佳实践：先缩容写队列128-\u003e64，写队列由0 1 2 ……127缩至 0 1 2 ……..63。等到64 65 66……127中的消息全部消费完后，再缩容读队列128-\u003e64.(同时缩容写队列和读队列可能会导致部分消息未被消费)\nrocketmq中的读写队列_八荒六合唯我独尊-CSDN博客_rocketmq读写队列 Message Queue\nMessage Queue（消息队列），主题被划分为一个或多个子主题，即消息队列。\n一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。\n消息的物理管理单位。一个Topic下可以有多个Queue，Queue的引入使得消息的存储可以分布式集群化，具有了水平扩展能力。\ntopic下有消息队列, 消息队列又分为读/写队列, 应该是这样吧?????\nOffset\n在RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用Offset 来访问，Offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限。\n也可以认为 Message Queue 是一个长度无限的数组，Offset 就是下标。\n消息消费模式\n消息消费模式有两种：Clustering（集群消费）和Broadcasting（广播消费）。\n默认情况下就是集群消费，该模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。\n而广播消费消息会发给消费者组中的每一个消费者进行消费。\nMessage Order\nMessage Order（消息顺序）有两种：Orderly（顺序消费）和Concurrently（并行消费）。\n顺序消费表示消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列。\n并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。\n3. 生产者 3.1 三种发送消息方式 rocketmq支持三种发送消息的方式，分别是同步发送（sync），异步发送（async）和直接发送（oneway）\n同步发送 sync 发送消息采用同步模式，这种方式只有在消息完全发送完成之后才返回结果，此方式存在需要同步等待发送结果的时间代价。\n这种方式具有内部重试机制，即在主动声明本次消息发送失败之前，内部实现将重试一定次数，默认为2次（DefaultMQProducer＃getRetryTimesWhenSendFailed）。 发送的结果存在同一个消息可能被多次发送给给broker，这里需要应用的开发者自己在消费端处理幂等性问题。\n异步发送 async 发送消息采用异步发送模式，消息发送后立刻返回，当消息完全完成发送后，会调用回调函数sendCallback来告知发送者本次发送是成功或者失败。异步模式通常用于响应时间敏感业务场景，即承受不了同步发送消息时等待返回的耗时代价。\n同同步发送一样，异步模式也在内部实现了重试机制，默认次数为2次（DefaultMQProducer#getRetryTimesWhenSendAsyncFailed）。发送的结果同样存在同一个消息可能被多次发送给给broker，需要应用的开发者自己在消费端处理幂等性问题。\n直接发送 one-way 采用one-way发送模式发送消息的时候，发送端发送完消息后会立即返回，不会等待来自broker的ack来告知本次消息发送是否完全完成发送。这种方式吞吐量很大，但是存在消息丢失的风险，所以其适用于不重要的消息发送，比如日志收集。\n4. 消费者 5. broker 5.1 动态、增减 Broker 扩容\n对集群进行扩容的时候，可以动态增加 Broker 角色的机器 。 只增加 Broker 不会对原有的 Topic 产生影响，原来创建好的 Topic 中数据的读写依然在原来的那些 Broker 上进行 。\n集群扩容后,\n一种是可以把新建的 Topic 指定到新的 Broker 机器上，均衡利用资源；\n另一种方式是通过 updateTopic 命令更改现有的 Topic 配置，在新加的 Broker 上创建新的队列 。 比如 TestTopic 是现有的一个 Topic ，因为数据量增大需要扩容，新增的一个 Broker 机器地址是 192 . 168.0.1:10911 ，这个时候执行下面的命令： sh ./bin/mqadmin updateTopic -b 192.168.0.1:10911 -t TestTopic -n 192.168.0.100:9876 ，结果是在新增的 Broker 机器上，为 TestTopic 新创建了 8个读写队列 。\n缩容\n当某个 Topic 有多个 Master Broker，停了其中一个，这时候是否会丢失消息呢？\n答案和 Producer 使用的发送消息的方式有关，\n如果使用同步方式 send ( msg ）发送，在DefaultMQProducer 内部有个自动重试逻辑，其中一个 Broker 停了，会自动向另一个 Broker 发消息，不会发生丢消息现象。\n如果使用异步方式发送 send ( msg, callback ），或者用 sendOneWay 方式，会丢失切换过程中的消息 。\n因为在异步和 sendOneWay 这两种发送方式下，Producer.setRetryTimesWhensendFailed 设置不起作用，发送失败不会重试 。DefaultMQProducer 默认每 30 秒到 NameServer 请求最新的路由消息， Producer如果获取不到已停止的 Broker 下的队列信息，后续就自动不再向这些队列发送消息 。\n5.2 消息存储 5.2.1 消息存储整体架构 消息存储架构图中主要有下面三个跟消息存储相关的文件构成。\n(1) CommitLog：消息主体以及元数据的存储主体，存储Producer端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G, 文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件；\n(2) ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能，由于RocketMQ是基于主题topic的订阅模式，消息消费是针对主题进行的，如果要遍历commitlog文件中根据topic检索消息是非常低效的。Consumer即可根据ConsumeQueue来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引,而不是存储生产者的消息本身，保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量offset(8字节)，消息大小size(4字节)和消息Tag的HashCode值(8字节)。consumequeue文件可以看成是基于topic的commitlog索引文件，故consumequeue文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样consumequeue文件采取定长设计，每一个条目共20个字节，分别为8字节的commitlog物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个ConsumeQueue文件大小约5.72M；\n(3) IndexFile：IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。Index文件的存储位置是：$HOME \\store\\index${fileName}，文件名fileName是以创建时的时间戳命名的，固定的单个IndexFile文件大小约为400M，一个IndexFile可以保存 2000W个索引，IndexFile的底层存储设计为在文件系统中实现HashMap结构，故rocketmq的索引文件其底层实现为hash索引。\nindexFile 这个文件用来加快消息查询的速度 。 按照Message Key查询消息就会用到这个文件\nindexFile落盘并不是采取定时刷盘机制，而是每更新一次索引文件就会将上一次的改动刷写到磁盘 。 见文件org.apache.rocketmq.store.index.IndexService#flush\n在上面的RocketMQ的消息存储整体架构图中可以看出，RocketMQ采用的是混合型的存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。RocketMQ的混合型存储结构(多个Topic的消息实体内容都存储于一个CommitLog中)针对Producer和Consumer分别采用了数据和索引部分相分离的存储结构，Producer发送消息至Broker端，然后Broker端使用同步或者异步的方式对消息刷盘持久化，保存至CommitLog中。只要消息被刷盘持久化至磁盘文件CommitLog中，那么Producer发送的消息就不会丢失。正因为如此，Consumer也就肯定有机会去消费这条消息。当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，如果一个消息拉取请求未拉取到消息，Broker允许等待30s的时间，只要这段时间内有新消息到达，将直接返回给消费端。这里，RocketMQ的具体做法是，使用Broker端的后台服务线程—ReputMessageService不停地分发请求并异步构建ConsumeQueue（逻辑消费队列）和IndexFile（索引文件）数据。\n5.2.2 页缓存与内存映射 页缓存（PageCache)是OS对文件的缓存，用于加速对文件的读写。一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于OS使用PageCache机制对读写访问操作进行了性能优化，将一部分的内存用作PageCache。对于数据的写入，OS会先写入至Cache内，随后通过异步的方式由pdflush内核线程将Cache内的数据刷盘至物理磁盘上。对于数据的读取，如果一次读取文件时出现未命中PageCache的情况，OS从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取。\n在RocketMQ中，ConsumeQueue逻辑消费队列存储的数据较少，并且是顺序读取，在page cache机制的预读取作用下，Consume Queue文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。而对于CommitLog消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取，严重影响性能。如果选择合适的系统IO调度算法，比如设置调度算法为“Deadline”（此时块存储采用SSD的话），随机读的性能也会有所提升。\n另外，RocketMQ主要通过MappedByteBuffer对文件进行读写操作。其中，利用了NIO中的FileChannel模型将磁盘上的物理文件直接映射到用户态的内存地址中（这种Mmap的方式减少了传统IO将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（正因为需要使用内存映射机制，故RocketMQ的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存）。\n6. topic 一个topic可以分布在多broker上, 所以只有多个broker合起来才是一个完整的topic内容\n5. RocketMQ 集群部署模式 0. 单 master 模式\n​ 也就是只有一个 master 节点，称不上是集群，一旦这个 master 节点宕机，那么整个服务就不可用，适合个人学习使用。\n多 master 模式 多个 master 节点组成集群，单个 master 节点宕机或者重启对应用没有影响。 优点：所有模式中性能最高 缺点：单个 master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息的实时性就受到影响。 注意：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 queue 应该分布在集群中各个节点，而不是只在某各节点上，否则，该节点宕机会对订阅该 topic 的应用造成影响。 多 master 多 slave 异步复制模式 在多 master 模式的基础上，每个 master 节点都有至少一个对应的 slave。master 节点可读可写，但是 slave 只能读不能写，类似于 mysql 的主备模式。 优点： 在 master 宕机时，消费者可以从 slave 读取消息，消息的实时性不会受影响，性能几乎和多 master 一样。 缺点：使用异步复制的同步方式有可能会有消息丢失的问题。 多 master 多 slave 同步双写模式 同多 master 多 slave 异步复制模式类似，区别在于 master 和 slave 之间的数据同步方式。 优点：同步双写的同步模式能保证数据不丢失。 缺点：发送单个消息 RT 会略长，性能相比异步复制低10%左右。 刷盘策略：同步刷盘和异步刷盘（指的是节点自身数据是同步还是异步存储） 同步方式：同步双写和异步复制（指的一组 master 和 slave 之间数据的同步） 注意：要保证数据可靠，需采用同步刷盘和同步双写的方式，但性能会较其他方式低。 几个核心参数之间可按需组合 =\u003e master和slave的数量/ 刷盘策略/ master和salve之间的同步方式\nA\u0026Q commitLog的消息是怎么同步到consumerQueue中的?\nRocketMQ 通过开启 一个线程 ReputMessageServcie 来准实时转发 CommitLog 文件更新事件 ， 相应 的任务处理器根据转发的消息及时更新 ConsumeQueue 、 IndexFile 文件 。broker启动时就会启动同步线程,\nbroker启动后会取 math.max(所有队列中物理偏移量,commitLog低水位), 得应该从哪个位置开始追加队列消息 然后就会启动线程, 每1毫秒执行一次 从commitLog里取出消息来,通过调用 doDispatch方法 。 最终将分别调用 CommitLogDispatcherBuildConsumeQueue （构建消息消费队列 ）、CommitLogDispatcherBuildlndex （构建索引文件） 消息消 费 队列转发任务实现类为 ： CommitLogDispatcherBuildConsumeQueue ，内部最终将调用 putMessagePositioninfo 方法 , 根据消息 主题与 队列 ID ，先获取对应的 ConumeQueue, 依 次将消息偏移量、消息长度、 tag的 hashcode 写入到 ByteBuffer 中，并根据 consumeQueueOffset 计算 ConumeQu eue 中的物理地址，将内 容追加到 ConsumeQueue 的内存映射文件中（本操作只追击并不刷盘 ）， ConumeQueue 的刷盘方式固定为异步刷盘模式 总结, broker启动后就会启动监听线程,每1毫秒执行一次, 把消息从commitlog取出来,分别去构建consumerQueue和indexFile,\n构建consumerQueue的消息写到MappedByteBuffer中(MappedFile类),只写内存不刷盘\nbroker 的高可用\n《浅入浅出》-RocketMQ - 知乎 (zhihu.com) rocketmq/docs/cn · apache/rocketmq (github.com) ","wordCount":"9947","inLanguage":"zh","datePublished":"2023-08-22T00:00:00Z","dateModified":"2023-09-04T06:32:51.416893927Z","author":{"@type":"Person","name":"xkj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/RocketMQ/%E4%BB%8B%E7%BB%8D.html"},"publisher":{"@type":"Organization","name":"米二","logo":{"@type":"ImageObject","url":"https://xiaokunji.com/img/Q.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaokunji.com/zh/ accesskey=h title="米二 (Alt + H)"><img src=https://xiaokunji.com/img/Q.svg alt aria-label=logo height=35>米二</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaokunji.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://xiaokunji.com/zh/ title=🏠主页><span>🏠主页</span></a></li><li><a href=https://xiaokunji.com/zh/search title="🔍搜索 (Alt + /)" accesskey=/><span>🔍搜索</span></a></li><li><a href=https://xiaokunji.com/zh/post.html title=📚文章><span>📚文章</span></a></li><li><a href=https://xiaokunji.com/zh/archives.html title=⏱时间轴><span>⏱时间轴</span></a></li><li><a href=https://xiaokunji.com/zh/tags.html title=🔖标签><span>🔖标签</span></a></li><li><a href=https://xiaokunji.com/zh/categories.html title=📖分类><span>📖分类</span></a></li><li><a href=https://xiaokunji.com/zh/links.html title=🤝友链><span>🤝友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><nav aria-label=breadcrumb><ul><a href=https://xiaokunji.com/zh/>🏠</a> <span>></span>
<a href=https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88.html>综合</a> <span>></span>
<a href=https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/RocketMQ.html>RocketMQ</a> <span>></span></ul></nav><h1 class=post-title>介绍</h1><div class=post-description></div><div class=post-meta>创建:&nbsp;<span title='2023-08-22 00:00:00 +0000 UTC'>2023-08-22</span>&nbsp;·&nbsp;更新:&nbsp;2023-09-04&nbsp;·&nbsp;xkj
&nbsp;|&nbsp;分类: &nbsp;<ul class=post-categories-meta><a href=https://xiaokunji.com/zh/categories/%E7%BB%BC%E5%90%88.html>综合</a></ul><span id=busuanzi_container_page_pv>&nbsp;| 访问: <span id=busuanzi_value_page_pv>1</span></span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#1-%e7%ae%80%e4%bb%8b aria-label="1. 简介">1. 简介</a></li><li><a href=#2-%e7%bb%84%e6%88%90 aria-label="2. 组成">2. 组成</a></li><li><a href=#3-%e7%94%9f%e4%ba%a7%e8%80%85 aria-label="3. 生产者">3. 生产者</a><ul><li><a href=#31-%e4%b8%89%e7%a7%8d%e5%8f%91%e9%80%81%e6%b6%88%e6%81%af%e6%96%b9%e5%bc%8f aria-label="3.1 三种发送消息方式">3.1 三种发送消息方式</a></li></ul></li><li><a href=#4-%e6%b6%88%e8%b4%b9%e8%80%85 aria-label="4. 消费者">4. 消费者</a></li><li><a href=#5-broker aria-label="5. broker">5. broker</a><ul><li><a href=#51-%e5%8a%a8%e6%80%81%e5%a2%9e%e5%87%8f-broker aria-label="5.1 动态、增减 Broker">5.1 动态、增减 Broker</a></li><li><a href=#52-%e6%b6%88%e6%81%af%e5%ad%98%e5%82%a8 aria-label="5.2 消息存储">5.2 消息存储</a><ul><li><a href=#521-%e6%b6%88%e6%81%af%e5%ad%98%e5%82%a8%e6%95%b4%e4%bd%93%e6%9e%b6%e6%9e%84 aria-label="5.2.1 消息存储整体架构">5.2.1 消息存储整体架构</a></li><li><a href=#522-%e9%a1%b5%e7%bc%93%e5%ad%98%e4%b8%8e%e5%86%85%e5%ad%98%e6%98%a0%e5%b0%84 aria-label="5.2.2 页缓存与内存映射">5.2.2 页缓存与内存映射</a></li></ul></li></ul></li><li><a href=#6-topic aria-label="6. topic">6. topic</a></li><li><a href=#5-rocketmq-%e9%9b%86%e7%be%a4%e9%83%a8%e7%bd%b2%e6%a8%a1%e5%bc%8f aria-label="5. RocketMQ 集群部署模式">5. RocketMQ 集群部署模式</a></li><li><a href=#aq aria-label=A&amp;amp;Q>A&amp;Q</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>[toc]</p><h1 id=1-简介>1. 简介<a hidden class=anchor aria-hidden=true href=#1-简介>#</a></h1><p>RocketMQ是一个纯Java、分布式、队列模型的开源消息中间件，前身是MetaQ，是阿里参考Kafka特点研发的一个队列模型的消息中间件，后开源给apache基金会成为了apache的顶级开源项目，具有高性能、高可靠、高实时、分布式特点</p><h1 id=2-组成>2. 组成<a hidden class=anchor aria-hidden=true href=#2-组成>#</a></h1><p>他主要有四大核心组成部分：NameServer、Broker、Producer以及Consumer四部分。</p><p><img loading=lazy src=https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_architecture_1.png alt=img></p><p>RocketMQ架构上主要分为四部分，如上图所示:</p><ol><li><p>Producer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳</p></li><li><p>Consumer：消息消费的角色，支持分布式集群方式部署。支持<u>以push推，pull拉两种模式</u>对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。Consumer与NameServer集群中的其中一个节点（随机选择）建立<strong>长连接</strong>，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳. Consumer既可以从Master订阅消息，也可以从Slave订阅消息. Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。</p></li><li><p>NameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。</p></li></ol><p>​ 主要包括两个功能：</p><ul><li><p>Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；</p></li><li><p>路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。</p><p><u>NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯</u>。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。</p></li></ul><ol start=4><li><p>BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。</p><ul><li><p>Remoting Module：整个Broker的实体，负责处理来自clients端的请求。</p></li><li><p>Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息</p></li><li><p>Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。</p></li><li><p>HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。</p></li><li><p>Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。</p></li></ul></li></ol><p><img loading=lazy src=https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_architecture_2.png alt=img></p><p>集群工作流程：</p><ul><li><p>启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。</p></li><li><p>Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。</p></li><li><p>收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。</p><blockquote><p>如果是自动创建主题, 则会沿用默认主题<code>TBW102</code>的配置,它在哪,自动创建的主题就在哪</p><p><a href=https://www.jianshu.com/p/c8fd57a7f741 target=_blank rel=noopener>RocketMQ自动创建topic - 简书 (jianshu.com)</a></p></blockquote></li><li><p>Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。</p></li><li><p>Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。</p></li></ul><p><strong>NameServer：</strong></p><p>主要负责对于源数据的管理，包括了对于Topic和路由信息的管理。</p><blockquote><p>NameServer压力不会太大，平时主要开销是在维持心跳和提供Topic-Broker的关系数据。</p><p>但有一点需要注意，Broker向NameServer发心跳时， 会带上当前自己所负责的所有Topic信息，如果Topic个数太多（万级别），会导致一次心跳中，就Topic的数据就几十M，网络情况差的话， 网络传输失败，心跳失败，导致NameServer误认为Broker心跳失败。</p></blockquote><p>NameServer 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。</p><p>每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。</p><p><strong>Producer</strong></p><p>消息生产者，负责产生消息，一般由业务系统负责产生消息。</p><p>RocketMQ 提供了三种方式发送消息：同步、异步和单向</p><ul><li><p>同步发送：同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。</p></li><li><p>异步发送：异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。</p></li><li><p>单向发送：单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。</p></li></ul><p><strong>Broker</strong></p><p>消息中转角色，负责存储消息，转发消息。</p><p>Broker是具体提供业务的服务器，单个Broker节点与所有的NameServer节点保持长连接及心跳，并会定时将Topic信息注册到NameServer，顺带一提底层的通信和连接都是基于Netty实现的。</p><p>Broker负责消息存储，以Topic为纬度支持轻量级的队列，单机可以支撑上万队列规模，支持消息推拉模型。</p><p><strong>Consumer</strong></p><p>消息消费者，负责消费消息，一般是后台系统负责异步消费。</p><p>Consumer也由用户部署，支持PUSH和PULL两种消费模式，支持集群消费和广播消息，提供实时的消息订阅机制。</p><ul><li><p>Pull：拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。(默认20s拉取一次)</p></li><li><p>Push：推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。</p><blockquote><p>push 方式其本质是加快版的pull(长轮询), 从结果来看就像是push一样</p></blockquote></li></ul><p><strong>消息领域模型</strong></p><p><img loading=lazy src=https://pic3.zhimg.com/v2-2edf1f6f40002a349690a9754f52fa5a_r.jpg alt=img></p><p><strong>Message</strong></p><p>Message（消息）就是要传输的信息。</p><p>一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题。</p><p><strong>Topic</strong></p><p>Topic 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。</p><p>一个 Topic 也可以被 0个、1个、多个消费者订阅。</p><p><strong>Tag</strong></p><p>Tag（标签）可以看作子主题，它是消息的第二级类型，用于为用户提供额外的灵活性。使用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识。比如交易消息又可以分为：交易创建消息、交易完成消息等，一条消息可以没有 Tag 。</p><p>标签有助于保持您的代码干净和连贯，并且还可以为 RocketMQ 提供的查询系统提供帮助。</p><p><strong>Group</strong></p><p>分组，一个组可以订阅多个Topic。</p><p>分为ProducerGroup，ConsumerGroup，代表某一类的生产者和消费者，一般来说同一个服务可以作为Group，同一个Group一般来说发送和消费的消息都是一样的</p><p><strong>Queue</strong></p><p>在Kafka中叫Partition，每个Queue内部是有序的，在RocketMQ中分为读和写两种队列，一般来说读写队列数量一致，如果不一致就会出现很多问题。</p><blockquote><p>读写队列，则是在做路由信息时使用。在消息发送时，使用写队列个数返回路由信息，而消息消费时按照读队列个数返回路由信息。在物理文件层面，只有写队列才会创建文件。举个例子：写队列个数是8，设置的读队列个数是4.这个时候，会创建8个文件夹，代表0 1 2 3 4 5 6 7，但在消息消费时，路由信息只返回4，在具体拉取消息时，就只会消费0 1 2 3这4个队列中的消息，4 5 6 7中的信息压根就不会被消费。反过来，如果写队列个数是4，读队列个数是8，在生产消息时只会往0 1 2 3中生产消息，消费消息时则会从0 1 2 3 4 5 6 7所有的队列中消费，当然 4 5 6 7中压根就没有消息 ，假设消费group有两个消费者，事实上只有第一个消费者在真正的消费消息(0 1 2 3)，第二个消费者压根就消费不到消息。由此可见，只有readQueueNums>=writeQueueNums,程序才能正常进行</p><blockquote><p>简单来说就是, 读队列表示消费的数量, 写队列表示生产的数量</p></blockquote><p>rocketmq设置读写队列数的目的在于方便队列的缩容和扩容。思考一个问题，一个topic在每个broker上创建了128个队列，现在需要将队列缩容到64个，怎么做才能100%不会丢失消息，并且无需重启应用程序？</p><p><u>最佳实践</u>：先缩容写队列128->64，写队列由0 1 2 &mldr;&mldr;127缩至 0 1 2 &mldr;&mldr;..63。等到64 65 66&mldr;&mldr;127中的消息全部消费完后，再缩容读队列128->64.(同时缩容写队列和读队列可能会导致部分消息未被消费)</p><p><a href=https://blog.csdn.net/qian_348840260/article/details/108975241 target=_blank rel=noopener>rocketmq中的读写队列_八荒六合唯我独尊-CSDN博客_rocketmq读写队列</a></p></blockquote><p><strong>Message Queue</strong></p><p>Message Queue（消息队列），主题被划分为一个或多个子主题，即消息队列。</p><p>一个 Topic 下可以设置多个消息队列，发送消息时执行该消息的 Topic ，RocketMQ 会轮询该 Topic 下的所有队列将消息发出去。</p><p>消息的物理管理单位。一个Topic下可以有多个Queue，Queue的引入使得消息的存储可以分布式集群化，具有了水平扩展能力。</p><blockquote><p>topic下有消息队列, 消息队列又分为读/写队列, 应该是这样吧?????</p></blockquote><p><strong>Offset</strong></p><p>在RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用Offset 来访问，Offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限。</p><p>也可以认为 Message Queue 是一个长度无限的数组，Offset 就是下标。</p><p><strong>消息消费模式</strong></p><p>消息消费模式有两种：Clustering（集群消费）和Broadcasting（广播消费）。</p><p>默认情况下就是集群消费，该模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。</p><p>而广播消费消息会发给消费者组中的每一个消费者进行消费。</p><p><strong>Message Order</strong></p><p>Message Order（消息顺序）有两种：Orderly（顺序消费）和Concurrently（并行消费）。</p><p>顺序消费表示消息消费的顺序同生产者为每个消息队列发送的顺序一致，所以如果正在处理全局顺序是强制性的场景，需要确保使用的主题只有一个消息队列。</p><p>并行消费不再保证消息顺序，消费的最大并行数量受每个消费者客户端指定的线程池限制。</p><h1 id=3-生产者>3. 生产者<a hidden class=anchor aria-hidden=true href=#3-生产者>#</a></h1><h2 id=31-三种发送消息方式>3.1 三种发送消息方式<a hidden class=anchor aria-hidden=true href=#31-三种发送消息方式>#</a></h2><p>rocketmq支持三种发送消息的方式，分别是同步发送（sync），异步发送（async）和直接发送（oneway）</p><ol><li><p><strong>同步发送 sync</strong>
发送消息采用同步模式，<u>这种方式只有在消息完全发送完成之后才返回结果</u>，此方式存在需要同步等待发送结果的时间代价。</p><p>这种方式具有内部重试机制，即在主动声明本次消息发送失败之前，内部实现将重试一定次数，默认为2次（<code>DefaultMQProducer＃getRetryTimesWhenSendFailed</code>）。 发送的结果存在同一个消息可能被多次发送给给broker，这里需要应用的开发者自己在消费端处理幂等性问题。</p></li><li><p><strong>异步发送 async</strong>
发送消息采用异步发送模式，<u>消息发送后立刻返回，当消息完全完成发送后，会调用回调函数sendCallback来告知发送者本次发送是成功或者失败</u>。异步模式通常用于响应时间敏感业务场景，即承受不了同步发送消息时等待返回的耗时代价。</p><p>同同步发送一样，异步模式也在内部实现了重试机制，默认次数为2次（<code>DefaultMQProducer#getRetryTimesWhenSendAsyncFailed</code>）。发送的结果同样存在同一个消息可能被多次发送给给broker，需要应用的开发者自己在消费端处理幂等性问题。</p></li><li><p><strong>直接发送 one-way</strong>
采用one-way发送模式发送消息的时候，<u>发送端发送完消息后会立即返回，不会等待来自broker的ack来告知本次消息发送是否完全完成发送</u>。这种方式吞吐量很大，但是存在消息丢失的风险，所以其适用于不重要的消息发送，比如日志收集。</p></li></ol><h1 id=4-消费者>4. 消费者<a hidden class=anchor aria-hidden=true href=#4-消费者>#</a></h1><h1 id=5-broker>5. broker<a hidden class=anchor aria-hidden=true href=#5-broker>#</a></h1><h2 id=51-动态增减-broker>5.1 动态、增减 Broker<a hidden class=anchor aria-hidden=true href=#51-动态增减-broker>#</a></h2><p><strong>扩容</strong></p><p>对集群进行扩容的时候，可以动态增加 Broker 角色的机器 。 只增加 Broker 不会对原有的 Topic 产生影响，原来创建好的 Topic 中数据的读写依然在原来的那些 Broker 上进行 。</p><p>集群扩容后,</p><p>一种是可以把新建的 Topic 指定到新的 Broker 机器上，均衡利用资源；</p><p>另一种方式是通过 updateTopic 命令更改现有的 Topic 配置，在新加的 Broker 上创建新的队列 。 比如 TestTopic 是现有的一个 Topic ，因为数据量增大需要扩容，新增的一个 Broker 机器地址是 192 . 168.0.1:10911 ，这个时候执行下面的命令： <code>sh ./bin/mqadmin updateTopic -b 192.168.0.1:10911 -t TestTopic -n 192.168.0.100:9876</code> ，结果是在新增的 Broker 机器上，为 TestTopic 新创建了 8个读写队列 。</p><p><strong>缩容</strong></p><p>当某个 Topic 有多个 Master Broker，停了其中一个，这时候是否会丢失消息呢？</p><p>答案和 Producer 使用的发送消息的方式有关，</p><ul><li><p>如果使用同步方式 send ( msg ）发送，在DefaultMQProducer 内部有个自动重试逻辑，其中一个 Broker 停了，会自动向另一个 Broker 发消息，不会发生丢消息现象。</p></li><li><p>如果使用异步方式发送 send ( msg, callback ），或者用 sendOneWay 方式，会丢失切换过程中的消息 。</p><blockquote><p>因为在异步和 sendOneWay 这两种发送方式下，Producer.setRetryTimesWhensendFailed 设置不起作用，发送失败不会重试 。DefaultMQProducer 默认每 30 秒到 NameServer 请求最新的路由消息， Producer如果获取不到已停止的 Broker 下的队列信息，后续就自动不再向这些队列发送消息 。</p></blockquote></li></ul><h2 id=52-消息存储>5.2 消息存储<a hidden class=anchor aria-hidden=true href=#52-消息存储>#</a></h2><h3 id=521-消息存储整体架构>5.2.1 消息存储整体架构<a hidden class=anchor aria-hidden=true href=#521-消息存储整体架构>#</a></h3><p><img loading=lazy src=https://github.com/apache/rocketmq/raw/master/docs/cn/image/rocketmq_design_1.png alt=img></p><p>消息存储架构图中主要有下面三个跟消息存储相关的文件构成。</p><p>(1) CommitLog：消息主体以及元数据的存储主体，存储Producer端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G, 文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件；</p><p>(2) ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能，由于RocketMQ是基于主题topic的订阅模式，消息消费是针对主题进行的，如果要遍历commitlog文件中根据topic检索消息是非常低效的。Consumer即可根据ConsumeQueue来查找待消费的消息。其中，<strong>ConsumeQueue（逻辑消费队列）作为消费消息的索引,而不是存储生产者的消息本身</strong>，保存了指定Topic下的队列消息在CommitLog中的<strong>起始物理偏移量offset</strong>(8字节)，<strong>消息大小size</strong>(4字节)和<strong>消息Tag的HashCode值</strong>(8字节)。consumequeue文件可以看成是基于topic的commitlog索引文件，故consumequeue文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样consumequeue文件采取定长设计，每一个条目共20个字节，分别为8字节的commitlog物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个ConsumeQueue文件大小约5.72M；</p><p>(3) IndexFile：IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。Index文件的存储位置是：$HOME \store\index${fileName}，文件名fileName是以创建时的时间戳命名的，固定的单个IndexFile文件大小约为400M，一个IndexFile可以保存 2000W个索引，IndexFile的底层存储设计为在文件系统中实现HashMap结构，故rocketmq的索引文件其底层实现为hash索引。</p><blockquote><p>indexFile 这个文件用来加快消息查询的速度 。 按照Message Key查询消息就会用到这个文件</p><p>indexFile落盘并不是采取定时刷盘机制，而是每更新一次索引文件就会将上一次的改动刷写到磁盘 。 见文件<code>org.apache.rocketmq.store.index.IndexService#flush</code></p></blockquote><p>在上面的RocketMQ的消息存储整体架构图中可以看出，RocketMQ采用的是混合型的存储结构，<strong>即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。RocketMQ的混合型存储结构(多个Topic的消息实体内容都存储于一个CommitLog中)针对Producer和Consumer分别采用了数据和索引部分相分离的存储结构，Producer发送消息至Broker端，然后Broker端使用同步或者异步的方式对消息刷盘持久化，保存至CommitLog中</strong>。只要消息被刷盘持久化至磁盘文件CommitLog中，那么Producer发送的消息就不会丢失。正因为如此，Consumer也就肯定有机会去消费这条消息。当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，如果一个消息拉取请求未拉取到消息，Broker允许等待30s的时间，只要这段时间内有新消息到达，将直接返回给消费端。这里，RocketMQ的具体做法是，使用Broker端的后台服务线程—ReputMessageService不停地分发请求并异步构建ConsumeQueue（逻辑消费队列）和IndexFile（索引文件）数据。</p><h3 id=522-页缓存与内存映射>5.2.2 页缓存与内存映射<a hidden class=anchor aria-hidden=true href=#522-页缓存与内存映射>#</a></h3><p>页缓存（PageCache)是OS对文件的缓存，用于加速对文件的读写。一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于OS使用PageCache机制对读写访问操作进行了性能优化，将一部分的内存用作PageCache。对于数据的写入，OS会先写入至Cache内，随后通过异步的方式由pdflush内核线程将Cache内的数据刷盘至物理磁盘上。对于数据的读取，如果一次读取文件时出现未命中PageCache的情况，OS从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取。</p><p>在RocketMQ中，ConsumeQueue逻辑消费队列存储的数据较少，并且是顺序读取，在page cache机制的预读取作用下，Consume Queue文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。而对于CommitLog消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取，严重影响性能。如果选择合适的系统IO调度算法，比如设置调度算法为“Deadline”（此时块存储采用SSD的话），随机读的性能也会有所提升。</p><p>另外，RocketMQ主要通过MappedByteBuffer对文件进行读写操作。其中，利用了NIO中的FileChannel模型将磁盘上的物理文件直接映射到用户态的内存地址中（这种Mmap的方式减少了传统IO将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（正因为需要使用内存映射机制，故RocketMQ的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存）。</p><h1 id=6-topic>6. topic<a hidden class=anchor aria-hidden=true href=#6-topic>#</a></h1><p><img loading=lazy src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDI1NzgwNC00ZWIxNDQ2MTcwYzFiYTE1LnBuZz9pbWFnZU1vZ3IyL2F1dG8tb3JpZW50L3N0cmlwfGltYWdlVmlldzIvMi93Lzg0My9mb3JtYXQvd2VicA?x-oss-process=image/format,png" alt=img></p><blockquote><p>一个topic可以分布在多broker上, 所以只有多个broker合起来才是一个完整的topic内容</p></blockquote><h1 id=5-rocketmq-集群部署模式>5. RocketMQ 集群部署模式<a hidden class=anchor aria-hidden=true href=#5-rocketmq-集群部署模式>#</a></h1><p><strong>0. 单 master 模式</strong></p><p>​ 也就是只有一个 master 节点，称不上是集群，一旦这个 master 节点宕机，那么整个服务就不可用，适合个人学习使用。</p><ol><li><strong>多 master 模式</strong>
多个 master 节点组成集群，单个 master 节点宕机或者重启对应用没有影响。
优点：所有模式中性能最高
缺点：单个 master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息的实时性就受到影响。
<strong>注意</strong>：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 queue 应该分布在集群中各个节点，而不是只在某各节点上，否则，该节点宕机会对订阅该 topic 的应用造成影响。</li><li><strong>多 master 多 slave 异步复制模式</strong>
在多 master 模式的基础上，每个 master 节点都有至少一个对应的 slave。master
节点可读可写，但是 slave 只能读不能写，类似于 mysql 的主备模式。
优点： 在 master 宕机时，消费者可以从 slave 读取消息，消息的实时性不会受影响，性能几乎和多 master 一样。
缺点：使用异步复制的同步方式有可能会有消息丢失的问题。</li><li><strong>多 master 多 slave 同步双写模式</strong>
同多 master 多 slave 异步复制模式类似，区别在于 master 和 slave 之间的数据同步方式。
优点：同步双写的同步模式能保证数据不丢失。
缺点：发送单个消息 RT 会略长，性能相比异步复制低10%左右。
刷盘策略：同步刷盘和异步刷盘（指的是节点自身数据是同步还是异步存储）
同步方式：同步双写和异步复制（指的一组 master 和 slave 之间数据的同步）
<strong>注意</strong>：要保证数据可靠，需采用同步刷盘和同步双写的方式，但性能会较其他方式低。</li></ol><blockquote><p>几个核心参数之间可按需组合 => master和slave的数量/ 刷盘策略/ master和salve之间的同步方式</p></blockquote><h1 id=aq>A&amp;Q<a hidden class=anchor aria-hidden=true href=#aq>#</a></h1><ol><li><p><strong>commitLog的消息是怎么同步到consumerQueue中的?</strong></p><p>RocketMQ 通过开启 一个线程 ReputMessageServcie 来准实时转发 CommitLog 文件更新事件 ， 相应 的任务处理器根据转发的消息及时更新 ConsumeQueue 、 IndexFile 文件 。broker启动时就会启动同步线程,</p><ol><li>broker启动后会取 <code>math.max(所有队列中物理偏移量,commitLog低水位)</code>, 得应该从哪个位置开始追加队列消息</li><li>然后就会启动线程, 每1毫秒执行一次</li><li>从commitLog里取出消息来,通过调用 doDispatch方法 。 最终将分别调用 CommitLogDispatcherBuildConsumeQueue （构建消息消费队列 ）、CommitLogDispatcherBuildlndex （构建索引文件）</li><li>消息消 费 队列转发任务实现类为 ： <code>CommitLogDispatcherBuildConsumeQueue</code> ，内部最终将调用 <code>putMessagePositioninfo</code> 方法 ,</li><li>根据消息 主题与 队列 ID ，先获取对应的 ConumeQueue,</li><li>依 次将消息偏移量、消息长度、 tag的 hashcode 写入到 ByteBuffer 中，并根据 consumeQueueOffset 计算 ConumeQu eue 中的物理地址，将内 容追加到 ConsumeQueue 的内存映射文件中（本操作只追击并不刷盘 ）， <strong>ConumeQueue 的刷盘方式固定为异步刷盘模式</strong></li></ol><blockquote><p>总结, broker启动后就会启动监听线程,每1毫秒执行一次, 把消息从commitlog取出来,分别去构建consumerQueue和indexFile,</p><p>构建consumerQueue的消息写到MappedByteBuffer中(<code>MappedFile</code>类),只写内存不刷盘</p></blockquote></li><li><p><strong>broker 的高可用</strong></p></li></ol><p><a href=https://zhuanlan.zhihu.com/p/94662788 target=_blank rel=noopener>《浅入浅出》-RocketMQ - 知乎 (zhihu.com)</a></p><p><a href=https://github.com/apache/rocketmq/tree/master/docs/cn target=_blank rel=noopener>rocketmq/docs/cn · apache/rocketmq (github.com)</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://xiaokunji.com/zh/tags/RocketMQ.html>RocketMQ</a></li><li><a href=https://xiaokunji.com/zh/tags/%E7%BB%BC%E5%90%88.html>综合</a></li></ul><nav class=paginav><a class=prev href=https://xiaokunji.com/zh/%E7%BB%BC%E5%90%88/Kubernetes/%E4%BB%8B%E7%BB%8D.html><span class=title>« 上一页</span><br><span>介绍</span></a>
<a class=next href=https://xiaokunji.com/zh/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/JVM/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8.html><span class=title>下一页 »</span><br><span>垃圾回收器</span></a></nav></footer></article></main><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><footer class=footer><span>Copyright
&copy;
-2023
<a href=https://xiaokunji.com/zh/ style=color:#939393>米二</a>
All Rights Reserved</span>
<span id=busuanzi_container><span class="fa fa-user">用户数:</span><span id=busuanzi_value_site_uv></span>
<span class="fa fa-eye">访问数:</span><span id=busuanzi_value_site_pv></span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>