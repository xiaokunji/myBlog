<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Hugo tranquilpeak theme</title><link>https://xiaokunji.github.io/myBlog/post/</link><description>Recent content in Posts on Hugo tranquilpeak theme</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://xiaokunji.github.io/myBlog/post/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/airflow/airflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/airflow/airflow/</guid><description>[toc]
1. 简介 Airflow是一个可编程，调度和监控的工作流平台，基于有向无环图(DAG)，airflow可以定义一组有依赖的任务，按照依赖依次执行。airflow提供了丰富的命令行工具用于系统管控，而其web管理界面同样也可以方便的管控调度任务，并且对任务运行状态进行实时监控，方便了系统的运维和管理。
Airflow 中常见的名词概念：
DAG
DAG 意为有向无循环图，在 Airflow 中则定义了整个完整的作业。同一个 DAG 中的所有 Task 拥有相同的调度时间。
Task
Task 为 DAG 中具体的作业任务，它必须存在于某一个 DAG 之中。Task 在 DAG 中配置依赖关系，跨 DAG 的依赖是可行的，但是并不推荐。跨 DAG 依赖会导致 DAG 图的直观性降低，并给依赖管理带来麻烦。
DAG Run
当一个 DAG 满足它的调度时间，或者被外部触发时，就会产生一个 DAG Run。可以理解为由 DAG 实例化的实例。
Task Instance
当一个 Task 被调度启动时，就会产生一个 Task Instance。可以理解为由 Task 实例化的实例。
2、Airflow 的服务构成 一个正常运行的 Airflow 系统一般由以下几个服务构成
WebServer
​ Airflow 提供了一个可视化的 Web 界面。启动 WebServer 后，就可以在 Web 界面上查看定义好的 DAG 并监控及改变运行状况。也可以在 Web 界面中对一些变量进行配置。
Worker
​ 一般来说我们用 Celery Worker 来执行具体的作业。Worker 可以部署在多台机器上，并可以分别设置接收的队列。当接收的队列中有作业任务时，Worker 就会接收这个作业任务，并开始执行。Airflow 会自动在每个部署 Worker 的机器上同时部署一个 Serve Logs 服务，这样我们就可以在 Web 界面上方便的浏览分散在不同机器上的作业日志了。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/flume%E5%92%8Cscribe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/flume%E5%92%8Cscribe/</guid><description>Scribe是Facebook开源的分布式日志搜集系统，架构简单，日志格式灵活，且支持异步发送消息和队列
对比项 Flume-NG Scribe 使用语言 Java c/c++ 容错性 Agent和Collector间，Collector和Store间都有容错性，且提供三种级别的可靠性保证； Agent和Collector间, Collector和Store之间有容错性； 负载均衡 Agent和Collector间，Collector和Store间有LoadBalance和Failover两种模式 无 可扩展性 好 好 Agent丰富程度 提供丰富的Agent，包括avro/thrift socket, text, tail等 主要是thrift端口 Store丰富程度 可以直接写hdfs, text, console, tcp；写hdfs时支持对text和sequence的压缩； 提供buffer, network, file(hdfs, text)等 代码结构 系统框架好，模块分明，易于开发 代码简单 来自* &amp;lt;http://www.aboutyun.com/thread-8317-1-1.html&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%8E%9F%E7%90%86/</guid><description>[toc]
1. flume是什么？ flume是分布式的，可靠的，高可用的，用于对不同来源的大量的日志数据进行有效收集、聚集和移动，并以集中式的数据存储的系统。
整个系统分为三层：Agent层(客户机)，Collector层(中心服务器)和Store层(存储服务器)。
来自* &amp;lt;http://www.aboutyun.com/thread-8317-1-1.html&amp;gt;
flume由agent组成,agent由source，channel，sink组成，Agent使用JVM 运行Flume
source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据, (数据来源)包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义
channel: source组件把数据收集来以后，临时存放在channel中,即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 它可以和任意数量的source和sink链接 来自 http://www.cnblogs.com/gongxijun/p/5656778.html
sink：sink组件是用于把数据发送到目的地的组件，从channel接受内容,目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 Thrift: thrift是一个软件框架，用来进行可扩展且跨语言的服务的开发(Facebook)。
Avro:Avro是一个数据序列化系统，设计用于支持大批量数据交换的应用(Apache),flume内置
event: 传输数据的基本单位。一个完整的event包括：event headers、event body、event信息(即文本文件中的单行记录)
event将传输的数据进行封装，如果是文本文件，通常是一行记录，
event也是事务的基本单位。event从source，流向channel，再到sink，
本身为一个字节数组并可携带headers(头信息)信息。
event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。
Flume提供了三种级别的可靠性保障，从强到弱依次分别为：
end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。）， Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送）， Best effort（数据发送到接收方后，不会进行确认） 来自 &amp;lt;http://www.cnblogs.com/oubo/archive/2012/05/25/2517751.html&amp;gt;
2. 关于flume启动 1)、flume组件启动顺序：channels——&amp;gt;sinks——&amp;gt;sources，关闭顺序：sources——&amp;gt;sinks——&amp;gt;channels；
2)、自动加载配置文件功能，会先关闭所有组件，再重启所有组件；
3)、关于AbstractConfigurationProvider中的Map&amp;lt;Class&amp;lt;? extends Channel&amp;gt;, Map&amp;lt;String, Channel&amp;raquo; channelCache这个对象，始终存储着agent中得所有channel对象，因为在动态加载时，channel中可能还有未消费完的数据，但是需要对channel重新配置，所以用以来缓存channel对象的所有数据及配置信息；
4)、通过在启动命令中添加&amp;quot;no-reload-conf&amp;quot;参数为true来取消自动加载配置文件功能；
来自* &amp;lt;http://www.cnblogs.com/lxf20061900/p/4012847.html&amp;gt;
3. flume的运行过程 ​ 把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。
为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),
待(全部)数据真正到达目的地(sink)后，flume在删除自己缓存的数据。
flume可以支持多级flume的agent,
即flume可以前后相继，例如sink可以将数据写到下一个agent的source中，这样的话就可以连成串了，可以整体处理了。flume还支持扇入(fan-in)、扇出(fan-out)。所谓扇入就是source可以接受多个输入，所谓扇出就是sink可以将数据输出多个目的地destination中。
来自* &amp;lt;http://blog.csdn.net/a2011480169/article/details/51544664&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E5%91%BD%E4%BB%A4/</guid><description>注:Flume框架对hadoop和zookeeper的依赖只是在jar包上，并不要求flume启动时必须将hadoop和zookeeper服务也启动
来自* &amp;lt;http://www.cnblogs.com/oubo/archive/2012/05/25/2517751.html&amp;gt;
启动flume服务器,需要用一个服务端的配置文件
flume-ng agent --conf conf --conf-file /mysoftware/flume-1.7.0/conf/flume-server.properties --name a1 -Dflume.root.logger=INFO,console &amp;gt; /mysoftware/flume-1.7.0/logs/flume-server.log 2&amp;gt;&amp;amp;1 &amp;amp; 启动flume客户端,需要用一个客户端的配置文件
flume-ng agent --conf conf --conf-file /mysoftware/flume-1.7.0/conf/flume-client.properties --name agent1 -Dflume.root.logger=INFO,console &amp;gt; /mysoftware/flume-1.7.0/logs/flume-client.log 2&amp;gt;&amp;amp;1 &amp;amp; 这里的用到了avro,貌似必须先启动服务器,后启动客户端(这里是从客户端发消息到服务端)
以下是两个例子:
注:文件均需自己创建!,两个配置文件
1. 从网络端口接收,存入HDFS
flume-server.properties: 服务端 (从avro端口到hdfs)
#set Agent name a1.sources = r1 a1.channels = c1 a1.sinks = k1 #set channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # other node,nna to nns source a1.sources.r1.type = avro a1.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/flume/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid><description>1. 安装 解压即可,可配置单机和集群版,将配置后的文件夹发到 从机 即可.
说是集群版,不是flume自己配的,几乎所有配置在自己写的配置文件中,自己配上 组 就可以关联到其他电脑组成集群
2. 配置 flume.env.sh :
export JAVA_HOME=/mysoftware/jdk1.8.0_101
当运行flume报错说找不到 org.apache.flume.tools.GetJavaProperty 类时,替换hbase配置文件中的HBASE_CLASSPATH
export JAVA_CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/hdfs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/hdfs/</guid><description>[toc]
1.介绍 一句话(官方):分布式存储系统HDFS( Hadoop Distributed File System)。 其实就是一个文件系统，类似于linux的文件系统。有目录，目录下可以存 储文件。但它又是一个分布式的文件系统。
基本原理
将文件切分成等大的数据块，分别存储到多台机器上。 每个数据块存在多个备份。 将数据切分、容错、负载均衡等功能透明化。 可将HDFS看成是一个巨大、具有容错性的磁盘。 缺点
不适合存储大量小文件。 不适合低延迟数据访问 。 不支持多用户写入及任意修改文件 。 2.HDFS实现原理 数据块
每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位。构建于单个磁盘之 上的文件系统通过磁盘块来管理该文件系统中的块，该文件系统块的大小可以是磁盘块的整数倍。 HDFS 同样也有块 (block) 的概念，但是大得多，默认为 128 MB 。与单一磁盘上的文件系统相似， HDFS 上的文件也被划分为块大小的多个分块 (chunk) ，作为独立的存储单元。但与其他 文件系统不同的是， HDFS 中小于一个块大小的文件不会占据整个块的空间。(好像老版本的会占用整个空间) 2.1 NameNode
NameNode是HDFS架构中的主节点。 当NameNode启动时会从fsimage中读取数据,缩短启动时间,并会写入editlog,Namenode只有 在启动时候合并fsimage和edits log.
功能
管理各个从节点的状态(DataNode)。 记录存储在HDFS上的所有数据的元数据信息。例如:block存储的位置，文件大小，文件权限，文件层级等等。这些信息以两个文件形式永久保存在本地磁盘上。 命名空间镜像文件(FsImage): fsimage是HDFS文件系统存于硬盘中的元数据检查点，里面记录 了自最后一次检查点之前HDFS文件系统中所有目录和文件的序列化信息 编辑日志(edit-logs)文件:保存了自最后一次检查点之后所有针对HDFS文件系统的操作，比如: 增加文件、重命名文件、删除目录等等。 记录了存储在HDFS上文件的所有变化，例如文件被删除，namenode会记录到editlog中。
接受DataNode的心跳和各个datanode上的block报告信息，确保DataNode是否存活。
负责处理所有块的复制因子。
如果DataNode节点宕机，NameNode会选择另外一个DataNode均衡复制因子，并做负载均衡。
2.2 Sencondary NameNode S NameNode是NameNode的助手，不要将其理解成是NameNode的备份。Secondary NameNode 的整个目的在HDFS中提供一个Checkpoint Node，所以也被叫做checkpoint node。
功能
定时的从NameNode获取EditLogs,并更新到FsImage上。 一旦它有新的fsimage文件，它将其拷贝回NameNode上，NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。 关于NameNode是什么时候将改动 写到edit logs中的?</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/yarn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/yarn/</guid><description>[toc]
1.YARN 概述 YARN YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操 作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序
YARN 是 Hadoop2.x 版本中的一个新特性。它的出现其实是为了解决第一代 MapReduce 编程 框架的不足，提高集群环境下的资源利用率，这些资源包括内存，磁盘，网络，IO等。Hadoop2.X 版本中重新设计的这个 YARN 集群，具有更好的扩展性，可用性，可靠性，向后兼容性，以 及能支持除 MapReduce 以外的更多分布式计算程序
1、YARN 并不清楚用户提交的程序的运行机制
2、YARN 只提供运算资源的调度（用户程序向 YARN 申请资源，YARN 就负责分配资源）
3、YARN 中的主管角色叫 ResourceManager
4、YARN 中具体提供运算资源的角色叫 NodeManager
5、这样一来，YARN 其实就与运行的用户程序完全解耦，就意味着 YARN 上可以运行各种类 型的分布式运算程序（MapReduce 只是其中的一种），比如 MapReduce、Storm 程序，Spark 程序，Tez ……
6、所以，Spark、Storm 等运算框架都可以整合在 YARN 上运行，只要他们各自的框架中有 符合 YARN 规范的资源请求机制即可
7、yarn 就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整 合在一个物理集群上，提高资源利用率，方便数据共享
YARN/MRv2 最基本的想法是将原 JobTracker 主要的资源管理和 Job 调度/监视功能分开作为 两个单独的守护进程。有一个全局的 ResourceManager(RM)和每个 Application 有一个 ApplicationMaster(AM)，Application 相当于 MapReduce Job 或者 DAG jobs。ResourceManager 和 NodeManager(NM)组成了基本的数据计算框架。ResourceManager 协调集群的资源利用， 任何 Client 或者运行着的 applicatitonMaster 想要运行 Job 或者 Task 都得向 RM 申请一定的资 源。ApplicatonMaster 是一个框架特殊的库，对于 MapReduce 框架而言有它自己的 AM 实现， 用户也可以实现自己的 AM，在运行的时候，AM 会与 NM 一起来启动和监视 Tasks。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</guid><description>1.hadoop:
lucene (全文检索) &amp;ndash;&amp;gt;Nutch(搜索引擎)
GFS(谷歌文件系统) &amp;mdash;&amp;gt;HDFS hadoop分布式文件系统
MapReduce(数据的处理算法) &amp;ndash;&amp;gt;MapReduce 分布式计算
bigtable(列式数据库) &amp;mdash;&amp;gt;HBase
主要包括:
分布式存储系统:HDFS
资源管理系统 :YARN &amp;ndash;&amp;gt;负责集群资源的统一管理和调度((hadoop2.0才有))
分布式计算框架: MapReduce
元数据:
对数据信息描述的数据(比如,某个资源在哪个块,起始地址是什么,大小为多少,)
namenode 持久化 有两个文件:fsimage,edits
SecondaryNameNode 合并fsimage,edits两个文件,加快启动速度
DN(dateNote) 和NN(namenode)保持心跳机制
副本放置策略:
找负载比较低的
放在不与第一个副本同一个地方
放在第二个副本同一个机架
hadoop作业流程:
客户向ResourceManager提交作业
ResourceManager的ApplicationManager通知一个NodeManager启动contrainer并在contrainer中启动ApplicationMaster负责这次作业
ApplicationMaster向ApplicationManager注册
ApplicationMaster向ResourceManager的ResourceSchedule轮询申请资源
申请到资源后,通知相应的NodeManager启动作业
NodeManager启动contrainer并执行相应的Map/Reduce Task
执行的Task向ApplicationMaster汇报作业情况
作业执行完成后,Application想ApplicationMaster注销作业
注:namenode: ResourceManager -&amp;gt; ResourceSchedule , ApplicationManager ;
datanode : NodeManager ; ApplicationManager ; MapTask ;ReduceTask</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E5%91%BD%E4%BB%A4/</guid><description>hdfs namenode -format #第一次启动需要格式化namenode
jps 可以查看java进程,以此来确定hadoop是否启动成功
sbin/start-dfs.sh # 启动hadoop分布式存储进程 启动了: namenode(主节点) SecondaryNameNode(备份节点) datenode(从节点)
sbin/start-yarn.sh # 启动hadoop资源管理进程
// (可以使用start-all.sh来启动hdfs和yarn,但是已被抛弃,因为这个命令启动resourceManager有问题)
mr-jobhistory-daemon.sh start historyserver # 启动历史服务器
hdfs dfsadmin -safemode leave # 离开安全模式 统计test.txt文件中的单词数
yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /test.txt /wc_out
scp a.txt hadoop@slave01:~/data 将a.txt文件传输到slave01的~/data文件夹下
scp -r aa hadoop@slave01:~/data 将aa文件夹传输到slave01的~/data文件夹下
HDFS常用命令 (大多命令基本和linux命令一样) :
// 全部路径是 绝对路径 hdfs dfs -put *.txt //上传文件到hdfs hdfs dfs -rm /hdfs001.iml // 删除文件,支持正则 hdfs dfs -rm -r /hq // 删除文件夹 https://www.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid><description>[toc]
来自 :https://www.aboutyun.com/forum.php?mod=viewthread&amp;amp;tid=20620
这节开始讲解集群搭建：
这儿选用的linux环境是CentOS-7.0-1406-x86_64-GnomeLive.iso GNOME桌面版。安装虚拟机的过程就不说了，这儿使用的网络模式是NAT模式。目前已aboutyun用户登录master机器。本次我们要搭建的是一个三节点的Hadoop、Spark集群。
一、Linux环境准备 1. 设置静态ip 2. 关闭SELINUX 修改 /etc/sysconfig/selinux文件
vim /etc/sysconfig/selinux
3. 关闭防火墙 sudo systemctl stop firewalld.service #停止firewall sudo systemctl disable firewalld.service #禁止firewall开机启动
4. 开启ssh sudo systemctl start sshd.service #开启ssh sudo systemctl enablesshd.service #开机启动ssh
5. 修改hosts sudo vim /etc/hosts
以下内容加入到hosts文件中：
6. 修改主机名 sudo vim /etc/hostname
将文件内容改为master
7. 配置ntp服务 用于同步时间
sudo vim /etc/ntp.conf
设置服务器为以下几个（默认为以下服务器的不用修改）：
server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst 保存后执行：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E9%94%99%E8%AF%AF%E5%90%88%E9%9B%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hadoop/%E9%94%99%E8%AF%AF%E5%90%88%E9%9B%86/</guid><description>[toc]
1.Datanode denied communication with namenode because hostname cannot be resolved( nameNode找不到dateNode**)** 大意是nameNode找不到dateNode,
原因: 暂未知
其表现:
1.在web界面中看得的活着节点是0, 进程都很正常,
\2. 日志中能看得报错,具体错误日志在
less /usr/software/hadoop-2.7.3/logs/hadoop-xkj-datanode-slave1.log
3.当上传文件时会报错,(大意是,无法复制内容到dataNode上)
解决:
先停止hdfs 先检查 /etc/hosts 和 /etc/hostname 两个文件的配置(每个服务器都改),改后需要重启服务器 (参考搭建教程) 删除 dfs.namenode.name.dir 和 dfs.datanode.data.dir 目录的文件 重新格式化NameNode，重新启动NameNode hadoop namenode -format // 该命令已废弃,建议用 hdfs代替,但是还没试
再启动即可
(https://blog.csdn.net/JavaDestiny/article/details/87294090)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/rowkey/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/rowkey/</guid><description>[toc]
1. RowKey的作用 1.1 介绍 Rowkey是每一行的主键,在每行的开头, 也是可以存储数据的,只是它具有索引的作用,一般不是存在业务数据,而是发挥索引作用,所以里面填充什么值很重要,此为Rowkey设计
1.2 RowKey在查询中的作用 HBase中RowKey可以唯一标识一行记录，在HBase中检索数据有以下三种方式：
通过 get 方式，指定 RowKey 获取唯一一条记录 通过 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 链接：https://www.jianshu.com/p/89bcd80890d6
当大量请求访问HBase集群的一个或少数几个节点，造成少数RegionServer的读写请求过多、负载过大，而其他RegionServer负载却很小，这样就造成热点现象。大量访问会使热点Region所在的主机负载过大，引起性能下降，甚至导致Region不可用。所以我们在向HBase中插入数据的时候，应尽量均衡地把记录分散到不同的Region里去，平衡每个Region的压力。
下面根据一个例子分别介绍下根据RowKey进行查询的时候支持的情况。 如果我们RowKey设计为uid+phone+name，那么这种设计可以很好的支持一下的场景:
uid=873969725 AND phone=18900000000 AND name=zhangsan uid= 873969725 AND phone=18900000000 uid= 873969725 AND phone=189? uid= 873969725 难以支持的场景：
phone=18900000000 AND name = zhangsan phone=18900000000 name=zhangsan 从上面的例子中可以看出，在进行查询的时候，根据RowKey从前向后匹配，所以我们在设计RowKey的时候选择好字段之后，还应该结合我们的实际的高频的查询场景来组合选择的字段，越高频的查询字段排列越靠左。
1.3 RowKey在Region中的作用 在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有StartRowKey和StopRowKey，这是表示 Region 存储的 RowKey 的范围，HBase 表的数据时按照 RowKey 来分散到不同的 Region，要想将数据记录均衡的分散到不同的Region中去，因此需要 RowKey 满足这种散列的特点。此外，在数据读写过程中也是与RowKey 密切相关，RowKey在读写过程中的作用：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</guid><description/></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hbase/%E5%91%BD%E4%BB%A4/</guid><description>[toc]
1. 大纲 启动hbase服务: start-hbase.sh
停止hbase服务: stop-hbase.sh
启动shelll: hbase shell
hbase shell命令 描述 alter 修改列族（column family）模式 count 统计表中行的数量 create 创建表 describe 显示表相关的详细信息 delete 删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值） deleteall 删除指定行的所有元素值 disable 使表无效 drop 删除表 enable 使表有效 exists 测试表是否存在 exit 退出hbase shell get 获取行或单元（cell）的值 incr 增加指定表，行或列的值 list 列出hbase中存在的所有表 put 向指向的表单元添加值 tools 列出hbase所支持的工具 scan 通过对表的扫描来获取对用的值 status 返回hbase集群的状态信息 shutdown 关闭hbase集群（与exit不同） truncate 重新创建指定表 version 返回hbase版本信息 来自 &amp;lt;https://www.cnblogs.com/cxzdy/p/5583239.html&amp;gt; 2. ddl命令 2.1. 创建表create 注意：创建表时只需要指定列族名称，不需要指定列名。
# 语法 create &amp;#39;表名&amp;#39;, {NAME =&amp;gt; &amp;#39;列族名1&amp;#39;}, {NAME =&amp;gt; &amp;#39;列族名2&amp;#39;}, {NAME =&amp;gt; &amp;#39;列族名3&amp;#39;} # 此种方式是上上面的简写方式，使用上面方式可以为列族指定更多的属性，如VERSIONS、TTL、BLOCKCACHE、CONFIGURATION等属性 create &amp;#39;表名&amp;#39;, &amp;#39;列族名1&amp;#39;, &amp;#39;列族名2&amp;#39;, &amp;#39;列族名3&amp;#39; create &amp;#39;表名&amp;#39;, {NAME =&amp;gt; &amp;#39;列族名1&amp;#39;, VERSIONS =&amp;gt; 版本号, TTL =&amp;gt; 过期时间, BLOCKCACHE =&amp;gt; true} # 示例 create &amp;#39;tbl_user&amp;#39;, &amp;#39;info&amp;#39;, &amp;#39;detail&amp;#39; create &amp;#39;t1&amp;#39;, {NAME =&amp;gt; &amp;#39;f1&amp;#39;, VERSIONS =&amp;gt; 1, TTL =&amp;gt; 2592000, BLOCKCACHE =&amp;gt; true} 2.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/sql%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/sql%E4%BC%98%E5%8C%96/</guid><description>[toc]
1.少用count（distinct） count（distinct）是由一个reduce task来完成的，这一个reduce需要处理的数据量太大，就会导致整个job很难完成。 count（distinct）可以使用先group by再count的方式来替换 2. 合理使用MapJoin 3. 合理使用动态分区 动态分区可以优化诸如需要往指定分区插入数据的这种操作。
配置参数
hive.exec.dynamic.partition：是否开启动态分区，默认为false，设置成true hive.exec.dynamic.partition.mode：默认值表示必须指定至少一个静态分区，默认为strict，设置成nonstrict hive.exec.max.dynamic.partitions.pernode：在每个执行MR的节点上，最大可以创建多少个动态分区，默认100，按实际情况来定 hive.exec.max.created.files：整个MR Job中，最大可以创建多少个HDFS文件，默认值：100000，一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。 hive.error.on.empty.partition：当有空分区生成时，是否抛出异常，默认值：false，一般不需要设置。 4. 可以替代 in/exists 语句 hive1.2.1 也支持 in/exists 操作，但还是推荐使用 hive 的一个高效替代方案：left semi join
select a.id, a.name from a left semi join b on a.id = b.id;
5. 合理设置map/reduce数量 6. 避免数据倾斜 https://www.cnblogs.com/qingyunzong/p/8847775.html#_label4 https://www.cnblogs.com/duanxingxing/p/6874318.html https://blog.csdn.net/qq_32038679/article/details/80557286</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%87%BD%E6%95%B0/</guid><description>[toc]
条件函数 assert_true(BOOLEAN condition) 解释 如果condition不为true，则抛出异常，否则返回null
使用案例 select assert_true(1&amp;lt;2) -- 返回null select assert_true(1&amp;gt;2) -- 抛出异常 coalesce(T v1, T v2, &amp;hellip;) 解释 返回第一个不为null的值，如果都为null，则返回null
使用案例 select coalesce(null,1,2,null) -- 返回1 select coalesce(1,null) -- 返回1 select coalesce(null,null) -- 返回null if if ( BOOLEAN testCondition, T valueTrue , T valueFalseOrNull )
解释 如果testCondition条件为true，则返回第一个值，否则返回第二个值
使用案例 select if(1 is null,0,1) -- 返回1 select if(null is null,0,1) -- 返回0 isnotnull(a) 解释 如果参数a不为null，则返回true，否则返回false
使用案例 select isnotnull(1) -- 返回true select isnotnull(null) -- 返回false isnull(a) 解释 与isnotnull相反，如果参数a为null，则返回true，否则返回false</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%8B%E7%BB%8D/</guid><description>[toc]
1.什么是hive 1. Hive 由 Facebook 实现并开源
2. 是基于 Hadoop 的一个数据仓库工具
3. 可以将结构化的数据映射为一张数据库表
4. 并提供 HQL(Hive SQL)查询功能
5. 底层数据是存储在 HDFS 上
6. Hive的本质是将 SQL 语句转换为 MapReduce 任务运行
7. 使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，适用于离线的批量数据计算。
数据仓库之父比尔·恩门（Bill Inmon）在 1991 年出版的“Building the Data Warehouse”（《建 立数据仓库》）一书中所提出的定义被广泛接受——数据仓库（Data Warehouse）是一个面 向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史 变化（Time Variant）的数据集合，用于支持管理决策(Decision Making Support)。
Hive 依赖于 HDFS 存储数据，Hive 将 HQL 转换成 MapReduce 执行，所以说 Hive 是基于 Hadoop 的一个数据仓库工具，实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理
1.1 Hive 特点 优点：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%91%BD%E4%BB%A4/</guid><description>create database [IF NOT EXISTS] userdb; 创建数据库 user userdb; 使用userdb数据库 客户端的链接:
Cli的方式
hive(jdbc方式)
web方式
jdbc方式
hive --service hiveserver2 &amp;amp; //先后台运行hive beeline -u jdbc:hive2:// -n hive -p a web方式
hive --service hwi
建表语法
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 说明：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid><description>[toc]
一.Hive安装包下载 下载地址http://mirrors.hust.edu.cn/apache/
选择合适的Hive版本进行下载，进到stable-2文件夹可以看到稳定的2.x的版本是2.3.3
二.Hive安装 使用MySQL做为Hive的元数据库，所以先安装MySQL。参考 https://www.runoob.com/mysql/mysql-install.html
1. 解压
tar -zxvf apache-hive-2.3.3-bin.tar.gz -C /usr/software
2.修改 hive-env.sh
cp -p hive-env.sh.template hive-env.sh &amp;amp;&amp;amp; vim hive-env.sh
并添加如下内容:
HADOOP_HOME=/usr/software/hadoop-2.7.3 export HIVE_CONF_DIR=/usr/software/apache-hive-2.1.1-bin/conf 3. 修改hive-site.xml
cp -p hive-default.xml.template hive-site.xml &amp;amp;&amp;amp; vim hive-site.xml
修改如下内容:
&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.cli.print.header&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;!-- false 改为true --&amp;gt; &amp;lt;description&amp;gt;Whether to print the names of the columns in query output.&amp;lt;/description&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.cli.print.current.db&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;!-- false 改为true --&amp;gt; &amp;lt;description&amp;gt;Whether to include the current database in the Hive prompt.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%B0%8F%E6%96%87%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E5%B0%8F%E6%96%87%E4%BB%B6/</guid><description>哪里会产生小文件 ?
源数据本身有很多小文件 动态分区会产生大量小文件 reduce个数越多, 小文件越多 按分区插入数据的时候会产生大量的小文件, 文件个数 = maptask个数 * 分区数 小文件太多造成的影响 ?
从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。 HDFS存储太多小文件, 会导致namenode元数据特别大, 占用太多内存, 制约了集群的扩展 小文件解决方法
方法一: 通过调整参数进行合并
#每个Map最大输入大小(这个值决定了合并后文件的数量) set mapred.max.split.size=256000000; #一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并) set mapred.min.split.size.per.node=100000000; #一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并) set mapred.min.split.size.per.rack=100000000; #执行Map前进行小文件合并 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; #===设置map输出和reduce输出进行合并的相关参数： #设置map端输出进行合并，默认为true set hive.merge.mapfiles = true #设置reduce端输出进行合并，默认为false set hive.merge.mapredfiles = true #设置合并文件的大小 set hive.merge.size.per.task = 256*1000*1000 #当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。 set hive.merge.smallfiles.avgsize=16000000 **方法二: **
针对按分区插入数据的时候产生大量的小文件的问题, 可以使用DISTRIBUTE BY rand() 将数据随机分配给Reduce，这样可以使得每个Reduce处理的数据大体一致.
# 设置每个reducer处理的大小为5个G set hive.exec.reducers.bytes.per.reducer=5120000000; # 使用distribute by rand()将数据随机分配给reduce, 避免出现有的文件特别大, 有的文件特别小 insert overwrite table test partition(dt) select * from iteblog_tmp DISTRIBUTE BY rand(); 方法三: 使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</guid><description>[toc]
1.什么是数据倾斜 简单的讲，数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了集群中的一台或者几台机器上计算，而集群中的其他节点空闲。这些倾斜了的数据的计算速度远远低于平均计算速度，导致整个计算过程过慢。
不是数据集中导致倾斜了,++而是计算太过集中++导致倾斜
2. 数据倾斜发生时的现象： 绝大多数task执行得都非常快，但个别task执行的极慢。 原本能正常执行的Spark作业，某天突然爆出OOM（内存溢出）异常。观察异常栈，是我们写的业务代码造成的 3. 数据倾斜发生的原理 : 在进行shuffle的时候，必须将各个节点上相同的Key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或者join操作。如果某个key对应的数据量特别大的话，会发生数据倾斜。比如大部分key对应的10条数据，但个别key却对应了100万条数据，那么大部分task会只分配到10条数据，而个别task可能会分配了100万数据。整个spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致OOM。
反正就是过多的key分配到同一个reduce去执行了,而那台机器无法处理那么多,解决方案基本往减轻压力方向靠
原文链接：https://blog.csdn.net/weixin_35353187/article/details/84303518
4. 产生原因 数据倾斜原因有以下几方面：
key分布不均匀 业务数据本身的特性 (比如北京/上海数据量比长沙数据要多) 建表时考虑不周 (用户表和日志表,共同拥有访问ip字段,但是类型却不一样或者默认值不同之类) 某些SQL语句本身就有数据倾斜 (出现shuffle操作) 5. 解决方案 hive和MR/spark的方案不一样,毕竟一个是写sql,一个是写程序,但是方向是一样的
hive的sql写好一点会有效避免数据倾斜
1. 空值产生的数据倾斜
场景说明 在日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和用户表中的 user_id 相关联，就会碰到数据倾斜的问题。
解决方案 1：user_id 为空的不参与关联 解决方案 2：赋予空值新的 key 值 方法 2 比方法 1 效率更好，不但 IO 少了，而且作业数也少了，方案 1 中，log 表 读了两次，jobs 肯定是 2，而方案 2 是 1。这个优化适合无效 id（比如-99，’’，null）产 生的数据倾斜，把空值的 key 变成一个字符串加上一个随机数，就能把造成数据倾斜的 数据分到不同的 reduce 上解决数据倾斜的问题。
改变之处：使本身为 null 的所有记录不会拥挤在同一个 reduceTask 了，会由于有替代的 随机字符串值，而分散到了多个 reduceTask 中了，由于 null 值关联不上，处理后并不影响最终结果。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hive/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</guid><description>[toc]
1. 介绍 Hive自定义函数包括三种UDF、UDAF、UDTF
UDF(User-Defined-Function) 一进一出 UDAF(User- Defined Aggregation Funcation) 聚集函数，多进一出。Count/max/min UDTF(User-Defined Table-Generating Functions) 一进多出，如lateral view explore) 2. 函数类型 2.1. UDF(一进一出) 继承UDF类
重写evaluate方法
将该java文件编译成jar
2.2. UDAF(多进一出) 实现方法:
用户的UDAF必须继承了org.apache.hadoop.hive.ql.exec.UDAF；
用户的UDAF必须包含至少一个实现了org.apache.hadoop.hive.ql.exec的静态类，诸如实现了 UDAFEvaluator
一个计算函数必须实现的5个方法的具体含义如下：
init()：主要是负责初始化计算函数并且重设其内部状态，一般就是重设其内部字段。一般在静态类中定义一个内部字段来存放最终的结果。
iterate()：每一次对一个新值进行聚集计算时候都会调用该方法，计算函数会根据聚集计算结果更新内部状态。当输 入值合法或者正确计算了，则 就返回true。
terminatePartial()：Hive需要部分聚集结果的时候会调用该方法，必须要返回一个封装了聚集计算当前状态的对象。
merge()：Hive进行合并一个部分聚集和另一个部分聚集的时候会调用该方法。
terminate()：Hive最终聚集结果的时候就会调用该方法。计算函数需要把状态作为一个值返回给用户。
部分聚集结果的数据类型和最终结果的数据类型可以不同。
2.3. UDTF(一进多出) 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF
initialize()：UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）
process：初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward() 调用产生一行；如果产生多列 可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数
最后close()方法调用，对需要清理的方法进行清理
来自: https://blog.csdn.net/weixin_42181917/article/details/82865140
https://www.cnblogs.com/lrxvx/p/10974341.html
3. 加载函数的方式 3.1 使用add jar [classPath]语句(临时加载) #加载jar add jar [classPath] #创建函数 create temporary function [functionName] as [calssPath] 这种方式不建议在生产环境中使用，通过该方式添加的jar文件只存在于当前会话中，当会话关闭后不能够继续使用该jar文件，最常见的问题是创建了永久函数到metastore中，再次使用该函数时却提示ClassNotFoundException。所以使用该方式每次都要使用add jar [classPath]语句添加相关的jar文件到classPath中。倒是可以用在临时使用函数的情况</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hue/hue%E4%B8%8Ezeppelin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/hue/hue%E4%B8%8Ezeppelin/</guid><description>（1）功能
Zeppelin和Hue都能提供一定的数据可视化的功能，都提供了多种图形化数据表示形式。单从这点来说，个人认为功能类似，大同小异，Hue可以通过经纬度进行地图定位，这个功能我在Zeppelin 0.6.0上没有找到。 Zeppelin支持的后端数据查询程序较多，0.6.0版本缺省有18种，原生支持Spark。而Hue的3.9.0版本缺省只支持Hive、Impala、Pig和数据库查询。 Zeppelin只提供了单一的数据处理功能，包括前面提到的数据摄取、数据发现、数据分析、数据可视化等都属于数据处理的范畴。而Hue的功能相对丰富的多，除了类似的数据处理，还有元数据管理、Oozie工作流管理、作业管理、用户管理、Sqoop集成等很多管理功能。从这点看，Zeppelin只是一个数据处理工具，而Hue更像是一个综合管理工具。 （2）架构
Zeppelin采用插件式的翻译器，通过插件开发，可以添加任何后端语言和数据处理程序。相对来说更独立和开放。 Hue与Hadoop生态圈的其它组件密切相关，一般都与CDH一同部署。 （3）使用场景
Zeppelin适合单一数据处理、但后端处理语言繁多的场景，尤其适合Spark。 Hue适合与Hadoop集群的多个组件交互、如Oozie工作流、Sqoop等联合处理数据的场景，尤其适合与Impala协同工作。 来自 &amp;lt;http://blog.csdn.net/wzy0623/article/details/52370045&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/impala/impala%E4%B8%8Ehive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/impala/impala%E4%B8%8Ehive/</guid><description>[toc]
1.Impala和Hive的关系 Impala是基于Hive的大数据实时分析查询引擎，直接使用Hive的元数据库Metadata,意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断的完善中。
与Hive的关系
Impala 与Hive都是构建在Hadoop之上的数据查询工具各有不同的侧重适应面，但从客户端使用来看Impala与Hive有很多的共同之处，如数据表元数 据、ODBC/JDBC驱动、SQL语法、灵活的文件格式、存储资源池等。Impala与Hive在Hadoop中的关系如下图所示。
Hive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询，Impala给数据分析人员提供了快速实验、验证想法的大数 据分析工具。可以先使用hive进行数据转换处理，之后使用Impala在Hive处理后的结果数据集上进行快速的数据分析。
2. Impala相对于Hive所使用的优化技术 1、没有使用 MapReduce进行并行计算，虽然MapReduce是非常好的并行计算框架，但它更多的面向批处理模式，而不是面向交互式的SQL执行。与 MapReduce相比：Impala把整个查询分成一执行计划树，而不是一连串的MapReduce任务，在分发执行计划后，Impala使用拉式获取 数据的方式获取结果，把结果数据组成按执行树流式传递汇集，减少的了把中间结果写入磁盘的步骤，再从磁盘读取数据的开销。Impala使用服务的方式避免 每次执行查询都需要启动的开销，即相比Hive没了MapReduce启动时间。 2、使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。 3、充分利用可用的硬件指令（SSE4.2）。 4、更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。 5、通过选择合适的数据存储格式可以得到最好的性能（Impala支持多种存储格式）。 6、最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。 3. Impala与Hive的异同 数据存储：使用相同的存储数据池都支持把数据存储于HDFS, HBase。 元数据：两者使用相同的元数据。 SQL****解释处理：比较相似都是通过词法分析生成执行计划。 执行计划：
Hive: 依赖于MapReduce执行框架，执行计划分成 map-&amp;gt;shuffle-&amp;gt;reduce-&amp;gt;map-&amp;gt;shuffle-&amp;gt;reduce…的模型。如果一个Query会 被编译成多轮MapReduce，则会有更多的写中间结果。由于MapReduce执行框架本身的特点，过多的中间过程会增加整个Query的执行时间。 Impala: 把执行计划表现为一棵完整的执行计划树，可以更自然地分发执行计划到各个Impalad执行查询，而不用像Hive那样把它组合成管道型的 map-&amp;gt;reduce模式，以此保证Impala有更好的并发性和避免不必要的中间sort与shuffle。 数据流：
Hive: 采用推的方式，每一个计算节点计算完成后将数据主动推给后续节点。 Impala: 采用拉的方式，后续节点通过getNext主动向前面节点要数据，以此方式数据可以流式的返回给客户端，且只要有1条数据被处理完，就可以立即展现出来，而不用等到全部处理完成，更符合SQL交互式查询使用。 内存使用：
Hive: 在执行过程中如果内存放不下所有数据，则会使用外存，以保证Query能顺序执行完。每一轮MapReduce结束，中间结果也会写入HDFS中，同样由于MapReduce执行架构的特性，shuffle过程也会有写本地磁盘的操作。 Impala: 在遇到内存放不下数据时，当前版本1.0.1是直接返回错误，而不会利用外存，以后版本应该会进行改进。这使用得Impala目前处理Query会受到一 定的限制，最好还是与Hive配合使用。Impala在多个阶段之间利用网络传输数据，在执行过程不会有写磁盘的操作（insert除外）。 调度：
Hive: 任务调度依赖于Hadoop的调度策略。 Impala: 调度由自己完成，目前只有一种调度器simple-schedule，它会尽量满足数据的局部性，扫描数据的进程尽量靠近数据本身所在的物理机器。调度器 目前还比较简单，在SimpleScheduler::GetBackend中可以看到，现在还没有考虑负载，网络IO状况等因素进行调度。但目前 Impala已经有对执行过程的性能统计分析，应该以后版本会利用这些统计信息进行调度吧。 容错：
Hive: 依赖于Hadoop的容错能力。 Impala: 在查询过程中，没有容错逻辑，如果在执行过程中发生故障，则直接返回错误（这与Impala的设计有关，因为Impala定位于实时查询，一次查询失败， 再查一次就好了，再查一次的成本很低）。但从整体来看，Impala是能很好的容错，所有的Impalad是对等的结构，用户可以向任何一个 Impalad提交查询，如果一个Impalad失效，其上正在运行的所有Query都将失败，但用户可以重新提交查询由其它Impalad代替执行，不 会影响服务。对于State Store目前只有一个，但当State Store失效，也不会影响服务，每个Impalad都缓存了State Store的信息，只是不能再更新集群状态，有可能会把执行任务分配给已经失效的Impalad执行，导致本次Query失败。 适用面：
Hive: 复杂的批处理查询任务，数据转换任务。 Impala：实时数据分析，因为不支持UDF，能处理的问题域有一定的限制，与Hive配合使用,对Hive的结果数据集进行实时分析。 来自* &amp;lt;https://www.cnblogs.com/zlslch/p/6785207.html?utm_source=itdadao&amp;amp;utm_medium=referral&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/impala/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/impala/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</guid><description>[toc]
1. 前言 Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。已有的Hive系统虽然也提供了SQL语义，但由于Hive底层执行使用的是MapReduce引擎，仍然是一个批处理过程，难以满足查询的交互性。相比之下，Impala的最大特点也是最大卖点就是它的快速。但也依赖Hive (impala元数据都存储在Hive的metastore中,或者说两者元数据共存)
来自* &amp;lt;https://baike.baidu.com/item/Impala/7458017?fr=aladdin&amp;gt;
2.Impala核心组件 2.1 Impala Daemon Impala的核心组件是运行在各个节点上面的impalad这个守护进程（Impala daemon），它负责读写数据文件，接收从impala-shell、Hue、JDBC、ODBC等接口发送的查询语句，并行化查询语句和分发工作任务到Impala集群的各个节点上，同时负责将本地计算好的查询结果发送给协调器节点（coordinator node）。
你可以向运行在任意节点的Impala daemon提交查询，这个节点将会作为这个查询的协调器（coordinator node），其他节点将会传输部分结果集给这个协调器节点。由这个协调器节点构建最终的结果集。在做实验或者测试的时候为了方便，我们往往连接到同一个Impala daemon来执行查询，但是在生产环境运行产品级的应用时，我们应该循环（按顺序）的在不同节点上面提交查询，这样才能使得集群的负载达到均衡。
Impala daemon不间断的跟statestore进行通信交流，从而确认哪个节点是健康的能接收新的工作任务。它同时接收catalogd daemon（从Impala 1.2之后支持）传来的广播消息来更新元数据信息，当集群中的任意节点create、alter、drop任意对象、或者执行INSERT、LOAD DATA的时候触发广播消息。
2.2 Impala Statestore Impala Statestore检查集群各个节点上Impala daemon的健康状态，同时不间断地将结果反馈给各个Impala daemon。这个服务的物理进程名称是statestored，在整个集群中我们仅需要一个这样的进程即可。如果某个Impala节点由于硬件错误、软件错误或者其他原因导致离线，statestore就会通知其他的节点，避免其他节点再向这个离线的节点发送请求。
由于statestore是当集群节点有问题的时候起通知作用，所以它对Impala集群并不是有关键影响的。如果statestore没有运行或者运行失败，其他节点和分布式任务会照常运行，只是说当节点掉线的时候集群会变得没那么健壮。当statestore恢复正常运行时，它就又开始与其他节点通信并进行监控。
2.3 Impala Catalog Imppalla catalog服务将SQL语句做出的元数据变化通知给集群的各个节点，catalog服务的物理进程名称是catalogd，在整个集群中仅需要一个这样的进程。由于它的请求会跟statestore daemon交互，所以最好让statestored和catalogd这两个进程在同一节点上。
Impala 1.2中加入的catalog服务减少了REFRESH和INVALIDATE METADATA语句的使用。在之前的版本中，当在某个节点上执行了CREATE DATABASE、DROP DATABASE、CREATE TABLE、ALTER TABLE、或者DROP TABLE语句之后，需要在其它的各个节点上执行命令INVALIDATE METADATA来确保元数据信息的更新。同样的，当你在某个节点上执行了INSERT语句，在其它节点上执行查询时就得先执行REFRESH table_name这个操作，这样才能识别到新增的数据文件。需要注意的是，通过Impala执行的操作带来的元数据变化，有了catalog就不需要再执行REFRESH和INVALIDATE METADATA，但如果是通过Hive进行的建表、加载数据，则仍然需要执行REFRESH和INVALIDATE METADATA来通知Impala更新元数据信息。
来自* &amp;lt;http://www.cnblogs.com/chenz/articles/3947147.html&amp;gt;
3. Impala执行步骤 Impala执行的查询有以下几个步骤：
1. 客户端通过ODBC、JDBC、或者Impala shell向Impala集群中的任意节点发送SQL语句，这个节点的impalad实例作为这个查询的协调器（coordinator）。
2. Impala解析和分析这个查询语句来决定集群中的哪个impalad实例来执行某个任务。
3. HDFS和HBase给本地的impalad实例提供数据访问。
4. 各个impalad向协调器impalad返回数据，然后由协调器impalad向client发送结果集。
来自 &amp;lt;http://www.cnblogs.com/chenz/articles/3947147.html&amp;gt;
Impala为什么比Hive速度快
Impala自称数据查询效率比Hive快几倍甚至数十倍，它之所以这么快的原因大致有以下几点：
真正的MPP查询引擎。 使用C++开发而不是Java，降低运行负荷。 运行时代码生成（LLVM IR），提高效率。 在执行SQL语句的时候，Impala不会把中间数据写入到磁盘，而是在内存中完成了所有的处理。 使用Impala的时候，查询任务会马上执行而不是生产Mapreduce任务，这会节约大量的初始化时间。 Impala查询计划解析器使用更智能的算法在多节点上分布式执行各个查询步骤，同时避免了sorting和shuffle这两个非常耗时的阶段，这两个阶段往往是不需要的。 Impala拥有HDFS上面各个data block的信息，当它处理查询的时候能够在各个datanode上面更均衡的分发查询。 另外一个关键原因是，Impala为每个查询产生汇编级的代码，当Impala在本地内存中运行的时候，这些汇编代码执行效率比其它任何代码框架都更快，因为代码框架会增加额外的延迟。 来自* &amp;lt;http://www.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka-stream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka-stream%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</guid><description>[toc]
1.1. 目标 本快速入门指南的目标是提供与KafkaStreams的第一个应用程序示例。我们将演示在你的第一个示例程序中，如果使用Kafka Streams库和演示一个简单的端到端的数据流。 值得注意的是，这种快速入门只涵盖了KafkaStreams的表面，这篇文档的剩余部分将会提供更多的细节，我们将在快速入门指南中为你指明方向。
1.2. 我们想做什么 在这个快速入门中，我们将运行包含Apachekafka的一个wordcount演示应用程序。下面代码的关键在于使用Java8的lambda表达式，易于阅读。(摘自WordCountLambdaExample):
[java] view plain copy //序列化/反序列化Sting和Long类型 final Serde&amp;lt;String&amp;gt; stringSerde = Serdes.String(); final Serde&amp;lt;Long&amp;gt; longSerde = Serdes.Long(); //通过指定输入topic “mystream”来构造KStream实例，
//输入数据就以文本的形式保存在topic “mystream” 中。
//(在本示例中，我们忽略所有消息的key.)
KStream&amp;lt;String, String&amp;gt; textLines = builder.stream(stringSerde, stringSerde, &amp;quot;mystream&amp;quot;); KStream&amp;lt;String, Long&amp;gt; wordCounts = textLines
//以空格为分隔符，将每行文本数据拆分成多个单词。
//这些文本行就是从输入topic中读到的每行消息的Value。
//我们使用flatMapValues方法来处理每个消息Value，而不是更通用的flatMap .flatMapValues(value -&amp;gt; Arrays.asList(value.toLowerCase().split(&amp;quot;\\W+&amp;quot;)))
//我们随后将调用countByKey来计算每个单词出现的次数
//所以我们将每个单词作为map的key。
.map((key, value) -&amp;gt; new KeyValue&amp;lt;&amp;gt;(value, value)) //通过key来统计每个单词的次数
//
//这会将流类型从KStream&amp;lt;String,String&amp;gt;转为KTable&amp;lt;String,Long&amp;gt; (word-count).
//因此我们必须提供String和long的序列化反序列化方法。
//
.countByKey(stringSerde, &amp;quot;Counts&amp;quot;) //转化KTable&amp;lt;String,Long&amp;gt;到KStream&amp;lt;String,Long&amp;gt;
.toStream();
//将KStream&amp;lt;String,Long&amp;gt;写入到输出topic中。
wordCounts.to(stringSerde, longSerde, &amp;quot;streams-wordcount-output&amp;quot;); 在上面的代码执行过程中，我们将执行如下步骤：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</guid><description>[toc]
1、Kafka使用背景 在我们大量使用分布式数据库、分布式计算集群的时候，是否会遇到这样的一些问题：
a.我们想分析下用户行为（pageviews），以便我们设计出更好的广告位
b.我想对用户的搜索关键词进行统计，分析出当前的流行趋势
c.有些数据，存储数据库浪费，直接存储硬盘效率又低
这些场景都有一个共同点：
数据是由上游模块产生，上游模块，使用上游模块的数据计算、统计、分析，这个时候就可以使用消息系统，尤其是分布式消息系统！
2、Kafka的定义 What is Kafka：它是一个分布式消息系统，由linkedin使用scala编写，用作LinkedIn的活动流（Activity Stream）和运营数据处理管道（Pipeline）的基础。具有高水平扩展和高吞吐量。
3.kafka的特点 Zookeeper是一种在分布式系统中被广泛用来作为：分布式状态管理、分布式协调管理、分布式配置管理、和分布式锁服务的集群。
kafka增加和减少服务器都会在Zookeeper节点上触发相应的事件kafka系统会捕获这些事件，进行新一轮的负载均衡，客户端也会捕获这些事件来进行新一轮的处理。
Kafka在底层摒弃了Java堆缓存机制，采用了操作系统级别的页缓存，同时将随机写操作改为顺序写，再结合Zero-Copy的特性极大地改善了IO性能。但是，这只是一个方面，毕竟单机优化的能力是有上限的。如何通过水平扩展甚至是线性扩展来进一步提升吞吐量呢？ Kafka就是使用了分区(partition)，通过将topic的消息打散到多个分区并分布保存在不同的broker上实现了消息处理(不管是producer还是consumer)的高吞吐量。
4.Kafka相关概念 1.AMQP协议 Advanced Message Queuing Protocol （高级消息队列协议）
The Advanced Message Queuing Protocol (AMQP)：是一个标准开放的应用层的消息中间件（Message Oriented Middleware）协议。AMQP定义了通过网络发送的字节流的数据格式。因此兼容性非常好，任何实现AMQP协议的程序都可以和与AMQP协议兼容的其他程序交互，可以很容易做到跨语言，跨平台。
上面说的3种比较流行的消息队列协议，要么支持AMQP协议，要么借鉴了AMQP协议的思想进行了开发、实现、设计。
2.什么是消息系统？ ​ 消息系统负责将数据从一个应用程序传输到另一个应用程序，因此应用程序可以专注于数据，但不必担心如何共享数据。 分布式消息传递基于可靠消息队列的概念。 消息在客户端应用程序和消息传递系统之间异步排队。 有两种类型的消息传递模式可用 - 一种是点对点的，另一种是发布 - 订阅(pub-sub)消息传递系统。 大多数消息传递模式遵循pub-sub。
点对点消息系统 在点对点系统中，消息被保存在一个队列中。 一个或多个消费者可以消费队列中的消息，但是特定的消息只能由最多一个消费者消费。 一旦消费者在队列中读取消息，消息就从该队列中消失。 这个系统的典型例子是一个订单处理系统，其中每个订单将由一个订单处理器处理，但是多订单处理器也可以同时工作。 下图描述了结构。
发布-订阅消息系统 在发布-订阅系统中，消息被保存在一个主题中。 与点对点系统不同，消费者可以订阅一个或多个主题并使用该主题中的所有消息。 在发布-订阅系统中，消息生产者称为发布者，消息消费者称为订阅者。 一个真实的例子是Dish TV，它发布体育，电影，音乐等不同的频道，任何人都可以订阅他们自己的一套频道，并在他们的订阅频道可用时获得内容。
来自* &amp;lt;https://www.yiibai.com/kafka/apache_kafka_introduction.html#article-start&amp;gt;
3. 一些基本的概念 1、消费者（Consumer）：从消息队列中请求消息的客户端应用程序
2、生产者（Producer） ：向broker发布消息的应用程序
3、AMQP服务端（broker）：用来接收生产者发送的消息并将这些消息路由给服务器中的队列，便于fafka将生产者发送的消息，动态的添加到磁盘并给每一条消息一个偏移量，所以对于kafka一个broker就是一个应用程序的实例
4、复制因子: 数据的备份数
5、偏移量(offset) : 来表示哪些消息是否被消费,偏移量唯一,消费后只是消费者的偏移量改变了,直到过期消息才会被清除(偏移量有很多,如下图) 来自 http://orchome.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E5%91%BD%E4%BB%A4/</guid><description>[toc]
注:kafka依赖zookeeper,所以启动kafka前需开启zookeeper,kafka依赖zookeeper来分发消息,并会在zookeeper中存储分区和broker的信息
1、启动服务 #从后台启动Kafka集群（3台都需要启动）
cd /mysoftware/kafka_2.11-0.10.1.0//bin
#进入到kafka的bin目录
./kafka-server-start.sh -daemon ../config/server.properties
(每台都)启动:
kafka-server-start.sh config/server.properties &amp;gt;/dev/null &amp;amp; 停止: kafka-server-stop.sh &amp;gt;/dev/null &amp;amp; 如果启动报错,必须先停止,才能启动
启动后会有kafka进程
2. 创建Topic ./kafka-topics.sh --create --zookeeper slave02:2181 --replication-factor 2 --partitions 1 --topic mystream
解释
&amp;ndash;replication-factor 2 #复制两份 &amp;ndash;partitions 1 #创建1个分区 &amp;ndash;topic #主题为shuaige 3. 在一台服务器上创建一个发布者(相当于生产者) #创建一个broker，发布者
./kafka-console-producer.sh --broker-list master:9092 --topic shuaige
4 在一台服务器上创建一个订阅者(相当于消费者) &amp;gt;--bootstrap-server 新的kafka,可以用这个参数取代--zookeeper 端口号要变成9092 old: kafka-console-consumer.sh --zookeeper localhost:2181 --topic shuaige --from-beginning new: kafka-console-consumer.sh --bootstrap-server master:9092 --topic shuaige --from-beginning 5 、查看topic .</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E7%9A%84%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/kafka%E7%9A%84%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/</guid><description>[toc]
1.前言: 对于整个选举算法的详情需要先了解Raft选举算法，kafka是基于该算法来实现leader选举的。有兴趣的读者可以参考之前的文章【分布式一致性协议：Raft算法详解】。
kafka 的选举有三类:
控制器（Broker）选主 分区多副本选主 消费组选主 2.选举类型: 2.1 控制器（Broker）选举 所谓控制器就是一个Borker，在一个kafka集群中，有多个broker节点，但是它们之间需要选举出一个leader，其他的broker充当follower角色。集群中第一个启动的broker会通过在zookeeper中创建临时节点/controller来让自己成为控制器(其实大家竞争成为控制器,一般先启动的,先注册成功嘛)，其他broker启动时也会在zookeeper中创建临时节点，但是发现节点已经存在，所以它们会收到一个异常，意识到控制器已经存在，那么就会在zookeeper中创建watch对象，便于它们收到控制器变更的通知。
那么如果控制器由于网络原因与zookeeper断开连接或者异常退出(此时其他borker也是竞争成为控制器)，那么其他broker通过watch收到控制器变更的通知，就会去尝试创建临时节点/controller，如果有一个broker创建成功，那么其他broker就会收到创建异常通知，也就意味着集群中已经有了控制器，其他broker只需创建watch对象即可。
如果集群中有一个broker发生异常退出了，那么控制器就会检查这个broker是否有分区的副本leader，如果有那么这个分区就需要一个新的leader，此时控制器就会去遍历其他副本，决定哪一个成为新的leader，同时更新分区的ISR集合。
如果有一个broker加入集群中，那么控制器就会通过Broker ID去判断新加入的broker中是否含有现有分区的副本，如果有，就会从分区副本中去同步数据。
集群中每选举一次控制器，就会通过zookeeper创建一个controller epoch，每一个选举都会创建一个更大，包含最新信息的epoch，如果有broker收到比这个epoch旧的数据，就会忽略它们，kafka也通过这个epoch来防止集群产生“脑裂”。
原文链接：https://blog.csdn.net/qq_37142346/article/details/91349100
controller作用: 维护ISR集合,选举leader,增加分区时的重新分配工作
2.2 分区副本选举机制 在kafka的集群中，会存在着多个主题topic，在每一个topic中，又被划分为多个partition，为了防止数据不丢失，每一个partition又有多个副本，在整个集群中，总共有三种副本角色：
首领副本（leader）：也就是leader主副本，每个分区都有一个首领副本，所有的生产者与消费者的请求都会经过该副本来处理。 跟随者副本（follower）：除了首领副本外的其他所有副本都是跟随者副本，跟随者副本不处理来自客户端的任何请求，只负责从首领副本同步数据，保证与首领保持一致。如果首领副本发生崩溃，就会从这其中选举出一个leader。 首选首领副本：创建分区时指定的首选首领。如果不指定，则为分区的第一个副本。 我们希望每个分区的leader可以分布到不同的broker中，尽可能的达到负载均衡，所以会有一个首选首领，如果我们设置参数auto.leader.rebalance.enable为true，那么它会检查首选首领是否是真正的首领，如果不是，则会触发选举，让首选首领成为首领(如果设置了首选首领,则一定会让它成为leader)。
原文链接：https://blog.csdn.net/qq_37142346/article/details/91349100
Kafka在ZooKeeper中动态维护了一个ISR（in-sync replicas），这个ISR里的所有Replica都复制了leader，只有ISR里的成员才有被选为Leader的可能。 默认的，如果follower与leader之间超过10s内没有发送请求，或者说两者数据差太多(指的是条数,估计是用offset来判断,差值可配置,默认为4000)，此时该follower就会被认为“不同步副本”(Out-Sync Relipcas)。而持续请求的副本就是“同步副本”，当leader发生故障时，会从“同步副本”(In-Sync Replicas)中选举为leader。其中的请求超时时间可以通过参数replica.lag.time.max.ms参数来配置。
由一个控制器认定谁是leader,
在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，如果由于服务宕机,导致某个分区的所有副本都失效,就无法保证数据不丢失了。这种情况下有两种可行的方案：
等待ISR中的任一个Replica“活”过来，并且选它作为Leader 选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader 如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。
选择第一个“活”过来的Replica作为Leader，而这个Replica不是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由Leader完成）。(此时称为脏leader选举)
Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不同的使用场景选择高可用性还是强一致性。 unclean.leader.election.enable 参数决定使用哪种方案，默认是true，采用第二种方案
链接：https://www.jianshu.com/p/1f02328a4f2e
链接: https://www.cnblogs.com/qingyunzong/p/9004703.html
这个leader的作用是接收读写操作,follwer只是个副本,如果leader挂了,则其中之一成为leader,继续接收读写操作
2.3 消费组选主 在kafka的消费端，会有一个消费者协调器以及消费组，组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，那么如何选举的呢？
如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader，如果某一个时刻leader消费者由于某些原因退出了消费组，那么就会重新选举leader，如何选举？
原文链接：https://blog.csdn.net/qq_37142346/article/details/91349100
消费组里有Rebalance 过程,做的是consumer如何达成一致来分配订阅topic的每个分区,其中就得先选leader,具体看 原理介绍</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E7%AE%80%E8%BF%B0%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E7%AE%80%E8%BF%B0%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/</guid><description>[toc]
在kafka原理中介绍,kafka在消费组分配分区时,有两种算法: range 和 round-robin和Sticky(0.11.x版本),前两种都存在弊端
消费者客户端参数partition.asssignment.strategy可以配置多个分配策略，彼此之间以逗号分隔。
RangeAssignor分配策略(默认使用该策略) RangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。(就是挨个给,轮着来)
原文链接：https://blog.csdn.net/u013256816/article/details/81123600
RoundRobin strategy 使用RoundRobin策略有两个前提条件必须满足：
同一个Consumer Group里面的所有消费者的num.streams必须相等； 每个消费者订阅的主题必须相同。 所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，这里文字可能说不清，看下面的代码应该会明白：
val allTopicPartitions = ctx.partitionsForTopic.flatMap { case(topic, partitions) =&amp;gt; info(&amp;ldquo;Consumer %s rebalancing the following partitions for topic %s: %s&amp;rdquo; .format(ctx.consumerId, topic, partitions)) partitions.map(partition =&amp;gt; { TopicAndPartition(topic, partition) }) }.toSeq.sortWith((topicPartition1, topicPartition2) =&amp;gt; { /* * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending * up on one consumer (if it has a high enough stream count).</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E7%AE%80%E8%BF%B0%E5%B9%82%E7%AD%89%E6%80%A7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E7%AE%80%E8%BF%B0%E5%B9%82%E7%AD%89%E6%80%A7/</guid><description>[toc]
一. 什么是幂等性? 幂等性其实是消息的一致性,分为生产者幂等性和消费者幂等性.
使用Kafka时,需要保证exactly-once语义。要知道在分布式系统中，出现网络分区是不可避免的，如果kafka broker 在回复ack时，出现网络故障或者是full gc导致ack timeout，producer将会重发，如何保证producer重试时不造成重复or乱序？又或者producer 挂了，新的producer并没有old producer的状态数据，这个时候如何保证幂等？即使Kafka 发送消息满足了幂等，consumer拉取到消息后，把消息交给线程池workers，workers线程对message的处理可能包含异步操作，
来自 https://www.cnblogs.com/jingangtx/p/11330338.html
就是用来解决数据重复问题，保证kafka单会话单分区内数据不会重复消费在kafka0.11之前通过isr+ack机制可保证数据不丢，却不能保证不重复 有一些情况可能会导致数据重复。比如：网络请求延时导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。
数据重复的解决方案就是加唯一id，通过id判断数据是否重复
原文链接：https://blog.csdn.net/qq_37923600/article/details/88583170
二. 生产者幂等性 保证在发送同一条消息时，在服务端只会被持久化一次，数据不丢不重。
但是是有条件的
kafka的幂等性只能保证单会话有效，如果broker挂掉重启，幂等就无效了，因为无法获取之前的状态信息 幂等性不能跨多个Topic-Partition，只能保证单个partition的幂等性。 所以生产者分为了 幂等型producer 和 事务型producer,前者解决了单会话幂等性等问题，后者解决了多会话幂等性
单回话的意思大概是 发送一次消息表示一次回话吧
2.1 单回话幂等性 为解决producer重试引起的乱序和重复。Kafka增加了pid和seq。Producer中每个RecordBatch都有一个单调递增的seq; Broker上每个tp也会维护pid-seq的映射，并且每Commit都会更新lastSeq。这样recordBatch到来时，broker会先检查RecordBatch再保存数据：如果batch中 baseSeq(第一条消息的seq)比Broker维护的序号(lastSeq)大1，则保存数据，否则不保存(inSequence方法)。
简单的说,相当于把存一份标识符,来确定是否已生产了
在生产者配置文件中加入配置即可实现: enable.idempotence=true
参考: https://blog.csdn.net/qq_37923600/article/details/88583170
2.2 多回话幂等性 kafka事务引入了transactionId 和Epoch，设置transactional.id后，一个transactionId只对应一个pid, 且Server 端会记录最新的 Epoch 值。这样有新的producer初始化时，会向TransactionCoordinator发送InitPIDRequest请求， TransactionCoordinator 已经有了这个 transactionId对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样当old producer恢复过来请求操作时，将被认为是无效producer抛出异常。 如果没有开启事务，TransactionCoordinator会为新的producer返回new pid，这样就起不到隔离效果，因此无法实现多会话幂等。
其实就是利用事务,只要没完成就不会自增(原子性),完成操作后手动提交
2.2.1 实现多会话幂等性 提供了API,使用API即可. 其中分为
只有写 有写有读(最常见) 只有读(没有实际意义,因为只有读不会发生异常) 这里只描述第二种</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E9%AB%98%E6%B0%B4%E4%BD%8D%E4%B8%8Eleader-epoch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/kafka/%E9%AB%98%E6%B0%B4%E4%BD%8D%E4%B8%8Eleader-epoch/</guid><description>[toc]
前言 你可能听说过高水位，但不一定听说过Leader Epoch。前者是Kafka中非常重要的概念。而后者是0.11版本中新推出的。主要是为了弥补前者水位机制的一些缺陷。
1.高水位 1.1 什么是高水位 Kafka的水位不是时间戳，更与时间无关。它是和位置信息绑定的，具体来说，它是用消息位移来表征的。
这个offset是所有ISR的LEO的最小位置（minimum LEO across all the ISR of this partition），consumer不能读取超过HW的消息，因为这意味着读取到未完全同步（因此没有完全备份）的消息。换句话说就是：HW是所有ISR中的节点都已经复制完的消息.也是消费者所能获取到的消息的最大offset（注意，并不是所有replica都一定有这些消息，而只是ISR里的那些才肯定会有）。
1.2 高水位的作用 定义消息可见性，用来标识分区下的哪些消息是可以被消费者消费的 帮助kafka完成副本同步 1.3 已提交消息和未提交消息 在分区高水位以下的消息就被认为是已提交消息，反之就是未提交消息 消费者只能消费已提交消息，即位移值小于8的消息。 这里不存在kafka的事务，因为事务机制会影响消息者所能看到的消息的范围，他不只是简单依赖高水位来判断，是依赖于一个名为LSO的位移值来判断事务性消费者的可见性 位移值等于高水位的消息也属于为未提交消息。即高水位的消息也是不能被消费者消费的 LEO表示副本写入下一条消息的位移值。同一个副本对象，起高水位值不会超过LEO 1.4 高水位更新机制
Kafka中所有副本对象都保存一组高水位值和LEO值，但Leader副本中还保留着其他Follower副本的LEO值。
Kafka副本机制在运行过程中，会更新Broker1上Follower副本的高水位和LEO值，同时也会更新Broker0上Leader副本的高水位和LEO以及Follow副本的LEO，但不会更新其HW。
1.5 副本同步机制解析 当生产者发送一条消息时，Leader和Follower副本对应的高水位是怎么被更新的呢？
Follower副本也成功地更新LEO为1.此时，Leader和Follower副本的LEO都是1，但各自的高水位依然是0，还没有被更新。他们需要在下一轮的拉取中被更新
在新一轮的拉去请求中，由于位移值是0的消息已经拉取成功，因此Follower副本这次请求拉去的位移值为1的消息。Leader副本接收此请求后，更新远程副本LEO为1，然后更新Leader高水位为1，然后才会将更新过的高水位值1发送给Follower副本。Follower副本接收到以后，也将自己的高水位值更新为1.至此，一个完整的消息同步周期就结束了。
总的来说: 第一次拉取只会更新LEO,第二次拉取时才会更新HW
2. Leader Epoch Follower副本的高水位更新是需要额外一轮的拉取请求才能实现的。若有多个副本的情况下，则需要多轮的拉取请求。也就是说，Leader副本高水位更新和Follower副本高水位更新在时间上是存在错配的。而这种错配往往是数据丢失，数据不一致问题现象的根源。因此kafka社区在0.11版本中引入了Leader Epoch。
2.1 Leader Epoch的组成 Epoch。一个单调递增的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的Leader被认为是过期的Leader，不能再行使Leader的权力。 起始位移（Start Offset）。Leader副本在该Epoch上写入的首条消息的位移。 Leader Epoch&amp;lt;0,0&amp;gt;和&amp;lt;1,100&amp;gt;。第一个Epoch指的是0版本，位移0开始保存消息，一共保存100条消息。之后Leader发生了变更，版本号增加到1，新版本起始位移为100.
Kafka Broker会在内存中为每个分区都缓存Leader Epoch数据，同时它还会定期的将这信息持久化一个checkpoint文件中。当Leader副本写入消息到磁盘时，Broker会尝试更新这部分缓存，如果该Leader是首次写入消息，那么Broker会向缓存中增加一个Leader Epoch条目，否则就不做更新。
2.2 Leader Epoch使用 Leader Epoch是怎样防止数据丢失的呢？
单纯依赖高水位是怎么造成数据丢失的。开始时，副本A和副本B都处于正常状态，A是Leader副本，B是Follower副本。当生产者使用ack=1（默认）往Leader副本A中发送两条消息。且A全部写入成功，此时Kafka会通知生产者说这两条消息写入成功。
现在假设A,B都写入了这两条消息，而且Leader副本的高水位也已经更新了，但Follower副本高水位还未更新。因为Follower端高水位的更新与Leader端有时间错配。假如现在副本B所在Broker宕机了，那么当它重启回来后，副本B就会执行日志截断操作，将LEO值调整为之前的高水位值，也就是1.所以副本B当中位移值为1的消息就丢失了。副本B中只保留了位移值0的消息。
当执行完截断操作之后，副本B开始从A中拉取消息，执行正常的消息同步。假如此时副本A所在的Broker也宕机了。那么kafka只能让副本B成为新的Leader，然后副本A重启回来之后，也需要执行日志截断操作，即调整高水位为与B相同的值，也就是1。这样操作之后，位移值为1的那条消息就永远丢失了。
Leader Epoch机制如何规避这种数据丢失现象呢？
延续上文场景，引用了Leader Epoch机制之后，Follower副本B重启回来后，需要向A发送一个特殊的请求去获取Leader的LEO值，该例子中为2。当知道Leader LEO为2时，B发现该LEO值不必自己的LEO值小，而且缓存中也没有保存任何起始位移值&amp;gt;2的Epoch条目，因此B无需执行日志截断操作。这是对高水位机制的一次明显改进，即不是依赖于高水位判断是否进行日志截断操作。
现在，副本A宕机了，B成立新Leader。同样的，在A重启回来后，执行与B逻辑相同的判断，也不需要执行日志截断操作，所以位移值为1的那条消息就全部得以保存。后面当生产者程序向 B 写入新消息时，副本 B 所在的 Broker 缓存中，会生成新的 Leader Epoch 条目：[Epoch=1, Offset=2]。之后，副本 B 会使用这个条目帮助判断后续是否执行日志截断操作。这样，kafka就规避掉了数据丢失的场景。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/scala/equalseqne%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E7%94%A8%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/scala/equalseqne%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E7%94%A8%E6%B3%95/</guid><description>equals
比较的是值 , 和java一样,利用hashcode()方法进行比较 例如(&amp;quot;he&amp;quot;+&amp;quot;llo&amp;quot;)
eq
比较的是引用,比较的对象的引用地址
ne
是eq的反义
==
当要比较的值是否null,
如果为null则使用eq,
如果不为null,则使用equals
https://blog.csdn.net/do_yourself_go_on/article/details/72758380</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/scala/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/scala/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</guid><description>整体类型架构图(待替换)
iterable类型架构图
def lazyFunc(x: Int, y: =&amp;gt; Int) = 1 // 表示接受的是一个变量 // x 参数为传值(call by value), y 参数是传名称(call by name) // 传名称相当于惰性参数,使用到该参数时才会求值,用一次求一次; 而传值则会先求值,不管函数内部是否使用或者使用几次 def func(() =&amp;gt; Int) =1 // 表示接受的是一个函数,没有入参,返参是Int的函数 println &amp;#34;23&amp;#34; // 等同于 println(&amp;#34;23&amp;#34;) // 在scala里，函数都可以写成操作符的形式，这使得函数定义更像数学表达式,若参数只有一个，圆括号也是可以省略的。 隐式转换: 1.当调用每个类的方法没有时,编译器会试图隐式转换,从作用域中寻找方法(可能放在伴生对象中) 2.当传函数的参数时,不是函数所需要的类型,会发生隐式转换,在作用域中寻找可以转变类型的函数(不在乎此函数的变量名,只关心入参和返参类型),配合柯西化,可以不传隐式参数</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark-shuffle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark-shuffle/</guid><description>[toc]
1. 介绍 Shuffle描述着数据从map task输出到reduce task输入的这段过程。shuffle是连接Map和Reduce之间的桥梁，Map的输出要用到Reduce中必须经过shuffle这个环节，shuffle的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task需要跨节点去拉取其它节点上的map task结果。这一过程将会产生网络资源消耗和内存，磁盘IO的消耗。通常shuffle分为两部分：Map阶段的数据准备和Reduce阶段的数据拷贝处理。一般将在map端的Shuffle称之为Shuffle Write，在Reduce端的Shuffle称之为Shuffle Read.
在Spark的中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种(前者在2.0版本后已遗弃)
spark shuffle 演进的历史
Spark 0.8及以前 Hash Based Shuffle Spark 0.8.1 为Hash Based Shuffle引入File Consolidation机制 Spark 0.9 引入ExternalAppendOnlyMap Spark 1.1 引入Sort Based Shuffle，但默认仍为Hash Based Shuffle Spark 1.2 默认的Shuffle方式改为Sort Based Shuffle Spark 1.4 引入Tungsten-Sort Based Shuffle Spark 1.6 Tungsten-sort并入Sort Based Shuffle Spark 2.0 Hash Based Shuffle退出历史舞台 所以现在就只有 sortshuffle
2. HashShuffleManager 这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。
图中有3个 Reducer，从Task 开始那边各自把自己进行 Hash 计算(分区器：hash/numreduce取模)，分类出3个不同的类别，每个 Task 都分成3种类别的数据，想把不同的数据汇聚然后计算出最终的结果，所以Reducer 会在每个 Task 中把属于自己类别的数据收集过来，汇聚成一个同类别的大集合，每1个 Task 输出3份本地文件，这里有4个 Mapper Tasks，所以总共输出了4个 Tasks x 3个分类文件 = 12个本地小文件。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark-sql/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark-sql/</guid><description>[toc]
1.前言: SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，但是Shark对Hive有太多依赖(如采用Hive的语法解析器、查询优化器等等),所以SparkSQL抛弃原有Shark的代码，汲取了Shark的一些优点，如内存列存储（In-Memory Columnar Storage）、Hive兼容性等，重新开发了SparkSQL代码.
不再受限于Hive，只是兼容Hive
而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎
来自* &amp;lt;http://www.cnblogs.com/shishanyuan/p/4723604.html&amp;gt;
可以这么说, sparkSQL做到了大部分HIVE的功能,但是比它快(因为重写了底层,而且用的是RDD),将操作写到代码里了
对SparkSQL来说,主要的工作就是写SQL,和写自定义函数了(类似于HIVE),
大致流程: 从数据源(HIVE,HDFS,文本等)读取数据,然后写SQL查询,(有业务处理就处理),复杂的就写自定义函数(继承UserDefinedFunction或者 UserDefinedAggregateFunction)
查sql前当然要有数据和表,在代码里只能写临时表,要想创建永久表,还是得用hive那一套(相关语句可以直接运行也可以写代码里)
目前比较流行的是SparkSQL 连接Hive,这样可以操作Hive中的数据,(在spark1.2.1后,自带Hive,但是sparksql用到了hive的元数据,所以需要额外提供mysql(用来存储元数据,当然也有自带的叫derby),但局限性比较大,估计生产中还是会用外部Hive),这样就又得学Hive,暂时放弃
1.1 Spark SQL基础 使用spark SQL有两种方式，一种是作为分布式SQL引擎,此时只需要写SQL就可以进行计算。另一种是吃饭spark程序中通过领域API的形式来操作数据(被抽象为DateFrame)。
简单的说: 一种是和SQL引擎去查数据(已经有数据了,一般和HIVE一起用); 一种是通过API的方式加载数据并操作(一般是从本地,HDFS,JDBC等加载文本数据)
2.为什么sparkSQL的性能得到提升 这个简单了解即可，
内存列存储（In-Memory Columnar Storage） 字节码生成技术 scala代码优化 3.sparkSQL组成 sparkSQL1.1总体上由四个模块组成：core、catalyst、hive、hive-Thriftserver：
core 处理数据的输入输出，从不同的数据源获取数据（RDD、Parquet、json等），将查询结果输出成schemaRDD； catalyst 处理查询语句的整个处理过程，包括解析、绑定、优化、物理计划等，说其是优化器，还不如说是查询引擎； hive 对hive数据的处理 hive-ThriftServer 提供CLI和JDBC/ODBC接口 在这四个模块中，catalyst处于最核心的部分，其性能优劣将影响整体的性能。由于发展时间尚短，还有很多不足的地方，但其插件式的设计，为未来的发展留下了很大的空间。下面是catalyst的一个设计图：
其中虚线部分是以后版本要实现的功能，实线部分是已经实现的功能。从上图看，catalyst主要的实现组件有：
sqlParse，完成sql语句的语法解析功能，目前只提供了一个简单的sql解析器；
Analyzer，主要完成绑定工作，将不同来源的Unresolved LogicalPlan和数据元数据（如hive metastore、Schema catalog）进行绑定，生成resolved LogicalPlan；
optimizer对resolved LogicalPlan进行优化，生成optimized LogicalPlan；
Planner将LogicalPlan转换成PhysicalPlan；
CostModel，主要根据过去的性能统计数据，选择最佳的物理执行计划
这些组件的基本实现方法：
先将sql语句通过解析生成Tree，然后在不同阶段使用不同的Rule应用到Tree上，通过转换完成各个组件的功能。
Analyzer使用Analysis Rules，配合数据元数据（如hive metastore、Schema catalog），完善Unresolved LogicalPlan的属性而转换成resolved LogicalPlan；
optimizer使用Optimization Rules，对resolved LogicalPlan进行合并、列裁剪、过滤器下推等优化作业而转换成optimized LogicalPlan；</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark-streaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/spark-streaming/</guid><description>[toc]
1.概述 Spark Streaming 是Spark核心API的一个扩展，可以实现高吞吐量的、具备容错机制的实时流数据的处理。支持从多种数据源获取数据，包括Kafk、Flume、Twitter、ZeroMQ、Kinesis 以及TCP sockets，从数据源获取数据之后，可以使用诸如map、reduce、join和window等高级函数进行复杂算法的处理。最后还可以将处理结果存储到文件系统，数据库和现场仪表盘。
来自* &amp;lt;http://www.cnblogs.com/shishanyuan/p/4747735.html&amp;gt;
2. Streaming架构 2.1 计算流程： Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark Core，也就是把Spark Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加或者存储到外部设备。下图显示了Spark Streaming的整个流程。
图Spark Streaming构架
2.2 容错性： 对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。
对于Spark Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性，所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。
Spark Streaming中RDD的lineage关系图
2.3 实时性： 对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark Job，对于每一段数据的处理都会经过Spark DAG图分解以及Spark的任务集的调度过程。对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右）
2.4 扩展性与吞吐量： Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍。
来自* &amp;lt;http://www.cnblogs.com/shishanyuan/p/4747735.html&amp;gt;
3.使用 3.1 基本使用(接收套接字) import org.apache.spark._ import org.apache.spark.streaming._ import org.apache.spark.streaming.StreamingContext._ // Create a local StreamingContext with two working thread and batch interval of 1 second.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/storm%E4%B8%8Espark-streaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/storm%E4%B8%8Espark-streaming/</guid><description>Spark Streaming与Storm的优劣分析
事实上，Spark Streaming绝对谈不上比Storm优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。
Spark Streaming仅仅在吞吐量上比Storm要优秀，而吞吐量这一点，也是历来挺Spark Streaming，贬Storm的人着重强调的。但是问题是，是不是在所有的实时计算场景下，都那么注重吞吐量？不尽然。因此，通过吞吐量说Spark Streaming强于Storm，不靠谱。
事实上，Storm在实时延迟度上，比Spark Streaming就好多了，前者是纯实时，后者是准实时。而且，Storm的事务机制、健壮性 / 容错性、动态调整并行度等特性，都要比Spark Streaming更加优秀。
Spark Streaming，有一点是Storm绝对比不上的，就是：它位于Spark生态技术栈中，因此Spark Streaming可以和Spark Core、Spark SQL无缝整合，也就意味着，我们可以对实时处理出来的中间数据，立即在程序中无缝进行延迟批处理、交互式查询等操作。这个特点大大增强了Spark Streaming的优势和功能。
来自* &amp;lt;http://blog.csdn.net/kwu_ganymede/article/details/50296831&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/structured-streaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/structured-streaming/</guid><description>[toc]
1. 介绍 Structured Streaming是Spark2.0版本提出的新的实时流框架（2.0和2.1是实验版本，从Spark2.2开始为稳定版本），相比于Spark Streaming，优点如下：
1.同样能支持多种数据源的输入和输出，参考如上的数据流图
2.以结构化的方式操作流式数据，能够像使用Spark SQL处理离线的批处理一样，处理流数据，代码更简洁，写法更简单
3.基于Event-Time，相比于Spark Streaming的Processing-Time更精确，更符合业务场景
4.解决了Spark Streaming存在的代码升级，DAG图变化引起的任务失败，无法断点续传的问题（Spark Streaming的硬伤！！！）
原文链接：https://blog.csdn.net/lovechendongxing/article/details/81748237</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%91%BD%E4%BB%A4/</guid><description>注:使用spark时,需要开启HDFS,(如果运行在yarn上还需开YARN)
启动Spark: (hadoop这个命令不起作用了) start-all.sh
启动后主机有Master进程, 从机有Worker进程
停止Spark: stop-all.sh
进入Spark-Shell:(进入scala环境) spark-shell
上传jar包:(这里是运行在Standalone上,&amp;ndash;master后是指定资源管理器)
spark-submit --master spark://master:7077 --class com.yc.hello hello.jar
更多命令:http://dataunion.org/10345.html</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid><description>[toc]
注:配置环境时,$SPARK_HOME/sbin一定放在hadoop的sbin前面,因为这两个文件夹中都含有start-all.sh和stop-all.sh,而spark启动/关闭用到了这两个脚本,而hadoop中这么命令已经遗弃了
源笔记:
1.修改slave文件,配置从机
如下:
slave01
slave02
slave03
2.修改spark.env.sh,如下:
export JAVA_HOME=/mysoftware/jdk1.8.0_101 export SPARK_MASTER_IP=master export SCALA_HOME=/mysoftware/scala-2.12.3 export HADOOP_HOME=/mysoftware/hadoop-2.7.3 export HADOOP_CONF_DIR=/mysoftware/hadoop-2.7.3/etc/hadoop/ 3.发送到从机即可,集群环境搭建完毕
一、安装Spark集群 1. 解压安装包 tar -zxvf ~/jar/spark-1.6.3-bin-hadoop2.6.tgz -C /data
2. 配置spark 涉及到的配置文件有以下几个：
${SPARK_HOME}/conf/spark-env.sh
${SPARK_HOME}/conf/slaves
${SPARK_HOME}/conf/spark-defaults.conf
这三个文件都是由原始的template文件复制过来的，比如cp spark-env.sh.template spark-env.sh
配置文件1：spark-env.sh
JAVA_HOME=/data/jdk1.8.0_111 SCALA_HOME=/data/scala-2.11.8 SPARK_MASTER_HOST=master SPARK_MASTER_PORT=7077 HADOOP_CONF_DIR=/data/hadoop-2.6.5/etc/hadoop # shuffled以及RDD的数据存放目录 SPARK_LOCAL_DIRS=/data/spark_data # worker端进程的工作目录 SPARK_WORKER_DIR=/data/spark_data/spark_works 注意：需要在本地创建/data/spark_data/spark_works目录
配置文件2：slaves
master slave1 slave2 配置文件3：spark-defaults.conf
spark.master spark://master:7077 spark.serializer org.apache.spark.serializer.KryoSerializer spark.eventLog.enabled true spark.eventLog.dir file:///data/spark_data/history/event-log spark.history.fs.logDirectory file:///data/spark_data/history/spark-events spark.eventLog.compress true 注意：需要在本地创建/data/spark_data/history/event-log、/data/spark_data/history/spark-events
3. 复制到其他节点 在master上：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</guid><description>[toc]
1.介绍,原理,原因 见 hive中的数据倾斜
2. 解决方案 ++自定义分区++,这需要用户自己继承partition类,指定分区策略,这种方式效果比较显著。
重新设计key,++有一种方案是在MAP阶段时给KEY加上一个随机数++,有了随机数的key就不会被大量的分配到同一节点(小几率),++待到REDUCE后再把随机数去掉++即可。(大表连接大表的情况可以用)
++使用combinner合并++,combinner是在map阶段,reduce之前的一个中间阶段,在这个阶段可以选择性的把大量的相同key数据先进行一个合并,可以看做是local reduce,然后再交给reduce来处理,这样做的好处很多,即减轻了map端向reduce端发送的数据量(减轻了网络带宽),也减轻了map端和reduce端中间的shuffle阶段的数据拉取数量(本地化磁盘IO速率)
原文链接：https://blog.csdn.net/weixin_35353187/article/details/84303518</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/spark/%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%8E%9F%E7%90%86/</guid><description>[toc]
1. 前言 Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架,Spark是用Scala程序设计语言编写而成，运行于Java虚拟机（JVM）环境之上
Spark运行在现有的Hadoop分布式文件系统基础之上（HDFS）提供额外的增强功能。它支持将Spark应用部署到现存的Hadoop v1集群（with SIMR – Spark-Inside-MapReduce）或Hadoop v2 YARN集群甚至是Apache Mesos之中。也有自己的资源管理器(Standalone),可以脱离Hadoop生态圈独立存在
Spark通过在数据处理过程中成本更低的洗牌（Shuffle）方式，将MapReduce提升到一个更高的层次。
Spark将中间结果保存在内存中而不是将其写入磁盘，当内存放了足够多的数据时,会放在磁盘上(有存储策略),所以Spark可以用于处理大于集群内存容量总和的数据集
Spark允许程序开发者使用有向无环图（DAG）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。
2. 生态系统 Spark生态圈以Spark Core为核心，从HDFS、Amazon S3和HBase等持久层读取数据，以MESS、YARN和自身携带的Standalone为资源管理器调度Job完成Spark应用程序的计算。 这些应用程序可以来自于不同的组件，如Spark Shell/Spark Submit的批处理、Spark Streaming的实时处理应用、Spark SQL的即时查询、BlinkDB的权衡查询、MLlib/MLbase的机器学习、GraphX的图处理和SparkR的数学计算等等。
Spark Core**:** 实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集（resilient distributed dataset，简称RDD）的API定义。 Spark Core提供了创建和操作这些集合的多个API。
来自* &amp;lt;https://www.douban.com/note/536766108/?from=tag&amp;gt;
Spark Streaming: Spark Streaming基于微批量方式的计算和处理，可以用于处理实时的流数据。它使用DStream，简单来说就是一个弹性分布式数据集（RDD）系列，处理实时数据。
Spark SQL: Spark SQL可以通过JDBC API将Spark数据集暴露出去，而且还可以用传统的BI(商业智能:提供报表展示分析帮助企业作出决策)和可视化工具在Spark数据上执行类似SQL的查询。用户还可以用Spark SQL对不同格式的数据（如JSON，Parquet以及数据库等）执行ETL，将其转化，然后暴露给特定的查询。
Spark MLlib: MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。
Spark GraphX: GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。
除了这些库以外，还有一些其他的库，如BlinkDB和Tachyon。
Spark常用术语
术语 描述 Application Spark的应用程序，包含一个Driver program和若干Executor SparkContext Spark应用程序的入口，负责调度各个运算资源，协调各个Worker Node上的Executor Driver Program 运行Application的main()函数并且创建SparkContext Executor 是为Application运行在Worker node上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。每个Application都会申请各自的Executor来处理任务 Cluster Manager 在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn) Worker Node 集群中任何可以运行Application代码的节点，运行一个或多个Executor进程 Task 运行在Executor上的工作单元(rdd的转换过程) Job SparkContext提交的具体Action操作，常和Action对应,一个JOB包含多个RDD及作用于相应RDD上的各种Operation(理解为一个action表示一个job) Stage 每个Job会被拆分很多组task，每组任务被称为Stage，也称TaskSet(窄依赖是一个stage,宽依赖是一个stage) RDD 是Resilient distributed datasets的简称，中文为弹性分布式数据集;是Spark最核心的模块和类 DAGScheduler 根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler TaskScheduler 将Taskset提交给Worker node集群运行并返回结果 Transformations 是Spark API的一种类型，Transformation返回值还是一个RDD，所有的Transformation采用的都是懒策略，如果只是将Transformation提交是不会执行计算的 Action 是Spark API的一种类型，Action返回值不是一个RDD，而是一个scala集合；计算只有在Action被提交的时候计算才被触发(懒计算)。 worker 集群中任何可以运行Application代码的节点，类似于YARN中的NodeManager节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点； 来自* &amp;lt;http://www.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%8E%9F%E7%90%86/</guid><description>[toc]
1.介绍 ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。
来自: 链接&amp;gt;
2.ZooKeeper的设计目标 简单的数据结构 : Zookeeper 使得分布式程序能够通过一个共享的树形结构的名字空间来进行相互协调，即Zookeeper 服务器内存中的数据模型由一系列被称为ZNode的数据节点组成，Zookeeper 将全量的数据存储在内存中，以此来提高服务器吞吐、减少延迟的目的。 可以构建集群 : Zookeeper 集群通常由一组机器构成，组成 Zookeeper 集群的而每台机器都会在内存中维护当前服务器状态，并且每台机器之间都相互通信。 顺序访问 : 对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序。 高性能 : Zookeeper 和Redis一样全量数据存储在内存中，100%读请求压测QPS 12-13W 3.关于 ZooKeeper 的一些重要概念 3.1 Zookeeper 集群： Zookeeper 是一个由多个 server 组成的集群,一个 leader，多个 follower。（这个不同于我们常见的Master/Slave模式）leader 为客户端服务器提供读写服务，除了leader外其他的机器只能提供读服务。 每个 server 保存一份数据副本全数据一致，分布式读 follower，写由 leader 实施更新请求转发，由 leader 实施更新请求顺序进行，来自同一个 client 的更新请求按其发送顺序依次执行数据更新原子性，一次数据更新要么成功，要么失败。全局唯一数据视图，client 无论连接到哪个 server，数据视图都是一致的实时性，在一定事件范围内，client 能读到最新数据。
3.2 集群角色 ①.Leader：是整个 Zookeeper 集群工作机制中的核心 。Leader 作为整个 ZooKeeper 集群的主节点，负责响应所有对 ZooKeeper 状态变更的请求。 主要工作：
事务请求的唯一调度和处理，保障集群处理事务的顺序性。 集群内各服务器的调度者。 Leader 选举是 Zookeeper 最重要的技术之一，也是保障分布式数据一致性的关键所在,选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪 我们以三台机器为例，在服务器集群初始化阶段，当有一台服务器Server1启动时候是无法完成选举的，当第二台机器 Server2 启动后两台机器能互相通信，每台机器都试图找到一个leader，于是便进入了 leader 选举流程.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E5%91%BD%E4%BB%A4/</guid><description>启动服务 : zkServer.sh start 打开客户端: zkCli.sh -server slave01:2181</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</guid><description>一:
修改zoo.cfg配置:
tickTime=2000 initLimit=5 syncLimit=2 dataDir=/opt/zookeeper/server1/data dataLogDir=/opt/zookeeper/server1/dataLog clientPort=2181 server.1=192.168.2.101:2888:3888 server.2=192.168.2.102:2889:3889 server.3=192.168.2.103:2890:3890 参数描述: tickTime：zookeeper中使用的基本时间单位, 毫秒值。 initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个 tickTime 时间间隔数。这里设置为5表名最长容忍时间为 5 * 2000 = 10 秒。 syncLimit：这个配置标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是 2 * 2000 = 4 秒。 dataDir 和 dataLogDir 看配置就知道干吗的了，不用解释。 clientPort：监听client连接的端口号，这里说的client就是连接到Zookeeper的代码程序。 server.{myid}={ip}:{leader服务器交换信息的端口}:{当leader服务器挂了后, 选举leader的端口} maxClientCnxns：对于一个客户端的连接数限制，默认是60，这在大部分时候是足够了。但是在我们实际使用中发现，在测试环境经常超过这个数，经过调查发现有的团队将几十个应用全部部署到一台机器上，以方便测试，于是这个数字就超过了。 二:
创建的data目录下创建文件 myid (注意,无后缀),并按照上面的配置文件中ip对应的数字
比如: 在 192.168.2.101机器上的myid文件写1</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E7%AE%80%E8%BF%B0%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/zookeeper/%E7%AE%80%E8%BF%B0%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/</guid><description>[TOC]
前言 Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。
服务器初始化启动。 服务器运行期间无法和Leader保持连接。 leader的选择机制，zookeeper提供了三种方式：
LeaderElection AuthFastLeaderElection FastLeaderElection （最新默认,也是本文主角） 下面就两种情况进行分析讲解。
服务器[启动]时期的Leader选举 个人简述: 初始启动时,每台机器都投自己,然后通知其他机器(通知的这个过程就叫投票),通知本质是发送消息,携带了(leaderId,zxid,Epoch),初始值就是(1,0,0),
(每个机器都投票,都判别)然后其他机器收到消息,进行判断,用收到的消息和自己的消息判别,
首先看是否为本轮选举(别人是新选举就更新自己,自己是新选举就通知它更新),然后比较zxid,如果一样再比较leaderId(都是取大值,所以配置文件中要求brokenId不能重复),比较后的结果如果与本地不同,则更新本地并通知其他机器,反之不动. 服务器还会判断,是否收集到了所有服务器的选举状态或者是否有过半机器(此时还会等200ms,看有没有新的消息),根据结果设置自己该成为leader还是follower,然后退出选举,kafka就能正常用了,如果已经产生leader,新加入的机器自动成为follower,同步数据即可
说明:
leaderId:机器所认为的leader ID,这个ID值就是配置文件中的brokenId zxid: 数据值,每次kafka中更新值就会更新这个值(zxid越大,数据越新) Epoch: 逻辑时钟值, 用于判断是否为本轮选举,每次选举都自动递增 若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下
(1) 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。
(2) 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。
(3) 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下
优先检查ZXID。ZXID比较大的服务器优先作为Leader。 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。
(4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。
(5) 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。
以上这是两台的场景,
如果是三台, 按顺序启动,leader也会是server2, 因为在第二轮投票时 server1和server2会投给server2, 达到过半,(server3会投给自己) (集群中没有leader时每个节点优先投给自己)
如果是四台, 按顺序启动, leader也会是server4, 但会有三轮选举. 在第二轮中, 每个节点的票数: server1:0 ; server:2 ; server3:1 ; server4:1
server2票数没有过半, 所以需要会发起一轮投票, 而server4的myid最大, 所以大家都会投给它,</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E5%88%9D%E5%A7%8B%E7%94%9F%E6%80%81%E5%9C%88/%E4%B8%80%E5%BC%A0%E5%BE%88%E7%89%9B%E9%80%BC%E7%9A%84%E5%9B%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E5%88%9D%E5%A7%8B%E7%94%9F%E6%80%81%E5%9C%88/%E4%B8%80%E5%BC%A0%E5%BE%88%E7%89%9B%E9%80%BC%E7%9A%84%E5%9B%BE/</guid><description>链接：https://www.zhihu.com/question/26568496/answer/224439650
**a.蓝色部分:**是Hadoop生态系统组件，黄色部分是Spark生态组件，虽然他们是两种不同的大数据处理框架，但它们不是互斥的，Spark与hadoop 中的MapReduce是一种相互共生的关系。Hadoop提供了Spark许多没有的功能，比如分布式文件系统，而Spark 提供了实时内存计算，速度非常快。有一点大家要注意，Spark并不是一定要依附于Hadoop才能生存，除了Hadoop的HDFS，还可以基于其他的云平台，当然啦，大家一致认为Spark与Hadoop配合默契最好摆了。
**b.技术趋势：**Spark在崛起，hadoop和Storm中的一些组件在消退。大家在学习使用相关技术的时候，记得与时俱进掌握好新的趋势、新的替代技术，以保持自己的职业竞争力。
HSQL未来可能会被Spark SQL替代，现在很多企业都是HIVE SQL和Spark SQL两种工具共存，当Spark SQL逐步成熟的时候，就有可能替换HSQL；
MapReduce也有可能被Spark 替换，趋势是这样，但目前Spark还不够成熟稳定，还有比较长的路要走；
Hadoop中的算法库Mahout正被Spark中的算法库MLib所替代，为了不落后，大家注意去学习Mlib算法库；
Storm会被Spark Streaming替换吗?在这里，Storm虽然不是hadoop生态中的一员，但我仍然想把它放在一起做过比较。由于Spark和hadoop天衣无缝的结合，Spark在逐步的走向成熟和稳定，其生态组件也在逐步的完善，是冉冉升起的新星，我相信Storm会逐步被挤压而走向衰退。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E5%88%9D%E5%A7%8B%E7%94%9F%E6%80%81%E5%9C%88/%E7%AB%AF%E5%8F%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E5%88%9D%E5%A7%8B%E7%94%9F%E6%80%81%E5%9C%88/%E7%AB%AF%E5%8F%A3/</guid><description>组件 节点 默认端口 配置 用途说明 HDFS DataNode 50010 dfs.datanode.address datanode服务端口，用于数据传输 HDFS DataNode 50075 dfs.datanode.http.address http服务的端口 HDFS DataNode 50475 dfs.datanode.https.address https服务的端口 HDFS DataNode 50020 dfs.datanode.ipc.address ipc服务的端口 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口(web界面) HDFS NameNode 50470 dfs.namenode.https-address https服务的端口 HDFS NameNode 8020 fs.defaultFS 接收Client连接的RPC端口，用于获取文件系统metadata信息。 HDFS journalnode 8485 dfs.journalnode.rpc-address RPC服务 HDFS journalnode 8480 dfs.journalnode.http-address HTTP服务 HDFS ZKFC 8019 dfs.ha.zkfc.port ZooKeeper FailoverController，用于NN HA YARN ResourceManager 8032 yarn.resourcemanager.address RM的applications manager(ASM)端口 YARN ResourceManager 8030 yarn.resourcemanager.scheduler.address scheduler组件的IPC端口 YARN ResourceManager 8031 yarn.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E6%95%B0%E4%BB%93/%E6%90%AD%E5%BB%BA%E6%95%B0%E4%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E6%95%B0%E4%BB%93/%E6%90%AD%E5%BB%BA%E6%95%B0%E4%BB%93/</guid><description>来源: https://mp.weixin.qq.com/s/PwnQl6uji85m7BGALmOVrw
数仓建模的过程分为业务建模、领域建模、逻辑建模和物理建模，但是这 些步骤比较抽象。为了便于落地，我根据自己的经验，总结出上面的七个步骤：梳理业务流程、垂直切分、指标体系梳理、实体关系调研、维度梳理、数仓分层以及物理模型建立。每个步骤不说理论，直接放工具、模板和案例。
1. 业务流程
找到公司核心业务流程，找到谁，在什么环节，做什么关键动作，得到什么结果。 梳理每个业务节点的客户及关注重点，找到数据在哪。 分域/主题
决定数仓的建设方式，快速交活，就用自下而上的建设。要全面支撑，就顶层规划，分步实施，交活稍微慢点。
同时按照业务领域划分主题域。主题域的划分方法有：按业务流划分（推荐）、按需求分、按职责分、按产品功能分等。 指标体系
指标的意义在于统一语言，统一口径。所以指标的定义必须有严格的标准。否则如无根之水。
指标可分为原子指标、派生指标和衍生指标，其含义及命名规则举例如下：
依照指标体系建设标准，开始梳理指标体系。整个体系同样要以业务为核心进行梳理。同时梳理每个业务过程所需的维度。维度就是你观察这个业务的角度，指标就是衡量这个业务结果 好坏的量化结果。 请注意，此时不能被现有数据局限。如果分析出这个业务过程应该有这个指标，但是没有数据，请标注出来，提出收集数据的需求。
4. 实体关系
每个业务动作都会有数据产生。我们将能够获取到的数据，提取实体，绘制ER图，便于之后的维度建模。 同样以业务过程为起点向下梳理，此时的核心是业务表。把每张表中涉及的维度、指标都整理出来。 维度整理 维度标准化是将各个业务系统中相同的维度进行统一的过程。其字段名称、代码、名字都可能不一样，我们需要完全掌握，并标准化。 维度的标准尽可能参照国家标准、行业标准。例如地区可以参照国家行政区域代码。
另外，有些维度存在层级，如区域的省、市、县。绝大多数业务系统中的级联就是多层级维度。
数仓分层 数据仓库一般分为4层，名字可能会不一样，但是其目的和建设方法基本一致： 每一层采用的建模方法都不一样，其核心是逐层解耦。越到底层，越接近业务发生的记录，越到上层，越接近业务目标。
依托数仓分层的设计理论，根据实际业务场景，我们就可以梳理出整体的数据流向图。这张图会很清晰的告诉所有人，数据从那来，到哪里去，最终提供什么样的服务。 模型建立 此时才真正进入纯代码阶段。数仓、ETL工具选型；ETL流程开发；cube的建立；任务调度，设定更新方式、更新频率；每日查看日志、监控etl执行情况等等。 前面梳理清楚了，ETL会变的非常清晰</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/hbase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/hbase/</guid><description>[toc]
1. 为何HBase速度很快？ HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree(Log-Structured Merge-Tree) + HTable(region分区) + Cache决定——客户端可以直接定位到要查数据所在的HRegion server服务器，然后直接在服务器的一个region上查找要匹配的数据，并且这些数据部分是经过cache缓存的。
HBase的写入速度快是因为它其实并不是真的立即写入文件中，而是先写入内存，随后异步刷入HFile。所以在客户端看来，写入速度很快。另外，写入时候将随机写入转换成顺序写，数据写入速度也很稳定。
读取速度快是因为它使用了LSM树型结构，而不是B或B+树.HBase的存储结构导致它需要磁盘寻道时间在可预测范围内，并且读取与所要查询的rowkey连续的任意数量的记录都不会引发额外的寻道开销。而且，HBase读取首先会在缓存（BlockCache）中查找，它采用了LRU（最近最少使用算法），如果缓存中没找到，会从内存中的MemStore中查找，再去HFile查找
https://zhuanlan.zhihu.com/p/83233850
2. hbase 实时查询的原理 实时查询，可以认为是从内存中查询，一般响应时间在 1 秒内。 HBase 的机制是数据先写入到内存中，当数据量达到一定的量（如 128M），再写入磁盘中， 在内存中，是不进行数据的更新或合并操作的，只增加数据，这使得用户的写操作只要进入内存中就可以立即返回，保证了 HBase I/O 的高性能。
3. 列簇怎么创建比较好？(&amp;lt;=2) rowKey 最好要创建有规则的 rowKey，即最好是有序的。 HBase 中一张表最好只创建一到两个列族比较好，因为 HBase 不能很好的处理多个列族。
4. 描述 HBase 中 scan 和 get 的功能以及实现的异同？ HBase 的查询实现只提供两种方式：
按指定 RowKey 获取唯一一条记录，get 方法（org.apache.hadoop.hbase.client.Get）Get 的方法处理分两种 : 设置了 ClosestRowBefore 和没有设置 ClosestRowBefore 的rowlock。主要是用来保证行的事务性，即每个 get 是以一个 row 来标记的。一个 row 中可以有很多 family 和 column。
按指定的条件获取一批记录，scan 方法(org.apache.Hadoop.hbase.client.Scan）实现条件查询功能使用的就是 scan 方式。
原文链接：https://blog.csdn.net/shujuelin/article/details/89035272</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/hive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/hive/</guid><description>[toc]
1. Hive 的 sort by 和 order by 的区别 order by 会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
sort by不是全局排序，其在数据进入reducer前完成排序.
因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&amp;gt;1， 则sort by只保证每个reducer的输出有序，不保证全局有序。
2. Hbase 和 hive 有什么区别hive 与 hbase 的底层存储是什么？hive是产生的原因是什么？habase是为了弥补hadoop的什么缺陷？
答案
​ 共同点：
hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储 区别：
Hive是建立在Hadoop之上为了减少MapReducejobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。
象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。
Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。
Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。
6. hive借用hadoop的MapReduce来完成一些hive中的命令的执行
7. hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。
8. hbase是列存储。
9. hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。
10. hive需要用到hdfs存储文件，需要用到MapReduce计算框架。
3. 数据仓库hive中，启动hive服务器的命令有哪些？分别代表什么意思？内部表与外部表有啥区别？分区与分桶，指的是什么？ 命令：
hive &amp;ndash;service metastore 启动元数据
hive：本地运行hive命令
hiveserver2：远程服务，开放默认端口 10000
内部表：内部表删除表时，数据也会被删除，
外部表：外部表在创建时需要加external，删除表时，表中的数据仍然会存储在hadoop中，不会丢失
分区：分文件夹：分目录，把一个大的数据集根据业务需要分割成小的数据集
分桶：分数据：分桶是将数据集分解成更容易管理的若干部分
原文链接：https://blog.csdn.net/pingsha_luoyan/article/details/97750251
4. hive中集合数据类型什么？有什么作用？什么情况下，hive需要使用集合类型？ 数据类型：
6个基本类型：整数，布尔类型，浮点数，字符，时间类型。字节数组
2个集合数据类型： struct，map，array</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/kafka/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/kafka/</guid><description>[toc]
1.kafka的message包括哪些信息 一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，比如是否压缩、压缩格式等等)；如果magic的值为0，那么不存在attributes属性body是由N个字节构成的一个消息体，包含了具体的key/value消息
2. 怎么解决kafka的数据丢失 producer端： 宏观上看保证数据的可靠安全性，肯定是依据分区数做好数据备份，设立副本数。
broker端： topic设置多分区，分区自适应所在机器，为了让各分区均匀分布在所在的broker中，分区数要大于broker数。
分区是kafka进行并行读写的单位，是提升kafka速度的关键。
Consumer端 consumer端丢失消息的情形比较简单：如果在消息处理完成前就提交了offset，那么就有可能造成数据的丢失。由于Kafka consumer默认是自动提交位移的，所以在后台提交位移前一定要保证消息被正常处理了，因此不建议采用很重的处理逻辑，如果处理耗时很长，则建议把逻辑放到另一个线程中去做。为了避免数据丢失，现给出两点建议：
enable.auto.commit=false 关闭自动提交位移
在消息被完整处理之后再手动提交位移
3.为什么Kafka不支持读写分离？ 也就是说: kafka对某一个主题的读写是在一个节点完成的,分区的从节点都是用来同步的
既然是在一个节点,为什么能做到高并发?(大概是因为: 对主题的读写本质是对主分区的读写,只要主分区尽可能的分配均匀至每个节点,就能分散压力)
因为这样有两个明显的缺点：
数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。 延时问题。数据从写入主节点到同步至从节点中的过程需要经历网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。 对于Kafka来说，必要性不是很高，因为在Kafka集群中，如果存在多个副本，经过合理的配置，可以让leader副本均匀的分布在各个broker上面，使每个 broker 上的读写负载都是一样的。
4. Kafka中的延迟队列 在发送延时消息的时候并不是先投递到要发送的真实主题（real_topic）中，而是先投递到一些 Kafka 内部的主题（delay_topic）中，这些内部主题对用户不可见，然后通过一个自定义的服务拉取这些内部主题中的消息，并将满足条件的消息再投递到要发送的真实的主题中，消费者所订阅的还是真实的主题。
5. Kafka为什么吞吐量大、速度快？ 一、顺序读写
众所周知Kafka是将消息记录持久化到本地磁盘中的，一般人会认为磁盘读写性能差，可能会对Kafka性能如何保证提出质疑。实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，一些情况下磁盘顺序读写性能甚至要高于内存随机读写。
二、Page Cache
为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：
1避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。
2避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题
相比于使用JVM或in-memory cache等数据结构，利用操作系统的Page Cache更加简单可靠。首先，操作系统层面的缓存利用率会更高，因为存储的都是紧凑的字节结构而不是独立的对象。其次，操作系统本身也对于Page Cache做了大量优化，提供了 write-behind、read-ahead以及flush等多种机制。再者，即使服务进程重启，系统缓存依然不会消失，避免了in-process cache重建缓存的过程。
通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。
三、零拷贝
linux操作系统 “零拷贝” 机制使用了sendfile方法， 允许操作系统将数据从Page Cache 直接发送到网络，只需要最后一步的copy操作将数据复制到 NIC 缓冲区， 这样避免重新复制数据 。示意图如下：
![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA3LzA4L1pyNTF6Vi5wbmc) 通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/spark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/spark/</guid><description>[toc]
1. spark shuffle过程 spark中管理shuffle的过程有一个shuffleManage负责管理, 在spark 2.X 之后,主要负责的是sortshufflemanager, 其中主要的是sortshuffle,shuffle分为两个阶段,shufflewriter和shuffleread
a) map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是5M
b) 在shuffle的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过5M时，比如现在内存结构中的数据为5.01M，那么他会申请5.01*2-5=5.02M内存给内存数据结构。
c) 如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。
d) 在溢写之前内存结构中的数据会进行排序分区
e) 然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是1万条数据，
f) map task执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。
g) reduce task去map端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。
https://blog.csdn.net/qq_21835703/article/details/79104733
2. spark streaming 读取kafka数据的两种方式 这两种方式分别是：
Receiver-base 使用Kafka的高层次Consumer API来实现。receiver从Kafka中获取的数据都存储在Spark Executor的内存中，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。
Direct Spark1.3中引入Direct方式，用来替代掉使用Receiver接收数据，这种方式会周期性地查询Kafka，获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。(使用比received更底层的api)
3. Spark提供的两种共享变量 Spark 程序的大部分操作都是 RDD 操作，通过传入函数给 RDD 操作函数来计算，这些函数在不同的节点上并发执行，内部的变量有不同的作用域，不能相互访问，有些情况下不太方便。
广播变量，是一个只读对象，在所有节点上都有一份缓存，创建方法是 SparkContext.broadcast()。创建之后再更新它的值是没有意义的，一般用 val 来修改定义。 计数器，只能增加，可以用计数或求和，支持自定义类型。创建方法是 SparkContext.accumulator(V, name)。只有 Driver 程序可以读这个计算器的变量，RDD 操作中读取计数器变量是无意义的。但节点可以对该计算器进行增加（？？？） 以上两种类型都是 Spark 的共享变量。
https://zhuanlan.zhihu.com/p/49169166
4. 解释一下Spark Master的选举过程 与hadoop一样，spark也存在单点故障问题，为此，spark的standalone模式提供了master的HA，与hadoop一样，一个是active，一个是standby状态，当active挂了，standby会顶上。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/%E9%80%9A%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/hadoop%E7%94%9F%E6%80%81%E5%9C%88/%E9%9D%A2%E8%AF%95/%E9%80%9A%E7%94%A8/</guid><description>[toc]
1.假如我有 100亿条数据，但是我们的内存只有1M，但是我们磁盘很大 我们现在要对这100亿条数据进行排序，是没法把所有的数据一次性的load进行内存进行排序的，这就涉及到一个外部排序的问题，我们的1M内存只能装进1亿条数据，每次都只能对这 1亿条数据进行排序，排好序后输出到磁盘，总共输出100个文件，最后怎么把这100个文件进行merge成一个全局有序的大文件。
我们可以每个文件（有序的）都取一部分头部数据最为一个 buffer， 并且把这 100个 buffer放在一个堆里面，进行堆排序，比较方式就是对所有堆元素（buffer）的head元素进行比较大小， 然后不断的把每个堆顶的 buffer 的head 元素 pop 出来输出到最终文件中，(想象链表的有序合并) 然后继续堆排序，继续输出。如果哪个buffer 空了，就去对应的文件中继续补充一部分数据。最终就得到一个全局有序的大文件。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E4%BB%8B%E7%BB%8D/</guid><description>Dubbo是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。简单的说，dubbo就是个服务框架，如果没有分布式的需求，其实是不需要用的，只有在分布式的时候，才有dubbo这样的分布式服务框架的需求，并且本质上是个服务调用的东东，说白了就是个远程服务调用的分布式框架
其核心部分包含:
远程通讯: 提供对多种基于长连接的NIO框架抽象封装，包括多种线程模型，序列化，以及“请求-响应”模式的信息交换方式。
集群容错: 提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。
自动发现: 基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器。
来自* &amp;lt;https://blog.csdn.net/wilsonke/article/details/39896595&amp;gt;
架构
节点角色说明
节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 来自* &amp;lt;http://dubbo.apache.org/zh-cn/docs/user/preface/architecture.html&amp;gt;
调用关系说明:
服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 来自* &amp;lt;http://dubbo.apache.org/zh-cn/docs/user/preface/architecture.html&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E4%BD%BF%E7%94%A8/</guid><description>使用框架:
Spring+Dubbo
分为 提供者/消费者
provider:
**配置:**新建文件 : beans-dubbo.xml,并写入, 可以把具体值写在properties中
&amp;lt;!-- 配置系统应用名称 --&amp;gt; &amp;lt;dubbo:application name=&amp;#34;${APP_NAME}&amp;#34; /&amp;gt; &amp;lt;!-- 通过注册中心发现监控中心服务 --&amp;gt; &amp;lt;dubbo:monitor protocol=&amp;#34;registry&amp;#34; /&amp;gt; &amp;lt;!-- 配置注册中心地址 --&amp;gt; &amp;lt;dubbo:registry protocol=&amp;#34;zookeeper&amp;#34; address=&amp;#34;${ZOOKEEPER_ADDRESS}&amp;#34; /&amp;gt; &amp;lt;!-- 配置服务发布方式（Dubbo 支持多种协议发布，Hessian只是其中的一种,这里用了原生方式） --&amp;gt; &amp;lt;dubbo:protocol name=&amp;#34;dubbo&amp;#34; port=&amp;#34;${APP_PORT}&amp;#34; /&amp;gt; application.xml: 写入 以下配置, 作用: 读取配置文件
&amp;lt;bean id=&amp;#34;propertyConfigurer&amp;#34; class=&amp;#34;com.jieshun.jht.framework.config.properties.PropertyPlaceholderConfigurer&amp;#34;&amp;gt; &amp;lt;property name=&amp;#34;locations&amp;#34;&amp;gt; &amp;lt;list&amp;gt; &amp;lt;value&amp;gt;classpath:${profile.properties}/*.properties&amp;lt;/value&amp;gt; &amp;lt;/list&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/bean&amp;gt; 作用:使beans-dubbo.xml 文件生效(在beans路径下,这应该看得懂吧,傻子)
&amp;lt;!-- hessian调用配置 --&amp;gt; &amp;lt;import resource=&amp;#34;classpath:beans/beans-*.xml&amp;#34; /&amp;gt; 文件:
1.写一个接口,写个方法
2.写个实现类,实现这个接口,这个就是你对外提供的方法,实现类上写个注解 @Service(&amp;ldquo;userIntegralManageService&amp;rdquo;),名字与消费者中配置文件中的订阅服务的id保持一致
别人就能通过接口来调用这个方法,就达到了远程调用
consume:
&amp;lt;!-- 配置系统应用名称 --&amp;gt; &amp;lt;dubbo:application name=&amp;#34;${APP_NAME}&amp;#34; /&amp;gt; &amp;lt;!</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E9%94%99%E8%AF%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/dubbo/%E9%94%99%E8%AF%AF/</guid><description>1. 调用某个方法时报错
Forbid consumer 10.101.90.181 access service com.jieshun.jht.oms.service.IPropertyManagerService from registry localhost:2181 use dubbo version 2.5.3, Please check registry access list (whitelist/blacklist).
原因: 大的来说,是连接不到 IPropertyManagerService 类
具体可以分:
服务端没启动或者启动失败(也就是说实现类异常) ip写错了,导致提供方注册到其他地方去了 没有在配置文件中指明接口与别名的关系(也有可能你傻逼,写错单词了),补充如下: &amp;lt;dubbo:reference id=&amp;#34;propertyManagerService&amp;#34; interface=&amp;#34;com.jieshun.jht.oms.service.IPropertyManagerService&amp;#34; /&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/io%E6%B5%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/io%E6%B5%81/</guid><description>字节流: 大多以Stream结尾
字符流:大多以Reader和Writer结尾
字符流与字节流的区别 经过以上的描述，我们可以知道字节流与字符流之间主要的区别体现在以下几个方面：
字节流操作的基本单元为字节；字符流操作的基本单元为Unicode码元。 字节流默认不使用缓冲区；字符流使用缓冲区。 字节流通常用于处理二进制数据，实际上它可以处理任意类型的数据，但它不支持直接写入或读取Unicode码元；字符流通常处理文本数据，它支持写入及读取Unicode码元。 参考地址:
字符流和字节流简述
IO类图</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/java8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/java8/</guid><description>[TOC]
介绍 stream的优势:
如表1-1中所示，Stream中的操作可以分为两大类：中间操作与结束操作，
中间操作只是对操作进行了记录，只有结束操作才会触发实际的计算（即惰性求值），这也是Stream在迭代大集合时高效的原因之一。
中间操作又可以分为无状态（Stateless）操作与有状态（Stateful）操作，前者是指元素的处理不受之前元素的影响；后者是指该操作只有拿到所有元素之后才能继续下去。
结束操作又可以分为短路与非短路操作，这个应该很好理解，前者是指遇到某些符合条件的元素就可以得到最终结果；而后者是指必须处理所有元素才能得到最终结果。
来自 http://www.cnblogs.com/Dorae/p/7779246.html
大概总结一下: 流式迭代集合操作,中间操作不会实际计算,而且会并行处理,(一个数据会同时被处理),等到了结束操作才会触发操作(和spark很像),java8的foreach还有并发处理,在数据量很大时foreach和流式的优势才会体现
1. 案例 函数式接口(导读): 那些地方可以用,看流的入参就行了
Function =&amp;gt; 函数,有输入有输出 参入参数T , 返回 R (用得多)
predicate =&amp;gt; 谓词/判定, 有输入,返回布尔值,主要作为一个谓词演算推导真假值存在 (用得多)
consumer =&amp;gt; 谓词/消费,有输入无输出,
supplier =&amp;gt; 提供, 无输入有输出,
(特殊) Operator =&amp;gt; 继承于 BiFunction ,所以也属于function, 算子Operator包括：UnaryOperator和BinaryOperator。分别对应单元算子和二元算子。其中BinaryOperator 可用于reduce
来自 https://blog.csdn.net/lz710117239/article/details/76192629
来自 http://www.sohu.com/a/123958799_465959
1.1 基本使用 // Function =&amp;gt; 就是一个函数,有输入输出 // 接收一个参数 Function&amp;lt;Integer, Integer&amp;gt; add = x -&amp;gt; x + 1; System.out.println(add.apply(4)); // 输出: 5 // 接收两个参数 BiFunction&amp;lt;String, Integer, Person&amp;gt; addxy2Str = (x, y) -&amp;gt; new Person(x, y); System.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm-%E5%8A%A0%E8%BD%BD-class-%E6%96%87%E4%BB%B6%E7%9A%84%E5%8E%9F%E7%90%86%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm-%E5%8A%A0%E8%BD%BD-class-%E6%96%87%E4%BB%B6%E7%9A%84%E5%8E%9F%E7%90%86%E6%9C%BA%E5%88%B6/</guid><description>JVM 中类的装载是由类加载器（ ClassLoader）和它的子类来实现的， Java 中的类加载器是一个重要的 Java 运行时系统组件，它负责在运行时查找和装入类文件中的类。
当 Java 程序需要使用某个类时， JVM 会确保这个类已经被加载、连接（验证、准备和解析）和初始化。类的加载是指把类的.class 文件中的数据读入到内存中，通常是创建一个字节数组读入.class 文件，然后产生与所加载类对应的 Class 对象。此时的类还不可用。
当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。
最后 JVM 对类进行初始化，包括：
1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；
2)如果类中存在初始化语句，就依次执行这些初始化语句。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm-%E8%B0%83%E4%BC%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm-%E8%B0%83%E4%BC%98/</guid><description>前言 无论是YGC或是FullGC，都会导致stop-the-world，即整个程序停止一些事务的处理，只有GC进程允许以进行垃圾回收，因此如果垃圾回收时间较长，部分web或socket程序，当终端连接的时候会报connetTimeOut或readTimeOut异常，
从JVM调优的角度来看，我们应该尽量避免发生YGC或FullGC，或者使得YGC和FullGC的时间足够的短。
所谓调优,就是找到适合自己程序的配置(jvm配置,比如,老年代大小,新生代大小,线程数,垃圾回收器等等)
原文链接：https://blog.csdn.net/Javazhoumou/article/details/99298624
GC日志 GC日志分类 Minor GC/Young GC，表示新生代GC，指发生在新生代的垃圾收集动作，所有的Minor GC都会触发全世界的暂停（stop-the-world），停止应用程序的线程，不过这个过程非常短。 Major GC/ Full GC：老年代GC，指发生在老年代的 GC，也称之为 Full GC。 日志分析 Young GC回收日志:
日志内容如下: [GC (Allocation Failure) [PSYoungGen: 637744K-&amp;gt;43513K(620544K)] 714874K-&amp;gt;144918K(730624K), 0.0652514 secs] [Times: user=0.20 sys=0.05, real=0.07 secs]
GC (Allocation Failure) : 表示发送了GC类型, 以及GC的原因, 此处是 内存分配失败
Full GC回收日志:
日志内容如下: [Full GC (Ergonomics) [PSYoungGen: 43513K-&amp;gt;23126K(620544K)] [ParOldGen: 101404K-&amp;gt;109897K(221184K)] 144918K-&amp;gt;133024K(841728K), [Metaspace: 77276K-&amp;gt;76763K(1120256K)], 0.3217926 secs] [Times: user=1.55 sys=0.06, real=0.32 secs]
GC原因类型 Allocation Failure
内存分配失败导致的GC，常见于年轻代当中。
GCLocker Initiated GC</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/jvm%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid><description>[toc]
jps jps 命令类似与 linux 的 ps 命令，但是它只列出系统中所有的 Java 应用程序。 通过 jps 命令可以方便地查看 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息。
-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数
-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null
-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名
-v 输出传递给JVM的参数
jinfo jinfo 是 JDK 自带的命令，可以用来查看正在运行的 java 应用程序的扩展参数，包括Java System属性和JVM命令行参数；也可以动态的修改正在运行的 JVM 一些参数。当系统崩溃时，jinfo可以从core文件里面知道崩溃的Java应用程序的配置信息
jinfo Pid 输出进程的基本信息。
jstat 用于监控虚拟机各种运行状态信息的命令行工具。他可以显示本地或远程虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据，在没有GUI图形的服务器上，它是运行期定位虚拟机性能问题的首选工具。
jstat -gcutil Pid 5s：每5秒输出一次GC情况。
jstack Jstat是JDK自带的一种堆栈跟踪工具。全称“Java Virtual Machine statistics monitoring tool”，可以用于生成java虚拟机当前时刻的线程快照
jstack -l Pid &amp;gt; /data/jstack.txt：将指定进行的线程情况进行输出到指定文件中。
jmap 命令jmap是一个多功能的命令。它可以生成 java 程序的 dump 文件， 也可以查看堆内对象示例的统计信息、查看 ClassLoader 的信息以及 finalizer 队列。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/</guid><description>[toc]
1. 大纲 1、根据Java虚拟机规范，Java虚拟机所管理的内存包括方法区、虚拟机栈、本地方法栈、堆、程序计数器等。
2、我们通常认为JVM中运行时数据存储包括堆和栈。这里所提到的栈其实指的是虚拟机栈，或者说是虚拟栈中的局部变量表。
3、栈中存放一些基本类型的变量数据（int/short/long/byte/float/double/Boolean/char）和对象引用。
4、堆中主要存放对象，即通过new关键字创建的对象。
5、数组引用变量是存放在栈内存中，数组元素是存放在堆内存中。
hotspot jdk8中移除了永久代以后的内存结构
JVM结构及堆的划分 - 光何 - 博客园 (cnblogs.com)
2. 方法区(元空间/永久代) 怎么回收对象? 《Java虚拟机规范》 中提到过可以不要求虚拟机在方法区中实现垃圾收集， 事实上也确实有未实现或未能完整实现方法区类型卸载的收集器存在（如JDK 11时期的ZGC收集器就不支持类卸载）
方法区的垃圾收集主要回收两部分内容： 废弃的常量和不再使用的类型
回收废弃常量与回收Java堆中的对象非常类似。 举个常量池中字面量回收的例子， 假如一个字符串“java”曾经进入常量池中， 但是当前系统又没有任何一个字符串对象的值是“java”， 换句话说， 已经没有任何字符串对象引用常量池中的“java”常量， 且虚拟机中也没有其他地方引用这个字面量。 如果在这时发生内存回收， 而且垃圾收集器判断确有必要的话， 这个“java”常量就将会被系统清理出常量池。 常量池中其他类（接口） 、 方法、 字段的符号引用也与此类似
判定一个类型是否属于“不再被使用的类”的条件需要同时满足下面三个条件：
该类所有的实例都已经被回收， 也就是Java堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收， 这个条件除非是经过精心设计的可替换类加载器的场景， 如OSGi、 JSP的重加载等， 否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用， 无法在任何地方通过反射访问该类的方法 Java虚拟机被允许对满足上述三个条件的无用类进行回收， 这里说的仅仅是“被允许”， 而并不是和对象一样， 没有引用了就必然会回收 ,关于是否要对类型进行回收 , HotSpot虚拟机提供了-Xnoclassgc参数进行控制
在大量使用反射、 动态代理、 CGLib等字节码框架， 动态生成JSP以及OSGi这类频繁自定义类加载器的场景中， 通常都需要Java虚拟机具备类型卸载的能力， 以保证不会对方法区造成过大的内存压力。
3. 堆的分代 Java虚拟机根据对象存活的周期不同，把堆内存划分为几块，一般分为新生代、老年代和永久代（对HotSpot虚拟机而言），这就是JVM的内存分代策略。
Java虚拟机将堆内存划分为新生代、老年代和永久代，永久代是HotSpot虚拟机特有的概念（JDK1.8之后为metaspace替代永久代），它采用永久代的方式来实现方法区，其他的虚拟机实现没有这一概念，永久代主要存放常量、类信息、静态变量等数据，与垃圾回收关系不大，新生代和老年代是垃圾回收的主要区域。
3.1 新生代（Young Generation） 新生成的对象优先存放在新生代中，新生代对象朝生夕死，存活率很低，在新生代中，常规应用进行一次垃圾收集一般可以回收70% ~ 95% 的空间，回收效率很高。 HotSpot将新生代划分为三块，一块较大的Eden（伊甸）空间和两块较小的Survivor（幸存者）空间，默认比例为8：1：1。划分的目的是因为HotSpot采用复制算法来回收新生代，设置这个比例是为了充分利用内存空间，减少浪费。新生成的对象在Eden区分配（大对象除外，大对象直接进入老年代），当Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。 GC开始时，对象只会存在于Eden区和From Survivor区，To Survivor区是空的（作为保留区域）。GC进行时，Eden区中所有存活的对象都会被复制到To Survivor区，而在From Survivor区中，仍存活的对象会根据它们的年龄值决定去向，要么移到老年代要么复制到To survivor区</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9B%9E%E6%94%B6%E5%89%8D%E6%8F%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9B%9E%E6%94%B6%E5%89%8D%E6%8F%90/</guid><description>[TOC]
1. 如果判断对象可以回收? 1.1 引用计数算法 定义: 在对象中添加一个引用计数器， 每当有一个地方引用它时， 计数器值就加一； 当引用失效时， 计数器值就减一； 任何时刻计数器为零的对象就是不可能再被使用的
主流的Java虚拟机里面都没有选用引用计数算法来管理内存， 主要原因是有些场景无法准确标记， 譬如单纯的引用计数就很难解决对象之间相互循环引用的问题
微软COM（Component Object Model） 技术、 使用ActionScript 3的FlashPlayer、 Python语言以及Squirrel中都使用了引用计数算法进行内存管理
1.2 可达性分析算法 这个算法的基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集， 从这些节点开始， 根据引用关系向下搜索， 搜索过程所走过的路径称为“引用链”（Reference Chain） ， 如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时， 则证明此对象是不可能再被使用的
在Java技术体系里面， 固定可作为GC Roots的对象包括以下几种：
在虚拟机栈（栈帧中的本地变量表） 中引用的对象， 譬如各个线程被调用的方法堆栈中使用到的参数、 局部变量、 临时变量等。 在方法区中类静态属性引用的对象， 譬如Java类的引用类型静态变量。 在方法区中常量引用的对象， 譬如字符串常量池（String Table） 里的引用。 在本地方法栈中JNI（即通常所说的Native方法） 引用的对象。 Java虚拟机内部的引用， 如基本数据类型对应的Class对象， 一些常驻的异常对象（比如NullPointExcepiton、 OutOfMemoryError） 等， 还有系统类加载器。 所有被同步锁（synchronized关键字） 持有的对象。 反映Java虚拟机内部情况的JMXBean、 JVMTI中注册的回调、 本地代码缓存等。 除了这些固定的GC Roots集合以外， 根据用户所选用的垃圾收集器以及当前回收的内存区域不同， 还可以有其他对象“临时性”地加入， 共同构成完整GC Roots集合。 譬如分代收集和局部回收（Partial GC） ， 如果只针对Java堆中某一块区域发起垃圾收集时（如最典型的只针对新生代的垃圾收集） ， 必须考虑到内存区域是虚拟机自己的实现细节（在用户视角里任何内存区域都是不可见的， 更不是孤立封闭的）， 所以某个区域里的对象完全有可能被位于堆中其他区域的对象所引用， 这时候就需要将这些关联区域的对象也一并加入GC Roots集合中去， 才能保证可达性分析的正确性</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8/</guid><description>[toc]
查询默认垃圾收集器 java -XX:+PrintCommandLineFlags -version
结果如下:
-XX:InitialHeapSize=257798976 -XX:MaxHeapSize=4124783616 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC -XX:+PrintCommandLineFlags java version &amp;#34;1.8.0_211&amp;#34; Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) 可以看到 jdk1.8 使用的ParallelGC , Parallel 垃圾收集器，即 PS Scavenge(Parallel Scavenge) 和 PS MarkSweep( Parallel Old)。
注意这里的 + 号，有 +/- 号表示启用/关闭，没有的表示是配置属性。
JDK8 及以前使用的是Parallel 垃圾收集器，jdk9 到 jdk16使用的都是 G1 收集器。
虽然推出了 ZGC, 但 可能是由于稳定性考虑没有马上更换
CMS默认情况下没有用在任何版本的回收器, CMS 使用标记清除算法，如果用作默认确实不怎么合适。
【Java】JVM - 各版本默认垃圾收集器 - 掘金 (juejin.cn)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E6%8C%87%E9%92%88%E7%A2%B0%E6%92%9E%E7%A9%BA%E9%97%B2%E5%88%97%E8%A1%A8tlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E6%8C%87%E9%92%88%E7%A2%B0%E6%92%9E%E7%A9%BA%E9%97%B2%E5%88%97%E8%A1%A8tlab/</guid><description>[toc]
Java堆是被所有线程共享的一块内存区域，主要用于存放对象实例，在堆上为对象分配内存就是把一块大小确定的内存从堆内存中划分出来，将对象放进去。常用的分配方法有指针碰撞和空闲列表以及TLAB
指针碰撞 适用于堆内存完整的情况，已分配的内存和空闲内存分表在不同的一侧，通过一个指针指向分界点，当需要分配内存时，把指针往空闲的一端移动与对象大小相等的距离即可，用于Serial和ParNew等不会产生内存碎片的垃圾收集器。
空闲列表 适用于堆内存不完整的情况，已分配的内存和空闲内存相互交错，JVM通过维护一张内存列表记录可用的内存块信息，当分配内存时，从列表中找到一个足够大的内存块分配给对象实例，并更新列表上的记录，最常见的使用此方案的垃圾收集器就是CMS。
TLAB 全称: （Thread Local Allocation Buffer 线程本地分配缓存区）
在使用指针碰撞的场景下, 由于堆内存本身是线程共享的，在多线程场景下，当一个线程需要创建对象，这时指针还没来得及修改（指针是在新对象占完位之后才能进行修改），如果另一个线程也需要分配空间，就会造成两个对象空间冲突，TLAB可以解决这种场景.
TALB的思路 TALB的解决思路比较简单粗暴，既然是因为堆内存多线程共享引发了问题，那我就直接给每个线程一个私有的区域分配对象不就解决了吗？
TALB的原理 TALB就是在堆内存上额外为每个线程分配一块线程私有区域，其大小一般比较小，默认占Eden区的1%。其本质就是通过start、top、end三个指针实现，其中start和end分别指向这个TALB的开始和结尾位置，用于确定该TALB在堆上对应区域，避免其他线程再过来分配内存，top实时指向TALB区域内当前可分配的第一个位置，当一个TALB满了或剩余空间不足以存储新申请的对象时，线程会向JVM再申请一块TALB。
到这里为止，指针碰撞多线程问题似乎已经得到了解决，不过由于TALB空间本身较小（默认只占Eden区1%），所以就很容易出现TALB剩余区域不足以存储新对象的情况，这时线程会把新对象存到新申请的TALB中，这样原有的TALB中剩余区域就会被浪费，造成内存泄漏。那么如何解决内存泄漏呢？
最大浪费空间 由于TALB内存浪费现象较为严重，所以JVM开发人员提出了一个最大浪费空间对TALB进行约束。 当TALB剩余空间存不下新对象时，会进行一个判断： ① 如果当前TALB剩余空间小于最大浪费空间，则TALB所属线程会向JVM申请一个新的TALB区域存储新对象，如果依旧存储不下，则对象会放在Eden区创建。 ② 如果当前TALB剩余空间大于最大浪费空间，则对象直接去Eden区创建。
TALB的局限性 虽然TALB解决了指针碰撞在多线程场景下的问题，并且通过最大浪费空间可以减少内存泄漏，但其本身依旧有一些缺点： ① GC更频繁： 由于每个TALB所占用的空间都要比线程实际需要的空间大小大一些（因为不可能每个TALB都刚好存满，也就是TALB空间浪费更严重），所以一批对象直接存储在Eden区会比存储在TALB区占用更少的空间，进而容易引发Minor GC。 ② TALB允许内存浪费，会导致Eden区内存不连续。
JVM指针碰撞和空闲列表
Java基础知识点总结系列
指针碰撞、空闲列表、TLAB是什么关系？ - 知乎 (zhihu.com)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</guid><description>[toc]
1. 什么是类的加载机制 ​ 类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。
类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误
加载.class文件的方式
从本地系统中直接加载 通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将Java源文件动态编译为.class文件 2. 类的生命周期 其中类加载的过程包括了加载、验证、准备、解析、初始化五个阶段。在这五个阶段中，加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持Java语言的运行时绑定（也成为动态绑定或晚期绑定）。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。
2.1 加载：查找并加载类的二进制数据 加载时类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情：
1、通过一个类的全限定名来获取其定义的二进制字节流。
2、将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。
3、在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。
相对于类加载的其他阶段而言，加载阶段（准确地说，是加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。
2.2 连接 验证：确保被加载的类的正确性 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作：
文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。
元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。
字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。
符号引用验证：确保解析动作能正确执行。
验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。
准备：为类的静态变量分配内存，并将其初始化为默认值
准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：
(1)、这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。
(2)、这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。
假设一个类变量的定义为：public static int value = 3；
那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的putstatic指令是在程序编译后，存放于类构造器（）方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。
· 这里还需要注意如下几点：
对基本数据类型来说，对于类变量（static）和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。
详见地址
​ (3)、如果类字段的字段属性表中存在ConstantValue属性，即同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值。
假设上面的类变量value被定义为： public static final int value = 3；
编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为3。回忆上一篇博文中对象被动引用的第2个例子，便是这种情况。我们可以理解为static final常量在编译期就将其结果放入了调用它的类的常量池中
解析：把类中的符号引用转换为直接引用 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。
符号引用: 就是一组符号来描述目标，可以是任何字面量。
直接引用: 就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。
2.3 初始化 ​ 初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式：
①声明类变量是指定初始值
②使用静态代码块为类变量指定初始值</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/lombok/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/lombok/</guid><description>[toc]
介绍 能用注解的方式来写代码.
比如:在编译的时候回自动生成get和set方法(不是生成代码,是生成字节码,反编译后可以看到)
安装 windows环境
下载lombok.jar包https://projectlombok.org/download.html
运行Lombok.jar:
Java -jar D:\software\lombok.jar D:\software\lombok.jar 这是windows下lombok.jar所在的位置 数秒后将弹出一框，以确认eclipse的安装路径 (无法运行就跳过2,3步骤)
确认完eclipse的安装路径后，点击install/update按钮，即可安装完成
安装完成之后，请确认eclipse安装路径下(与eclipse.ini 同级目录下)是否多了一个lombok.jar包，并且其 配置文件eclipse.ini中是否 添加了如下内容: -javaagent:lombok.jar -Xbootclasspath/a:lombok.jar 如果上面的答案均为true，那么恭喜你已经安装成功，否则将缺少的部分添加到相应的位置即可
重启eclipse或myeclipse,然后clean工程
来自 https://www.cnblogs.com/justuntil/p/7120534.html
使用 &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.projectlombok&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;lombok&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.16.20&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; @Getter @Setter @ToString public class SysUserEntity{ private String name; private String age; } 然后就可以直接使用get和set方法,tostring方法,还有其他注解,就不写出来了
来自 https://www.cnblogs.com/qnight/p/8997493.html
注解 @NoNull
对方法上的参数进行判null处理,如果为空抛NullPointerException异常
@CleanUp
自动资源管理器,能自动关闭流,这个注解会吞噬原有的异常(就是自己的代码有流异常),小心使用
使用: @Cleanup InputStream in = new FileInputStream(args[0]);
@Getter/@Setter
生成get/set方法
@ToString
生成toString方法
@EqualsAndHashCode
生成equal和hash方法
@NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/spi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/spi/</guid><description>[toc]
1. 介绍 SPI 全称：Service Provider Interface，是Java提供的一套用来被第三方实现或者扩展的接口，它可以用来启用框架扩展和替换组件。比如java.sql.Driver接口，其他不同厂商可以针对同一接口做出不同的实现，MySQL和PostgreSQL都有不同的实现提供给用户，而Java的SPI机制可以为某个接口寻找服务实现。Java中SPI机制主要思想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是 解耦。
简单来说, 框架只提供接口(规范), 具体实现由业务自行实现, 调用的时候自然就调用了具体的业务类,
和spring 的ioc很类似, 定义接口, 由实现类完成业务, 只是 ioc使用 @service 等注解完成类注入, SPI 通过java.util.ServiceLoader 注入,
如此, 即使没有spring框架也能做到定义接口, 由业务方来实现, 比如在common包中,没有spring环境, 但需要提供这种能力,就可以用SPI
2. 案例介绍 定义一个接口：
Phone.java
package com.light.sword; /** * @author: Jack * 2021/1/31 上午1:44 */ public interface Phone { String getSystemInfo(); } 两个实现
package com.light.sword; /** * 华为手机 * @author: Jack * 2021/1/31 上午1:48 */ public class Huawei implements Phone { @Override public String getSystemInfo() { return &amp;#34;Hong Meng&amp;#34;; } } package com.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%8A%A8%E6%80%81%E5%AD%97%E8%8A%82%E7%A0%81%E6%8A%80%E6%9C%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%8A%A8%E6%80%81%E5%AD%97%E8%8A%82%E7%A0%81%E6%8A%80%E6%9C%AF/</guid><description>[toc]
1.字节码 1.1 什么是字节码？ Java之所以可以“一次编译，到处运行”，一是因为JVM针对各种操作系统、平台都进行了定制，二是因为无论在什么平台，都可以编译生成固定格式的字节码（.class文件）供JVM使用。因此，也可以看出字节码对于Java生态的重要性。之所以被称之为字节码，是因为字节码文件由十六进制值组成，而JVM以两个十六进制值为一组，即以字节为单位进行读取。在Java中一般是用javac命令编译源代码为字节码文件，一个.java文件从编译到运行的示例如图1所示。
1.2 字节码结构 .java文件通过javac编译后将得到一个.class文件，比如编写一个简单的ByteCodeDemo类，如下图2的左侧部分：
编译后生成ByteCodeDemo.class文件，打开后是一堆十六进制数，按字节为单位进行分割后展示如图2右侧部分所示。上文提及过，JVM对于字节码是有规范要求的，那么看似杂乱的十六进制符合什么结构呢？JVM规范要求每一个字节码文件都要由十部分按照固定的顺序组成，整体结构如图3所示。接下来我们将一一介绍这十部分：
（1） 魔数（Magic Number）
所有的.class文件的前四个字节都是魔数，魔数的固定值为：0xCAFEBABE。魔数放在文件开头，JVM可以根据文件的开头来判断这个文件是否可能是一个.class文件，如果是，才会继续进行之后的操作。
有趣的是，魔数的固定值是Java之父James Gosling制定的，为CafeBabe（咖啡宝贝），而Java的图标为一杯咖啡。
魔数是表示一个文件的类型的,图片有图片的魔数,文本有文本的魔数,且魔数是可以更改的
Magic Number(魔数）是什么_i_can1的博客-CSDN博客_魔数是什么
（2） 版本号
版本号为魔数之后的4个字节，前两个字节表示次版本号（Minor Version），后两个字节表示主版本号（Major Version）。上图2中版本号为“00 00 00 34”，次版本号转化为十进制为0，主版本号转化为十进制为52，在Oracle官网中查询序号52对应的主版本号为1.8，所以编译该文件的Java版本号为1.8.0。
（3） 常量池（Constant Pool）
紧接着主版本号之后的字节为常量池入口。常量池中存储两类常量：字面量与符号引用。字面量为代码中声明为Final的常量值，符号引用如类和接口的全局限定名、字段的名称和描述符、方法的名称和描述符。常量池整体上分为两部分：常量池计数器以及常量池数据区，如下图4所示。
常量池计数器（constant_pool_count）：由于常量的数量不固定，所以需要先放置两个字节来表示常量池容量计数值。图2中示例代码的字节码前10个字节如下图5所示，将十六进制的24转化为十进制值为36，排除掉下标“0”，也就是说，这个类文件中共有35个常量。 图5 前十个字节及含义
常量池数据区：数据区是由（constant_pool_count-1）个cp_info结构组成，一个cp_info结构对应一个常量。在字节码中共有14种类型的cp_info（如下图6所示），每种类型的结构都是固定的。 图6 各类型的cp_info
具体以CONSTANT_utf8_info为例，它的结构如下图7左侧所示。首先一个字节“tag”，它的值取自上图6中对应项的Tag，由于它的类型是utf8_info，所以值为“01”。接下来两个字节标识该字符串的长度Length，然后Length个字节为这个字符串具体的值。从图2中的字节码摘取一个cp_info结构，如下图7右侧所示。将它翻译过来后，其含义为：该常量类型为utf8字符串，长度为一字节，数据为“a”。
图7 CONSTANT_utf8_info的结构（左）及示例（右）
其他类型的cp_info结构在本文不再赘述，整体结构大同小异，都是先通过Tag来标识类型，然后后续n个字节来描述长度和（或）数据。先知其所以然，以后可以通过javap -verbose ByteCodeDemo命令，查看JVM反编译后的完整常量池，如下图8所示。可以看到反编译结果将每一个cp_info结构的类型和值都很明确地呈现了出来。
（4） 访问标志
常量池结束之后的两个字节，描述该Class是类还是接口，以及是否被Public、Abstract、Final等修饰符修饰。JVM规范规定了如下图9的访问标志（Access_Flag）。需要注意的是，JVM并没有穷举所有的访问标志，而是使用按位或操作来进行描述的，比如某个类的修饰符为Public Final，则对应的访问修饰符的值为ACC_PUBLIC | ACC_FINAL，即0x0001 | 0x0010=0x0011。
图9 访问标志
（5） 当前类名
访问标志后的两个字节，描述的是当前类的全限定名。这两个字节保存的值为常量池中的索引值，根据索引值就能在常量池中找到这个类的全限定名。
（6） 父类名称
当前类名后的两个字节，描述父类的全限定名，同上，保存的也是常量池中的索引值。
（7） 接口信息
父类名称后为两字节的接口计数器，描述了该类或父类实现的接口数量。紧接着的n个字节是所有接口名称的字符串常量的索引值。
（8） 字段表
字段表用于描述类和接口中声明的变量，包含类级别的变量以及实例变量，但是不包含方法内部声明的局部变量。字段表也分为两部分，第一部分为两个字节，描述字段个数；第二部分是每个字段的详细信息fields_info。字段表结构如下图所示：
图10 字段表结构
以图2中字节码的字段表为例，如下图11所示。其中字段的访问标志查图9，0002对应为Private。通过索引下标在图8中常量池分别得到字段名为“a”，描述符为“I”（代表int）。综上，就可以唯一确定出一个类中声明的变量private int a。
图11 字段表示例</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/forkjoin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/forkjoin/</guid><description>[toc]
前言 它通过 「 分而治之 」 的方法尝试将所有可用的处理器内核使用起来帮助加速并行处理。
在实际使用过程中，这种 「 分而治之 」的方法意味着框架首先要 fork ，递归地将任务分解为较小的独立子任务，直到它们足够简单以便异步执行。然后，join 部分开始工作，将所有子任务的结果递归地连接成单个结果，或者在返回 void 的任务的情况下，程序只是等待每个子任务执行完毕。
Fork/Join框架是一个实现了ExecutorService接口的多线程处理器，它专为那些可以通过递归分解成更细小的任务而设计，最大化的利用多核处理器来提高应用程序的性能。
与其他ExecutorService相关的实现相同的是，Fork/Join框架会将任务分配给线程池中的线程。而与之不同的是，Fork/Join框架在执行任务时使用了工作窃取算法。
ForkJoin 有两大核心思想：
分治算法； 工作密取：为了充分利用 cpu 资源，一个工作线程执行完自己队列的任务之后，不会空闲，而是从其它队列里寻找任务。 **使用场景: **
ForkJoinPool 不是为了替代 ExecutorService，而是它的补充，在某些应用场景下性能比 ExecutorService 更好。
ForkJoinPool 主要用于实现“分而治之”的算法，特别是分治之后递归调用的函数，例如 quick sort 等。
ForkJoinPool 最适合的是计算密集型的任务，如果存在 I/O，线程间同步，sleep() 等会造成线程长时间阻塞的情况时，最好配合使用 ManagedBlocker。
ManagedBlocker 它可以控制在阻塞时增加并行数, 这样就不会卡死了
18 Fork/Join框架 · 深入浅出Java多线程 (redspider.group)
一文秒懂 Java Fork/Join - Java 一文秒懂 - 简单教程，简单编程 (twle.cn)
在Java8 parallelStream()中使用I/O + ManagedBlocker有什么问题吗? - IT宝库 (itbaoku.cn)
关于ForkJoinPool使用ManagedBlocker防线程阻塞而降低吞吐量的说明_heng_zou的博客-CSDN博客_forkjoin 阻塞
ManagedBlocker的使用和深入理解ForkJoin 有待提升
1. 工作窃取算法 工作窃取算法指的是在多线程执行不同任务队列的过程中，某个线程执行完自己队列的任务后从其他线程的任务队列里窃取任务来执行。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/lock/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/lock/</guid><description>[toc]
1. 定义 在 Lock 接口出现之前，Java 程序是靠 synchronized 关键字实现锁功能的，而 Java SE 5之后，并发包中新增了 Lock 接口（以及相关实现类）用来实现锁功能，它提供了与synchronized 关键字类似的同步功能,只是在使用时需要显式地获取和释放锁。
虽然它缺少了（通过 synchronized 块或者方法所提供的）隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性、可中断的获取锁以及超时获取锁等多种 synchronized 关键字所不具备的同步特性。
Lock也是维护了一个锁（state），和一个等待队列(AQS)，这也是Lock在底层实现的两个核心元素。AQS队列解决了线程同步的问题，volatile定义的锁状态解决保证了线程对于临界区代码访问的互斥，并且解决了各个线程对于锁状态的可见性问题。
Lock 是一个接口，它定义了锁获取和释放的基本操作
2. 使用 一般使用Lock会使用它的实现类, java中常用的就是ReentrantLock 类
Lock lock = new ReentrantLock(); lock.lock(); try { // do some thing } finally { lock.unlock(); } 不要将获取锁的过程写在 try 块中，因为如果在获取锁（自定义锁的实现） 时发生了异常，异常抛出的同时，也会导致锁无故释放。
关键字synchronized与wait()和notify()/notifyAll()方法相结合可以实现等待/通知模式，类ReentrantLock也可以实现同样的功能，但需要借助于Condition对象
condition需要获得锁后使用
private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); 一把锁可以生成多个condition, (变量各自控制各自线程的等待和唤起)
当调用 await()方法后，当前线程会释放锁并在此等待，而其他线程调用 Condition 对象的 signal()方法，通知当前线程后，当前线程才从 await()方法返回，并且在返回前已经获取了锁</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/threadlocal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/threadlocal/</guid><description>[toc]
1. ThreadLocal是什么？ 从名字我们就可以看到ThreadLocal 叫做本地线程变量，意思是说，ThreadLocal 中填充的的是当前线程的变量，该变量对其他线程而言是封闭且隔离的，ThreadLocal 为变量在每个线程中创建了一个副本，这样每个线程都可以访问自己内部的副本变量。
简单实用
@Test public void testThreadLocal() { ThreadLocal&amp;lt;String&amp;gt; local = new ThreadLocal&amp;lt;&amp;gt;(); IntStream.range(0, 10) .forEach(i -&amp;gt; new Thread(() -&amp;gt; { local.set(Thread.currentThread().getName() + &amp;#34;:&amp;#34; + i); System.out.println(&amp;#34;线程：&amp;#34; + Thread.currentThread().getName() + &amp;#34;,local:&amp;#34; + local.get()); }).start() ); } 输出结果： 线程：Thread-0,local:Thread-0:0 线程：Thread-1,local:Thread-1:1 线程：Thread-2,local:Thread-2:2 线程：Thread-3,local:Thread-3:3 线程：Thread-4,local:Thread-4:4 线程：Thread-5,local:Thread-5:5 线程：Thread-6,local:Thread-6:6 线程：Thread-7,local:Thread-7:7 线程：Thread-8,local:Thread-8:8 线程：Thread-9,local:Thread-9:9 2. ThreadLocal的具体实现 2.1 ThreadLocal结构 每一个线程都有一个 ThreadLocalMap ； 该 Map 底层由 Entry 数组构成，含有多个 Entry ； Entry 中 key 为 ThreadLocal 的弱引用， value 为我们保存的值 线程下的threadLocalMap</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%85%B3%E9%94%AE%E8%AF%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%85%B3%E9%94%AE%E8%AF%8D/</guid><description>[TOC]
1. volatile 1.1 volatile的内存语义 在Java中，volatile关键字有特殊的内存语义。volatile主要有以下两个功能：
保证变量的内存可见性 禁止volatile变量与普通变量重排序（JSR133提出，Java 5 开始才有这个“增强的volatile内存语义”） 这两个功能都是通过内存屏障实现的
public class VolatileExample { int a = 0; volatile boolean flag = false; public void writer() { a = 1; // step 1 flag = true; // step 2 } public void reader() { if (flag) { // step 3 System.out.println(a); // step 4 } } } 在这段代码里，我们使用volatile关键字修饰了一个boolean类型的变量flag。
所谓内存可见性，指的是当一个线程对volatile修饰的变量进行写操作（比如step 2）时，JMM会立即把该线程对应的本地内存中的共享变量的值刷新到主内存；当一个线程对volatile修饰的变量进行读操作（比如step 3）时，JMM会把立即该线程对应的本地内存置为无效，从主内存中读取共享变量的值。
在这一点上，volatile与锁具有相同的内存效果，volatile变量的写和锁的释放具有相同的内存语义，volatile变量的读和锁的获取具有相同的内存语义。 所以上面的代码是线程安全的
单纯的赋值操作是原子性的。
而如果flag变量没有用volatile修饰，在step 2，线程A的本地内存里面的变量就不会立即更新到主内存，那随后线程B也同样不会去主内存拿最新的值，仍然使用线程B本地内存缓存的变量的值a = 0，flag = false。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>[toc]
1. 并发模型 两种并发模型可以解决这两个问题：
消息传递并发模型 (CSP-&amp;gt;Go语言 ; Actor模型-&amp;gt;akka框架) 共享内存并发模型 (java) 基于消息传递的并发模型 - 知乎 (zhihu.com)
2. 内存不可见问题 现代计算机为了高效，往往会在高速缓存区中缓存共享变量，因为cpu访问缓存区比访问内存要快得多。
线程之间的共享变量存在主内存中，每个线程都有一个私有的本地内存，存储了该线程以读、写共享变量的副本。本地内存是Java内存模型的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器等。
Java线程之间的通信由Java内存模型（简称JMM）控制，从抽象的角度来说，JMM定义了线程和主内存之间的抽象关系。JMM的抽象示意图如图所示：
所以，线程A无法直接访问线程B的工作内存，线程间通信必须经过主内存。
注意，根据JMM的规定，线程对共享变量的所有操作都必须在自己的本地内存中进行，不能直接从主内存中读取。
所以出现了内存不可见问题!
那么怎么知道这个共享变量的被其他线程更新了呢？这就是JMM的功劳了，也是JMM存在的必要性之一。JMM通过控制主内存与每个线程的本地内存之间的交互，来提供内存可见性保证。
具体操作如下图:
6 Java内存模型基础知识 · 深入浅出Java多线程 (redspider.group)
3. 重排序 计算机在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排。
指令重排对于提高CPU处理性能十分必要。虽然由此带来了乱序的问题，但是这点牺牲是值得的。
指令重排一般分为以下三种：
编译器优化重排
编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
指令并行重排
现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序。
内存系统重排
由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。
指令重排可以保证串行语义一致，但是没有义务保证多线程间的语义也一致。所以在多线程下，指令重排序可能会导致一些问题。
4. happens-before 一方面，程序员需要JMM提供一个强的内存模型来编写代码；另一方面，编译器和处理器希望JMM对它们的束缚越少越好，这样它们就可以最可能多的做优化来提高性能，希望的是一个弱的内存模型。
JMM考虑了这两种需求，并且找到了平衡点，对编译器和处理器来说，只要不改变程序的执行结果（单线程程序和正确同步了的多线程程序），编译器和处理器怎么优化都行。
而对于程序员，JMM提供了happens-before规则（JSR-133规范），满足了程序员的需求——**简单易懂，并且提供了足够强的内存可见性保证。**换言之，程序员只要遵循happens-before规则，那他写的程序就能保证在JMM中具有强的内存可见性。
happens-before关系的定义如下：
如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM也允许这样的重排序。 happens-before关系本质上和as-if-serial语义是一回事。
总之，如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的，不管它们在不在一个线程。
天然的happens-before关系
在Java中，有以下天然的happens-before关系：
程序顺序规则：一个线程中的每一个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start规则：如果线程A执行操作ThreadB.start()启动线程B，那么A线程的ThreadB.start（）操作happens-before于线程B中的任意操作、 join规则：如果线程A执行操作ThreadB.join（）并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 5. as-if-seial as-if-serial语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。
为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E9%9B%86%E5%90%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E9%9B%86%E5%90%88/</guid><description>[TOC]
1. 并发容器类介绍 2. ConcurrentHashMap 2.1 jdk1.7实现 在JDK1.7版本中，ConcurrentHashMap的数据结构是由一个Segment数组和多个HashEntry组成，如下图所示：
Segment数组的意义就是将一个大的table分割成多个小的table来进行加锁，而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样
put操作 对于ConcurrentHashMap的数据插入，这里要进行两次Hash去定位数据的存储位置
static class Segment&amp;lt;K,V&amp;gt; extends ReentrantLock implements Serializable { } 从上Segment的继承体系可以看出，Segment实现了ReentrantLock,也就带有锁的功能，
当执行put操作时，会进行第一次key的hash来定位Segment的位置，如果该Segment还没有初始化，即通过CAS操作进行赋值，然后进行第二次hash操作，找到相应的HashEntry的位置，这里会利用继承过来的锁的特性，在将数据插入指定的HashEntry位置时（链表的尾端），会通过继承ReentrantLock的tryLock（）方法尝试去获取锁，如果获取成功就直接插入相应的位置，如果已经有线程获取该Segment的锁，那当前线程会以自旋的方式去继续的调用tryLock（）方法去获取锁，超过指定次数就挂起，等待唤醒
简单的说, 会通过两次hash分别找到对应的Segment和hashEntry的位置, 然后再插进去, 每个Segment都是ReentrantLock锁, 所以在Segment上加锁来保证线程安全
2.2 jdk1.8 实现 JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本
这些是构成ConcurrentHashMap实现结构的基础，下面看一下基本属性：
// node数组最大容量：2^30=1073741824 private static final int MAXIMUM_CAPACITY = 1 &amp;lt;&amp;lt; 30 ; // 默认初始值，必须是2的幂数 private static final int DEFAULT_CAPACITY = 16 ; //数组可能最大值，需要与toArray（）相关方法关联 static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8 ; //并发级别，遗留下来的，为兼容以前的版本 private static final int DEFAULT_CONCURRENCY_LEVEL = 16 ; // 负载因子 private static final float LOAD_FACTOR = 0.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E6%B1%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E6%B1%A0/</guid><description>[TOC]
1. 前言 线程池解决的核心问题就是资源管理问题。在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。
为解决资源分配这个问题，线程池采用了“池化”（Pooling）思想。池化，顾名思义，是为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。
“池化”思想不仅仅能应用在计算机领域，在金融、设备、人员管理、工作管理等领域也有相关的应用。
在计算机领域中的表现为：统一管理IT资源，包括服务器、存储、和网络资源等等。通过共享资源，使用户在低投入中获益。除去线程池，还有其他比较典型的几种使用策略包括：
内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。 连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。 实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。 jdk 提供了快捷创建线程池的方式
Executors 类下提供了定长/延时/单一等线程池,但由于上线使用系统资源,一般不建议使用
手动创建ThreadPoolExecutor对象
其核心参数:
/** * corePoolSize – 要保留在池中的线程数，即使它们处于空闲状态，除非设置了allowCoreThreadTimeOut maximumPoolSize – 池中允许的最大线程数 keepAliveTime – 当线程数大于核心数时，这是多余空闲线程在终止前等待新任务的最长时间。 unit – keepAliveTime参数的时间单位 workQueue – 用于在执行任务之前保存任务的队列。 这个队列将只保存execute方法提交的Runnable任务 */ public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&amp;lt;Runnable&amp;gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } 其执行流程
为什么一般不建议使用Executors创建线程池？_damokelisijian866的博客-CSDN博客
2、线程池核心设计与实现 2.1 总体设计 Java中的线程池核心实现类是ThreadPoolExecutor，本章基于JDK 1.8的源码来分析Java线程池的核心设计与实现。我们首先来看一下ThreadPoolExecutor的UML类图，了解下ThreadPoolExecutor的继承关系。
ThreadPoolExecutor实现的顶层接口是Executor，顶层接口Executor提供了一种思想：将任务提交和任务执行进行解耦。用户无需关注如何创建线程，如何调度线程来执行任务，用户只需提供Runnable对象，将任务的运行逻辑提交到执行器(Executor)中，由Executor框架完成线程的调配和任务的执行部分。ExecutorService接口增加了一些能力：
（1）扩充执行任务的能力，补充可以为一个或一批异步任务生成Future的方法；
（2）提供了管控线程池的方法，比如停止线程池的运行。
AbstractExecutorService则是上层的抽象类，将执行任务的流程串联了起来，保证下层的实现只需关注一个执行任务的方法即可。
最下层的实现类ThreadPoolExecutor实现最复杂的运行部分，ThreadPoolExecutor将会一方面维护自身的生命周期，另一方面同时管理线程和任务，使两者良好的结合从而执行并行任务。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%BC%82%E5%B8%B8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E5%BC%82%E5%B8%B8/</guid><description>error 是不可以被捕获的</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%97%B6%E9%97%B4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%97%B6%E9%97%B4/</guid><description>Java日期格式化参数对照表
常用格式: yyyy-MM-dd HH:mm:ss
纪元标记 AD 备注 y/u 年份 2001 u 在使用jdk8格式化使用过 Y 周年年份 2001 当天所在的周属于的年份, (一年中的最后一周存在跨年的情况) 来自 http://www.cnblogs.com/zheting/p/7702470.html M 月份 July or 07 d 一个月的第几天 10 h A.M./P.M. (1~12)格式小时 12 H 一天中的小时 (0~23) 22 m 分钟数 30 s 秒数 55 S 毫秒数 234 E 星期几 Tuesday D 一年中的第几天 360 F 一个月中第几周的周几 2 (second Wed. in July) w 一年中第几周 40 W 一个月中第几周 1 a A.M./P.M. 标记 PM k 一天中的小时(1~24) 24 K A.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%9E%9A%E4%B8%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%9E%9A%E4%B8%BE/</guid><description>[toc]
用法一：常量 public enum Color { RED, GREEN, BLANK, YELLOW } 也可以写构造方法,给枚举加值
public enum Color { RED(&amp;#34;红色&amp;#34;, 1), GREEN(&amp;#34;绿色&amp;#34;, 2), BLANK(&amp;#34;白色&amp;#34;, 3), YELLO(&amp;#34;黄色&amp;#34;, 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) { this.name = name; this.index = index; } // 普通方法 , 配个switch使用更佳 public static String getName(int index) { for (Color c : Color.values()) { if (c.getIndex() == index) { return c.name; } } return null; } // get set 方法 public String getName() { return name; } public void setName(String name) { this.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%B3%9B%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E6%B3%9B%E5%9E%8B/</guid><description>[toc]
一. 为啥要泛型? 简单的说,我 new一个list,我想放字符串,数字等多种数据类型,怎么办? 那就整个泛型!(理解意思就行)
https://www.cnblogs.com/lwbqqyumidi/p/3837629.html
二. 什么是泛型？ 泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。
来自 https://www.cnblogs.com/lwbqqyumidi/p/3837629.html
三. 泛型的使用 泛型类 //此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型(见名知意), T : Type K V : Key Value E : Element ? : 泛型通配符(后面会讲到) 来自 https://segmentfault.com/a/1190000014824002
//在实例化泛型类时，必须指定T的具体类型 public class Generic&amp;lt;T&amp;gt;{ //key这个成员变量的类型为T,T的类型由外部指定 private T key; public Generic(T key) { //泛型构造方法形参key的类型也为T，T的类型由外部指定 this.key = key; } public T getKey(){ //泛型方法getKey的返回值类型为T，T的类型由外部指定 return key; } } 注意：
泛型的类型参数只能是类类型，不能是简单类型。
不能对确切的泛型类型使用instanceof操作。如下面的操作是非法的，编译时会出错。 if(ex_num instanceof Generic&amp;lt;Number&amp;gt;){ } 来自https://blog.csdn.net/s10461/article/details/53941091#commentBox
泛型接口 //定义一个泛型接口 public interface Generator&amp;lt;T&amp;gt; { public T next(); } 来自 https://blog.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%AE%9E%E6%88%98-%E7%BE%8E%E5%9B%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%AE%9E%E6%88%98-%E7%BE%8E%E5%9B%A2/</guid><description>设计模式二三事 设计模式是众多软件开发人员经过长时间的试错和应用总结出来的，解决特定问题的一系列方案。现行的部分教材在介绍设计模式时，有些会因为案例脱离实际应用场景而令人费解，有些又会因为场景简单而显得有些小题大做。
本文会根据在美团金融服务平台设计开发时的经验，结合实际的案例，并采用“师生对话”这种相对诙谐的形式去讲解几类常用设计模式的应用。希望能对想提升系统设计能力的同学有所帮助或启发。
本篇涉及
策略模式 适配器模式 单例模式 状态模式 观察者模式 建造者模式 装饰器模式 引言 话说这是在程序员世界里一对师徒的对话：
“老师，我最近在写代码时总感觉自己的代码很不优雅，有什么办法能优化吗？”
“嗯，可以考虑通过教材系统学习，从注释、命名、方法和异常等多方面实现整洁代码。”
“然而，我想说的是，我的代码是符合各种编码规范的，但是从实现上却总是感觉不够简洁，而且总是需要反复修改！”学生小明叹气道。
老师看了看小明的代码说：“我明白了，这是系统设计上的缺陷。总结就是抽象不够、可读性低、不够健壮。”
“对对对，那怎么能迅速提高代码的可读性、健壮性、扩展性呢？”小明急不可耐地问道。
老师敲了敲小明的头：“不要太浮躁，没有什么方法能让你立刻成为系统设计专家。但是对于你的问题，我想设计模式可以帮到你。”
“设计模式？”小明不解。
“是的。”老师点了点头，“世上本没有路，走的人多了，便变成了路。在程序员的世界中，本没有设计模式，写代码是人多了，他们便总结出了一套能提高开发和维护效率的套路，这就是设计模式。设计模式不是什么教条或者范式，它可以说是一种在特定场景下普适且可复用的解决方案，是一种可以用于提高代码可读性、可扩展性、可维护性和可测性的最佳实践。”
“哦哦，我懂了，那我应该如何去学习呢？”
“不急，接下来我来带你慢慢了解设计模式。”
奖励的发放策略 第一天，老师问小明：“你知道活动营销吗？”
“这我知道，活动营销是指企业通过参与社会关注度高的已有活动，或整合有效的资源自主策划大型活动，从而迅速提高企业及其品牌的知名度、美誉度和影响力，常见的比如有抽奖、红包等。”
老师点点头：“是的。我们假设现在就要做一个营销，需要用户参与一个活动，然后完成一系列的任务，最后可以得到一些奖励作为回报。活动的奖励包含美团外卖、酒旅和美食等多种品类券，现在需要你帮忙设计一套奖励发放方案。”
因为之前有过类似的开发经验，拿到需求的小明二话不说开始了编写起了代码：
// 奖励服务 class RewardService { // 外部服务 private WaimaiService waimaiService; private HotelService hotelService; private FoodService foodService; // 使用对入参的条件判断进行发奖 public void issueReward(String rewardType, Object ... params) { if (&amp;#34;Waimai&amp;#34;.equals(rewardType)) { WaimaiRequest request = new WaimaiRequest(); // 构建入参 request.setWaimaiReq(params); waimaiService.issueWaimai(request); } else if (&amp;#34;Hotel&amp;#34;.equals(rewardType)) { HotelRequest request = new HotelRequest(); request.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%AE%80%E8%BF%B0%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%AE%80%E8%BF%B0%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</guid><description>[toc]
1. 适配器模式 类比插座,有欧洲标准,中国标准,为了在欧洲使用中国的插头,所以需要一个适配器,做到兼容
在正常开发中,一般是针对一个已有的类,想要用它的功能,由于各种原因,不能直接用,需要写一个中间类,自己调用这个中间类,由中间类去调用已有的类
优点：
1.通过适配器，客户端可以直接调用同一接口，因而对客户端来说是透明的。这样做更简单，更直接，更紧凑。
2.复用了现存的类，解决了现存类和复用环境要求不一致的问题。
3.将目标类和适配者类解耦，通过引入一个适配器现有的适配者类，而无须修改原有代码。
一个对象适配器可以把多个不同的适配器类适配到同一个目标，也就是说，同一个适配器可以把适配者类和他的子类都适配到目标接口。
缺点：
1.对于适配器来说，更换适配器的实现过程比较复杂 。
2. 代理模式 优点:
\1. 动态代理采用在运行时动态生成代码的方式,取消了对被代理类的扩展限制,遵循了开闭原则
\2. 若动态代理要对目标类的增强逻辑进行扩展,结合策略模式,只需要新增策略类便可完成,无需修改代理类的代码
3.在编码时，代理逻辑与业务逻辑是互相独立的，没有耦合
https://blog.csdn.net/wb_snail/article/details/80632038
2.1. 静态代理 定义：为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介的作用。
作用：增强一个类中的某个方法.对程序进行扩展，Spring框架中AOP。
2.2 动态代理 1、动态代理：(它与装饰者模式有点相似，它比装饰者模式还要灵活)
动态代理它可以直接给某一个目标(被代理 对象)对象(实现了某个或者某些接口)生成一个代理对象，而不需要代理类存在，如上图中经理人需要存在。 动态代理与代理模式原理是一样的，只是它没有具体的代理类，直接通过反射生成了一个代理对象 2、动态代理的分类
jdk提供一个Proxy类可以直接给实现接口类的对象直接生成代理对象 API: 调用, 缺少什么参数,传什么 spring中动态代理:cglib 继承 3. 单例模式 一个类能返回对象一个引用(永远是同一个)和一个获得该实例的方法（必须是静态方法，通常使用getInstance这个名 称）；当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用；同时我们 还将该类的构造函数定义为私有方法，这样其他处的代码就无法通过调用该类的构造函数来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例。
优点：
1.在单例模式中，活动的单例只有一个实例，对单例类的所有实例化得到的都是相同的一个实例。这样就 防止其它对象对自己的实例化，确保所有的对象都访问一个实例
2.单例模式具有一定的伸缩性，类自己来控制实例化进程，类就在改变实例化进程上有相应的伸缩性。
3.提供了对唯一实例的受控访问。
4.由于在系统内存中只存在一个对象，因此可以 节约系统资源，当 需要频繁创建和销毁的对象时单例模式无疑可以提高系统的性能。
5.允许可变数目的实例。
6.避免对共享资源的多重占用。
缺点：
1.不适用于变化的对象，如果同一类型的对象总是要在不同的用例场景发生变化，单例就会引起数据的错误，不能保存彼此的状态。
2.由于单利模式中没有抽象层，因此单例类的扩展有很大的困难。
3.单例类的职责过重，在一定程度上违背了“单一职责原则”。
4.滥用单例将带来一些负面问题，如为了节省资源将数据库连接池对象设计为的单例类，可能会导致共享连接池对象的程序过多而出现连接池溢出；如果实例化的对象长时间不被利用，系统会认为是垃圾而被回收，这将导致对象状态的丢失
适用场景：
单例模式只允许创建一个对象，因此节省内存，加快对象访问速度，因此对象需要被公用的场合适合使用，如多个模块使用同一个数据源连接对象等等。如：
1.需要频繁实例化然后销毁的对象。
2.创建对象时耗时过多或者耗资源过多，但又经常用到的对象。
3.有状态的工具类对象。
4.频繁访问数据库或文件的对象。
以下都是单例模式的经典使用场景：
1.资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如上述中的日志文件，应用配置。
2.控制资源的情况下，方便资源之间的互相通信。如线程池等。
4. 策略模式 在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%B0%83%E8%AF%95%E4%BD%93%E7%B3%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E8%B0%83%E8%AF%95%E4%BD%93%E7%B3%BB/</guid><description>[toc]
1. 前言 我们在使用idea启动debug时可以做好多事情,看变量的值; 随处打断点; 计算变量的值等等, 这些功能全都依赖了java的调试体系JPDA(Java Platform Debugger Architecture), 而java也提供了命令行级别的调试
2. 调试使用 关键命令
javac -g src/HelloJDB.java -g 表示展示调试信息
这样就启动了程序,然后打开调试程序
jdb HelloJDB.class 使用JDB调试Java程序 - 娄老师 - 博客园 (cnblogs.com)
由此可见,idea那么丰富的调试功能基本来源jvm
3. JPDA JPDA 由三个规范组成：JVMTI（JVM Tool Interface）、JDWP（Java Debug Wire Protocol）、JDI（Java Debug Interface）
这三个规范的层次由低到高分别是 JVMTI、JDWP、JDI
这三个规范把调试过程分解成几个概念：调试者（debugger）、被调试者（debuggee）、以及它们中间的通信器
JPDA 被抽象为三层实现。其中 JVMTI 是 JVM 对外暴露的接口。JDI 是实现了 JDWP 通信协议的客户端，调试器通过它和 JVM 中被调试程序通信。
模块 层次 编程语言 作用 JVMTI 底层 C 获取及控制当前虚拟机状态 JDWP 中介层 C 定义 JVMTI 和 JDI 交互的数据格式 JDI 高层 Java 提供 Java API 来远程控制被调试虚拟机 IDEA 的 debug 怎么实现？出于这个好奇心，我越挖越深！ - 知乎 (zhihu.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/java%E5%9F%BA%E7%A1%80/%E9%9D%A2%E8%AF%95/</guid><description>[TOC]
1.抽象和接口 区别:
设计角度:
抽象是事物的对象,即对类抽象; 接口是对行为的抽象 抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象(继承只能是单继承嘛,所以是对类整体的抽象) 语法角度:
1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法(java8的默认方法可以写实现)；
2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的；
3）抽象类可以有静态代码块和静态方法,而接口中不能含有静态代码块(java8可以写静态方法)；
4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。
https://www.cnblogs.com/dolphin0520/p/3811437.html
2.Java内存模型是什么(JMM)？ Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。比如，先行发生关系确保了：
线程内的代码能够按先后顺序执行，这被称为程序次序规则。 对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。 前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。 一个线程内的任何操作必需在这个线程的start()调用之后，也叫作线程启动规则。 一个线程的所有操作都会在线程终止之前，线程终止规则。 一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。 可传递性 Java内存模型围绕着三个特征建立起来的。分别是：原子性，可见性，有序性
注意这里是java的内存模型, 堆栈那些分布是指jvm内存模型
3. JMM中内存操作流程 8 种基本操作，如图：
lock 将对象变成线程独占的状态 unlock 将线程独占状态的对象的锁释放出来 read 从主内存读数据 load 将从主内存读取的数据写入工作内存 use 工作内存使用对象 assign 对工作内存中的对象进行赋值 store 将工作内存中的对象传送到主内存当中 write 将对象写入主内存当中，并覆盖旧值 对于有些操作lock和unlock是没有的,比如volatile,毕竟加了这两个东西线程就安全了
JMM对8种内存交互操作制定的规则吧：
不允许read、load、store、write操作之一单独出现，也就是read操作后必须load，store操作后必须write。 不允许线程丢弃他最近的assign操作，即工作内存中的变量数据改变了之后，必须告知主存。 不允许线程将没有assign的数据从工作内存同步到主内存。 一个新的变量必须在主内存中诞生，不允许工作内存直接使用一个未被初始化的变量。就是对变量实施use、store操作之前，必须经过load和assign操作。 一个变量同一时间只能有一个线程对其进行lock操作。多次lock之后，必须执行相同次数unlock才可以解锁。 如果对一个变量进行lock操作，会清空所有工作内存中此变量的值。在执行引擎使用这个变量前，必须重新load或assign操作初始化变量的值。 如果一个变量没有被lock，就不能对其进行unlock操作。也不能unlock一个被其他线程锁住的变量。 一个线程对一个变量进行unlock操作之前，必须先把此变量同步回主内存。 为什么volatile也无法保证线程安全_IT农场-CSDN博客_volatile线程安全吗
Java内存模型原理，你真的理解吗？ - 知乎 (zhihu.com)
面试官问我什么是JMM - 知乎 (zhihu.com)
3. Java中的volatile 变量是什么 可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。也就是一个线程修改的结果。另一个线程马上就能看到。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是一个非原子操作，也就是这个操作同样存在线程安全问题。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/maven%E6%89%93%E5%8C%85%E6%8F%92%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/maven%E6%89%93%E5%8C%85%E6%8F%92%E4%BB%B6/</guid><description>[toc]
1. maven介绍 我们都知道Maven本质上是一个插件框架，具有打包和jar管理的功能.
对于打包来说,它的核心并不执行任何具体的构建任务，所有这些任务都交给插件来完成，例如编译源代码是由maven- compiler-plugin完成的。进一步说，每个任务对应了一个插件目标（goal），每个插件会有一个或者多个目标，例如maven- compiler-plugin的compile目标用来编译位于src/main/java/目录下的主源码，testCompile目标用来编译位于src/test/java/目录下的测试源码。
用户可以通过两种方式调用Maven插件目标:
第一种方式是将插件目标与生命周期阶段（lifecycle phase）绑定，这样用户在命令行只是输入生命周期阶段而已，例如Maven默认将maven-compiler-plugin的compile目标与 compile生命周期阶段绑定，因此命令mvn compile实际上是先定位到compile这一生命周期阶段，然后再根据绑定关系调用maven-compiler-plugin的compile目标。
第二种方式是直接在命令行指定要执行的插件目标，例如mvn archetype:generate 就表示调用maven-archetype-plugin的generate目标，这种带冒号的调用方式与生命周期无关。
2. 生命周期及插件 以下测试用的是maven默认的打包插件(打common包这种), 执行 mvn clean install 对应的输出
2.1 总览生命周期 生命周期（lifecycle）由各个阶段组成，每个阶段由maven的插件plugin来执行完成。生命周期（lifecycle）主要包括clean、resources、complie、install、package、testResources、testCompile等，其中带test开头的都是用业编译测试代码或运行单元测试用例的。
由上图可知，各个插件的执行顺序一般是：1：clean、２：resources、３：compile、４：testResources、５：testCompile、６：test、７：jar、８：install。
在图中标记的地方每一行都是由冒号分隔的，前半部分是对应的插件，后半部分是插件的执行目标也就是插件执行产生的结果。
现在我们来看下上面的pom文件，我们如配置了maven-compiler-plugin这个插件，其它的插件没有配置，但最后项目构建成功，说明maven内置的各种插件，如果pom中没有配置就调用默认的内置插件，如果pom中配置了就调用配置的插件。
到此我们理解maven的构建过程或者有更多的人称是打包，就是由各种插件按照一定的顺序执行来完成项目的编译，单元测试、打包、布署的完成。各种插件的执行过程也就构成的maven的生命周期（lifecycle）。生命周期（lifecycle）各个阶段并不是独立的，可以单独执行如mvn clean，也可以一起执行如mvn clean install。而且有的mvn命令其是包括多个阶段的，如mvn compile其是包括了resources和compile两个阶段。下面分别来分析各个阶段需要的插件和输出的结果
也就是说,每个步骤的插件包都能指定特定版本和配置其下参数
2.2 打包插件 这个插件是把class文件、配置文件打成一个jar(war或其它格式)包。依赖包是不在jar里面的，需要建立lib目录，且jar和lib目录在同级目录。常用的打包插件有maven-jar-plugin、maven-assembly-plugin、maven-shade-plugin三种，下面分别介绍下各自己pom配置和使用特点。
2.2.1 maven-jar-plugin 可执行jar与依赖包是分开，需建立lib目录里来存放需要的j依赖包，且需要jar和lib目录在同级目录, 这也是默认打包方式,所以打common包不写插件也能打包
2.2.2 maven-assembly-plugin 这个插件可以把所有的依赖包打入到可执行jar包。需要在pom文件的plugin元素中引入才可以使用，功能非常强大，是maven中针对打包任务而提供的标准插件。它是Maven最强大的打包插件，它支持各种打包文件格式，包括zip、tar.gz、tar.bz2等等，通过一个打包描述文件设置（src/main/assembly.xml），它能够帮助用户选择具体打包哪些资源文件集合、依赖、模块，甚至本地仓库文件，每个项的具体打包路径用户也能自由控制。
但是该插件有个bug会缺失spring的xds文件，导致无法运行jar，同时如果同级目录还有其它可执行jar文件依赖可能会产生冲突。
2.2.3 maven-shade-plugin 需要在pom文件的plugin元素中引入才可以使用，它可以让用户配置Main-Class的值，然后在打包的时候将值填入/META-INF/MANIFEST.MF文件。关于项目的依赖，它很聪明地将依赖的JAR文件全部解压后，再将得到的.class文件连同当前项目的.class文件一起合并到最终的CLI包(可以直接运行的jar包)中，这样，在执行CLI JAR文件的时候，所有需要的类就都在Classpath中了。(springboot打包插件用的就是这个)
如何选用这几个插件
如果在开发一个库，直接使用默认的maven-jar-plugin插件即可；
如果是开发一个应用程序，可以考虑使用maven-shade-plugin进行打包生成Über jar(Über jar是将应用程序打包到单独的jar包中，该jar包包含了应用程序依赖的所有库和二进制包)
如果打包生成了Über jar都不能满足你的需求的话，那么推荐使用maven-assembly-plugin插件来自定义打包内容。
2.2.4 maven-war-plugin war项目默认的打包工具，默认情况下会打包项目编译生成的.class文件、资源文件以及项目依赖的所有jar包。
2.2.4.1 jar和war 1、war是一个web模块，其中需要包括WEB-INF，是可以直接运行的WEB模块；jar一般只是包括一些class文件，在声明了Main_class之后是可以用java命令运行的。
2、war包是做好一个web应用后，通常是网站，打成包部署到容器中；jar包通常是开发时要引用通用类，打成包便于存放管理。
3、war是Sun提出的一种Web应用程序格式，也是许多文件的一个压缩包。这个包中的文件按一定目录结构来组织；classes目录下则包含编译好的Servlet类和Jsp或Servlet所依赖的其它类（如JavaBean）可以打包成jar放到WEB-INF下的lib目录下。
1.我的一个springboot项目，用mvn install打包成jar，换一台有jdk的机器就直接可以用java -jar 项目名.jar的方式运行，没任何问题，为什么这里不需要tomcat也可以运行了？
答: 通过jar运行实际上是启动了内置的tomcat,所以用的是应用的配置文件中的端口</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/pom%E6%89%93%E5%8C%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/pom%E6%89%93%E5%8C%85/</guid><description>目录结构
&amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;!--springboot自带的--&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;!-- 打包 --&amp;gt; &amp;lt;!-- 使用这个插件打包 --&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;maven-jar-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;!-- 在1.8的环境下--&amp;gt; &amp;lt;source&amp;gt;1.8&amp;lt;/source&amp;gt; &amp;lt;target&amp;gt;1.8&amp;lt;/target&amp;gt; &amp;lt;encoding&amp;gt;UTF-8&amp;lt;/encoding&amp;gt; &amp;lt;!--排除这些文件--&amp;gt; &amp;lt;excludes&amp;gt; &amp;lt;exclude&amp;gt;*.properties&amp;lt;/exclude&amp;gt; &amp;lt;exclude&amp;gt;*.xml&amp;lt;/exclude&amp;gt; &amp;lt;/excludes&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;!-- 再使用这个插件打包 --&amp;gt; &amp;lt;!-- 允许打多个包 --&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;make-zip&amp;lt;/id&amp;gt; &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;!-- 确定包名,输出路径,用assembly.xml文件 --&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;finalName&amp;gt;${project.artifactId}&amp;lt;/finalName&amp;gt; &amp;lt;outputDirectory&amp;gt;target&amp;lt;/outputDirectory&amp;gt; &amp;lt;descriptors&amp;gt; &amp;lt;descriptor&amp;gt;src/assembly/assembly.xml&amp;lt;/descriptor&amp;gt; &amp;lt;/descriptors&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;!-- 再使用这个插件打包 --&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;artifactId&amp;gt;maven-resources-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;copy-xmls&amp;lt;/id&amp;gt; &amp;lt;phase&amp;gt;process-sources&amp;lt;/phase&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;copy-resources&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;outputDirectory&amp;gt;${basedir}/target/classes&amp;lt;/outputDirectory&amp;gt; &amp;lt;resources&amp;gt; &amp;lt;resource&amp;gt; &amp;lt;directory&amp;gt;${basedir}/src/main/java&amp;lt;/directory&amp;gt; &amp;lt;includes&amp;gt; &amp;lt;include&amp;gt;**/*.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/pom%E6%A0%87%E7%AD%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/pom%E6%A0%87%E7%AD%BE/</guid><description>[toc]
parent 现在有这样一个场景，有两个web项目A、B，一个java项目C，它们都需要用到同一个jar包：common.jar。如果分别在三个项目的pom文件中定义各自对common.jar的依赖，那么当common.jar的版本发生变化时，三个项目的pom文件都要改，项目越多要改的地方就越多，很麻烦。这时候就需要用到parent标签, 我们创建一个parent项目，打包类型为pom，parent项目中不存放任何代码，只是管理多个项目之间公共的依赖。在parent项目的pom文件中定义对common.jar的依赖，ABC三个子项目中只需要定义&amp;lt;parent&amp;gt;&amp;lt;/parent&amp;gt;，++parent标签中写上parent项目的pom坐标就可以引用到common.jar了++。
上面的问题解决了，我们在切换一个场景，有一个springmvc.jar，只有AB两个web项目需要，C项目是java项目不需要，那么又要怎么去依赖。如果AB中分别定义对springmvc.jar的依赖，当springmvc.jar版本变化时修改起来又会很麻烦。解决办法是在parent项目的pom文件中使用&amp;lt;dependencyManagement&amp;gt;&amp;lt;/dependencyManagement&amp;gt;将springmvc.jar管理起来，++如果有哪个子项目要用，那么子项目在自己的POM文件中使用++
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 标签中写上springmvc.jar的坐标，不需要写版本号，可以依赖到这个jar包了。这样springmvc.jar的版本发生变化时只需要修改parent中的版本就可以了。
原文：https://blog.csdn.net/qq_41254677/article/details/</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/settings%E6%96%87%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/maven/settings%E6%96%87%E4%BB%B6/</guid><description>settings.xml 中的参数看文件中的描述即可
仓库和镜像的区别
其中有两个标签, ,, 它们写的东西差不多,前者是仓库,后者是仓库
​ repository是指在局域网内部搭建的repository，它跟central repository, jboss repository等的区别仅仅在于其URL是一个内部网址 . ​ mirror则相当于一个代理，它会拦截去指定的远程repository下载构件的请求，然后从自己这里找出构件回送给客户端。配置mirror的目的一般是出于网速考虑
不过，很多internal repository搭建工具往往也提供mirror服务，比如Nexus就可以让同一个URL,既用作internal repository，又使它成为所有repository的mirror
&amp;lt;settings&amp;gt; ... &amp;lt;mirrors&amp;gt; &amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;maven.net.cn&amp;lt;/id&amp;gt; &amp;lt;name&amp;gt;one of the central mirrors in china&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;http://maven.net.cn/content/groups/public/&amp;lt;/url&amp;gt; &amp;lt;mirrorOf&amp;gt;central&amp;lt;/mirrorOf&amp;gt; &amp;lt;/mirror&amp;gt; &amp;lt;/mirrors&amp;gt; ... &amp;lt;/settings&amp;gt; 表示该镜像所指向的仓库id是哪个?
该例中，的值为central，表示该配置为中央仓库的镜像，任何对于中央仓库的请求都会转至该镜像，用户也可以使用同样的方法配置其他仓库的镜像。另外三个元素id,name,url与一般仓库配置无异，表示该镜像仓库的唯一标识符、名称以及地址
Maven还支持更高级的镜像配置：
* 匹配所有远程仓库。 external:* 匹配所有远程仓库，使用localhost的除外，使用file://协议的除外。也就是说，匹配所有不在本机上的远程仓库。 repo1,repo2 匹配仓库repo1和repo2，使用逗号分隔多个远程仓库。 *,!repo1 匹配所有远程仓库，repo1除外，使用感叹号将仓库从匹配中排除。 这里的仓库配置,指的在pom.xml中配置的; 好像在也能配置
&amp;lt;profiles&amp;gt; &amp;lt;profile&amp;gt; &amp;lt;repositories&amp;gt; &amp;lt;repository&amp;gt; &amp;lt;id&amp;gt;jdk14&amp;lt;/id&amp;gt; &amp;lt;name&amp;gt;Repository for JDK 1.4 builds&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;http://www.myhost.com/maven/jdk14&amp;lt;/url&amp;gt; &amp;lt;/repository&amp;gt; &amp;lt;/repositories&amp;gt; &amp;lt;/profile&amp;gt; &amp;lt;/profiles&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis-plus/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis-plus/%E4%BB%8B%E7%BB%8D/</guid><description>MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。
特性
无侵入：只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑 损耗小：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作 强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求 支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错 支持多种数据库：支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer2005、SQLServer 等多种数据库 支持主键自动生成：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题 支持 XML 热加载：Mapper 对应的 XML 支持热加载，对于简单的 CRUD 操作，甚至可以无 XML 启动 支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作 支持自定义全局通用操作：支持全局通用方法注入（ Write once, use anywhere ） 支持关键词自动转义：支持数据库关键词（order、key&amp;hellip;&amp;hellip;）自动转义，还可自定义关键词 内置代码生成器：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用 内置分页插件：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List 查询 内置性能分析插件：可输出 Sql 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询 内置全局拦截插件：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作 内置 Sql 注入剥离器：支持 Sql 注入剥离，有效预防 Sql 注入攻击 官网写的超详细: https://mp.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis-plus/%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis-plus/%E4%BD%BF%E7%94%A8/</guid><description>[toc]
该文章包括:
使用提供的curd 条件构造器 自定义sql 分页插件 sql执行效率插件 配置文件: application.properties
spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://10.10.203.10:3306/jplatdvv?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;amp;useSSL=false&amp;amp;allowMultiQueries=true&amp;amp;allowPublicKeyRetrieval=true spring.datasource.username=test spring.datasource.password=Jht123456 #spring.datasource.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.initialSize=5 spring.datasource.minIdle=5 spring.datasource.maxActive=20 spring.datasource.maxWait=60000 #mybatisMapper文件配置地址 #mybatis框架的配置不起作用了 mybatis-plus.mapper-locations=classpath*:mapper/*Mapper.xml 启动类: SpringBootTestMybatisPlusApplication.java
package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.transaction.annotation.EnableTransactionManagement; import com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor; import com.baomidou.mybatisplus.extension.plugins.PerformanceInterceptor; @SpringBootApplication @EnableTransactionManagement public class SpringBootTestMybatisPlusApplication { public static void main(String[] args) { SpringApplication.run(SpringBootTestMybatisPlusApplication.class, args); } /** * SQL执行效率插件 */ @Bean // @Profile({&amp;#34;dev&amp;#34;,&amp;#34;test&amp;#34;})// 设置 dev test 环境开启 public PerformanceInterceptor performanceInterceptor() { return new PerformanceInterceptor(); } /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } } 实体类 package com.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis-plus/%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E8%A7%A3%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis-plus/%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E8%A7%A3%E6%9E%90/</guid><description>[toc]
1. 使用 1.1. 引包 &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.baomidou&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;dynamic-datasource-spring-boot-starter&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.3.6&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 用最新版的就好 Maven Repository: dynamic-datasource-spring-boot-starter 不过做开发的经验来看, 应该选 倒数第二个大版本里最新的版本, 这样既能用更多的功能,又能保证没有大bug, 除非最新版本有你需要的功能
1.2 增加配置 spring: datasource: dynamic: primary: master #设置默认的数据源或者数据源组,默认值即为master, 所以此处不写也可以 strict: false #设置严格模式,默认false不启动. 启动后在未匹配到指定数据源时候回抛出异常,不启动会使用默认数据源. datasource: master: # 第一个数据源的名称 url: jdbc:mysql://127.0.0.1:3306/dynamic username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver db_1: # 第二个数据源的名称 url: jdbc:gbase://127.0.0.1:5258/dynamic username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 如果是多主多从或者一主多从，那么就用分组的功能实现, 即: 数据组名称_xxx，下划线前面的就是数据组名称，后面就是数据源名, 相同组名称的数据源会放在一个组下。切换数据源时，可以指定具体数据源名称，也可以指定组名然后会自动采用负载均衡算法切换,
例如: 上面配置中的&amp;quot;db_1&amp;quot;, 表示db分组, 1 名称的数据源, 如果再增加一个&amp;quot;db_2&amp;quot;, 则表示这两个数据源是同一个分组, 当在@DS中只指定&amp;quot;db&amp;quot;, 则会在这个分组中采用负载均衡方式选数据源(默认是轮询)
1.3 使用@DS注解 @DS注解, 是用来表示使用哪个数据源的, 也是切换数据源功能的核心注解, 它可以用来类和方法上, 方法上的优先级更高, 例如 service层, Mapper层,都可以, 如果没有配置,则会使用默认数据源</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</guid><description>[Toc]
一. 入参是Map,取key和value dao接口:
int updateByBatch(@Param(&amp;#34;content&amp;#34;) Map&amp;lt;String, Integer&amp;gt; alreadySoldNumMap); mapper文件:
&amp;lt;update id=&amp;#34;updateByBatch&amp;#34; parameterType=&amp;#34;java.util.Map&amp;#34;&amp;gt; update COUPON_CATEGORY &amp;lt;trim prefix=&amp;#34;set&amp;#34; suffixOverrides=&amp;#34;,&amp;#34;&amp;gt; &amp;lt;trim prefix=&amp;#34;ALREADY_SOLD_NUM = case&amp;#34; suffix=&amp;#34;end,&amp;#34;&amp;gt; &amp;lt;foreach collection=&amp;#34;content.keys&amp;#34; item=&amp;#34;key&amp;#34; index=&amp;#34;index&amp;#34;&amp;gt; when ID=#{key} then ALREADY_SOLD_NUM+#{content[${key}]} &amp;lt;/foreach&amp;gt; &amp;lt;/trim&amp;gt; &amp;lt;/trim&amp;gt; where &amp;lt;!-- 循环key--&amp;gt; &amp;lt;foreach collection=&amp;#34;content.keys&amp;#34; separator=&amp;#34;or&amp;#34; item=&amp;#34;key&amp;#34; index=&amp;#34;index&amp;#34;&amp;gt; ID=#{key} &amp;lt;/foreach&amp;gt; &amp;lt;/update&amp;gt; content.keys 得到所有的key; content.values 得到所有的value; 这种方式#{content[${key}]}获取map中的value，传递的map中的key只能是String类型， 如果是其他类型，得到的value是null。#{content[${key}]}还可以写成${content[key]}方式。 来自https://blog.csdn.net/shiqijiamengjie/article/details/77448829
1. 如果固定key: &amp;lt;select id=&amp;#34;selectRule&amp;#34; parameterType=&amp;#34;Map&amp;#34; resultType=&amp;#34;com.ourangel.weixin.domain.Rule&amp;#34;&amp;gt; SELECT ruleId,msgType,event,respId,reqValue,firstRespId,createDate,yn FROM oal_tb_rule WHERE yn = 1 &amp;lt;if test=&amp;#34;_parameter.containsKey(&amp;#39;msgType&amp;#39;)&amp;#34;&amp;gt; AND msgType = #{msgType,jdbcType=VARCHAR}) &amp;lt;/if&amp;gt; &amp;lt;if test=&amp;#34;_parameter.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E6%A0%87%E7%AD%BE%E7%9A%84%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E6%A0%87%E7%AD%BE%E7%9A%84%E4%BD%BF%E7%94%A8/</guid><description>[TOC]
一. if的使用 mybatis做if 判断 注意:下面这种写法只适用于 id 类型为字符串.
&amp;lt;if test=&amp;#34;id != null and id != &amp;#39;&amp;#39; &amp;#34;&amp;gt; id = #{id} &amp;lt;/if&amp;gt; 如果id类型为int 当id=0时 这个判断不会进入.
可以这样写&amp;lt;if test=&amp;quot;id != null and id != '' or id==0&amp;quot;&amp;gt;
或者这样写&amp;lt;if test=&amp;quot;id != null &amp;quot;&amp;gt; 来自: https://blog.csdn.net/qq_33626745/article/details/52955626
二. where 用在where条件不确定的情况下,where下的and和or会自动增加和去除,如下案例
&amp;lt;select id=&amp;#34;getUserList_whereIf&amp;#34; resultMap=&amp;#34;resultMap_User&amp;#34; parameterType=&amp;#34;com.yiibai.pojo.User&amp;#34;&amp;gt; SELECT u.user_id, u.username, u.sex, u.birthday FROM User u &amp;lt;where&amp;gt; &amp;lt;if test=&amp;#34;username !=null &amp;#34;&amp;gt; u.username LIKE CONCAT(CONCAT(&amp;#39;%&amp;#39;, #{username, jdbcType=VARCHAR}),&amp;#39;%&amp;#39;) &amp;lt;/if&amp;gt; &amp;lt;if test=&amp;#34;sex != null and sex !</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E7%BC%93%E5%AD%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/mybatis/%E7%BC%93%E5%AD%98/</guid><description>[TOC]
前言 和大多数持久化框架一样，MyBatis 提供了一级缓存和二级缓存的支持。默认情况下，MyBatis 只开启一级缓存。
MyBatis提供了一级缓存和二级缓存
一级缓存：也称为本地缓存，用于保存用户在一次会话过程中查询的结果，用户一次会话中只能使用一个sqlSession，一级缓存是自动开启的，不允许关闭。 二级缓存：也称为全局缓存，是mapper级别的缓存，是针对一个表的查结果的存储，可以共享给所有针对这张表的查询的用户。也就是说对于mapper级别的缓存不同的sqlsession是可以共享的。 一级缓存 一级缓存是基于 PerpetualCache（MyBatis自带）的 HashMap 本地缓存，作用范围为 session 域内,它存储的SqlSession中的BaseExecutor之中。当 session flush（刷新）或者 close（关闭）之后，该 session 中所有的 cache（缓存）就会被清空。
在参数和 SQL 完全一样的情况下，我们使用同一个 SqlSession 对象调用同一个 mapper 的方法，往往只执行一次 SQL。因为使用 SqlSession 第一次查询后，MyBatis 会将其放在缓存中，再次查询时，如果没有刷新，并且缓存没有超时的情况下，SqlSession 会取出当前缓存的数据，而不会再次发送 SQL 到数据库。
由于 SqlSession 是相互隔离的，所以如果你使用不同的 SqlSession 对象，即使调用相同的 Mapper、参数和方法，MyBatis 还是会再次发送 SQL 到数据库执行，返回结果。
示例:
package net.biancheng.test; import java.io.IOException; import java.io.InputStream; import java.util.List; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import org.apache.log4j.Logger; import net.biancheng.po.Website; public class Test { public static Logger logger = Logger.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E4%BB%8B%E7%BB%8D/</guid><description>[toc]
Seata 是什么? Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。
Seata 整体工作流程 Seata 对分布式事务的协调和控制，主要是通过 XID 和 3 个核心组件实现的。
XID XID 是全局事务的唯一标识，它可以在服务的调用链路中传递，绑定到服务的事务上下文中。
核心组件 Seata 定义了 3 个核心组件：
TC（Transaction Coordinator）：事务协调器，它是事务的协调者（这里指的是 Seata 服务器），主要负责维护全局事务和分支事务的状态，驱动全局事务提交或回滚。 TM（Transaction Manager）：事务管理器，它是事务的发起者，负责定义全局事务的范围，并根据 TC 维护的全局事务和分支事务状态，做出开始事务、提交事务、回滚事务的决议。 RM（Resource Manager）：资源管理器，它是资源的管理者（这里可以将其理解为各服务使用的数据库）。它负责管理分支事务上的资源，向 TC 注册分支事务，汇报分支事务状态，驱动分支事务的提交或回滚。 以上三个组件相互协作，TC 以 Seata 服务器（Server）形式独立部署，TM 和 RM 则是以 Seata Client 的形式集成在微服务中运行，其整体工作流程如下图。
Seata 的整体工作流程如下：
TM 向 TC 申请开启一个全局事务，全局事务创建成功后，TC 会针对这个全局事务生成一个全局唯一的 XID； XID 通过服务的调用链传递到其他服务; RM 向 TC 注册一个分支事务，并将其纳入 XID 对应全局事务的管辖； TM 根据 TC 收集的各个分支事务的执行结果，向 TC 发起全局事务提交或回滚决议； TC 调度 XID 下管辖的所有分支事务完成提交或回滚操作。 四种模式 Seata 提供了 AT、TCC、SAGA 和 XA 四种事务模式，可以快速有效地对分布式事务进行控制。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%90%86%E8%AE%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%90%86%E8%AE%BA/</guid><description>[toc]
前言 一图解读分布式事务 名词解释 事务：事务是由一组操作构成的可靠的独立的工作单元，事务具备ACID的特性，即原子性、一致性、隔离性和持久性。 本地事务：当事务由资源管理器本地管理时被称作本地事务。本地事务的优点就是支持严格的ACID特性，高效，可靠，状态可以只在资源管理器中维护，而且应用编程模型简单。但是本地事务不具备分布式事务的处理能力，隔离的最小单位受限于资源管理器。 全局事务：当事务由全局事务管理器进行全局管理时成为全局事务，事务管理器负责管理全局的事务状态和参与的资源，协同资源的一致提交回滚。 TX协议：应用或者应用服务器与事务管理器的接口。 XA协议：全局事务管理器与资源管理器的接口。XA是由X/Open组织提出的分布式事务规范。该规范主要定义了全局事务管理器和局部资源管理器之间的接口。主流的数据库产品都实现了XA接口。XA接口是一个双向的系统接口，在事务管理器以及多个资源管理器之间作为通信桥梁。之所以需要XA是因为在分布式系统中从理论上讲两台机器是无法达到一致性状态的，因此引入一个单点进行协调。由全局事务管理器管理和协调的事务可以跨越多个资源和进程。全局事务管理器一般使用XA二阶段协议与数据库进行交互。 AP：应用程序，可以理解为使用DTP（Data Tools Platform）的程序。 RM：资源管理器，这里可以是一个DBMS或者消息服务器管理系统，应用程序通过资源管理器对资源进行控制，资源必须实现XA定义的接口。资源管理器负责控制和管理实际的资源。 TM：事务管理器，负责协调和管理事务，提供给AP编程接口以及管理资源管理器。事务管理器控制着全局事务，管理事务的生命周期，并且协调资源。 两阶段提交协议：XA用于在全局事务中协调多个资源的机制。TM和RM之间采取两阶段提交的方案来解决一致性问题。两节点提交需要一个协调者（TM）来掌控所有参与者（RM）节点的操作结果并且指引这些节点是否需要最终提交。两阶段提交的局限在于协议成本，准备阶段的持久成本，全局事务状态的持久成本，潜在故障点多带来的脆弱性，准备后，提交前的故障引发一系列隔离与恢复难题。 BASE理论：BA指的是基本业务可用性，支持分区失败，S表示柔性状态，也就是允许短时间内不同步，E表示最终一致性，数据最终是一致的，但是实时是不一致的。原子性和持久性必须从根本上保障，为了可用性、性能和服务降级的需要，只有降低一致性和隔离性的要求。 CAP定理：对于共享数据系统，最多只能同时拥有CAP其中的两个，任意两个都有其适应的场景，真是的业务系统中通常是ACID与CAP的混合体。分布式系统中最重要的是满足业务需求，而不是追求高度抽象，绝对的系统特性。C表示一致性，也就是所有用户看到的数据是一样的。A表示可用性，是指总能找到一个可用的数据副本。P表示分区容错性，能够容忍网络中断等故障。 CAP定理 CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性：
一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用，操作依然可以完成 具体地讲在分布式系统中，一个Web应用至多只能同时支持上面的两个属性。
CAP 权衡 通过 CAP 理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？
对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。
对于涉及到钱财这样不能有一丝让步的场景，C 必须保证。网络发生故障宁可停止服务，这是保证 CA，舍弃 P。貌似这几年国内银行业发生了不下 10 起事故，但影响面不大，报道也不多，广大群众知道的少。还有一种是保证 CP，舍弃 A。例如网络故障是只读不写。
BASE定理 CAP是分布式系统设计理论，BASE是CAP理论中AP方案的延伸，对于C我们采用的方式和策略就是保证最终一致性；
eBay的架构师Dan Pritchett源于对大规模分布式系统的实践总结，在ACM上发表文章提出BASE理论，BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（StrongConsistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。
BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE基于CAP定理演化而来，核心思想是即时无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。
分布式事务分类 刚性事务 刚性事务：通常无业务改造，强一致性，原生支持回滚/隔离性，低并发，适合短事务。
原则：刚性事务满足足CAP的CP理论
刚性事务指的是，要使分布式事务，达到像本地式事务一样，具备数据强一致性，从CAP来看，就是说，要达到CP状态。
刚性事务：XA 协议（2PC、JTA、JTS）、3PC，但由于同步阻塞，处理效率低，不适合大型网站分布式场景。
柔性事务 柔性事务指的是，不要求强一致性，而是要求最终一致性，允许有中间状态，也就是Base理论，换句话说，就是AP状态。
与刚性事务相比，柔性事务的特点为：有业务改造，最终一致性，实现补偿接口，实现资源锁定接口，高并发，适合长事务。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/seata/%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</guid><description>[toc]
介绍 安装 单机 集群 Seata-server 的高可用依赖于注册中心、配置中心和数据库来实现。
Server端存储模式（store.mode）现有file、db、redis三种（后续将引入raft,mongodb），file模式无需改动，直接启动即可，下面专门讲下db启动步骤。
注： file模式为单机模式，全局事务会话信息内存中读写并持久化本地文件root.data，性能较高;
db模式为高可用模式，全局事务会话信息通过db共享，相应性能差些;
redis模式Seata-Server 1.3及以上版本支持,性能较高,存在事务信息丢失风险,请提前配置合适当前场景的redis持久化配置.
seata-server 的高可用的实现，主要基于db和注册中心，通过db获取全局事务，实现多实例事务共享。通过注册中心来实现seata-server多实例的动态管理。架构图原理图如 下：
案例 seata-server 集群搭建 - 掘金 (juejin.cn)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/service-mesh/istio/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/service-mesh/istio/%E4%BB%8B%E7%BB%8D/</guid><description>根据Istio官方文档的介绍，Istio在服务网络中主要提供了以下关键功能：
流量管理：控制服务之间的流量和API调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。 可观察性：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。 策略执行：将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。策略的更改是通过配置网格而不是修改应用程序代码。 服务身份和安全：为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。 平台支持：Istio旨在在各种环境中运行，包括跨云、Kubernetes、Mesos等。最初专注于Kubernetes，但很快将支持其他环境。 集成和定制：策略执行组件可以扩展和定制，以便与现有的ACL、日志、监控、配额、审核等解决方案集成。 Istio针对可扩展性进行了设计，以满足不同的部署需要。这些功能极大的减少了应用程序代码、底层平台和策略之间的耦合，使得微服务更加容易实现。
下图为Istio的架构设计图，主要包括了Envoy、Pilot、Mixer和Istio-Auth等。
Envoy: 扮演Sidecar的功能，协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集与流量相关的性能指标。
Pilot: 负责部署在Service Mesh中的Envoy实例的生命周期管理。本质上是负责流量管理和控制，将流量和基础设施扩展解耦，这是Istio的核心。可以把Pilot看做是管理Sidecar的Sidecar, 但是这个特殊的Sidacar并不承载任何业务流量。Pilot让运维人员通过Pilot指定它们希望流量遵循什么规则，而不是哪些特定的pod/VM应该接收流量。有了Pilot这个组件，我们可以非常容易的实现 A/B 测试和金丝雀Canary测试。
Mixer: Mixer在应用程序代码和基础架构后端之间提供通用中介层。它的设计将策略决策移出应用层，用运维人员能够控制的配置取而代之。应用程序代码不再将应用程序代码与特定后端集成在一起，而是与Mixer进行相当简单的集成，然后Mixer负责与后端系统连接。Mixer可以认为是其他后端基础设施（如数据库、监控、日志、配额等）的Sidecar Proxy。
Istio-Auth: 提供强大的服务间认证和终端用户认证，使用交互TLS，内置身份和证书管理。可以升级服务网格中的未加密流量，并为运维人员提供基于服务身份而不是网络控制来执行策略的能力。Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括基于属性和角色的访问控制以及授权钩子）来控制和监视访问服务、API或资源的访问者。
Istio的设计理念先进，功能也比较强大，加之Google、IBM的影响力让Istio迅速传播，让广大开发者认识到了Istio项目在Service Mesh领域的重要性。但是Istio目前版本也存在了一些不足：
目前的Istio大部分能力与Kubernetes是强关联的。而我们在构建微服务的时候往往是希望服务层与容器层是解耦的，服务层在设计上需要能够对接多种容器层平台。 Istio至今未有稳定版本，截至本文编写时为止，Istio的最新版本为0.8版本，预计在2018年内会发布1.0版本。 来自: https://blog.csdn.net/dylloveyou/article/details/81951286</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/service-mesh/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/service-mesh/%E4%BB%8B%E7%BB%8D/</guid><description>前言: 微服务:将系统很多功能拆分成一个个小服务(就是很多工程)
服务网格:服务有多了,如果服务之间随意通信,就会形成蜘蛛网,乱七八糟,不好管理,(目前捷顺是人为避免server之间调用,各个业务系统之间也没有联系,唯一的联系是项目和组织,这些都在B门户,通过API接口来通信,柳波说,华为的微服务也是用api的方式通信,多达几千个接口)服务网格就是处理服务之间的通信,让程序猿只需关注业务,类比于现在把网络分发和程序,程序不需要关注网络怎么分发,因为已经抽了tcp/ip出来,如图
服务网格就是在每个服务&amp;quot;旁边&amp;quot;加个东西,这个东西专门用来出流量处理与通信之类的,这个东西叫sidecar(边车,抗日剧中那种三轮车,很形象吧),如果有很多服务,每个服务&amp;quot;旁边&amp;quot;有个sidercar,sidecar用来通信,就形成了网格的样子,如图
注: 蓝色的是sidecar,绿色的就是(微服务)应用
第一代Service Mesh 的 代表为 Linkerd和Envoy
第二代Service Mesh 的 代表为 Istio
目前 第一代 都在为 Istio做支持,就是说第一代基本放弃竞争了,和Istio共同推进Service Mesh的发展,更牛逼的是Istio是由 Google、IBM 和 Lyft 联合开发</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/actuator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/actuator/</guid><description>[toc]
1.介绍 在Spring boot应用中，要实现可监控的功能，依赖的是 spring-boot-starter-actuator 这个组件。它提供了很多监控和管理你的spring boot应用的HTTP或者JMX端点，并且你可以有选择地开启和关闭部分功能。当你的spring boot应用中引入下面的依赖之后，将自动的拥有审计、健康检查、Metrics监控功能。
基于springboot2.X版本
链接：https://www.jianshu.com/p/1aadc4c85f51
2.使用 // 先有parent和web包 &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 然后就能访问了,哈哈哈,是不是超简单,http://127.0.0.1:8767/actuator/health
端口是项目的访问地址,即使是用Feign访问的server层
返回值:
up:服务正常 down:服务异常 它是号称可以做监控中心的插件,展示东西远不止这些
加上配置:management.endpoint.health.show-details=always
表示总是展示详细信息,因为还有个选择是when-authorized,默认是不展示(never)
再次访问,会展示出项目中使用的插件,比如redis,数据库等使用情况还有磁盘的使用情况,基本上,用这个能很好的看到项目的当前状态.
默认情况下(Http)还能访问一个链接,/info (http://127.0.0.1:8767/actuator/info),这个能获取应用程序的定制信息，这些信息由 info 打头的属性提供,比如:info.app.version=V1.0.0,访问地址就能得到这个样的json字符串
{ &amp;#34;app&amp;#34;:{ &amp;#34;version&amp;#34;:&amp;#34;V1.0.0&amp;#34; } } 还有其他链接:
Http 方法 路径 描述 get /autoconfig 提供了一份自动配置报告，记录哪些自动配置条件通过了，哪些没通过 get /configprops 描述配置属性（包含默认值）如何注入 Bean get /beans 描述应用程序上下文里全部的 Bean，以及它们的关系 get /dump 获取线程活动的快照 get /env 获取全部环境属性 get /env/{name} 根据名称获取特定的环境属性值 get /info 获取应用程序的定制信息，这些信息由 info 打头的属性提供 get /mappings 描述全部的 URI 路径，以及它们和控制器（包含 Actuator 端点）的映射关系 get /metrics 报告各种应用程序度量信息，比如内存用量和 HTTP 请求计数 get /metrics/{name} 报告指定名称的应用程序度量值 post /shutdown 关闭应用程序，要求 endpoints.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/filter/</guid><description>类似于spring的过滤器,只是引用方式不同,
spring: 引用过滤器使用xml方式,在web.xml中引用,过滤器越先引用(写在前面)优先级越高
spring boot: 使用一个FilterRegistrationBean类来引用过滤器
过滤器书写与spring一致:
public class MyFilter implements Filter { //写了一个MyFilter 过滤器 Logger logger = LoggerFactory.getLogger(this.getClass()); @Override public void destroy() { } @Override public void doFilter(ServletRequest srequest, ServletResponse sresponse, FilterChain filterChain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) srequest; //可以做登录验证等等….. logger.info(&amp;#34;this is MyFilter,url :&amp;#34;+request.getRequestURI()); filterChain.doFilter(srequest, sresponse); } @Override public void init(FilterConfig arg0) throws ServletException { } } 使用 过滤器:
@Bean public FilterRegistrationBean&amp;lt;MyFilter&amp;gt; testFilterRegistration() { FilterRegistrationBean&amp;lt;MyFilter&amp;gt; registration = new FilterRegistrationBean&amp;lt;MyFilter&amp;gt;(); registration.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/jpa/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/jpa/</guid><description>首先了解JPA是什么？ JPA(Java Persistence API)是Sun官方提出的Java持久化规范。它为Java开发人员提供了一种对象/关联映射工具来管理Java应用中的关系数据.
他的出现主要是为了简化现有的持久化开发工作和整合ORM技术，结束现在Hibernate，TopLink，JDO等ORM框架各自为营的局面。值得注意的是，JPA是在充分吸收了现有Hibernate，TopLink，JDO等ORM框架的基础上发展而来的，具有易于使用，伸缩性强等优点。
从目前的开发社区的反应上看，JPA受到了极大的支持和赞扬，其中就包括了Spring与EJB3.0的开发团队。
注意:JPA是一套规范，不是一套产品，那么像Hibernate,TopLink,JDO他们是一套产品，如果说这些产品实现了这个JPA规范，那么我们就可以叫他们为JPA的实现产品。
spring data jpa Spring Data JPA 是 Spring (所以这不是spring boot的)基于 ORM 框架、JPA 规范的基础上封装的一套JPA应用框架，可使开发者用极简的代码即可实现对数据的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展！学习并使用 Spring Data JPA 可以极大提高开发效率！ spring data jpa让我们解脱了DAO层的操作，基本上所有CRUD都可以依赖于它来实现
来自 http://www.ityouknow.com/springboot/2016/08/20/spring-boo-jpa.html
注:在项目中应该用得不多,毕竟项目中都是复杂查询,不会用简单的,即使有也不用(要保持代码风格一致),一些用sql一些用jpa,阅读会有障碍,虽然jpa支持一定的复杂查询,但还是对sql比较熟悉
导入包:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-data-jpa&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.5.6.RELEASE&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 继承JpaRepository public interface UserRepository extends JpaRepository&amp;lt;User, Long&amp;gt; { } 实体类 import javax.persistence.Entity; import javax.persistence.Id; import javax.persistence.Table; @Entity @Table(name=&amp;#34;person&amp;#34;) public class User { @Id private String id; @Column(name = &amp;#34;name&amp;#34;)//映射数据库字段名 private String userName; private String password; // set , get .</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/shiro-%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/shiro-%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</guid><description>工作原理是通过过滤器拦截,做一些操作,来达到身份认证和权限管理的作用
有拦截功能,跳转功能
shiro自带拦截器,也有相应实现,具体实现不需要我们操作,只需做在什么情况下跳转什么页面(或者操作),比如:当密码错误时,要做什么(自己写操作) ; 没有登录时,做什么等等
详情: https://blog.csdn.net/ityouknow/article/details/73836159
@Bean(name = &amp;#34;shiroFilter&amp;#34;) public ShiroFilterFactoryBean getShiroFilterFactoryBean(DefaultWebSecurityManager securityManager) { logger.info(&amp;#34;-------------getShiroFilterFactoryBean()+++++++++++&amp;#34;); ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 必须设置 SecurityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 如果不设置默认会自动寻找Web工程根目录下的&amp;#34;/login.jsp&amp;#34;页面 shiroFilterFactoryBean.setLoginUrl(&amp;#34;/login&amp;#34;); // // 登录成功后要跳转的连接 // shiroFilterFactoryBean.setSuccessUrl(&amp;#34;/user&amp;#34;); // shiroFilterFactoryBean.setUnauthorizedUrl(&amp;#34;/403&amp;#34;); Map&amp;lt;String, String&amp;gt; filterChainDefinitionMap = new LinkedHashMap&amp;lt;String, String&amp;gt;(); // authc：该过滤器下的页面必须验证后才能访问，它是Shiro内置的一个拦截器org.apache.shiro.web.filter.authc.FormAuthenticationFilter // anon：它对应的过滤器里面是空的,什么都没做 filterChainDefinitionMap.put(&amp;#34;/getUser&amp;#34;, &amp;#34;anon&amp;#34;);// 这里为了测试，只限制/user，实际开发中请修改为具体拦截的请求规则 logger.info(&amp;#34;##################从数据库读取权限规则，加载到shiroFilter中##################&amp;#34;); filterChainDefinitionMap.put(&amp;#34;/user/edit/**&amp;#34;, &amp;#34;authc,perms[user:edit]&amp;#34;);// 这里为了测试，固定写死的值，也可以从数据库或其他配置中读取 filterChainDefinitionMap.put(&amp;#34;/hello&amp;#34;, &amp;#34;authc&amp;#34;); filterChainDefinitionMap.put(&amp;#34;/**&amp;#34;, &amp;#34;anon&amp;#34;);//anon 可以理解为不拦截 shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; }</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E4%BB%8B%E7%BB%8D/</guid><description>spring boot 想杜绝spring mvc辣么多xml配置,所有在spring boot中没有几乎没有xml文件(当然你用xml也兼容),全部用properties 和 注解去代替</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/es-springboot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/es-springboot/</guid><description>[TOC]
前言 到目前为止，ES有4种客户端，分别是：Jest client、Rest client、Transport client、Node client，相信大家在项目集成中选择客户端比较纠结，搜索案例的时候一会是这个客户端实现的，一会儿又是别的客户端实现的，自己又不了解每个客户端的优劣势，但又想集成最好的，下面就来说说各个客户端的区别，以及优劣势
ES支持两种协议
HTTP协议，支持的客户端有Jest client和Rest client Native Elasticsearch binary协议，也就是Transport client和Node client Jest client和Rest client区别
​ Jest client非官方支持，在ES5.0之前官方提供的客户端只有Transport client、Node client。在5.0之后官方发布Rest client，并大力推荐
Transport client和Node client区别
​ Transport client（7.0弃用）和Node client（2.3弃用）区别：最早的两个客户端，Transport client是不需要单独一个节点。Node client需要单独建立一个节点，连接该节点进行操作，ES2.3之前有独立的API，ES2.3之后弃用该API，推荐用户创建一个节点，并用Transport client连接进行操作
综合：以上就是各个客户端现在的基本情况，可以看出Rest client目前是官方推荐的，但是springBoot默认支持的依然Transport client，这可能和ES更新速度有关
Springboot 2.3.x版本开始, Spring Data Elasticsearch 4.0.x 开始使用 restClient
版本对应-官方文档
1. Rest client 此处仅使用HighLevelClient
1. application.properties spring.elasticsearch.rest.uris=http://127.0.0.1:9200 2. pom.xml &amp;lt;!--rest--&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.elasticsearch.client&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;elasticsearch-rest-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;6.4.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.elasticsearch.client&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;elasticsearch-rest-high-level-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;6.4.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 3. RestHighTest.java package com.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/kafka-springboot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/kafka-springboot/</guid><description>kafka 的 server.properties 中加入 host.name=10.1.21.37, 不然springBoot连不上kafka
1.pom.xml
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.kafka&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-kafka&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 2.application.properties
#============== kafka =================== # 指定kafka 代理地址，可以多个 spring.kafka.bootstrap-servers=10.1.21.37:9092 #=============== provider ======================= spring.kafka.producer.retries=2 # 每次批量发送消息的数量 spring.kafka.producer.batch-size=16384 spring.kafka.producer.buffer-memory=33554432 # 指定消息key和消息体的编解码方式 spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer #=============== consumer ======================= # 指定默认消费者group id spring.kafka.consumer.group-id=test spring.kafka.consumer.auto-offset-reset=earliest spring.kafka.consumer.enable-auto-commit=true spring.kafka.consumer.auto-commit-interval=100 # 指定消息key和消息体的编解码方式 spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer topic=msg.topic 代码 @Autowired private KafkaTemplate&amp;lt;String, Object&amp;gt; kafka; // 生产者 @RequestMapping(&amp;#34;/sendMsg&amp;#34;) public LocalDateTime sendMsg(String helpNo) throws Exception { HelpInfo helpInfo=new HelpInfo(); LocalDateTime now = LocalDateTime.now(); helpInfo.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/mongodb-springboot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/mongodb-springboot/</guid><description>配置文件,application.yml
spring: data: mongodb: database: JHT_CPR uri: mongodb://10.10.203.16:27017/JHT_CPR 工具类:
public interface MongodbMapper&amp;lt;T&amp;gt; { public void save(String tableName, T entity); public void batchSave(String tableName, List&amp;lt;T&amp;gt; lists); // ... 这是接口,后面可以写很多 } 实现mongo工具类
@Configuration @Component @Slf4j public abstract class MongodbMapperImpl&amp;lt;T&amp;gt; implements MongodbMapper&amp;lt;T&amp;gt; { @Autowired private MongoTemplate mongoTemplate; protected abstract Class&amp;lt;T&amp;gt; getEntityClass(); protected abstract String getCollectionName(); @Override public void save(String tableName, T entity) { // 获取对象中属性对应的字段及其值 Map&amp;lt;String, Object&amp;gt; map = ClassUtils.getColumnValue(entity); log.info(&amp;#34;新增数据：{}&amp;#34;, JSON.toJSONString(map)); // 数据 Document datas = new Document(); for (Map.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/mybatis-springboot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/mybatis-springboot/</guid><description>在application.properties 中添加配置文件
spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/test?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;amp;useSSL=false&amp;amp;allowMultiQueries=true spring.datasource.username=xkj spring.datasource.password=xiaokunji spring.datasource.initialSize=50 spring.datasource.minIdle=10 spring.datasource.maxActive=100 spring.datasource.maxWait=60000 mybatis.mapper-locations=classpath*:mapper/*Mapper.xml mybatis.type-aliases-package=com.xkj.demo.entity # 打印sql查询结果值 mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl #默认插入空值 mybatis.configuration.jdbc-type-for-null=null # 支持驼峰 mybatis.configuration.map-underscore-to-camel-case=true 注:扫描实体类和Mapper文件用配置文件了,在spring中是用xml方式
导入包:
&amp;lt;!-- MyBatis --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.mybatis.spring.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mybatis-spring-boot-starter&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.2.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- MySQL --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;5.1.41&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 实例化接口:
@Autowired private UserService userService;//service接口不贴了,普通接口 方法调用:
@RequestMapping(&amp;#34;/getUser&amp;#34;) public User getUser() { User user = userService.getUser(); logger.info(&amp;#34;用户数据:{}&amp;#34;,user); return user; } 实现类:
@Service public class UserServiceImpl implements UserService { @Autowired public UserDao userDao; @Override public User getUser() { User user = userDao.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/swagger-springboot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%88%86%E6%94%AF/swagger-springboot/</guid><description>[TOC]
介绍 Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。Swagger 让部署管理和使用功能强大的API从未如此简单。
使用 pom.xml &amp;lt;!-- swagger2 --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.springfox&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;springfox-swagger2&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.9.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- swagger 原生ui --&amp;gt; &amp;lt;!-- &amp;lt;dependency&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;groupId&amp;gt;io.springfox&amp;lt;/groupId&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;artifactId&amp;gt;springfox-swagger-ui&amp;lt;/artifactId&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;version&amp;gt;2.9.2&amp;lt;/version&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;/dependency&amp;gt; --&amp;gt; &amp;lt;!-- bootstrap 写的ui--&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.github.xiaoymin&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;swagger-bootstrap-ui&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.9.3&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- spring4All自己封装的,用的swagger自带的ui,这个一个包就足够了, 1.7版本用的是swagger2.9.0,但很多库没有这个包...所以跨过这个版本 --&amp;gt; &amp;lt;!-- &amp;lt;dependency&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;groupId&amp;gt;com.spring4all&amp;lt;/groupId&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;artifactId&amp;gt;swagger-spring-boot-starter&amp;lt;/artifactId&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;version&amp;gt;1.9.0.RELEASE&amp;lt;/version&amp;gt; --&amp;gt; &amp;lt;!-- &amp;lt;/dependency&amp;gt; --&amp;gt; &amp;lt;!--高本版好像没有guava包,报错就加上--&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.google.guava&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;guava&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;27.0.1-jre&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 重点是springfox-swagger2 包, UI可以随便,bootstrp的界面更友好</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</guid><description>导包:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-test&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; //单元测试类 @RunWith(SpringRunner.class) @SpringBootTest @AutoConfigureMockMvc @WebAppConfiguration public class SpringbootDemoApplicationTests { @Autowired private WebApplicationContext context; private MockMvc mvc; @Before public void setUp() throws Exception { // mvc = MockMvcBuilders.standaloneSetup(new CustomerController()).build(); mvc = MockMvcBuilders.webAppContextSetup(context).build();//建议使用这种 } @Test public void contextLoads() throws Exception { MvcResult mvcR = mvc.perform(MockMvcRequestBuilders.get(&amp;#34;/hello&amp;#34;).accept(MediaType.APPLICATION_JSON)) // .andExpect(MockMvcResultMatchers.status().isOk()) // .andDo(MockMvcResultHandlers.print()) .andReturn(); String content = mvcR.getResponse().getContentAsString(); System.out.println(&amp;#34;内容:&amp;#34;+content); } } MockMvc解析
我在上面代码中进行了标记，我们按照标记进行讲解，这样会更明白一些：
perform方法其实只是为了构建一个请求，并且返回ResultActions实例，该实例则是可以获取到请求的返回内容。 MockMvcRequestBuilders该抽象类则是可以构建多种请求方式，如：Post、Get、Put、Delete等常用的请求方式，其中参数则是我们需要请求的本项目的相对路径，/则是项目请求的根路径。 param方法用于在发送请求时携带参数，当然除了该方法还有很多其他的方法，大家可以根据实际请求情况选择调用。 andReturn方法则是在发送请求后需要获取放回时调用，该方法返回MvcResult对象，该对象可以获取到返回的视图名称、返回的Response状态、获取拦截请求的拦截器集合等。 我们在这里就是使用到了第4步内的MvcResult对象实例获取的MockHttpServletResponse对象从而才得到的Status状态码。 同样也是使用MvcResult实例获取的MockHttpServletResponse对象从而得到的请求返回的字符串内容。【可以查看rest返回的json数据】 使用Junit内部验证类Assert判断返回的状态码是否正常为200 判断返回的字符串是否与我们预计的一样。 链接：https://www.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%9B%BD%E9%99%85%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%9B%BD%E9%99%85%E5%8C%96/</guid><description>[TOC]
介绍 在我们实际开发中，一个web应用可能要在多个地区使用，面对不同地区的不同语言，为了适应不同的用户，我们可以尝试在前端页面实现多语言的支持，那么同样对于后端返回的一些提示信息，异常信息等，我们后端也可以根据不同的语言环境来进行国际化处理，返回相应的信息。
但是这里我只用于做返回码,因为天启的返回码用得很太笨重
因为国际化会以key-val形式读取配置文件,很方便取,其本质通过流将数据写到properties中再写到map中
@SuppressWarnings({&amp;#34;unchecked&amp;#34;, &amp;#34;rawtypes&amp;#34;}) public PropertyResourceBundle (InputStream stream) throws IOException { Properties properties = new Properties(); properties.load(stream); lookup = new HashMap(properties); } 只能用properties文件
使用 application.properties
# 指定默认properties文件, # 默认为 : messages , 形如: # 1. messages.properties 默认文件(在指定语言中找不到配置时会来找这个文件) # 2. messages_zh_CN.properties 简体中文 spring.messages.basename=application-responseCode application-responseCode.properties
JS001=成功 hello=你好：{0} ， 你的验证码为 ：{1} controller
@Autowired private MessageSource msg; @RequestMapping(&amp;#34;hello&amp;#34;) public String hello() { String message = msg.getMessage(&amp;#34;JS001&amp;#34;,null,&amp;#34;默认值&amp;#34; LocaleContextHolder.getLocale());// local写啥都一样,反正用默认配置 String message1=msg.getMessage(&amp;#34;hello&amp;#34;, new Object[]{&amp;#34;zhangsan&amp;#34;,&amp;#34;123456&amp;#34;}, LocaleContextHolder.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%B0%8F%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E5%B0%8F%E7%9F%A5%E8%AF%86/</guid><description>[toc]
1. 配置多个properties文件: 在application.properties文件中添加
spring.profiles.active=jdbc,constants,resultCode
表示,使用application-jdbc.properties 和 application-constants.properties 和 application-resultCode.properties
命名方式:application-xxx.properties
2.属性变量引用方式@@ 这种属性应用方式是field_name=@field_value@。
两个@符号是springboot为替代${}属性占位符产生，原因是${}会被maven处理，所以应该是起不到引用变量的作用。 @@方式可以引用springboot 非默认配置文件（即其他配置文件) 中的变量； springboot默认配置文件是src/main/resources/application.properties 原文：https://blog.csdn.net/u011672034/article/details/79130001
你可以使用Maven的资源过滤（resource filter）自动暴露来自Maven项目的属性，如果使用spring-boot-starter-parent，你可以通过@..@占位符引用Maven项目的属性，例如：
app.encoding=@project.build.sourceEncoding@
app.java.version=@java.version@
app.project.version=@project.version@
本地环境可以用,但是打包后,不一定能用,这种方式下springboot读取了classpath中的配置,但是现在配置一般外置,所以要用这种方式,得把配置文件打包至jar中.
还有一种方式是读取jar中的pom.properties文件,打成jar包后,会生成一个pom.properties文件,里面包含版本信息,这种方式暂未实现
链接：https://www.jianshu.com/p/8c281e822c4a
3. application.yml 和bootstrap.yml 区别 加载顺序 若application.yml 和bootstrap.yml 在同一目录下：bootstrap.yml 先加载 application.yml后加载
配置区别 bootstrap.yml 和 application.yml 都可以用来配置参数。 bootstrap.yml 用来程序引导时执行，应用于更加早期配置信息读取。可以理解成系统级别的一些参数配置，这些参数一般是不会变动的。一旦bootStrap.yml 被加载，则内容不会被覆盖。 application.yml 可以用来定义应用级别的， 应用程序特有配置信息，可以用来配置后续各个模块中需使用的公共参数等。
3.属性覆盖问题
启动上下文时，Spring Cloud 会创建一个 Bootstrap Context，作为 Spring 应用的 Application Context 的父上下文。
初始化的时候，Bootstrap Context 负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的 Environment。Bootstrap 属性有高优先级，默认情况下，它们不会被本地配置覆盖。
也就是说如果加载的 application.yml 的内容标签与 bootstrap 的标签一致，application 也不会覆盖 bootstrap，而 application.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E6%97%A5%E5%BF%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E6%97%A5%E5%BF%97/</guid><description>Spring Boot 默认使用了logback日志,里面引用的==SLF4J==,默认是带颜色的哦(老帅老帅了),甚至都不需要主动引包, …web等等包中有引用
可以使用properties和logback.xml两种方式
properties logging.path=F:\\demo logging.file=demo.log logging.level.root=info # mybatis 打印返回值 mybatis.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl yml logging: level: io: netty: warn org: apache: error logback.xml:
默认在resource文件夹中找 logback_spring.xml &amp;gt; logback.xml
(推荐使用前者,springboot会提供更多的功能,例如:springProfile 节点,这个节点可以多环境输出:
&amp;lt;!--测试环境+开发环境,多个使用逗号隔开--&amp;gt; &amp;lt;springProfile name=&amp;#34;test,dev&amp;#34;&amp;gt; &amp;lt;logger name=&amp;#34;com.dudu.controller&amp;#34; level=&amp;#34;info&amp;#34;/&amp;gt; &amp;lt;/springProfile&amp;gt; &amp;lt;!--生产环境--&amp;gt; &amp;lt;springProfile name=&amp;#34;prod&amp;#34;&amp;gt; &amp;lt;logger name=&amp;#34;com.dudu.controller&amp;#34; level = &amp;#34;error&amp;#34;/&amp;gt; &amp;lt;/springProfile&amp;gt; 可以启动服务的时候指定 profile （如不指定使用默认），如指定prod 的方式为： java -jar xxx.jar –spring.profiles.active=prod 或者 在配置文件中 spring.profiles.active=dev
如果你的logback.xml有特殊的名字,可以指定(在application.properties中) logging.config=classpath:logging-config.xml
使用:
Logger logger = LoggerFactory.getLogger(this.getClass()); logger.info(&amp;#34;nihao&amp;#34;); 案例:
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;configuration scan=&amp;#34;true&amp;#34; scanPeriod=&amp;#34;60 seconds&amp;#34; debug=&amp;#34;false&amp;#34;&amp;gt; &amp;lt;substitutionProperty name=&amp;#34;log.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E9%94%99%E8%AF%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E9%94%99%E8%AF%AF/</guid><description>[TOC]
1. 安装lombok后仍不能生产get和set方法: 这分eclipse版本,我用的是sts,一切操作完成后,需要用管理员身份启动sts.exe
2. 时间格式化配置失效: 配置:
# 指定日期格式 spring.jackson.date-format=yyyy-MM-dd HH:mm:ss 预计是按指定格式转换,实际上前端返回时间戳
原因:
拦截器中继承了 WebMvcConfigurationSupport 或者没有导包 spring-boot-starter-data-rest
解决:
(实现) implements WebMvcConfigurer 导包 (项目中是不使用配置……通过java格式化时间字符串) 来自 https://www.jianshu.com/p/7211dfdbbb9d
https://www.cnblogs.com/myitnews/p/12329126.html
3. 热部署相关问题 原因: 热部署有自己的加载类的方法,这样导致与jdk的加载类不一样(序列化和反序列化)
情况:
启动项目报错. XXXXXXXXXXXXXXXX, is not visible from class loader (大致是类加载的问题) 访问报错: java.lang.ClassCastException: com.xkj.ExampleServiceImpl cannot be cast to com.xkj.ExampleServiceImpl(同一个类,但是类型转换异常) 解决:
在resources目录下面创建META_INF文件夹，然后创建spring-devtools.properties文件，文件加上类似下面的配置： restart.exclude.companycommonlibs=/mycorp-common-[\w-]+.jar restart.include.projectcommon=/mycorp-myproj-[\w-]+.jar &amp;gt; 来自 &amp;lt;https://www.cnblogs.com/ldy-blogs/p/8671863.html&amp;gt; 在启动的Main方法中:
​ System.setProperty(&amp;quot;spring.devtools.restart.enabled&amp;quot;, &amp;quot;false&amp;quot;);
ConfigurableObjectInputStream配合Thread.currentThread().getContextClassLoader() 来使用 (我不会….)
来自 https://www.jianshu.com/p/e6d5a3969343</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-boot/%E9%9D%A2%E8%AF%95/</guid><description>[toc]
springboot 启动过程 启动的流程主要分为两大阶段：
初始化SpringApplication运行SpringApplication (在这里就扫描了所有的spring.factories,并缓存在内存中) 运行SpringApplication的过程 其中运行SpringApplication的过程又可以细分为以下几个部分：
1）SpringApplicationRunListeners 引用启动监控模块,
2）ConfigrableEnvironment配置环境模块和监听：包括创建配置环境、加载属性配置文件和配置监听
3）ConfigrableApplicationContext配置应用上下文：包括配置应用上下文对象、配置基本属性和刷新应用上下文, 其中最核心代码refreshContext(context) 刷新上下文, 将通过工程模式产生应用上下文中所需的bean。实现spring-boot-starter-*(mybatis、redis等)自动化配置的关键，包括spring.factories的加载、bean的实例化等核心工作
详解面试官经常问的SpringBoot启动流程机制 - 云+社区 - 腾讯云 (tencent.com)
高级面试题&amp;ndash;SpringBoot启动流程解析_hfmbook的博客-CSDN博客_springboot启动流程面试
@SpringBootApplication 注解 @SpringBootApplication是一个复合注解，包括@ComponentScan，和@SpringBootConfiguration，@EnableAutoConfiguration。
@SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到spring容器中，并且实例名就是方法名。 @EnableAutoConfiguration的作用启动自动的配置，@EnableAutoConfiguration注解的意思就是Springboot根据你添加的jar包来配置你项目的默认配置，比如根据spring-boot-starter-web ，来判断你的项目是否需要添加了webmvc和tomcat，就会自动的帮你配置web项目中所需要的默认配置。 @ComponentScan，扫描当前包及其子包下被@Component，@Controller，@Service，@Repository注解标记的类并纳入到spring容器中进行管理。是以前的&amp;lt;context:component-scan&amp;gt;（以前使用在xml中使用的标签，用来扫描包配置的平行支持）。 所以SpringBootApplication做了三件事, 能够识别并加载@bean的实例 / 开启自动去读配置(自己业务加的那些配置) / 扫描各种类对象并加载
@SpringBootApplication注解分析 - duanxz - 博客园 (cnblogs.com)
自动装配机制 1、main方法中SpringApplication.run(HelloBoot.class,args)的执行流程中有refreshContext(context)。
2、而这个refreshContext(context)内部会解析，配置类上自动装配功能的注解@EnableAutoConfiguration中的，@EnableAutoConfiguration中的，使用@Import引入类AutoConfigurationImportSelector。
3、AutoConfigurationImportSelector这个类中的方法SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()会读取jar包中的/项目中的META-INF/spring.factories文件。
4、spring.factories配置了自动装配的类，最后根据配置类的条件，自动装配Bean。
SpringBoot自动装配原理 - 简书 (jianshu.com)
（Spring Boot的自动装配原理及流程）_一碗谦谦粉的博客-CSDN博客_springboot自动装配面试</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/bus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/bus/</guid><description>搭了config后,bus就很简单了
https://www.jianshu.com/p/e48de30aab76</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/config/</guid><description>[toc]
介绍 可以理解为配置中心,把配置放在某个地方(jdbc,git,svn,vault),统一管理.
和注册中心类似,分为客户端和服务端,服务端就是提供配置的,客户端就获取配置, 仅SpringCloudConfig不支持自动动态获取配置,需要结合bus(总线)来实现才行 使用配置中心后,会优先使用配置中心的地址,一旦配置中心连不通才会使用本地配置文件 以后再补充吧 使用 1. 基本使用 以snv为例
服务端:
pom.xml
&amp;lt;!-- 基础配置 --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-config-server&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- svn --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.tmatesoft.svnkit&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;svnkit&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 启动主类:
@SpringBootApplication @EnableConfigServer //表示使用配置中心 public class SpringBootTestConfigApplication { public static void main(String[] args) { SpringApplication.run(SpringBootTestConfigApplication.class, args); } } application.properties
server.port=8999 server.servlet.context-path=/test spring.cloud.config.server.svn.uri=http://10.101.222.10/jht/标准项目/JSCP/code/trunk/jportal/jportal-server/src/main/ spring.cloud.config.server.svn.username=xiaokunqi spring.cloud.config.server.svn.password=xiaokunqi spring.cloud.config.server.default-label=resources # 具体某个文件夹下 spring.profiles.active=subversion 使用网址可以访问到配置: http://localhost:8999/test/application/jdbc 2. 客户端 pom.xml
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-config&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 增加一个文件 bootstrap.properties</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/eureka/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/eureka/</guid><description>[toc]
前言 随着服务的越来越多，越来越杂，服务之间的调用会越来越复杂，越来越难以管理。而当某个服务发生了变化，或者由于压力性能问题，多部署了几台服务，怎么让服务的消费者知晓变化，就显得很重要了。不然就会存在调用的服务其实已经下线了，但调用者不知道等异常情况。
Eureka是Netflix开源的服务发现组件，本身是一个基于REST的服务。它包含Server和Client两部分。Spring Cloud将它集成在子项目Spring Cloud Netflix中，从而实现微服务的注册与发现。
著名的CAP理论指出，一个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。由于分区容错性在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。在此Zookeeper保证的是CP, 而Eureka则是AP。Zookeeper保证CP
来自 &amp;lt;https://blog.csdn.net/qq_38363255/article/details/80909731
1. 基础架构 Eureka服务端
​ 也称为注册中心，用于提供服务的注册与发现。支持高可用配置，依托与强一致性提供良好的服务实例可用性，可以应对多种不同的故障场景。
Eureka客户端
​ 主要处理服务的注册与发现。客户端服务通过注解和参数配置方式，嵌入在客户端的应用程序代码中，在应用程序启动时，向注册中心注册自身提供的服务并周期性地发送心跳来更新它的服务租约。同时，它也能从服务端查询当前注册的服务信息并把它们缓存到本地并周期性地刷新服务状态。
从这个简图中，可以看出，Eureka有三部分组成：
Service Provider： 暴露服务的提供方。 Service Consumer：调用远程服务的服务消费方。 EureKa Server： 服务注册中心和服务发现中心 2. 基本使用 服务端:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-eureka&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; eureka.instance.instance-id=${spring.cloud.client.ipAddress}:${server.port} eureka.instance.prefer-ip-address=true server.port=8766 eureka.client.serviceUrl.defaultZone=http://10.101.90.171:10000/eureka/ # 指定服务注册中心地址 这里直接指向了本服务 #eureka.client.service-url.defaultZone=http://${eureka.instance.hostname}:${server.port}/eureka/ 注: 默认不写时，是注册至：DEFAULT_URL中,默认就是http://localhost:8761/eureka。
/** * Eureka服务端 * @author oKong * */ @SpringBootApplication @EnableEurekaServer public class EureakServiceApplication { public static void main(String[] args) throws Exception { SpringApplication.run(EureakServiceApplication.class, args); log.info(&amp;#34;spring-cloud-eureka-service启动!&amp;#34;); } } 启动应用，访问：http://127.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/gateway/</guid><description>[toc]
1.介绍 Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。
Spring Cloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全、监控、埋点和限流等。
Spring Cloud Gateway 的特征：
基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0 动态路由 Predicates 和 Filters 作用于特定路由 集成 Hystrix 断路器 集成 Spring Cloud DiscoveryClient 易于编写的 Predicates 和 Filters 限流 路径重写 1.2术语 Route（路由）：这是网关的基本构建块。它由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配。-&amp;gt; 理解为分发(改变)路径 Predicate（断言）：这是一个 Java 8 的 Predicate。输入类型是一个 ServerWebExchange。我们可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。-&amp;gt; 按指定的路径拦截 Filter（过滤器）：这是org.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/nacos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/nacos/</guid><description>[toc]
前言 阿里开源的注册中心和配置中心
Nacos 官方文档
Nacos 源码
1. 概念 Name Space
用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置。Namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。
Configuration
配置文件
Data ID
Nacos 中的某个配置集的 ID。配置集 ID 是组织划分配置的维度之一。Data ID 通常用于组织划分系统的配置集。一个系统或者应用可以包含多个配置集，每个配置集都可以被一个有意义的名称标识。Data ID 通常采用类 Java 包（如 com.taobao.tc.refund.log.level）的命名规则保证全局唯一性。此命名规则非强制。
Group
Nacos 中的一组配置集，是组织配置的维度之一。通过一个有意义的字符串（如 Buy 或 Trade ）对配置集进行分组，从而区分 Data ID 相同的配置集。当您在 Nacos 上创建一个配置时，如果未填写配置分组的名称，则配置分组的名称默认采用 DEFAULT_GROUP 。配置分组的常见场景：不同的应用或组件使用了相同的配置类型，如 database_url 配置和 MQ_topic 配置。
Virtual Cluster
同一个服务下的所有服务实例组成一个默认集群, 集群可以被进一步按需求划分，划分的单位可以是虚拟集群。
如图: Nacos 概念
2. 架构 3. 安装 Nacos Server下载地址：
https://github.com/alibaba/nacos/releases
下载解压后打开bin目录
/work/nacos/bin/startup.sh -m standalone</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/openfeign/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/openfeign/</guid><description>[toc]
前言 OpenFeign 全称 Spring Cloud OpenFeign，它是 Spring 官方推出的一种声明式服务调用与负载均衡组件，它的出现就是为了替代进入停更维护状态的 Feign。OpenFeign 是 Spring Cloud 对 Feign 的二次封装，它具有 Feign 的所有功能，并在 Feign 的基础上增加了对 Spring MVC 注解的支持，例如 @RequestMapping、@GetMapping 和 @PostMapping 等。
OpenFeign：Spring Cloud声明式服务调用组件（非常详细） (biancheng.net)
本文章基于openFeign 2.X
Feign 的启动原理 注入@Import 我们在使用OpenFegin时, 需要加@EnableFeignClients, 我们先看一下这个注解
@Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Documented @Import(FeignClientsRegistrar.class) public @interface EnableFeignClients {...} 重点在第四个 @Import 上，一般使用此注解都是想要动态注册 Spring Bean 的
class FeignClientsRegistrar implements ImportBeanDefinitionRegistrar, ResourceLoaderAware, EnvironmentAware { // ..... // 资源加载器，可以加载 classpath 下的所有文件 private ResourceLoader resourceLoader; // 上下文，可通过该环境获取当前应用配置属性等 private Environment environment; @Override public void setEnvironment(Environment environment) { this.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/sleuth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/sleuth/</guid><description>[toc]
Spring-Cloud-Sleuth是Spring Cloud的组成部分之一，为SpringCloud应用实现了一种分布式追踪解决方案，其兼容了Zipkin, HTrace和log-based追踪,官网:https://cloud.spring.io/spring-cloud-sleuth/spring-cloud-sleuth.html#_running_examples
1.术语(Terminology) **Span：**基本工作单元，例如，在一个新建的span中发送一个RPC等同于发送一个回应请求给RPC，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、关键值注释(tags)、span的ID、以及进度ID(通常是IP地址)
span在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。
**Trace：**一系列spans组成的一个树状结构，例如，如果你正在跑一个分布式大数据工程，你可能需要创建一个trace。
**Annotation：**用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束
cs - Client Sent -客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络延迟 ss - Server Sent -注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳便可得到服务端需要的处理请求时间 cr - Client Received -表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳便可得到客户端从服务端获取回复的所有所需时间 将Span和Trace在一个系统中使用Zipkin注解的过程图形化：
每个颜色的注解表明一个span(总计7个spans，从A到G)，如果在注解中有这样的信息：
Trace Id = X
Span Id = D
Client Sent
这就表明当前span将Trace-Id设置为X，将Span-Id设置为D，同时它还表明了ClientSent事件。
spans 的parent/child关系图形化：
来源: https://blog.csdn.net/u010257992/article/details/52474639
2.使用 单独使用sleuth十分简单
pom.xml
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-cloud-starter-sleuth&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 得先有才能不写版本号
将日志格式改为
&amp;lt;layout class=&amp;#34;ch.qos.logback.classic.PatternLayout&amp;#34;&amp;gt; &amp;lt;pattern&amp;gt;%date [%thread] [%X{X-B3-TraceId:-},%X{X-B3-SpanId:-}] %-5level %logger{80}:%line - %msg%n&amp;lt;/pattern&amp;gt; &amp;lt;/layout&amp;gt; 把traceId和SpanId打印出来
然后就可以在日志中按照traceId和spanId找请求了,如果日志收集到了es中,也可以给traceId和spanId单独做个列,用es来找也是可以的</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring-cloud/%E4%BB%8B%E7%BB%8D/</guid><description>[TOC]
SpringCloud是基于SpringBoot的一整套实现微服务的框架。它提供了微服务开发所需的配置管理、服务发现、断路器、智能路由、微代理、控制总线、全局锁、决策竞选、分布式会话和集群状态管理等组件。最重要的是，基于SpringBoot，会让开发微服务架构非常方便。列举只是部分,更多参照官网: https://spring.io/projects/spring-cloud
来自 https://blog.lqdev.cn/2018/09/04/SpringCloud/chapter-one/
核心组件 SpringCloudGateway Spring Cloud Gateway是Spring官方基于Spring 5.0，Spring Boot 2.0和Project Reactor等技术开发的网关，Spring Cloud Gateway旨在为微服务架构提供一种简单而有效的统一的API路由管理方式。
Spring Cloud Gateway作为Spring Cloud生态系中的网关，目标是替代Netflix ZUUL，其不仅提供统一的路由方式，并且基于Filter链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。
SpringCloudNetflix 这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka,Hystrix,Zuul… 太多了
Netflix Eureka 服务中心，云端服务发现，一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。
Netflix Hystrix 熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。比如突然某个小弟生病了(服务达到处理上限或者因为网络问题连不通等等,就是这个小弟不能正常处理请求的情况)，但是你还需要它的支持，然后调用之后它半天没有响应，你却不知道，一直在等等这个响应；有可能别的小弟也正在调用你的武功绝技，那么当请求多之后，就会发生严重的阻塞影响老大的整体计划。这个时候Hystrix就派上用场了，当Hystrix发现某个小弟不在状态不稳定立马马上让它下线，让其它小弟来顶上来，或者给你说不用等了这个小弟今天肯定不行，该干嘛赶紧干嘛去别在这排队了。
Netflix Zuul Zuul是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和Netflix流应用的 Web 网站后端所有请求的前门。当其它门派来找大哥办事的时候一定要先经过zuul,看下有没有带刀子什么的给拦截回去，或者是需要找那个小弟的直接给带过去。
SpringCloudConfig 俗称的配置中心，配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。就是以后大家武器、枪火什么的东西都集中放到一起，别随便自己带，方便以后统一管理、升级装备。
SpringCloudBus 事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。相当于水浒传中日行八百里的神行太保戴宗，确保各个小弟之间消息保持畅通。
SpringCloudforCloudFoundry Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题 其实就是与CloudFoundry进行集成的一套解决方案，抱了Cloud Foundry的大腿。
SpringCloudCluster Spring Cloud Cluster将被Spring Integration取代。提供在分布式系统中的集群所需要的基础功能支持，如：选举、集群的状态一致性、全局锁、tokens等常见状态模式的抽象和实现。 如果把不同的帮派组织成统一的整体，Spring Cloud Cluster已经帮你提供了很多方便组织成统一的工具。
SpringCloudConsul Consul是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/aop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/aop/</guid><description>[toc]
1. 描述一下Spring AOP Spring AOP(Aspect Oriented Programming，面向切面编程)是OOPs(面向对象编程)的补充，它也提供了模块化。在面向对象编程中，关键的单元是对象，AOP的关键单元是切面，或者说关注点（可以简单地理解为你程序中的独立模块）。一些切面可能有集中的代码，但是有些可能被分散或者混杂在一起，例如日志或者事务。这些分散的切面被称为横切关注点。一个横切关注点是一个可以影响到整个应用的关注点，而且应该被尽量地集中到代码的一个地方，例如事务管理、权限、日志、安全等。
AOP让你可以使用简单可插拔的配置，在实际逻辑执行之前、之后或周围动态添加横切关注点。这让代码在当下和将来都变得易于维护。如果你是使用XML来使用切面的话，要添加或删除关注点，你不用重新编译完整的源代码，而仅仅需要修改配置文件就可以了。
Spring AOP通过以下两种方式来使用。但是最广泛使用的方式是Spring AspectJ 注解风格(Spring AspectJ Annotation Style)
使用AspectJ 注解风格 使用Spring XML 配置风格 https://blog.csdn.net/dadiyang/article/details/82920139
2. AOP概念 首先让我们从一些重要的AOP概念和术语开始。这些术语不是Spring特有的。不过AOP术语并不是特别的直观，如果Spring使用自己的术语，将会变得更加令人困惑。
切面（Aspect）：一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中一个关于横切关注点的很好的例子。在Spring AOP中，切面可以使用基于模式）或者基于@Aspect注解的方式来实现。 连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是表示一个方法的执行。 通知（Advice）：在切面的某个特定的连接点上执行的动作。其中包括了“around”、“before”和“after”等不同类型的通知（通知的类型将在后面部分进行讨论）。许多AOP框架（包括Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 切入点（Pointcut）：匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 引入（Introduction）：用来给一个类型声明额外的方法或属性（也被称为连接类型声明（inter-type declaration））。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用引入来使一个bean实现IsModified接口，以便简化缓存机制。 目标对象（Target Object）： 被一个或者多个切面所通知的对象。也被称做*被通知（advised）对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）*对象。 AOP代理（AOP Proxy）：AOP框架创建的对象，用来实现切面契约（例如通知方法执行等等）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）：把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 通知类型：
前置通知（Before advice）：在某连接点之前执行的通知，但这个通知不能阻止连接点之前的执行流程（除非它抛出一个异常）。 后置通知（After returning advice）：在某连接点正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 异常通知（After throwing advice）：在方法抛出异常退出时执行的通知。 最终通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 环绕通知（Around Advice）：包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 环绕通知是最常用的通知类型。和AspectJ一样，Spring提供所有类型的通知，我们推荐你使用尽可能简单的通知类型来实现需要的功能。例如，如果你只是需要一个方法的返回值来更新缓存，最好使用后置通知而不是环绕通知，尽管环绕通知也能完成同样的事情。用最合适的通知类型可以使得编程模型变得简单，并且能够避免很多潜在的错误。比如，你不需要在JoinPoint上调用用于环绕通知的proceed()方法，就不会有调用的问题。
在Spring 2.0中，所有的通知参数都是静态类型，因此你可以使用合适的类型（例如一个方法执行后的返回值类型）作为通知的参数而不是使用Object数组。
通过切入点匹配连接点的概念是AOP的关键，这使得AOP不同于其它仅仅提供拦截功能的旧技术。 切入点使得通知可以独立对应到面向对象的层次结构中。例如，一个提供声明式事务管理 的环绕通知可以被应用到一组横跨多个对象的方法上（例如服务层的所有业务操作）。
http://shouce.jb51.net/spring/aop.html
AOP方式 机制 说明 静态织入 静态代理 直接修改原类，比如编译期生成代理类的 APT 静态织入 自定义类加载器 使用类加载器启动自定义的类加载器，并加一个类加载监听器，监听器发现目标类被加载时就织入切入逻辑，以 Javassist 为代表 动态织入 动态代理 字节码加载后，为接口动态生成代理类，将切面植入到代理类中，以 JDK Proxy 为代表 动态织入 动态字节码生成 字节码加载后，通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用织入逻辑。属于子类代理，以 CGLIB 为代表 会用就行了？你知道 AOP 框架的原理吗？ - 简书 (jianshu.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/ioc%E6%8E%A7%E5%88%B6%E7%BF%BB%E8%BD%AC%E5%92%8Cdi%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/ioc%E6%8E%A7%E5%88%B6%E7%BF%BB%E8%BD%AC%E5%92%8Cdi%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</guid><description>1. IoC是什么 **Ioc—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。**在Java开发中，**Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。****对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系.**如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下：
●**谁控制谁，控制什么****：**传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对 象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）****。
●**为何是反转，哪些方面反转了****：**有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。
我们通常做事的方式，如果我们需要某个对象，一般都是采用这种直接创建的方式(new BeautifulGirl())，这个过程复杂而又繁琐，而且我们必须要面对每个环节，同时使用完成之后我们还要负责销毁它，在这种情况下我们的对象与它所依赖的对象耦合在一起。
我们知道，我们依赖对象其实并不是依赖该对象本身(控制层调用实现层时,需要的是实现层里面内容)，而是依赖它所提供的服务，只要在我们需要它的时候，它能够及时提供服务即可，至于它是我们主动去创建的还是别人送给我们的，其实并不是那么重要。再说了，相比于自己千辛万苦去创建它还要管理、善后而言，直接有人送过来是不是显得更加好呢？
这个给我们送东西的“人” 就是 IoC，在上面的例子中，它就相当于一个婚介公司，作为一个婚介公司它管理着很多男男女女的资料，当我们需要一个女朋友的时候，直接跟婚介公司提出我们的需求，婚介公司则会根据我们的需求提供一个妹子给我们，我们只需要负责谈恋爱，生猴子就行了。你看，这样是不是很简单明了。
1.1 注入对象的方式 IOC Service Provider 为被注入对象提供被依赖对象也有如下几种方式：构造方法注入、stter方法注入、接口注入,静态工厂模式。
1.1.1 构造器注入 构造器注入，顾名思义就是被注入的对象通过在其构造方法中声明依赖对象的参数列表，让外部知道它需要哪些依赖对象。
YoungMan(BeautifulGirl beautifulGirl){ this.beautifulGirl = beautifulGirl; }
构造器注入方式比较直观，对象构造完毕后就可以直接使用，这就好比你出生你家里就给你指定了你媳妇。
1.1.2 setter 方法注入 对于 JavaBean 对象而言，我们一般都是通过 getter 和 setter 方法来访问和设置对象的属性。所以，当前对象只需要为其所依赖的对象提供相对应的 setter 方法，就可以通过该方法将相应的依赖对象设置到被注入对象中。如下：
public class YoungMan { private BeautifulGirl beautifulGirl; public void setBeautifulGirl(BeautifulGirl beautifulGirl) { this.beautifulGirl = beautifulGirl; } }
相比于构造器注入，setter 方式注入会显得比较宽松灵活些，它可以在任何时候进行注入（当然是在使用依赖对象之前），这就好比你可以先把自己想要的妹子想好了，然后再跟婚介公司打招呼，你可以要林志玲款式的，赵丽颖款式的，甚至凤姐哪款的，随意性较强。
1.1.3 接口方式注入 接口方式注入显得比较霸道，因为它需要被依赖的对象实现不必要的接口，带有侵入性。一般都不推荐这种方式。
1.1.4 静态工厂模式 https://blog.csdn.net/shadow_zed/article/details/72566611
1.2 springIOC的实现过程 1，资源定位，找到对应的(xml)的位置</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/springcache/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/springcache/</guid><description>[toc]
1. 介绍 SpringCache提供基本的Cache抽象，并没有具体的缓存能力，需要配合具体的缓存实现来完成，目前SpringCache支持redis、ehcache、simple（基于内存,使用ConcurrentHashMap）等方式来实现缓存。
因为spring的缓存机制会引入一些问题, 一些大型项目可能不适合, 比如并发场景下出现异常等等, 但大部分的项目还是可以适用的(毕竟中国没有多少具备高并发的公司)
2. 常用注解 2.1 @EnableCaching 开启缓存功能，一般使用在springboot的启动类或配置类上
2.2 @Cacheable 使用缓存。在方法执行前Spring先查看缓存中是否有数据，如果有数据，则直接返回缓存数据；没有则调用方法并将方法返回值放进缓存。
**注解参数解释: **
@Cacheable属性名 用途 备注 cacheNames/value 指定缓存空间的名称，不同缓存空间的数据是彼此隔离的 key 同一个cacheNames中通过key区别不同的缓存。如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合，如：@CachePut(value = “demo”, key = “‘user’+#user.id”)，字符串中spring表达式意外的字符串部分需要用单引号 SpringCache提供了与缓存相关的专用元数据，如target、methodMame、result、方法参数等，如：@CachePut(value = “demo”, key = “#result==null”)当方法的参数和值一样时, 会导致重复; 如果是基本参数: 参数名+值 如果是对象参数: 参数的类名+对象的成员变量+值 (类似于toString的结果) keyGenrator key的生成策略，SpringCache默认使用SimpleKeyGenerator，默认情况下将参数值作为键，但是可能会导致key重复出现，因此一般需要自定义key的生成策略 和key参数二选一 cacheManager 指定缓存管理器(例如ConcurrentHashMap、Redis等)。 cacheResolver 和cacheManager作用一样，使用时二选一。 condition (指定缓存的条件（对参数判断，满足什么条件时才缓存）)condition是在调用方法之前判断条件，满足条件才缓存, 支持spel语法 @Cacheable(cacheNames=“book”, condition=&amp;quot;#name.length() &amp;lt; 32&amp;quot;) unless (否定缓存的条件（对结果判断，满足什么条件时不缓存）)unless是在调用方法之后判断条件，如果SpEL条件成立，则不缓存【条件满足不缓存】,支持spel语法 @Cacheable(cacheNames = “hello”,unless=&amp;quot;#result.id.contains(‘1’)&amp;quot; ) sync 缓存过期之后，如果多个线程同时请求对某个数据的访问，会同时去到数据库，导致数据库瞬间负荷增高。Spring4.3为@Cacheable注解提供了一个新的参数“sync”（默认为false）。当设置它为true时，只有一个线程的请求会去到数据库，其他线程都会等待直到缓存可用。这个设置可以减少对数据库的瞬间并发访问。 2.3 @CachePut 更新缓存，将方法的返回值放到缓存中
2.4 @CacheEvict 清空缓存</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E4%BA%8B%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E4%BA%8B%E5%8A%A1/</guid><description>[TOC]
1. 介绍 事务是一系列的动作，它们综合在一起才是一个完整的工作单元，这些动作必须全部完成，如果有一个失败的话，那么事务就会回滚到最开始的状态，仿佛什么都没发生过一样。 在企业级应用程序开发中，事务管理必不可少的技术，用来确保数据的完整性和一致性。
2.事务有四个特性：ACID 原子性（Atomicity）：事务是一个原子操作，由一系列动作组成。事务的原子性确保动作要么全部完成，要么完全不起作用。 一致性（Consistency）：一旦事务完成（不管成功还是失败），系统必须确保它所建模的业务处于一致的状态，而不会是部分完成部分失败。在现实中的数据不应该被破坏。 隔离性（Isolation）：可能有许多事务会同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。 持久性（Durability）：一旦事务完成，无论发生什么系统错误，它的结果都不应该受到影响，这样就能从任何系统崩溃中恢复过来。通常情况下，事务的结果被写到持久化存储器中。 原文链接：https://blog.csdn.net/trigl/article/details/50968079
3. 事务管理器 Spring并不直接管理事务，而是提供了多种事务管理器，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。 Spring事务管理器的接口是org.springframework.transaction.PlatformTransactionManager，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。此接口的内容如下：
Public interface PlatformTransactionManager()...{ // 由TransactionDefinition得到TransactionStatus对象 TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // 提交 Void commit(TransactionStatus status) throws TransactionException; // 回滚 Void rollback(TransactionStatus status) throws TransactionException; } JDBC事务 Hibernate事务 Java持久化API事务（JPA） Java原生API事务 (跨越了多个事务管理源) 原文链接：https://blog.csdn.net/trigl/article/details/50968079
4. 基本事务属性的定义 4.1 传播行为 事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为：
传播行为 含义 PROPAGATION_REQUIRED 表示当前方法必须运行在事务中。如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务(springboot中注解默认的行为) PROPAGATION_SUPPORTS 表示当前方法不需要事务上下文，但是如果存在当前事务的话，那么该方法会在这个事务中运行 PROPAGATION_MANDATORY 表示该方法必须在事务中运行，如果当前事务不存在，则会抛出一个异常 PROPAGATION_REQUIRED_NEW 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NOT_SUPPORTED 表示该方法不应该运行在事务中。如果存在当前事务，在该方法运行期间，当前事务将被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NEVER 表示当前方法不应该运行在事务上下文中。如果当前正有一个事务在运行，则会抛出异常 PROPAGATION_NESTED 表示如果当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。如果当前事务不存在，那么其行为与PROPAGATION_REQUIRED一样。注意各厂商对这种传播行为的支持是有所差异的。可以参考资源管理器的文档来确认它们是否支持嵌套事务 原文链接：https://blog.csdn.net/trigl/article/details/50968079
4.2 隔离级别 事务的第二个维度就是隔离级别（isolation level）。隔离级别定义了一个事务可能受其他并发事务影响的程度。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E6%B3%A8%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E6%B3%A8%E8%A7%A3/</guid><description>[TOC]
一 @Autowired 通过 @Autowired 的使用来消除 set ，get方法 注:区别于Lombok,lombok是生成set和get方法,你能直接使用 ;
@Autowired 是用来注值的,spring注值本质上是用了set方法来找到对象并初始化的,在这个注解没出来前,需要写get,set方法
通过(1),可以看到,@Autowired 本质上是用来初始化对象的,(源城时好像是用来初始化实体类,捷顺用来初始化Dao层对象) @Autowired默认按类型装配
注:捷顺用来初始化Dao层对象,关键是怎么和Mapper连接呢,有两种方案:
在Dao层接口类上 写 @Mapper 注解,表示这个接口要对接mapper文件 在application.xml中 添加配置: &amp;lt;!-- 自动扫描了所有的XxxxMapper.java，这样就不用一个一个手动配置Mpper的映射了，只要Mapper接口类和Mapper映射文件对应起来就可以了 --&amp;gt; &amp;lt;bean class=&amp;#34;org.mybatis.spring.mapper.MapperScannerConfigurer&amp;#34;&amp;gt; &amp;lt;property name=&amp;#34;basePackage&amp;#34; value=&amp;#34;com.jieshun.jht.integral.dao&amp;#34; /&amp;gt; &amp;lt;/bean&amp;gt; 来自 http://www.cnblogs.com/zghull/archive/2012/06/27/2565480.html
二 @Resource 这个是用来初始化对象的,这和@Autowired注解不一样,
@Autowired默认按类型装配 , @Resource按名称装配, 这是使用注解是要自己写名称的,捷顺在Controller里用初始化对象,使用这个注解,在接口实现类(impl)上使用注解 @Service(&amp;ldquo;mebershipService&amp;rdquo;)来表示这个类要放入IOC中,并指定名字,然后使用这个指定的名字就能获得对象,
如果没有指定名字就会按类型装配
来自 http://www.cnblogs.com/zghull/archive/2012/06/27/2565480.html
三 @Service 当你需要定义某个类为一个bean，则在这个类的类名前一行使用@Service(&amp;ldquo;XXX&amp;rdquo;),就相当于讲这个类定义为一个bean，bean名称为XXX;
会配合@Resource来使用
来自 http://www.cnblogs.com/Struts-pring/p/4951661.html
​
四 @Async 用法: 在方法上或者类上标注, 这样调用方法时就会异步执行, 它需要搭配 @EnableAsync注解使用
它还有一个value值,表示指定使用哪个线程池,不指定就用叫&amp;rsquo;taskExecutor&amp;rsquo;的线程池, 连默认线程池都没有就注解自己的线程池(有内存泄露风险,而且不重用线程)
原理: Spring容器启动初始化bean时，判断类中是否使用了@Async注解，创建切入点和切入点处理器，根据切入点创建代理，在调用@Async注解标注的方法时，会调用代理，执行切入点处理器invoke方法，将方法的执行提交给线程池，实现异步执行。
需要注意的一个错误用法是，如果A类的a方法(没有标注@Async)调用它自己的b方法(标注@Async)是不会异步执行的，因为从a方法进入调用的都是它本身，不会进入代理。(称为本地调用)
mode=AdviceMode.ASPECTJ 指定切面模式就可以生效
Spring @Async之四：Aysnc的异步执行的线程池实现原理 - duanxz - 博客园 (cnblogs.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E8%8E%B7%E5%8F%96%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E5%B7%A5%E5%85%B7%E7%B1%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E8%8E%B7%E5%8F%96%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E5%B7%A5%E5%85%B7%E7%B1%BB/</guid><description>UserUtils.java package com.gree.ecommerce.utils; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.gree.ecommerce.config.InheritableThreadPoolTaskExecutor; import com.gree.ecommerce.constant.Constant; import com.gree.ecommerce.constant.ServerResultCode; import com.gree.ecommerce.exception.BusinessException; import com.gree.ecommerce.module.user.BaseUserRedisVO; import com.gree.ecommerce.module.user.EocLoginUserRedisVO; import com.gree.ecommerce.module.user.LoginUserRedisVO; import lombok.extern.slf4j.Slf4j; import org.apache.commons.collections4.CollectionUtils; import java.util.Collection; import java.util.List; import java.util.Optional; import java.util.concurrent.TimeUnit; import java.util.function.Supplier; import java.util.stream.Collectors; import java.util.stream.Stream; /** * 用户数据相关工具类 * * @createDate 2021/12/15 */ @Slf4j public class UserUtils { static RedisUtil redisUtil = new RedisUtil(); public UserUtils(RedisUtil ru) { redisUtil = ru; } static Supplier&amp;lt;BusinessException&amp;gt; resultUserNotLoginEx = () -&amp;gt; new BusinessException(ServerResultCode.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/spring/%E9%9D%A2%E8%AF%95/</guid><description>[TOC]
BeanFactory和ApplicationContext有什么区别？ Bean工厂(BeanFactory)是Spring框架最核心的接口，提供了高级Ioc的配置机制．
应用上下文(ApplicationContext)建立在BeanFacotry基础之上，提供了更多面向应用的功能，如果国际化，属性编辑器，事件等等．
beanFactory是spring框架的基础设施，是面向spring本身，ApplicationContext是面向使用Spring框架的开发者，几乎所有场合都会用到ApplicationContext.
https://www.jianshu.com/p/b82ceb084adf
bean加载过程 根据bean名称 查询缓存中是否已存在(因为是单例)
context.getBean(&amp;quot;person&amp;quot;, Person.class);
确认该bean在工厂中是否已定义,解决循环依赖问题,有定义则完善该bean并返回
如果没有就会创建个新的,并放到bean池中
根据配置文件(xml.yaml)给bean设置属性(处理循环依赖问题(仅仅是单例的情况下)) 初始化bean,如果有自定义的bean初始化方法,则会在这里执行 https://segmentfault.com/a/1190000012887776#item-4-3 .
https://www.jianshu.com/p/b82ceb084adf
Spring 框架中都用到了哪些设计模式 代理模式—在AOP和remoting中被用的比较多。
单例模式—在spring配置文件中定义的bean默认为单例模式。
模板方法—用来解决代码重复的问题 比如. RestTemplate, JmsTemplate, JpaTemplate。 前端控制器—Srping提供了DispatcherServlet来对请求进行分发。 视图帮助(View Helper )—Spring提供了一系列的JSP标签，高效宏来辅助将分散的代码整合在视图里。 依赖注入—贯穿于BeanFactory / ApplicationContext接口的核心理念。
工厂模式—BeanFactory用来创建对象的实例。
Builder模式- 自定义配置文件的解析bean是时采用builder模式，一步一步地构建一个beanDefinition
策略模式：Spring 中策略模式使用有多个地方，如 Bean 定义对象的创建以及代理对象的创建等。这里主要看一下代理对象创建的策略模式的实现。 前面已经了解 Spring 的代理方式有两个 Jdk 动态代理和 CGLIB 代理。这两个代理方式的使用正是使用了策略模式。
Spring框架中单例beans是线程安全的吗？ 不是，Spring框架中的单例beans不是线程安全的。
spring bean的生命周期 Spring启动，查找并加载需要被Spring管理的bean，进行Bean的实例化 Bean实例化后对将Bean的引入和值注入到Bean的属性中 如果Bean实现了BeanNameAware接口的话，Spring将Bean的Id传递给setBeanName()方法 如果Bean实现了BeanFactoryAware接口的话，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入 如果Bean实现了ApplicationContextAware接口的话，Spring将调用Bean的setApplicationContext()方法，将bean所在应用上下文引用传入进来 如果Bean实现了BeanPostProcessor接口，Spring就将调用他们的postProcessBeforeInitialization()方法。Spring 的 AOP 就是利用它实现的。 如果Bean 实现了InitializingBean接口，Spring将调用他们的afterPropertiesSet()方法。类似的，如果bean使用init-method声明了初始化方法，该方法也会被调用 如果Bean 实现了BeanPostProcessor接口，Spring就将调用他们的postProcessAfterInitialization()方法。 此时，Bean已经准备就绪，可以被应用程序使用了。他们将一直驻留在应用上下文中，直到应用上下文被销毁。 如果bean实现了DisposableBean接口，Spring将调用它的destory()接口方法，同样，如果bean使用了destory-method 声明销毁方法，该方法也会被调用。 简版:
Bean 的实例化 Bean 属性赋值 Bean 的初始化 Bean 的使用 Bean 的销毁 java对象创建实例化和初始化的区别</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%E5%9B%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%E5%9B%BE/</guid><description>如何设计高性能、高并发、高可用的系统。 - 一心二念 - 博客园 (cnblogs.com)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%9D%A2%E8%AF%95/</guid><description>[toc]
在做分布式集群时候一般会产生什么问题？ 并发性。加锁解决。
缺乏全局时钟。
通信异常。引起数据丢失或者接收数据延迟的问题。
网络分区，也叫脑裂。
三态。成功和失败以外的第三种状态，叫超时态。
节点故障。节点越多，发生故障的几率越大。
cap不能同时满足。
一致性问题。
资源倾斜问题。忙的忙死，闲的闲死。
扩容和缩容问题。
分布式幂等性问题。
session共享问题。
分布式全局生成id问题。
来自: https://www.yuque.com/fudadajiagoushimeiriyiti/rqui93/ecylig
度量指标 QPS
Query Per Second: 每秒响应请求数，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准, 即每秒的响应请求数，也即是最大吞吐能力。
TPS
Transactions Per Second，也就是事务数/秒。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。
QPS基本类似于TPS，但是不同的是，对于一个页面的一次访问，形成一个TPS；但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“QPS”之中。
并发用户数
并发用户数：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。
RT
response Time：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。一般而言，用户感知友好的高并发系统，时延应该控制在250毫秒以内。
PV
PV（Page View）：页面访问量，即页面浏览量或点击量，用户每次刷新即被计算一次。可以统计服务一天的访问日志得到。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E5%8F%AF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E5%8F%AF%E7%94%A8/</guid><description>[toc]
一、什么是高可用 **高可用HA（**High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。
二、如何保障系统的高可用 应用高可用 我们都知道，单点是系统高可用的大敌，单点往往是系统高可用最大的风险和敌人，应该尽量在系统设计的过程中避免单点。方法论上，高可用保证的原则是“集群化”，或者叫“冗余”：只有一个单点，挂了服务会受影响；如果有冗余备份，挂了还有其他backup能够顶上。
保证系统高可用，架构设计的核心准则是：冗余。
有了冗余之后，还不够，每次出现故障需要人工介入恢复势必会增加系统的不可服务实践。所以，又往往是通过“自动故障转移”来实现系统的高可用。
在流量达到系统可承受的上限时, 还得用 降级/限流/熔断 来保证系统的可用性
对于服务状态临界值时怎么保证高可用以及数据的安全?
引入NG的动态负载均衡, 自动剔除不正常的服务
Nginx 模块nginx_upstream_check_module了，这个模块可以让 Nginx 定期地探测后端服务的一个指定的接口，然后根据返回的状态码，来判断服务是否还存活,当探测不存活的次数达到一定阈值时，就自动将这个后端服务从负载均衡服务器中摘除。
upstream server { server 192.168.1.1:8080; server 192.168.1.2:8080; // 检测 URL check interval=3000 rise=2 fall=5 timeout=1000 type=http default_down=t 5 check_http_send &amp;#34;GET /health_check HTTP/1.0\r\n\r\n&amp;#34;; // 检测返回状态码为 200 时认为检测成功 check_http_expect_alive http_2xx; } Nginx 按照上面的方式配置之后，你的业务服务器也要实现一个“/health_check”的接口，在这个接口中返回的 HTTP 状态码，这个返回的状态码可以存储在配置中心中，这样在变更状态码时，就不需要重启服务了
**在服务刚刚启动时，**可以初始化默认的 HTTP 状态码是 500，这样 Nginx 就不会很快将这个服务节点标记为可用，也就可以等待服务中，依赖的资源初始化完成，避免服务初始启动时的波动
**在完全初始化之后，**再将 HTTP 状态码变更为 200，Nginx 经过两次探测后，就会标记服务为可用。在服务关闭时，也应该先将 HTTP 状态码变更为 500，等待 Nginx 探测将服务标记为不可用后，前端的流量也就不会继续发往这个服务节点。在等待服务正在处理的请求全部处理完毕之后，再对服务做重启，可以避免直接重启导致正在处理的请求失败的问题。
或者使用灰度机制(AB区机制)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E5%B9%B6%E5%8F%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E5%B9%B6%E5%8F%91/</guid><description>[toc]
一、什么是高并发 高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。
高并发相关常用的一些指标有响应时间（Response Time），吞吐量（Throughput），每秒查询率QPS（Query Per Second），并发用户数等。
响应时间：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。 吞吐量：单位时间内处理的请求数量。 QPS：每秒响应请求数。在互联网领域，这个指标和吞吐量区分的没有这么明显。 并发用户数：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。 二、如何提升系统的并发能力 加机器的方式就不说了,花钱就完事了
2.0 常见的互联网分层架构 常见互联网分布式架构如上，分为：
（1）客户端层：典型调用方是浏览器browser或者手机应用APP
（2）反向代理层：系统入口，反向代理
（3）站点应用层：实现核心应用逻辑，返回html或者json
（4）服务层：如果实现了服务化，就有这一层
（5）数据-缓存层：缓存加速访问存储
（6）数据-数据库层：数据库固化数据存储
2.1 客户端提升性能 资源静态化 - 使用服务端渲染 采用cdn , 加快动态资源加载 一些小点:
减少http请求 使用浏览器的缓存 减少单个请求的数据传输 2.2 代理层的水平扩展 反向代理层的水平扩展，是通过“DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。
当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。
(有多个NG的情况下)
优化ng的一些小细节,比如:
启用压缩 使用长链接 增加worker数 NG是七层模型中的第七层-应用层 , 所以又称它为七层负载, 因此性能会有一定的损耗
LVS(Linux Virtual Server: Linux虚拟服务器) 是七层模型中的第四层-网络层 , 所以又称它为四层负载, 因此性能会更高
LVS 是在网络栈的四层做请求包的转发，请求包转发之后，由客户端和后端服务直接建立连接，后续的响应包不会再经过 LVS 服务器，所以性能更高, LVS 缺陷是工作在四层，而请求的 URL 是七层的概念，不能针对 URL 做更细致地请求分发，而且 LVS 也没有提供探测后端服务是否存活的机制
如果你的 QPS 在十万以内，那么可以考虑不引入LVS 而直接使用 Nginx 作为唯一的负载均衡服务器</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E6%80%A7%E8%83%BD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99/%E9%AB%98%E6%80%A7%E8%83%BD/</guid><description>总的来说,让处理请求的时候变短,速度变快
前端
浏览器优化技术：合理布局，页面缓存，减少http请求数，页面压缩，减少 cookie 传输。
CDN
DNS负载均衡
动静分离
动态图片独立提供服务
加入redis
服务分流
代码良好
https://zhuanlan.zhihu.com/p/105160130</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/log4j2%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/log4j2%E5%BC%82%E6%AD%A5%E6%97%A5%E5%BF%97/</guid><description>[toc]
介绍 log4j2最大的特点就是异步日志，其性能的提升主要也是从异步日志中受益，我们来看看如何使用log4j2的异步日志。
Log4j2提供了两种实现日志的方式，一个是通过AsyncAppender，一个是通过AsyncLogger，分别对应前面我们说的Appender组件和Logger组件。注意这是两种不同的实现方式，在设计和源码上都是不同的体现。
AsyncAppender方式 AsyncAppender是通过引用别的Appender来实现的，当有日志事件到达时，会开启另外一个线程来处理它们。需要注意的是，如果在Appender的时候出现异常，对应用来说是无法感知的。 AsyncAppender应该在它引用的Appender之后配置，默认使用 java.util.concurrent.ArrayBlockingQueue实现而不需要其它外部的类库。 当使用此Appender的时候，在多线程的环境下需要注意，阻塞队列容易受到锁争用的影响，这可能会对性能产生影响。这时候，我们应该考虑使用无所的异步记录器（AsyncLogger）。
例子:
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;Configuration status=&amp;#34;warn&amp;#34; name=&amp;#34;MyApp&amp;#34; packages=&amp;#34;&amp;#34;&amp;gt; &amp;lt;Appenders&amp;gt; &amp;lt;File name=&amp;#34;MyFile&amp;#34; fileName=&amp;#34;logs/app.log&amp;#34;&amp;gt; &amp;lt;PatternLayout&amp;gt; &amp;lt;Pattern&amp;gt;%d %p %c{1.} [%t] %m%n&amp;lt;/Pattern&amp;gt; &amp;lt;/PatternLayout&amp;gt; &amp;lt;/File&amp;gt; &amp;lt;Async name=&amp;#34;Async&amp;#34;&amp;gt; &amp;lt;AppenderRef ref=&amp;#34;MyFile&amp;#34;/&amp;gt; &amp;lt;/Async&amp;gt; &amp;lt;/Appenders&amp;gt; &amp;lt;Loggers&amp;gt; &amp;lt;Root level=&amp;#34;error&amp;#34;&amp;gt; &amp;lt;AppenderRef ref=&amp;#34;Async&amp;#34;/&amp;gt; &amp;lt;/Root&amp;gt; &amp;lt;/Loggers&amp;gt; &amp;lt;/Configuration&amp;gt; AsyncLogger方式 AsyncLogger才是log4j2 的重头戏，也是官方推荐的异步方式。它可以使得调用Logger.log返回的更快。你可以有两种选择：全局异步和混合异步。
全局异步就是，所有的日志都异步的记录，在配置文件上不用做任何改动，只需要在jvm启动的时候增加一个参数；
混合异步就是，你可以在应用中同时使用同步日志和异步日志，这使得日志的配置方式更加灵活。因为Log4j文档中也说了，虽然Log4j2提供以一套异常处理机制，可以覆盖大部分的状态，但是还是会有一小部分的特殊情况是无法完全处理的，比如我们如果是记录审计日志，那么官方就推荐使用同步日志的方式，而对于其他的一些仅仅是记录一个程序日志的地方，使用异步日志将大幅提升性能，减少对应用本身的影响。混合异步的方式需要通过修改配置文件来实现，使用AsyncLogger标记配置。
例子:
引入配置
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.lmax&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;disruptor&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.4.2&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 全局异步
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;!-- Don&amp;#39;t forget to set system property -Dlog4j2.contextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector to make all loggers asynchronous.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/slf4j%E5%92%8Clog4j%E5%92%8Clogback%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/java%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/slf4j%E5%92%8Clog4j%E5%92%8Clogback%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB/</guid><description>1.SLF4J(Simple logging Facade for Java) 意思为简单日志门面，它是把不同的日志系统的实现进行了具体的抽象化，只提供了统一的日志使用接口，使用时只需要按照其提供的接口方法进行调用即可，由于它只是一个接口，并不是一个具体的可以直接单独使用的日志框架，所以最终日志的格式、记录级别、输出方式等都要通过接口绑定的具体的日志系统来实现，这些具体的日志系统就有log4j,logback,java.util.logging等，它们才实现了具体的日志系统的功能。
如何使用SLF4J?
既然SLF4J只是一个接口，那么实际使用时必须要结合具体的日志系统来使用，我们首先来看SLF4J和各个具体的日志系统进行绑定时的框架原理图：
其实slf4j原理很简单，他只提供一个核心slf4j api(就是slf4j-api.jar包)，这个包只有日志的接口，并没有实现，所以如果要使用就得再给它提供一个实现了些接口的日志包，比 如：log4j,common logging,jdk log日志实现包等，但是这些日志实现又不能通过接口直接调用，实现上他们根本就和slf4j-api不一致，因此slf4j又增加了一层来转换各日志实现包的使 用，当然slf4j-simple除外。其结构如下：
slf4j-api(接口层)
|
各日志实现包的连接层( slf4j-jdk14, slf4j-log4j)
|
各日志实现包
所以，结合各日志实现包使用时提供的jar包情况为：
SLF4J和logback结合使用时需要提供的jar:slf4j-api.jar,logback-classic.jar,logback-core.jar
SLF4J和log4j结合使用时需要提供的jar:slf4j-api.jar,slf4j-log412.jar,log4j.jar
SLF4J和JDK中java.util.logging结合使用时需要提供的jar:slf4j-api.jar,slf4j-jdk14.jar
SLF4J和simple(SLF4J本身提供的一个接口的简单实现)结合使用时需要提供的jar:slf4j-api.jar,slf4j-simple.jar
当然还有其他的日志实现包，以上是经常会使用到的一些。
注意，以上slf4j和各日志实现包结合使用时最好只使用一种结合，不然的话会提示重复绑定日志，并且会导致日志无法输出。
为什么要使用SLF4J?
slf4j是一个日志接口，自己没有具体实现日志系统，只提供了一组标准的调用api,这样将调用和具体的日志实现分离，使用slf4j后有利于根据自己实际的需求更换具体的日志系统，比如，之前使用的具体的日志系统为log4j,想更换为logback时，只需要删除log4j相关的jar,然后加入logback相关的jar和日志配置文件即可，而不需要改动具体的日志输出方法，试想如果没有采用这种方式，当你的系统中日志输出有成千上万条时，你要更换日志系统将是多么庞大的一项工程。如果你开发的是一个面向公众使用的组件或公共服务模块，那么一定要使用slf4的这种形式，这有利于别人在调用你的模块时保持和他系统中使用统一的日志输出。 slf4j日志输出时可以使用{}占位符，如，logger.info(&amp;ldquo;testlog: {}&amp;rdquo;, &amp;ldquo;test&amp;rdquo;)，而如果只使用log4j做日志输出时，只能以logger.info(&amp;ldquo;testlog:&amp;quot;+&amp;ldquo;test&amp;rdquo;)这种形式，前者要比后者在性能上更好，后者采用+连接字符串时就是new 一个String 字符串，在性能上就不如前者。 2.log4j(log for java) Log4j是Apache的一个开源项目，通过使用Log4j，我们可以控制日志信息输送的目的地是控制台、文件、GUI组件，甚至是套接口服务器、NT的事件记录器、UNIX Syslog守护进程等；我们也可以控制每一条日志的输出格式；通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。最令人感兴趣的就是，这些可以通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。
如何使用？
引入jar,使用log4j时需要的jar为：log4j.jar。
定义配置文件log4j.properties或log4j.xml
在具体的类中进行使用：
在需要日志输出的类中加入：private static final Logger logger = Logger.getLogger(Tester.class); //通过Logger获取Logger实例 在需要输出日志的地方调用相应方法即可：logger.debug(“System …..”); logger.log(Level.DEBUG,&amp;ldquo;这是debug&amp;rdquo;);// 可以指定打印级别,但logback不支持,需要自己实现 关于如何单独使用log4j，建议详细阅读以下文章：
https://blog.csdn.net/u012422446/article/details/51199724
https://blog.csdn.net/azheng270/article/details/2173430/
http://shmilyaw-hotmail-com.iteye.com/blog/2410764
3.logback logback同样是由log4j的作者设计完成的，拥有更好的特性，用来取代log4j的一个日志框架,是slf4j的原生实现(即直接实现了slf4j的接口，而log4j并没有直接实现，所以就需要一个适配器slf4j-log4j12.jar),logback一共有以下几个模块：
logback-core：其它两个模块的基础模块 logback-classic：它是log4j的一个改良版本，同时它完整实现了slf4j API使你可以很方便地更换成其它日志系统如log4j或JDK14 Logging logback-access：访问模块与Servlet容器集成提供通过Http来访问日志的功能 同样，单独使用它时，需要引入以上jar,然后进行配置文件的配置，最后就是在相关类中进行使用，使用时加入以下语句:
private final static Logger logger = LoggerFactory.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/centos/centos%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/centos/centos%E5%91%BD%E4%BB%A4/</guid><description>centos
6. 更换yum源 #下载wget yum install wget -y #备份原来的yum mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup #下载yum wget http://mirrors.163.com/.help/CentOS7-Base-163.repo #阿里的yum wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #更新缓存 yum makecache #查询源 yum -y update 来自 https://www.cnblogs.com/xjh713/p/7458437.html
安装自动提示包
yum install -y bash-completion
​ ​
把某个命令做成系统服务
可以把一些手动安装的应用, 统一纳入系统管理, 可以使用 systemctl 命令
方案一:
此方案是老的linux支持的方式, 不太建议
在目录etc/init.d 下创建一个可执行文件 /redisd, redisd可以从redis中复制 cp /opt/software/redis-4.0.11/utils/redis_init_script , /etc/init.d/redisd , 比如nacos没有类似文件, 可以自己创建一个, 里面写执行命令(当成写一个sh脚本)
在文件中加入如下命令 (别看它被注释了, 但是系统会读它)
# chkconfig: 2345 10 90 # description: Start and Stop redisd (这里是服务的描述, 可以随便写) 增加可执行权限, chmod +x /etc/init.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid><description>==写得很乱,以后知识体系完善了,再整理==
操作系统分类:
linux : derbin,ubuntu,redhat,contos,fcortor,redflag,suse
unix:
window NT:
MAC :
linux由Linux内核,系统基本库,应用程序组成
设置Ubuntu的静态ip地址
/etc/network/interfaces //ip地址文件路径
sudo /etc/init.d/networking restart //重启网卡
修改如下(静态ip):
auto ens32 iface ens32 inet static address 192.168.22.127 netmask 255.255.255.0 gateway 192.168.22.254 DNS设置:
临时设置:/etc/resolv.conf 永久设置:/etc/resolvconf/resolv.conf.d/base 使用 lrzsz
可以实现window与Ubuntu之间的文件上传与下载
​ sudo apt-get install lrzsz ​
Linux命令:
shutdown -h now 立刻关机
shutdown -r now 立刻重启
cd
pwd
ls
grep(筛选)
rm 删除文件
rm -rf 删除目录
mv 旧名字 新名字 //重命名文件
mv 旧位置 新位置 //移动文件
cp 旧文件 新位置 拷贝文件</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/shell/shell%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/shell/shell%E5%91%BD%E4%BB%A4/</guid><description>[toc]
if 一、条件测试的表达式：
[ expression ] 括号两端必须要有空格
[[ expression ]] 括号两端必须要有空格
test expression 组合测试条件：
-a: and -o: or !: 非 二、整数比较：
-eq 测试两个整数是否相等 -ne 测试两个整数是否不等 -gt 测试一个数是否大于另一个数 -lt 测试一个数是否小于另一个数 -ge 大于或等于 -le 小于或等于 命令间的逻辑关系
逻辑与：&amp;amp;&amp;amp; 第一个条件为假 第二个条件不用在判断，最总结果已经有 第一个条件为真，第二个条件必须得判断 逻辑或：|| 三、字符串比较
字符串比较： == 等于 两边要有空格 != 不等 &amp;gt; 大于 &amp;lt; 小于 四、文件测试
-z string 测试指定字符是否为空，空着真，非空为假 -n string 测试指定字符串是否为不空，空为假 非空为真 -e FILE 测试文件是否存在 -f file 测试文件是否为普通文件 -d file 测试指定路径是否为目录 -r file 测试文件对当前用户是否可读 -w file 测试文件对当前用户是否可写 -x file 测试文件对当前用户是都可执行 -z 是否为空 为空则为真 -a 是否不空 五、if 语法</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/shell/shell%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/shell/shell%E6%A1%88%E4%BE%8B/</guid><description>[toc]
通过命令返回值判断 if [ `sed -n &amp;#39;$=&amp;#39; a.txt` -lt 2 ] ; then echo &amp;#34;a.txt文件的行数大于2&amp;#34; 判断文件夹 if [ ! -d &amp;#34;$old_path$1&amp;#34; ];then echo &amp;#34; 路径 $old_path$1 不存在,文件夹 $1 不存在&amp;#34; return</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/%E7%AE%80%E8%BF%B0systemd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/%E7%AE%80%E8%BF%B0systemd/</guid><description>[toc]
Systemd 概述 Systemd 简介 历史上，Linux 的启动一直采用 init 进程。这种方法有两个缺点。
一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。
二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。
下面的命令用来启动服务。
$ sudo /etc/init.d/apache2 start # 或者 $ service apache2 start Systemd 就是为了解决这些问题而诞生的,字母d是守护进程（daemon）的缩写
Systemd 是一系列工具的集合，其作用也远远不仅是启动操作系统，它还接管了后台服务、结束、状态查询，以及日志归档、设备管理、电源管理、定时任务等许多职责，并支持通过特定事件（如插入特定 USB 设备）和特定端口数据触发的 On-demand（按需）任务。
Systemd 的后台服务还有一个特殊的身份——它是系统中 PID 值为 1 的进程。
更少的进程 Systemd 提供了 服务按需启动 的能力，使得特定的服务只有在真定被请求时才启动。
允许更多的进程并行启动 在 SysV-init 时代，将每个服务项目编号依次执行启动脚本。Ubuntu 的 Upstart 解决了没有直接依赖的启动之间的并行启动。而 Systemd 通过 Socket 缓存、DBus 缓存和建立临时挂载点等方法进一步解决了启动进程之间的依赖，做到了所有系统服务并发启动。对于用户自定义的服务，Systemd 允许配置其启动依赖项目，从而确保服务按必要的顺序运行。
使用 CGroup 跟踪和管理进程的生命周期 在 Systemd 之间的主流应用管理服务都是使用 进程树 来跟踪应用的继承关系的，而进程的父子关系很容易通过 两次 fork 的方法脱离。
而 Systemd 则提供通过 CGroup 跟踪进程关系，引补了这个缺漏。通过 CGroup 不仅能够实现服务之间访问隔离，限制特定应用程序对系统资源的访问配额，还能更精确地管理服务的生命周期。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/%E7%B3%BB%E7%BB%9F%E7%BA%A7%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E5%85%AC%E5%85%B1%E9%83%A8%E5%88%86%E6%9A%82%E5%AE%9A/%E7%B3%BB%E7%BB%9F%E7%BA%A7%E5%91%BD%E4%BB%A4/</guid><description>centos
配置ip地址 vim /etc/sysconfig/network-scripts/ifcfg-eno* 配置静态IP
HWADDR=00:0C:29:8D:24:73 TYPE=Ethernet BOOTPROTO=static #启用静态IP地址 DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_PEERDNS=yes IPV6_PEERROUTES=yes IPV6_FAILURE_FATAL=no NAME=eno16777736 UUID=ae0965e7-22b9-45aa-8ec9-3f0a20a85d11 ONBOOT=yes #开启自动启用网络连接 IPADDR=192.168.21.128 #设置IP地址 GATEWAY=192.168.21.2 #设置网关 DNS1=8.8.8.8 #设置主DNS DNS2=8.8.4.4 #设置备DNS 重启网卡 service network restart 或者 systemctl restart network.service
注意网卡物理地址要正确, 使用ip addr 命令查看的地址和要HWADDR一样,不然会启动失败
开启/关闭网卡 ip link set eth0 up/down
开启网络 vim /etc/sysconfig/network NETWORKING=yes
表示系统是否使用网络，一般设置为yes。如果设为no，则不能使用网络。
建议关闭防火墙 systemctl stop firewalld.service 查看防火墙状态 systemctl status firewalld.service 禁止firewall开机启动 systemctl disable firewalld.service service iptables stop
更换pip源 阿里云 http://mirrors.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/linux%E7%B3%BB%E7%BB%9F/%E9%9D%A2%E8%AF%95/</guid><description>[TOC]
cpu 100% 怎么排查? 找到最耗CPU的进程: top命令查看进程运行信息列表,键入P (大P)，进程按照CPU使用率从高到低排序。 找到最耗CPU的线程：top -Hp 进程PID命令查看该进程下所有的线程,键入P (大P)，进程按照CPU使用率从高到低排序。 将线程PID转化为16进制:printf “%X\n” 线程PID。是因为堆栈里，线程id是用16进制表示的(十进制6524转换为十六进制就是197c)。 查看堆栈，找到线程: jstack 进程PID |grep 线程PID转换后的16进制 -C10(显示匹配行前后各10行) &amp;ndash;color。例如:jstack 6505 |grep 0x197c -C10 &amp;ndash;color Linux-cpu100%排查_听风的小男孩的博客-CSDN博客_cpu排查 linux
CPU 100% 异常排查实践与总结 - leejun2005的个人页面 - OSCHINA - 中文开源技术交流社区</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/anaconda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/anaconda/</guid><description>[toc]
一、什么是Anaconda？ 1. 简介 Anaconda（官方网站）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。
简单的说就是提供了一个python的环境,还有很多工具包,不需要自己单独安装python了,这个工具自带python,还可以通过虚拟环境,指定不同版本的python
通过虚拟出来的环境,可以当成一个python用,要用时指定对应目录即可
链接：https://www.jianshu.com/p/62f155eb6ac5
2. Anaconda、conda、pip、virtualenv的区别 ① Anaconda Anaconda是一个包含180+的科学包及其依赖项的发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook等。 ② conda conda是包及其依赖项和环境的管理工具。
适用语言：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。
适用平台：Windows, macOS, Linux
用途：
快速安装、运行和升级包及其依赖项。 在计算机中便捷地创建、保存、加载和切换环境。 如果你需要的包要求不同版本的Python，你无需切换到不同的环境，因为conda同样是一个环境管理器。仅需要几条命令，你可以创建一个完全独立的环境来运行不同的Python版本，同时继续在你常规的环境中使用你常用的Python版本。——conda官方网站
conda为Python项目而创造，但可适用于上述的多种语言。
conda包和环境管理器包含于Anaconda的所有版本当中。
③ pip pip是用于安装和管理软件包的包管理器。 pip编写语言：Python。 Python中默认安装的版本： Python 2.7.9及后续版本：默认安装，命令为pip Python 3.4及后续版本：默认安装，命令为pip3 pip名称的由来：pip采用的是递归缩写进行命名的。其名字被普遍认为来源于2处： “Pip installs Packages”（“pip安装包”） “Pip installs Python”（“pip安装Python”） ④ virtualenv virtualenv：用于创建一个独立的Python环境的工具。 解决问题： 当一个程序需要使用Python 2.7版本，而另一个程序需要使用Python 3.6版本，如何同时使用这两个程序？ 如果将所有程序都安装在系统下的默认路径，如：/usr/lib/python2.7/site-packages，当不小心升级了本不该升级的程序时，将会对其他的程序造成影响。 如果想要安装程序并在程序运行时对其库或库的版本进行修改，都会导致程序的中断。 在共享主机时，无法在全局site-packages目录中安装包。 virtualenv将会为它自己的安装目录创建一个环境，这并不与其他virtualenv环境共享库；同时也可以选择性地不连接已安装的全局库。 ⑤ pip 与 conda 比较 → 依赖项检查 pip： 不一定会展示所需其他依赖包。 安装包时或许会直接忽略依赖项而安装，仅在结果中提示错误。 conda： 列出所需其他依赖包。 安装包时自动安装其依赖项。 可以便捷地在包的不同版本中自由切换。 → 环境管理 pip：维护多个环境难度较大。 conda：比较方便地在不同环境之间进行切换，环境管理较为简单。 → 对系统自带Python的影响 pip：在系统自带Python中包的**更新/回退版本/卸载将影响其他程序。 conda：不会影响系统自带Python。 → 适用语言 pip：仅适用于Python。 conda：适用于Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。 ⑥ conda与pip、virtualenv的关系 conda结合了pip和virtualenv的功能。 链接：https://www.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E5%91%BD%E4%BB%A4/</guid><description>创建一个Django项目 django-admin.py startproject HelloWorld
HelloWorld: 项目的容器。
manage.py: 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。
HelloWorld/init.py: 一个空文件，告诉 Python 该目录是一个 Python 包。
HelloWorld/settings.py: 该 Django 项目的设置/配置。
HelloWorld/urls.py: 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站&amp;quot;目录&amp;quot;。
HelloWorld/wsgi.py: 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。
启动服务 cd HelloWorld python manage.py runserver 0.0.0.0:8000
#0.0.0.0 让其它电脑可连接到开发服务器
创建一个app,名为sales (terminal中): django-admin[.py] startapp sales
建表: python manage.py makemigrations sale 让 Django 知道我们在我们的模型有一些变更
python manage.py migrate sale 创建表结构
收集静态资源 (css,js,image): python manage.py collectstatic
模糊查询: sciencenews = models.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E9%87%8D%E5%86%99resource/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E9%87%8D%E5%86%99resource/</guid><description>原因: 使用modleresource时 返回注入的值是模型,当你放回的值不是模型对应的字段则会报错,比如进行了聚合操作
解决: 重写resource,自定义返回的值
SaleResource
class SaleResource(Resource): avgPrice = fields.IntegerField(attribute=&amp;#39;avgPrice&amp;#39;) maxPrice = fields.DecimalField(attribute=&amp;#39;maxPrice&amp;#39;) minPrice = fields.DecimalField(attribute=&amp;#39;minPrice&amp;#39;) class Meta: resource_name = &amp;#39;sale&amp;#39; authorization = Authorization() filtering = { &amp;#39;id&amp;#39;: ALL_WITH_RELATIONS, &amp;#39;pub_date&amp;#39;: [&amp;#39;exact&amp;#39;, &amp;#39;lt&amp;#39;, &amp;#39;lte&amp;#39;, &amp;#39;gte&amp;#39;, &amp;#39;gt&amp;#39;], } def get_object_list(self, request): reID = request.GET[&amp;#39;id&amp;#39;] logger.debug(&amp;#34;需要查询三种价格的ID:&amp;#34; + reID) results = [] queryset = Sale.objects.filter(id=reID) \ .annotate(avgPrice=Avg(&amp;#34;sellPrice&amp;#34;), maxPrice=Max(&amp;#34;sellPrice&amp;#34;), minPrice=Min(&amp;#34;sellPrice&amp;#34;)) \ .values(&amp;#39;avgPrice&amp;#39;, &amp;#39;maxPrice&amp;#39;, &amp;#39;minPrice&amp;#39;) for item in queryset: \# logging.warn(item) new_obj = BaseJsonModel() new_obj.minPrice = item[&amp;#39;minPrice&amp;#39;] new_obj.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E9%94%99%E8%AF%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/django/%E9%94%99%E8%AF%AF/</guid><description>错误大意: 必须是对象 使用: 使用聚合操作访问MongoDB时
解决:在参数前面加上 * , eg. SaleMongo.objects.all().aggregate(*pipeline)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/pip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/pip/</guid><description>pip 使用:
导出各种包: pip freeze &amp;gt; requirments.txt
从文件中安装各种包 pip install -r requirments.txt
virtualenv`
(1).安装:
pip install virtualenv (2).创建一个隔离环境
virtualenv myproject (3)使用这个隔离环境
source venv/bin/activate (4)退出
deactivate
virtualenvwrapper
(1).安装:
linux: pip install virtualenvwrapper win: pip install virtualenvwrapper-win
(2).配置:
在~/.bashrc写入以下内容(在其他配置文件也是可以的)
export WORKON_HOME=~/Envs
source /usr/local/bin/virtualenvwrapper.sh
第一行：virtualenvwrapper存放虚拟环境目录
第二行：virtrualenvwrapper会安装到python的bin目录下，所以该路径是python安装目录下bin/virtualenvwrapper.sh
读入配置文件，立即生效
source ~/.bashrc (3).使用
①.创建虚拟环境:
mkvirtualenv venv ②.查看当前虚拟环境目录
workon ③.切换虚拟环境:
workon venv ④.退出:
deactivate ⑤.删除虚拟环境
rmvirtualenv venv</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/logging/</guid><description>1. format: 指定输出的格式和内容
%(levelno)s: 打印日志级别的数值
%(levelname)s: 打印日志级别名称
%(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]
%(filename)s: 打印当前执行程序名
%(funcName)s: 打印日志的当前函数
%(lineno)d: 打印日志的当前行号
%(asctime)s: 打印日志的时间
%(thread)d: 打印线程ID
%(threadName)s: 打印线程名称
%(process)d: 打印进程ID
%(message)s: 打印日志信息
datefmt: 指定时间格式，同time.strftime()
level: 设置日志级别，默认为logging.WARNING
stream: 指定将日志的输出流，可以指定输出到sys.stderr,sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略
例如:format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s datefmt=%a, %d %b %Y %H:%M:%S
2. logging的几种handle方式
logging.StreamHandler: 日志输出到流，可以是sys.stderr、sys.stdout或者文件
logging.FileHandler: 日志输出到文件
日志回滚方式，实际使用时用RotatingFileHandler和TimedRotatingFileHandler
logging.handlers.BaseRotatingHandler
logging.handlers.RotatingFileHandler
logging.handlers.TimedRotatingFileHandler
logging.handlers.SocketHandler: 远程输出日志到TCP/IP sockets
logging.handlers.DatagramHandler: 远程输出日志到UDP sockets
logging.handlers.SMTPHandler: 远程输出日志到邮件地址
logging.handlers.SysLogHandler: 日志输出到syslog
logging.handlers.NTEventLogHandler: 远程输出日志到Windows NT/2000/XP的事件日志
logging.handlers.MemoryHandler: 日志输出到内存中的制定buffer
logging.handlers.HTTPHandler: 通过&amp;quot;GET&amp;quot;或&amp;quot;POST&amp;quot;远程输出到HTTP服务器
TimedRotatingFileHandler用法:
#日志打印格式
log_fmt = &amp;#39;%(asctime)s\tFile \&amp;#34;%(filename)s\&amp;#34;,line %(lineno)s\t%(levelname)s: %(message)s&amp;#39; formatter = logging.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/pdb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/pdb/</guid><description>经典使用:
在需要调试的地方加入代码:
import pdb ; pdb.set_trace()
使用参数:
h（elp）[ 命令 ] 没有参数:打印可用命令的列表。
有参数:打印关于该命令的帮助。例如:h n
b（reak）[[ filename：] lineno | 函数 [，条件 ]] lineno:在当前文件中lineon行设置一个中断。
没有参数:列出所有中断，包括每个断点，断点被击中的次数，当前的忽略计数，以及相关的条件（如果有的话）。
tbreak [[ filename：] lineno | 函数 [，条件 ]] 临时断点，当它被首次击中时被自动删除。参数与break一样。
cl（ear）[ filename：lineno | bpnumber [ bpnumber … ]] 文件名：lineno : 清除此行中的所有断点。
无参:清除所有断点
s(tep) 执行当前行，有函数就进入函数。
n(ext) 继续执行，直到当前功能中的下一行达到或返回。
r(eturn) 继续执行，直到当前函数返回。
c(ont(inue)) 继续执行，仅在遇到断点时停止。
j（ump）lineno 跳到指定行
l（ist）[ first [，last ]] 列出当前文件的源代码。默认显示11行
a（rgs） 打印当前函数的参数列表。
p 变量名(表达式) 打印变量名的值
print也可以使用，但不是调试器命令 - 这将执行Python print语句。
pp 表达式 像p命令一样，除了该表达式的值使用该pprint模块漂亮打印。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/%E4%BD%9C%E7%94%A8%E5%9F%9F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/%E4%BD%9C%E7%94%A8%E5%9F%9F/</guid><description>变量的作用域
在Python程序中创建、改变、查找变量名时，都是在一个保存变量名的空间中进行，我们称之为命名空间，也被称之为作用域。python的作用域是静态的，在源代码中变量名被赋值的位置决定了该变量能被访问的范围。即Python变量的作用域由变量所在源代码中的位置决定。
只有当变量在Module(模块)、def(函数)中定义的时候，才会有作用域的概念
在作用域中定义的变量，一般只在作用域中有效。 需要注意的是：在if-elif-else、for-else、while、try-except\try-finally等关键字的语句块中并不会产成作用域
注:
在Python中，scope是由namespace按特定的层级结构组合起来的。 scope一定是namespace，但namespace不一定是scope.
来自* &amp;lt;http://python.jobbole.com/81367/&amp;gt;
class没有作用域(scope)，但有一个局部的名空间(namespace)，它并不构成一个作用域。 这意味着在类定义中的表达式可以访问该名空间。
搜索变量名的优先级：局部作用域 &amp;gt; 嵌套作用域 &amp;gt; 全局作用域 &amp;gt; 内置作用域
LEGB****法则： 当在函数中使用未确定的变量名时，Python会按照优先级依次搜索4个作用域，以此来确定该变量名的意义。首先搜索局部作用域(L)，之后是上一层嵌套结构中def或lambda函数的嵌套作用域(E)，之后是全局作用域(G)，最后是内置作用域(B)。按这个查找原则，在第一处找到的地方停止。如果没有找到，则会出发NameError错误。
**L(local)**局部作用域 局部变量：包含在def关键字定义的语句块中，即在函数中定义的变量。每当函数被调用时都会创建一个新的局部作用域。Python中也有递归，即自己调用自己，每次调用都会创建一个新的局部命名空间。在函数内部的变量声明，除非特别的声明为全局变量，否则均默认为局部变量。有些情况需要在函数内部定义全局变量，这时可以使用global关键字来声明变量的作用域为全局。局部变量域就像一个 栈，仅仅是暂时的存在，依赖创建该局部作用域的函数是否处于活动的状态。所以，一般建议尽量少定义全局变量，因为全局变量在模块文件运行的过程中会一直存在，占用内存空间。
注意：如果需要在函数内部对全局变量赋值，需要在函数内部通过global语句声明该变量为全局变量。
**E(enclosing)**嵌套作用域 E也包含在def关键字中，E和L是相对的，E相对于更上层的函数而言也是L。与L的区别在于，对一个函数而言，L是定义在此函数内部的局部作用域，而E是定义在此函数的上一层父级函数的局部作用域。主要是为了实现Python的闭包，而增加的实现。
**G(global)**全局作用域 即在模块层次中定义的变量，每一个模块都是一个全局作用域。也就是说，在模块文件顶层声明的变量具有全局作用域，从外部开来，模块的全局变量就是一个模块对象的属性。
注意：全局作用域的作用范围仅限于单个模块文件内
**B(built-in)**内置作用域 系统内固定模块里定义的变量，如预定义在builtin 模块内的变量。
不要使用 from xx import * , 会把xx模块中所有东西(变量,函数,类,等等)( 以 _ 开头的变量,函数不会引入)引入变为全局变量,就不知道引入了什么,怕 会引起作用域的冲突</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/python2/%E9%97%AE%E9%A2%98/</guid><description>安装spynner:
①. 需先安装pyqt4,
下载地址:https://www.riverbankcomputing.com/software/pyqt/download/https://www.riverbankcomputing.com/software/pyqt/download
②. 安装 autopy:
下载地址: https://pypi.python.org/pypi/autopy/0.51
注: 安装时会自动安装在python下,我用的是Anaconda中的python,所有不成功,下载二进制文件安装autopy-0.51-cp27-cp27m-win_amd64.whl
安装命令: pip install autopy-0.51-cp27-cp27m-win_amd64.whl</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/phantomjs%E5%92%8Cselenium/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/phantomjs%E5%92%8Cselenium/%E5%91%BD%E4%BB%A4/</guid><description>安装:
安装selenium: pip install selenium 安装phantomjs: 下载包,将bin/phantomjs.exe文件加入到环境变量path中即可 请求头设置:
dcap = dict(DesiredCapabilities.PHANTOMJS) headers = {&amp;#39;Accept&amp;#39;: &amp;#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&amp;#39;, &amp;#39;Accept-Language&amp;#39;: &amp;#39;zh-CN,zh;q=0.8&amp;#39;, &amp;#39;Cache-Control&amp;#39;: &amp;#39;max-age=0&amp;#39;, &amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36&amp;#39;, &amp;#39;Connection&amp;#39;: &amp;#39;keep-alive&amp;#39;, &amp;#39;Host&amp;#39;: &amp;#39;club.autohome.com.cn&amp;#39;, \# &amp;#39;Upgrade - Insecure - Requests&amp;#39;:1, &amp;#39;Referer&amp;#39;: &amp;#39;https://www.autohome.com.cn/shanghai/&amp;#39;, } for key, value in headers.iteritems(): dcap[&amp;#39;phantomjs.page.customHeaders.{}&amp;#39;.format(key)] = value driver = webdriver.PhantomJS(desired_capabilities=dcap) 添加代理:
from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities def get_cookie(url,source=&amp;#39;xcar&amp;#39;): &amp;#34;&amp;#34;&amp;#34; 爱卡出现安全页面,模拟输入验证码获得cookie &amp;#34;&amp;#34;&amp;#34; cookies={} dcap = dict(DesiredCapabilities.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>常用技术:
scrapy:一种爬虫框架,我用来爬取了静态页面 Xpath : 用来取路径,根据写的标签路径来取值 beautifulSoup: 与Xpath作用一样,根据标签路径来取值 splash:一个Javascript渲染服务</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/%E9%97%AE%E9%A2%98/%E7%88%B1%E5%8D%A1/%E7%88%B1%E5%8D%A1%E7%9A%84%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/python%E5%8F%8A%E5%85%B6%E6%A1%86%E6%9E%B6/%E7%88%AC%E8%99%AB/%E9%97%AE%E9%A2%98/%E7%88%B1%E5%8D%A1/%E7%88%B1%E5%8D%A1%E7%9A%84%E9%97%AE%E9%A2%98/</guid><description>1.错误次数太多,用户连接超时(不定期出现)
原因:太多次重复连接,导致网页缓慢,不能再指定时间(5秒)内连接上
解决:将代理ip中间件换成新框架的
2.代理池有卡顿
3.频繁请求ip,无论成功与否
4.爬取速度慢:
原因:不停在爬取重复页面,在爬取帖子详细内容时,重复请求该页面
解决:目前是注释掉那句代码
5.数据量不对:
原因:目前爱卡更改了对发布帖子排序的操作,增加了cookie(oderby参数)来控制是否排序
以前爬下来的是按回复排序
解决:增加cookie &amp;lsquo;oderby&amp;rsquo;:1,
6.被爱卡禁止访问:
原因:爱卡增加了cookie验证,来做反爬
解决:增加所需cookie,具体如下:
&amp;lsquo;_appuv_newcar&amp;rsquo;:&amp;lsquo;f0fafccbbed05fa57d0b404f1be0d158&amp;rsquo;,
&amp;lsquo;_fwck_newcar&amp;rsquo;:&amp;lsquo;c28879b9110138c43ca7ef25c7e7f52b&amp;rsquo;,
&amp;lsquo;_appuv_www&amp;rsquo;:&amp;lsquo;37d34d06ad43bde460744eedc6a82c98&amp;rsquo;,
&amp;lsquo;_fwck_www&amp;rsquo;:&amp;lsquo;b15320548fb602fafa7e384f4c0f568a&amp;rsquo;,
7.无法进行翻页:
原因:获取下一页数字的代码错误,例:
Href:forumdisplay.php?fid=741&amp;amp;orderby=dateline&amp;amp;page=4
原代码:按 &amp;ldquo;=&amp;rdquo; 分开取第三个,则会取出 dateline&amp;amp;page , 此为错误
解决: 按 &amp;ldquo;page=&amp;rdquo; 切分, 取第二个
8.用户禁言论坛修改
能爬取禁言论坛,修改了xcar.py文件</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E5%89%8D%E7%AB%AF/js/%E7%82%B9%E5%87%BB%E5%9C%B0%E5%9B%BE%E8%8E%B7%E5%8F%96%E7%BB%8F%E7%BA%AC%E5%BA%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E5%89%8D%E7%AB%AF/js/%E7%82%B9%E5%87%BB%E5%9C%B0%E5%9B%BE%E8%8E%B7%E5%8F%96%E7%BB%8F%E7%BA%AC%E5%BA%A6/</guid><description>//初始化地图对象，加载地图 var map = new AMap.Map(&amp;#39;mapContainer&amp;#39;, { resizeEnable: true, zoom:11 }); //为地图注册click事件获取鼠标点击出的经纬度坐标 var clickEventListener = map.on(&amp;#39;click&amp;#39;, function(e) { document.getElementById(&amp;#34;longitude&amp;#34;).value = e.lnglat.getLng(); document.getElementById(&amp;#34;latitude&amp;#34;).value = e.lnglat.getLat(); var lnglatXY = [e.lnglat.getLng(), e.lnglat.getLat()]; AMap.service(&amp;#39;AMap.Geocoder&amp;#39;,function(){//回调函数 //实例化Geocoder geocoder = new AMap.Geocoder({ city: &amp;#34;010&amp;#34;,//城市，默认：“全国” }); //逆地理编码 geocoder.getAddress(lnglatXY, function(status, result) { if (status === &amp;#39;complete&amp;#39; &amp;amp;&amp;amp; result.info === &amp;#39;OK&amp;#39;) { //获得了有效的地址信息: document.getElementById(&amp;#34;addressStr&amp;#34;).value = result.regeocode.formattedAddress; } else { //获取地址失败 alertBox(&amp;#34;暂无地址信息！&amp;#34;); } }); }); }); document.getElementById(&amp;#34;query&amp;#34;).onclick = function(e){ var inputAddress = document.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/_id%E5%92%8Cobjectid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/_id%E5%92%8Cobjectid/</guid><description>[toc]
1.ObjectId ObjectId是&amp;quot;_id&amp;quot;的默认类型。它设计成轻量型的，不同的机器都能用全局唯一的同种方法方便地生成它。
这是MongoDB采用ObjectId，而不是其他比较常规的做法（比如自动增加的主键）的主要原因，因为在多个服务器上同步自动增加主键值既费力还费时。MongoDB从一开始就设计用来作为分布式数据库，处理多个节点是一个核心要求。
ObjectId使用12字节的存储空间，每个字节两位十六进制数字，是一个24位的字符串。由于看起来很长，不少人会觉得难以处理。但关键是要知道这个长长的ObjectId是实际存储数据的两倍长。
如果快速连续创建多个ObjectId，会发现每次只有最后几位数字有变化。另外中间的几位数字也会变化（要是在创建的过程中停顿几秒钟）。这是ObjectId的创建方式导致的。12字节按照如下方式生成：
0|1|2|3|4|5|6|7|8 |9|10|11 时间戳|机器 |PID|计数器 前4字节是从标准纪元开始的时间戳，单位为秒。这会带来一些有用的属性。时间戳，与随后的5个字节组合起来，提供了秒级别的唯一性。
由于时间戳在前，这意味着ObjectId大致会按照插入的顺序排列。这对于某些方面很有用，如将其作为索引提高效率，但是这个是没有保证的，仅仅是&amp;quot;大致&amp;quot;。这4个字节也隐含了文档创建的时间。绝大多数驱动都会公开一个方法从ObjectId获取这个信息。
因为使用的是当前时间，很多用户担心要对服务器进行时间同步，其实这个没有必要，因为时间戳的实际值并不重要，只要其总是不停增加就好了（每秒一次）。
接下来的三个字节是所在主机的唯一标识符。通常是机器主机名的散列值。这样就可以确保不同主机生成不同的ObjectId，不产生冲突。
为了确保在同一台机器上并发的多个进程产生的ObjectId是唯一的。后3个字节就是一个自动增加的计数器，确保相同进程同一秒产生的ObjectId也是不一样的。同一秒钟最多允许每个进程拥有256（16777216）个不同的ObjectId。
2.自动生成_id 前面讲到，如果插入文档的时候没有&amp;quot;_id&amp;quot;键，系统会帮你自动创建一个。可以由MongoDB服务器来做这件事情，但通常会在客户端由驱动程序完成。理由如下：
虽然ObjectId设计成轻量型的，易于生成，但是毕竟生成的时候还是产生开销。在客户端生成体现了MongoDB的设计
理念：能从服务器端转移到驱动程序来做的事，就尽量转移。这种理念背后的原因是，即便是像MongoDB这样的可扩展数据库，扩展应用层也要比扩展数据库层容易的多。将事务交由客户端来处理，就减轻了数据库扩展的负担。
在客户端生成ObjectId，驱动程序能够提供更加丰富的API。例如，驱动程序可以有自己的insert方法，可以返回生成的ObjectId，也可以直接将其插入文档。如果驱动程序允许服务器生成ObjectId，那么将需要单独的查询，以确 定插入的文档中的&amp;quot;_id&amp;quot;值。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/sql%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/sql%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/</guid><description>2017-11-10T10:28:34.939+0800 I COMMAND [conn57] command DJangoLearn.sale_mongo command: aggregate { aggregate: &amp;#34;sale_mongo&amp;#34;, pipeline: [{ $match: { id: 45 } }, { $group: { _id: &amp;#34;$id&amp;#34;, maxPrice: { $max: &amp;#34;$sellPrice&amp;#34; }, minPrice: { $min: &amp;#34;\$sellPrice&amp;#34; }, avgPrice: {\ $avg: &amp;#34;\$sellPrice&amp;#34; } } }], cursor: {} } planSummary: COLLSCAN keysExamined: 0 docsExamined: 5000000 cursorExhausted: 1 numYields: 39194 nreturned: 1 reslen: 181 locks: { Global: { acquireCount: { r: 78396 } }, Database: { acquireCount: { r: 39198 } }, Collection: { acquireCount: { r: 39197 } } } protocol: op_query 3668 ms 参数:</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E5%91%BD%E4%BB%A4/</guid><description>[toc]
1. 按时间查询: db.getCollection('sale_mongo').find({'sellTime':{$lt:new Date('2014-02-01')}})
2. 查询三种价格: {$group : {_id : &amp;#34;$id&amp;#34;, maxPrice : {$max : &amp;#34;$sellPrice&amp;#34;} , minPrice : {$min : &amp;#34;$sellPrice&amp;#34;} , avgPrice : {$avg : &amp;#34;$sellPrice&amp;#34;}, } } ]) 3. 查询时间: db.getCollection('sale_mongo').find({'sellTime':{$lt:new Date('2014-02-01')}})
4. 插入时间: timeStr = &amp;#34;2009-08-02 05:00:09&amp;#34; db.insert_many([ {&amp;#39;id&amp;#39;: 0,&amp;#39;sellTime&amp;#39;:datetime.datetime.strptime(timeStr, &amp;#34;%Y-%m-%d %H:%M:%S&amp;#34;),&amp;#39;sellPrice&amp;#39;:1000.0 }, {&amp;#39;id&amp;#39;: 0,&amp;#39;sellTime&amp;#39;:datetime.datetime.strptime(timeStr, &amp;#34;%Y-%m-%d %H:%M:%S&amp;#34;),&amp;#39;sellPrice&amp;#39;:1030.0 }, ]) 5. 创建索引: db.COLLECTION_NAME.ensureIndex({KEY:1})
1/-1 : 升/降序
1.8版本之前创建索引使用createIndex() db.col.ensureIndex({&amp;ldquo;title&amp;rdquo;:1})
getIndexes():方法可以用来查看集合的所有索引
totalIndexSize() : 查看集合索引的总大小
dropIndex(key) : 删除指定的索引
6. 查询 and : db.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E5%AE%89%E8%A3%85%E5%8F%8A%E5%87%86%E5%A4%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E5%AE%89%E8%A3%85%E5%8F%8A%E5%87%86%E5%A4%87/</guid><description> 强烈建议设置SELinux: 修改配置文件(并重启): sudo /etc/selinux/config 改为: SELINUX=disabled或者SELINUX=disabled 继 1.(上一点),注释bindIp这一行
创建&amp;quot;创建管理员&amp;quot; use admin db.createUser({user:&amp;#39;xkj&amp;#39;,pwd:&amp;#39;a&amp;#39;, roles:[{role:&amp;#39;userAdminAnyDatabase&amp;#39;, db:&amp;#39;admin&amp;#39;}]}) 验证: db.auth(&amp;#39;xkj&amp;#39;,&amp;#39;a&amp;#39;) 创建用户: use DJangoLearn db.createUser({user:&amp;#39;mathartsys&amp;#39;,pwd:&amp;#39;a&amp;#39;, roles:[{role:&amp;#39;dbAdmin&amp;#39;, db:&amp;#39;DJangoLearn&amp;#39;}]})</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E7%B4%A2%E5%BC%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb/%E7%B4%A2%E5%BC%95/</guid><description>为什么 MongoDB 使用 B 树？
首先要了解Btree和B+tree之间的区别,详情见mysql的索引
作为 NoSQL 的 MongoDB，其目标场景就与更早的数据库就有着比较大的差异，我们来简单总结一下 MongoDB 最终选择使用 B 树的两个原因：
MySQL 使用 B+ 树是因为数据的遍历在关系型数据库中非常常见，它经常需要处理各个表之间的关系并通过范围查询一些数据；但是 MongoDB 作为面向文档的数据库，与数据之间的关系相比，它更看重以文档为中心的组织方式，所以选择了查询单个文档性能较好的 B 树，这个选择对遍历数据的查询也可以保证可以接受的时延；(没有那么多的范围查询) LSM 树是一种专门用来优化写入的数据结构，它将随机写变成了顺序写显著地提高了写入性能，但是却牺牲了读的效率，这与大多数场景需要的特点是不匹配的，所以 MongoDB 最终还是选择读取性能更好的 B 树作为默认的数据结构； https://blog.csdn.net/kexuanxiu1163/article/details/106821401</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/binlog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/binlog/</guid><description>[toc]
1. 前言 MySQL的二进制日志可以说是MySQL最重要的日志了，它记录了所有的DDL和DML(除了数据查询语句)语句，以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。一般来说开启二进制日志大概会有1%的性能损耗(参见MySQL官方中文手册 5.1.24版)。二进制有两个最重要的使用场景: 其一：MySQL Replication在Master端开启binlog，Mster把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 其二：自然就是数据恢复了，通过使用mysqlbinlog工具来使恢复数据。
二进制日志包括两类文件：
二进制日志索引文件（文件名后缀为.index）用于记录所有的二进制文件，
二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML(除了数据查询语句)语句事件。
binlog是一个二进制文件集合，每个binlog文件以一个4字节的魔数开头，接着是一组Events:
魔数：0xfe62696e对应的是0xfebin；
Event：每个Event包含header和data两个部分；header提供了Event的创建时间，哪个服务器等信息，data部分提供的是针对该Event的具体信息，如具体数据的修改；
第一个Event用于描述binlog文件的格式版本，这个格式就是event写入binlog文件的格式；
其余的Event按照第一个Event的格式版本写入；
最后一个Event用于说明下一个binlog文件；
binlog的索引文件是一个文本文件，其中内容为当前的binlog文件列表
当遇到以下3种情况时，MySQL会重新生成一个新的日志文件，文件序号递增：
MySQL服务器停止或重启时
使用 flush logs 命令；
当 binlog 文件大小超过 max_binlog_size 变量的值时；
max_binlog_size 的最小值是4096字节，最大值和默认值是 1GB (1073741824字节)。事务被写入到binlog的一个块中，所以它不会在几个二进制日志之间被拆分。
因此，如果你有很大的事务，为了保证事务的完整性，不可能做切换日志的动作，只能将该事务的日志都记录到当前日志文件中，直到事务结束，你可能会看到binlog文件大于 max_binlog_size 的情况。
写 Binlog 的时机
对支持事务的引擎如InnoDB而言，是在prepare完commit前写的。binlog 什么时候刷新到磁盘跟参数 sync_binlog 相关。
如果设置为0，则表示MySQL不控制binlog的刷新，由文件系统去控制它缓存的刷新；
如果设置为不为0的值，则表示每 sync_binlog 次事务，MySQL调用文件系统的刷新操作刷新binlog到磁盘中。
设为1是最安全的，在系统故障时最多丢失一个事务的更新，但是会对性能有所影响。
如果 sync_binlog=0 或 sync_binlog大于1，当发生电源故障或操作系统崩溃时，可能有一部分已提交但其binlog未被同步到磁盘的事务会被丢失，恢复程序将无法恢复这部分事务。
在MySQL 5.7.7之前，默认值 sync_binlog 是0，MySQL 5.7.7和更高版本使用默认值1，这是最安全的选择。一般情况下会设置为100或者0，牺牲一定的一致性来获取更好的性能。
这涉及几个知识点
redolog
二段式提交
当我们更新数据时，两阶段提交的具体流程：
更新操作先写入redolog，这时候这条log的状态是prepared状态 再将逻辑日志写入binlog 最后在binlog写好之后，把redolog里的这条日志的状态改为commit redolog和undolog：innodb事务日志包括redo log和undo log。redo log是重做日志，提供前滚操作，undo log是回滚日志，提供回滚操作。
三种日志的写入顺序 : undo log -&amp;gt; redo log(prepare阶段) -&amp;gt; bin log -&amp;gt; commit</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/crash-safe%E5%8E%9F%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/crash-safe%E5%8E%9F%E7%90%86/</guid><description>[toc]
一、前言 MySQL 保证数据不会丢的能力主要体现在两方面：
能够恢复到任何时间点的状态； 能够保证MySQL在任何时间段突然奔溃，重启后之前提交的记录都不会丢失； 对于第一点将MySQL恢复到任何时间点的状态，相信很多人都知道，只要保留有足够的binlog，就能通过重跑binlog来实现。
对于第二点的能力，也就是本文标题所讲的crash-safe。即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和unod log两个日志。
因为crash-safe主要体现在事务执行过程中突然奔溃，重启后能保证事务完整性，所以在讲解具体原理之前，先了解下MySQL事务执行有哪些关键阶段，后面才能依据这几个阶段来进行解析。下面以一条更新语句的执行流程为例，话不多说，直接上图：
从上图可以清晰地看出一条更新语句在MySQL中是怎么执行的，简单进行总结一下：
从内存中找出这条数据记录，对其进行更新； 将对数据页的更改记录到redo log中； 将逻辑操作记录到binlog中； 对于内存中的数据和日志，都是由后台线程，当触发到落盘规则后再异步进行刷盘； 二、WAL机制 问题：为什么不直接更改磁盘中的数据，而要在内存中更改，然后还需要写日志，最后再落盘这么复杂？
MySQL更改数据的时候，之所以不直接写磁盘文件中的数据，最主要就是性能问题。因为直接写磁盘文件是随机写，开销大性能低，没办法满足MySQL的性能要求。所以才会设计成先在内存中对数据进行更改，再异步落盘。但是内存总是不可靠，万一断电重启，还没来得及落盘的内存数据就会丢失，所以还需要加上写日志这个步骤，万一断电重启，还能通过日志中的记录进行恢复。
写日志虽然也是写磁盘，但是它是顺序写，相比随机写开销更小，能提升语句执行的性能
这个技术就是大多数存储系统基本都会用的WAL(Write Ahead Log)技术，也称为日志先行的技术，指的是对数据文件进行修改前，必须将修改先记录日志。保证了数据一致性和持久性，并且提升语句执行性能。
三、核心日志模块 更新SQL执行过程中，总共涉及MySQL日志模块其中的三个核心日志，分别是redo log（重做日志）、undo log（回滚日志）、binlog（归档日志）。crash-safe的能力主要依赖的就是这三大日志。
3.1、重做日志 redo log redo log也称为事务日志，由InnoDB存储引擎层产生。记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置，因为修改会覆盖之前的）。
前面提到的WAL技术，redo log就是WAL的典型应用，MySQL在有事务提交对数据进行更改时，只会在内存中修改对应的数据页和记录redo log日志，完成后即表示事务提交成功，至于磁盘数据文件的更新则由后台线程异步处理。由于redo log的加入，保证了MySQL数据一致性和持久性（即使数据刷盘之前MySQL奔溃了，重启后仍然能通过redo log里的更改记录进行重放，重新刷盘），此外还能提升语句的执行性能（写redo log是顺序写），由此可见redo log是必不可少的。
redo log是固定大小的，所以只能循环写，从头开始写，写到末尾就又回到开头，相当于一个环形。当日志写满了，就需要对旧的记录进行擦除，但在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。在redo log满了到擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求，所以有可能会导致MySQL卡顿。（所以针对并发量大的系统，适当设置redo log的文件大小非常重要！！！）
3.2、回滚日志 undo log undo log顾名思义，主要就是提供了回滚的作用和多个行版本控制(MVCC)，保证事务的原子性。在数据修改的流程中，会记录一条与当前操作相反的逻辑日志到undo log中（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录），如果因为某些原因导致事务异常失败了，可以借助该undo log进行回滚，保证事务的完整性，所以undo log也必不可少。
3.3、归档日志 binlog binlog在MySQL的server层产生，不属于任何引擎，主要记录用户对数据库操作的SQL语句（除了查询语句）。之所以将binlog称为归档日志，是因为binlog不会像redo log一样擦掉之前的记录循环写，而是一直记录（超过有效期(默认不删除)才会被清理），如果超过单日志的最大值（默认1G，可以通过变量 max_binlog_size 设置），则会新起一个文件继续记录。但由于日志可能是基于事务来记录的(如InnoDB表类型)，而事务是绝对不可能也不应该跨文件记录的，如果正好binlog日志文件达到了最大值但事务还没有提交则不会切换新的文件记录，而是继续增大日志，所以 max_binlog_size 指定的值和实际的binlog日志大小不一定相等。
正是由于binlog有归档的作用，所以binlog主要用作主从同步和数据库基于时间点的还原。
四、两阶段提交 问题：为什么redo log要分两步写，中间再穿插写binlog呢？
从上面可以看出，因为redo log影响主库的数据，binlog影响从库的数据，所以redo log和binlog必须保持一致才能保证主从数据一致，这是前提。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/ddl%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%E5%92%8C%E5%88%86%E7%B1%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/ddl%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%E5%92%8C%E5%88%86%E7%B1%BB/</guid><description>[toc]
前言 DDL(Data Definition Language) 众所周知，DDL定义了数据在数据库中的结构、关系以及权限等。比如CREATE，ALTER，DROP等等。
几种算法实现 分别是：copy、inplace、instant。
copy 算法为最古老的算法，在 MySQL 5.5 及以下为默认算法。 从 MySQL 5.6 开始，引入了 inplace 算法并且默认使用。inplace 算法还包含两种类型：rebuild-table 和 not-rebuild-table。MySQL 使用 inplace 算法时，会自动判断，能使用 not-rebuild-table 的情况下会尽量使用，不能的时候才会使用 rebuild-table。当 DDL 涉及到主键和全文索引相关的操作时，无法使用 not-rebuild-table，必须使用 rebuild-table。其他情况下都会使用 not-rebuild-table。 从 MySQL 8.0.12 开始，引入了 instant 算法并且默认使用。目前 instant 算法只支持增加列等少量 DDL 类型的操作，其他类型仍然会默认使用 inplace。 copy 算法 较简单的实现方法，MySQL 会建立一个新的临时表，把源表的所有数据写入到临时表，在此期间无法对源表进行数据写入。MySQL 在完成临时表的写入之后，用临时表替换掉源表。这个算法主要被早期（&amp;lt;=5.5）版本所使用。
inplace 算法 从 5.6 开始，常用的 DDL 都默认使用这个算法。inplace 算法包含两类：inplace-no-rebuild 和 inplace-rebuild，两者的主要差异在于是否需要重建源表。
inplace 算法的操作阶段主要分为三个：
Prepare阶段：- 创建新的临时 frm 文件(与 InnoDB 无关)。- 持有 EXCLUSIVE-MDL 锁，禁止读写。- 根据 alter 类型，确定执行方式（copy，online-rebuild，online-not-rebuild）。更新数据字典的内存对象。- 分配 row_log 对象记录数据变更的增量（仅 rebuild 类型需要）。- 生成新的临时ibd文件 new_table（仅rebuild类型需要）。 Execute 阶段：降级EXCLUSIVE-MDL锁，允许读写。扫描old_table聚集索引（主键）中的每一条记录 rec。遍历new_table的聚集索引和二级索引，逐一处理。根据 rec 构造对应的索引项。将构造索引项插入 sort_buffer 块排序。将 sort_buffer 块更新到 new_table 的索引上。记录 online-ddl 执行过程中产生的增量（仅 rebuild 类型需要）。重放 row_log 中的操作到 new_table 的索引上（not-rebuild 数据是在原表上更新）。重放 row_log 中的DML操作到 new_table 的数据行上。 Commit阶段：当前 Block 为 row_log 最后一个时，禁止读写，升级到 EXCLUSIVE-MDL 锁。重做 row_log 中最后一部分增量。更新 innodb 的数据字典表。提交事务（刷事务的 redo 日志）。修改统计信息。rename 临时 ibd 文件，frm文件。变更完成，释放 EXCLUSIVE-MDL 锁。 instant 算法 MySQL 8.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mycat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mycat/</guid><description>[toc]
1.前言 1.1 概念介绍: 1.1.1、分区 对业务透明，分区只不过把存放数据的文件分成了许多小块，例如mysql中的一张表对应三个文件.MYD,MYI,frm。
根据一定的规则把数据文件(MYD)和索引文件（MYI）进行了分割，分区后的表呢，还是一张表。分区可以把表分到不同的硬盘上，但不能分配到不同服务器上。
优点：数据不存在多个副本，不必进行数据复制，性能更高。 缺点：分区策略必须经过充分考虑，避免多个分区之间的数据存在关联关系，每个分区都是单点，如果某个分区宕机，就会影响到系统的使用。 1.1.2、分片 对业务透明，在物理实现上分成多个服务器，不同的分片在不同服务器上。如HDFS。
1.1.3、分表 同库分表：所有的分表都在一个数据库中，由于数据库中表名不能重复，因此需要把数据表名起成不同的名字。
优点：由于都在一个数据库中，公共表，不必进行复制，处理更简单。 缺点：由于还在一个数据库中，CPU、内存、文件IO、网络IO等瓶颈还是无法解决，只能降低单表中的数据记录数。表名不一致，会导后续的处理复杂（参照mysql meage存储引擎来处理） 不同库分表：由于分表在不同的数据库中，这个时候就可以使用同样的表名。
优点：CPU、内存、文件IO、网络IO等瓶颈可以得到有效解决，表名相同，处理起来相对简单。 缺点：公共表由于在所有的分表都要使用，因此要进行复制、同步。一些聚合的操作，join,group by,order等难以顺利进行。 1.1.4、分库 分表和分区都是基于同一个数据库里的数据分离技巧，对数据库性能有一定提升，但是随着业务数据量的增加，原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。
当业务系统的数据容量接近或超过单台服务器的容量、QPS/TPS接近或超过单个数据库实例的处理极限等。此时，往往是采用垂直和水平结合的数据拆分方法，把数据服务和数据存储分布到多台数据库服务器上。
分库只是一个通俗说法，更标准名称是数据分片，采用类似分布式数据库理论指导的方法实现，对应用程序达到数据服务的全透明和数据存储的全透明
来自: https://www.cnblogs.com/ijavanese/p/9512369.html
2.介绍 2.1 什么是MyCat？ 简单的说，MyCAT就是：
一个新颖的数据库中间件产品； 一个彻底开源的、面向企业应用开发的“大数据库集群”； 支持事务、ACID、可以替代MySQL的加强版数据库； 一个可以视为“MySQL”集群的企业级数据库，用来替代昂贵的Oracle集群； 一个融合内存缓存技术、Nosql技术、HDFS大数据的新型SQL Server； 结合传统数据库和新型分布式数据仓库的新一代企业级数据库产品。 2.2 MyCat的目标 MyCAT的目标是：低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。
2.3 MyCat的关键特性 ​ 支持 SQL 92标准
支持Mysql集群，可以作为Proxy使用
支持JDBC连接ORACLE、DB2、SQL Server，将其模拟为MySQL Server使用
支持NoSQL数据库
支持galera for mysql集群，percona-cluster或者mariadb cluster，提供高可用性数据分片集群
自动故障切换，高可用性
支持读写分离，支持Mysql双主多从，以及一主多从的模式
支持全局表，数据自动分片到多个节点，用于高效表关联查询
支持独有的基于E-R 关系的分片策略，实现了高效的表关联查询
支持一致性Hash分片，有效解决分片扩容难题
多平台支持，部署和实施简单
支持Catelet开发，类似数据库存储过程，用于跨分片复杂SQL的人工智能编码实现，143行Demo完成跨分片的两个表的JION查询。
支持NIO与AIO两种网络通信机制，Windows下建议AIO，Linux下目前建议NIO
支持Mysql存储过程调用
以插件方式支持SQL拦截和改写
支持自增长主键、支持Oracle的Sequence机制
2.4 总体架构 MyCAT的架构如下图所示：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/</guid><description>第一步：应用程序把查询SQL语句发给服务器端执行 我们在数据层执行SQL语句时，应用程序会连接到相应的数据库服务器，把SQL语句发送给服务器处理。
第二步：服务器解析请求的SQL语句 SQL计划缓存，经常用查询分析器的朋友大概都知道这样一个事实，往往一个查询语句在第一次运行的时候需要执行特别长的时间，但是如果你马上或者在一定时间内运行同样的语句，会在很短的时间内返回查询结果。原因是：
服务器在接收到查询请求后，并不会马上去数据库查询，而是在数据库中的计划缓存中找是否有相对应的执行计划。如果存在，就直接调用已经编译好的执行计划，节省了执行计划的编译时间。 如果所查询的行已经存在于数据缓冲存储区中，就不用查询物理文件了，而是从缓存中取数据，这样从内存中取数据就会比从硬盘上读取数据快很多，提高了查询效率。数据缓冲存储区会在后面提到。 如果在SQL计划缓存中没有对应的执行计划，服务器首先会对用户请求的SQL语句进行语法效验，如果有语法错误，服务器会结束查询操作，并用返回相应的错误信息给调用它的应用程序。
注意：此时返回的错误信息中，只会包含基本的语法错误信息，例如select 写成selec等，错误信息中如果包含一列表中本没有的列，此时服务器是不会检查出来的，因为只是语法验证，语义是否正确放在下一步进行。
语法符合后，就开始验证它的语义是否正确。例如，表名、列名、存储过程等等数据库对象是否真正存在，如果发现有不存在的，就会报错给应用程序，同时结束查询。
接下来就是获得对象的解析锁，我们在查询一个表时，首先服务器会对这个对象加锁，这是为了保证数据的统一性，如果不加锁，此时有数据插入，但因为没有加锁的原因，查询已经将这条记录读入，而有的插入会因为事务的失败会回滚，就会形成脏读的现象。
接下来就是对数据库用户权限的验证。SQL语句语法，语义都正确，此时并不一定能够得到查询结果，如果数据库用户没有相应的访问权限，服务器会报出权限不足的错误给应用程序，在稍大的项目中，往往一个项目里面会包含好几个数据库连接串，这些数据库用户具有不同的权限，有的是只读权限，有的是只写权限，有的是可读可写，根据不同的操作选取不同的用户来执行。稍微不注意，无论你的SQL语句写的多么完善，完美无缺都没用。
解析的最后一步，就是确定最终的执行计划。当语法、语义、权限都验证后，服务器并不会马上给你返回结果，而是会针对你的SQL进行优化，选择不同的查询算法以最高效的形式返回给应用程序。例如在做表联合查询时，服务器会根据开销成本来最终决定采用hash join,merge join ，还是loop join，采用哪一个索引会更高效等等。不过它的自动化优化是有限的，要想写出高效的查询SQL还是要优化自己的SQL查询语句。
当确定好执行计划后，就会把这个执行计划保存到SQL计划缓存中，下次在有相同的执行请求时，就直接从计划缓存中取，避免重新编译执行计划。
第三步：语句执行 服务器对SQL语句解析完成后，服务器才会知道这条语句到底表态了什么意思，接下来才会真正的执行SQL语句。
此时分两种情况：
如果查询语句所包含的数据行已经读取到数据缓冲存储区的话，服务器会直接从数据缓冲存储区中读取数据返回给应用程序，避免了从物理文件中读取，提高查询速度。 如果数据行没有在数据缓冲存储区中，则会从物理文件中读取记录返回给应用程序，同时把数据行写入数据缓冲存储区中，供下次使用。 来源: https://www.cnblogs.com/wuyun-blog/p/4697144.html
示意图 https://www.cnblogs.com/ChangAn223/p/10686639.html
扩展 sql的解析顺序: https://www.bilibili.com/video/BV12b411K7Zu?p=189</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</guid><description>[toc]
1.介绍 利用主从数据库来实现读写分离，从而分担主数据库的压力。在多个服务器上部署mysql，将其中一台认为主数据库，而其他为从数据库，实现主从同步。其中主数据库负责主动写的操作，而从数据库则只负责主动读的操作（slave从数据库仍然会被动的进行写操作，为了保持数据一致性），这样就可以很大程度上的避免数据丢失的问题，同时也可减少数据库的连接，减轻主数据库的负载。
原文：https://blog.csdn.net/qq_15092079/article/details/81672920
在上面的模型中，Mysql-A就是主服务器，即master，Mysql-B就是从服务器，即slave。
在Mysql-A的数据库事件（例如修改数据库的sql操作语句），都会存储到日志系统A中，在相应的端口（默认3306）通过网络发送给Mysql-B。Mysql-B收到后，写入本地日志系统B，然后一条条的将数据库事件在数据库Mysql-B中完成。
日志系统A，是MYSQL的日志类型中的二进制日志，也就是专门用来保存修改数据库表的所有动作，即bin log，注意MYSQL会在执行语句之后，释放锁之前，写入二进制日志，确保事务安全。
日志系统B，不是二进制日志，由于它是从MYSQL-A的二进制日志复制过来的，并不是自己的数据库变化产生的，有点接力的感觉，称为中继日志，即relay log。
通过上面的机制，可以保证Mysql-A和Mysql-B的数据库数据一致，但是时间上肯定有延迟，即Mysql-B的数据是滞后的。因此，会出现这样的问题，Mysql-A的数据库操作是可以并发的执行的，但是Mysql-B只能从relay log中一条一条的读取执行。若Mysql-A的写操作很频繁，Mysql-B很可能就跟不上了。
原文：https://blog.csdn.net/qq_15092079/article/details/81672920
大致流程: 主数据库 负责写,会把所有的写操作记录到日志文件(二进制文件)中, 然后 从服务器来 同步这个日志文件,然后以此更新 从服务器上 的日志
主从同步复制有以下几种方式：
（1）同步复制，master的变化，必须等待slave-1,slave-2,&amp;hellip;,slave-n完成后才能返回。
（2）异步复制，master只需要完成自己的数据库操作即可，至于slaves是否收到二进制日志，是否完成操作，不用关心。MYSQL的默认设置。
（3）半同步复制，master只保证slaves中的一个操作成功，就返回，其他slave不管。这个功能，是由google为MYSQL引入的。
本文说的是在centos 7系统上，实现的mysql5.7数据库的主从同步配置，从而实现读写分离操作。
原文：https://blog.csdn.net/qq_15092079/article/details/81672920
是不是我无限制地增加从库的数量就可以抵抗大量的并发呢？
实际上并不是的。因为随着从库数量增加，从库连接上来的 IO 线程比较多，主库也需要创建同样多的log dump 线程来处理复制的请求，对于主库资源消耗比较高，同时受限于主库的网络带宽，所以在实际使用中，一般一个主库最多挂 3～5 个从库。
2.实践 以下操作未经自己实践!!
原文：https://blog.csdn.net/qq_15092079/article/details/81672920
2.1 分别在两台centos 7系统上安装mysql 5.7
具体的安装步骤可以见此链接，https://blog.csdn.net/qq_15092079/article/details/81629238。
本文中的两台服务器的IP地址分别为主服务器（192.168.17.130）和从服务器（192.168.17.132）。
分别在这两个服务器上创建test数据库，以备后面测试。
2.2 master主服务器的配置 2.2.1 配置文件my.cnf的修改 #根据上一篇文章，编辑my.cnf文件 [root@localhost mysql]# vim /etc/my.cnf #在[mysqld]中添加： server-id=1 log_bin=master-bin log_bin_index=master-bin.index binlog_do_db=test #备注： #server-id 服务器唯一标识。 #log_bin 启动MySQL二进制日志，即数据同步语句，从数据库会一条一条的执行这些语句。 #binlog_do_db 指定记录二进制日志的数据库，即需要复制的数据库名，如果复制多个数据库，重复设置这个选项即可。 #binlog_ignore_db 指定不记录二进制日志的数据库，即不需要复制的数据库名，如果有多个数据库，重复设置这个选项即可。 #其中需要注意的是，binlog_do_db和binlog_ignore_db为互斥选项，一般只需要一个即可。 2.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%BA%8B%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%BA%8B%E5%8A%A1/</guid><description>[toc]
并发事务引起的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务。并发虽然是必须的，但可能会导致一下的问题(严重程度依次递增)。
脏读（Dirty reads）——脏读发生在一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读（Nonrepeatable read）——不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了更新。(用mvcc解决的, 读取改之前的副本就能保证读的数据一致了) 幻读（Phantom read）——幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。(用mvcc+临键锁实现的,把要查的范围内数据全部锁住就可以了) 原文链接：https://blog.csdn.net/trigl/article/details/50968079
数据库用的是mvcc+各种锁来解决的三种问题(简单理解为乐观锁)
简述三种的情况的处理
不可重复读与幻读的解决方案
隔离级别(围绕三种读的隔离) 隔离级别 含义 ISOLATION_READ_UNCOMMITTED (读未提交)最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 ISOLATION_READ_COMMITTED (读已提交)允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 ISOLATION_REPEATABLE_READ (可重复读)对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 ISOLATION_SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的 mysql默认的事务处理级别是REPEATABLE-READ,也就是可重复读
oracle默认系统事务隔离级别是READ COMMITTED,也就是读已提交
原文链接：https://blog.csdn.net/trigl/article/details/50968079
MySQL如何解决幻读和不可重复度？_兴趣使然的草帽路飞的博客-CSDN博客</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</guid><description> 从其他表中查询结果并插入新表 INSERT INTO jpf_project_service (project_no,service_id,customer_id,start_time,end_time,create_id,create_time) SELECT sp.PROJECT_NO , sp.SERVICE_ID , sp.MASTER_CUSTOMER_ID, sc.START_TIME, sc.END_TIME,1,sysdate() FROM jbp_service_project sp INNER JOIN jpf_customer_service sc ON sc.CUSTOMER_ID = sp.MASTER_CUSTOMER_ID AND sc.SERVICE_ID = sp.SERVICE_ID;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%91%BD%E4%BB%A4/</guid><description>下载mysql源安装包 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm
安装mysql源 yum localinstall mysql57-community-release-el7-8.noarch.rpm
检查mysql源是否安装成功 yum repolist enabled | grep &amp;quot;mysql.*-community.*&amp;quot; 安装MySQL yum install mysql-community-server
启动MySQL服务 systemctl start mysqld
查看MySQL的启动状态 systemctl status mysqld
查看root用户的密码，（冒号后面得就是密码） grep 'temporary password' /var/log/mysqld.log
登录root mysql -uroot -p
修改密码 ALTER USER 'root'@'localhost' IDENTIFIED BY 'newPassword';
添加远程用户 GRANT ALL PRIVILEGES ON *.* TO 'xkj'@'%' IDENTIFIED BY 'a' WITH GRANT OPTION;
修改编码集 vim /etc/my.cnf (在 [mysqld] 下加) character_set_server=utf8 init_connect=&amp;#39;SET NAMES utf8&amp;#39; 查看编码集 show variables like '%character%' 给用户添加某个database权限 grant all on dbName.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6mvcc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6mvcc/</guid><description>[toc]
1. 什么是多版本并发控制（ MVCC ） MySQL 的大多数事务型存储引擎基于提升并发性能的考虑，一般都实现了多版本并发控制（ MVCC ）。MVCC 是行级锁的一个变种，但实际上实现机制有所不同，避免了加锁的操作，因此有了更低的开销和更高的性能。
MVCC 的实现，是通过保持数据在某个时间点的快照来实现的，不管事务的执行时间有多久，MVCC 保障了每一个事务内看到的数据是一致的，而根据事务开始时间，不同事务看到同一张表，同一份数据可能是不同的。 MVCC的实现，是通过保存数据在某个时间点的快照来实现的。
需要注意的是，MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作，其他两个级别都和MVCC“不兼容”。因为READ UNCOMMITTED总是读取最新的数据行，而SERIALIZABLE则会对所有读取的行都加锁。
保存这两个额外的系统版本号，使大多数的读操作都不用加锁，这样的设计使得读数据操作很简单，性能很好，并且只会读取到符合标准的行。不足之处是每行记录都需要额外的存储空间，需要做更多额外的检查工作和维护工作。
不同存储引擎对 MVCC 的实现机制不尽相同，因为 MVCC 并没有一个统一的标准，下面以 InnoDB 为例，简要介绍 MVCC 的实现原理。
注: 多版本并发控制不支持myisam存储引擎。估计是因为myism是锁整个表,做不到版本控制
2. InnoDB 中的 MVCC InnoDB 中的 MVCC，是通过在每行记录后保存两个隐藏的列来实现的。这两个特殊的列一个保存了行的创建时间，一个保存了行的删除时间。当然这里存储的并不是真实的时间，实际上存储的是版本号（ system version number ）。每次开启一个新的事务，系统版本号会开始递增并分配给当前的事务，用于与数据的版本号进行比较。简要介绍下 InnoDB 是如何操作的：
SELECT：在 InnoDB 中，SELECT 操作有两个额外的条件： 只查找行的创建版本号小于或等于当前事务版本号的数据。这样可以保证事务读取的数据要么是在事务开始之前就已经存在，要么是事务本身插入或者修改过的。 行的删除版本号要么未定义，要么大于当前事务的版本号。这样是为了确保事务读取的数据在事务开始之前未被删除。 INSERT：InnoDB 为每行新增数据设置了当前事务的版本号作为创建版本号，删除版本号留空。 DELETE：InnoDB 为每行删除的数据设置了当前事务的版本号作为删除版本号。 UPDATE：InnoDB 额外插入一条新的记录，将当前事务版本号作为新记录的创建版本号，同时保存当前事务版本号到原来的数据中的删除版本号。 在 InnoDB 中，MVCC 只在 READ COMMITTED 和 REPEATABLE READ 这两个隔离级别下工作。其他的隔离级别和 MVCC 并不兼容，READ UNCOMMITTED 总是读取最新的数据，而 SERIALIZABLE 则会对所有读取的行都进行加锁操作。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</guid><description>函数 解释 例子 IFNULL(a,b) 如果a为null,则返回b, IFNULL(age,34) Group_concat(id) 将id字段(默认)用 &amp;ldquo;,&amp;rdquo; 连接 . 用 &amp;quot; SEPARATOR &amp;ldquo;可指定连接符号. (配合group使用,效果棒棒哒) SELECT GROUP_CONCAT( CAST(ccrt.RELEASE_AMT AS char) SEPARATOR &amp;lsquo;、&amp;rsquo;) FROM ccrt INSTR(a, b) 这个函数返回字符串b 在 字符串a中的位置, 没有找到字符串返回0，否则返回位置（从1开始）,也可以用来作为是否存在的条件 来自 &amp;lt;https://www.cnblogs.com/gisblogs/p/4011575.html&amp;gt; SELECT * FROM jbp_org_info WHERE INSTR( CONCAT( &amp;lsquo;,&amp;rsquo; , id , &amp;lsquo;,&amp;rsquo; ), ORG_LINK) SUBSTRING_INDEX（str, delim, count） str 需要拆分的字符串delim 分隔符，通过某字符进行拆分count 当 count 为正数，取第 n 个分隔符之前的所有字符； 当 count 为负数，取倒数第 n 个分隔符之后的所有字符。 来自 &amp;lt;https://blog.csdn.net/pjymyself/article/details/81668157&amp;gt; SUBSTRING_INDEX(&amp;lsquo;7654,7698,7782,7788&amp;rsquo;, &amp;lsquo;,&amp;rsquo; ,2) FIND_IN_SET(str,strlist) 查找str在strlist中的位置,strlist是由&amp;rdquo;,&amp;ldquo;分割的字符串.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%BC%95%E6%93%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E5%BC%95%E6%93%8E/</guid><description>MySQL引擎:
一.主要是以下两种(不止这两种)
Innodb(5.5版本以后默认引擎):
Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。++它的设计的目标就是处理大数据容量的数据库系统++。它本身实际上是基于Mysql后台的完整的系统。Mysql运行的时候，Innodb会在内存中建立缓冲池，用于缓冲数据和索引。但是，该引擎是不支持全文搜索的。同时，启动也比较的慢，它是不会保存表的行数的。当进行Select count(*) from table指令的时候，需要进行扫描全表。所以当需要使用数据库的事务时，该引擎就是首选。由于锁的粒度小，写操作是不会锁定全表的。所以在并发度较高的场景下使用会提升效率的。主键索引上就存储业务数据,二级索引存储的主键索引的位置 (擅长写)
MyIASM:
它是MySql的默认引擎，但不提供事务的支持，也不支持行级锁和外键。因此当执行Insert插入和Update更新语句时，即执行写操作的时候需要锁定这个表。所以会导致效率会降低。不过和Innodb不同的是，MyIASM引擎是保存了表的行数，于是当进行Select count(*) from table语句时，可以直接的读取已经保存的值而不需要进行扫描全表。所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的。可以将MyIASM作为数据库引擎的首选。
在索引树上仅存储数据的物理地址,再根据地址再去找到业务数据.(擅长读)
二. 查看MySQL提供哪些引擎:
show engines</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E7%B4%A2%E5%BC%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E7%B4%A2%E5%BC%95/</guid><description>[toc]
前言 说到索引，很多人都知道“索引是一个排序的列表，在这个列表中存储着索引的值和包含这个值的数据所在行的物理地址，在数据十分庞大的时候，索引可以大大加快查询的速度，这是因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据。”
但是索引是怎么实现的呢？因为索引并不是关系模型的组成部分，因此不同的DBMS有不同的实现，我们针对MySQL数据库的实现进行说明。
一、MySQL中索引的语法 创建索引
在创建表的时候添加索引
CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); 在创建表以后添加索引
ALTER TABLE my_table ADD [UNIQUE] INDEX index_name(column_name);
或者
CREATE INDEX index_name ON my_table(column_name);
注意：
1、索引需要占用磁盘空间，因此在创建索引时要考虑到磁盘空间是否足够
2、创建索引时需要对表加锁，因此实际操作中需要在业务空闲期间进行
原文链接：https://blog.csdn.net/tongdanping/article/details/79878302
删除索引
DROP INDEX my_index ON tablename；
或者
ALTER TABLE table_name DROP INDEX index_name;
查看表中的索引
SHOW INDEX FROM tablename
查看查询语句使用索引的情况
explain SELECT * FROM table_name WHERE column_1='123'; 原文链接：https://blog.csdn.net/tongdanping/article/details/79878302
二、索引的优缺点 优势：可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序；
劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的1.5倍；</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84%E5%9B%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84%E5%9B%BE/</guid><description>MySQL逻辑架构
MySQL逻辑架构图
MySQL逻辑架构分四层
1.连接层：主要完成一些类似连接处理，授权认证及相关的安全方案。
2.服务层：在 MySQL据库系统处理底层数据之前的所有工作都是在这一层完成的，包括权限判断，SQL接口，SQL解析，SQL分析优化， 缓存查询的处理以及部分内置函数执行(如日期,时间,数学运算,加密)等等。各个存储引擎提供的功能都集中在这一层，如存储过程，触发器，视图等。
3.引擎层：是底层数据存取操作实现部分，由多种存储引擎共同组成。真正负责MySQL中数据的存储和提取。就像Linux众多的文件系统 一样。每个存储引擎都有自己的优点和缺陷。服务器是通过存储引擎API来与它们交互的。这个接口隐藏 了各个存储引擎不同的地方。对于查询层尽可能的透明。这个API包含了很多底层的操作。如开始一个事物，或者取出有特定主键的行。存储引擎不能解析SQL，互相之间也不能通信。仅仅是简单的响应服务器 的请求。
4.存储层：将数据存储于裸设备的文件系统之上，完成与存储引擎的交互。
https://www.cnblogs.com/ChangAn223/p/10686639.html
https://www.bilibili.com/video/BV12b411K7Zu?p=186</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%94%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%94%81/</guid><description>[toc]
1 . 2. 死锁 2.1 死锁是怎么被发现的？ 2.1.1 死锁成因&amp;amp;&amp;amp;检测方法 与java中死锁的成因一样,都是互相等待资源造成,
innodb是怎么探知死锁的？
直观方法是在两个事务相互等待时，当一个等待时间超过设置的某一阀值时，对其中一个事务进行回滚，另一个事务就能继续执行。这种方法简单有效，在innodb中，参数innodb_lock_wait_timeout用来设置超时时间。
仅用上述方法来检测死锁太过被动，innodb还提供了wait-for graph算法来主动进行死锁检测，每当加锁请求无法立即满足需要并进入等待时，wait-for graph算法都会被触发
2.1.2 wait-for graph原理 他们相互等待对方的资源，而且形成环路！我们将每辆车看为一个节点，当节点1需要等待节点2的资源时，就生成一条有向边指向节点2，最后形成一个有向图。我们只要检测这个有向图是否出现环路即可，出现环路就是死锁！这就是wait-for graph算法
2.2 innodb隔离级别、索引与锁 2.3 死锁成因 2.3.1不同表相同记录行锁冲突 这种情况很好理解，事务A和事务B操作两张表，但出现循环等待锁情况
2.3.2相同表记录行锁冲突 这种情况比较常见，之前遇到两个job在执行数据批量更新时，jobA处理的的id列表为[1,2,3,4]，而job处理的id列表为[8,9,10,4,2]，这样就造成了死锁。
2.3.3不同索引锁冲突 这种情况比较隐晦，事务A在执行时，除了在二级索引加锁外，还会在聚簇索引上加锁，在聚簇索引上加锁的顺序是[1,4,2,3,5]，而事务B执行时，只在聚簇索引上加锁，加锁顺序是[1,2,3,4,5]，这样就造成了死锁的可能性
2.3.4 gap锁(间隙锁)冲突 innodb在RR级别下，如下的情况也会产生死锁，比较隐晦。不清楚的同学可以自行根据上节的gap锁原理分析下。
2.4 如何尽可能避免死锁 1）以固定的顺序访问表和行。比如对第2节两个job批量更新的情形，简单方法是对id列表先排序，后执行，这样就避免了交叉等待锁的情形；又比如对于3.1节的情形，将两个事务的sql顺序调整为一致，也能避免死锁。
2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。
3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。
4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。
5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。
2.5.怎么解决死锁 通过应用业务日志定位到问题代码，找到相应的事务对应的sql； 查看数据库死锁相关日志,确定死锁情况; 找到死锁的进程,手动kill一个或多个进程(破坏环路) 一般来说,数据库会帮我们检测死锁并破坏死锁情况
show engine innodb status; 查看引擎信息, 包含了死锁信息
MySQL死锁日志的查看和分析
3. 意向锁 在Innodb引擎支持表锁和行锁，意向锁将提高锁判断的效率,并可以让行锁和表锁同时存在 .
行锁分为共享锁，一个事务对一行的共享只读锁。排它锁，一个事务对一行的排他读写锁。
MyISAM 引擎中只有表锁
共享锁:加了读锁，只允许别的事务继续加读锁而不能加写锁，也就是只读
排它锁: 加了写锁，别的事务不允许加任何锁。
来自: https://www.zhihu.com/question/51513268
考虑一个例子:
事物A获得了一个行锁,事物B想来获得一个表锁,此时表锁应该不能成功,不然就冲突了.但是事物B要去判断是否会冲突,就会判断有没有表锁和行锁,而行锁只能一行行记录去判断,效率低下,所以此时需要意向锁. 当事务A想获得行锁时,会自动先获得一个意向锁,成功后才能获得行锁; 此时事务B判断完表锁后,再判断共享锁,这样就减少了判断次数
意向锁分为 意向共享锁和意向排它锁, 共享锁可以有多把, 从其特性看,意向锁是表锁,所以它可以让行锁和表锁同时存在</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%94%99%E8%AF%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%94%99%E8%AF%AF/</guid><description>1.数据库查出来是正常中文,用MySQLdb查出来却是问号 db = MySQLdb.connect(...); print db.character_set_name() 查看编码集
解决:
db = MySQLdb.connect(..., charset='utf8') 来自:http://blog.csdn.net/hu330459076/article/details/7828512</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%AB%98%E7%BA%A7sql/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/%E9%AB%98%E7%BA%A7sql/</guid><description>查询和&amp;quot; 01 &amp;ldquo;号的同学学习的课程 完全相同的其他同学的信息
​ &amp;ndash; 不存在这样的课程 , 01学了,但是学生x没学 -&amp;gt; 蕴含逻辑运算
​ &amp;ndash; (像这种蕴含逻辑运算,涉及到离散,做一些简单倒是可以,比如&amp;quot;完全相同&amp;rdquo;,&amp;ldquo;没有全部拥有的&amp;quot;等全量字眼)
SELECT * FROM student WHERE NOT EXISTS( SELECT * FROM stucou sc1 WHERE sc1.SId=&amp;#39;01&amp;#39; AND NOT EXISTS( SELECT * FROM stucou sc2 WHERE sc2.SId=student.SId AND sc1.CId=sc2.CId) ); 详解:https://blog.csdn.net/qsyzb/article/details/12525955
查询学过&amp;quot;张三&amp;quot;老师讲授的全部课程的学生姓名
​ &amp;ndash; 不存在这样的课程,张三老师教了,但是学生没学
SELECT s.* FROM student s WHERE NOT EXISTS ( SELECT 1 FROM teacher t INNER JOIN course c ON c.TId= t.TId WHERE t.Tname=&amp;#39;张三&amp;#39; AND NOT EXISTS ( SELECT 1 FROM stucou sc WHERE c.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/oracle/%E6%B8%B8%E6%A0%87%E5%92%8C%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/oracle/%E6%B8%B8%E6%A0%87%E5%92%8C%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/</guid><description>[toc]
1. 游标 相当于指针,通过游标PL/SQL程序可以一次处理查询结果集中的一行,并可以对该行数据执行特定操作
显示游标: 显式游标用于处理返回多行的查询。 步骤如下:
声明游标: cursor cur_name [ (in_param [ , in_param]…) ] [ return return_type ] is select_sentence; 打开游标: open cur_name [ (in_param [ , in_param]…) ]; 读取游标: fetch cur_name into (variable); 关闭游标: close cur_name 隐式游标: 在 PL/SQL 程序中执行DML SQL 语句时自动创建隐式游标，名字默认叫sql,感觉隐式就是普通的PL/SQL(特别是配合for使用),无形中使用,因为打开,读取等操作Oracle系统自动完成 REF 游标：REF 游标用于处理运行时才能确定的动态 SQL 查询的结果 https://www.2cto.com/database/201501/371435.html
http://www.cnblogs.com/sc-xx/archive/2011/12/03/2275084.html
2. 存储过程 存储过程相当于java中函数,将一些操作集合起来.存储过程报错在数据库中,可以被重复使用,(也正因为如此,存储过程不便迁移),存储过程是已经编译好的代码,所有被引用时,效率非常高(这也是项目中喜欢用存储过程的原因)
语法:
create [ or replace ] procedure pro_name [ (in_param in/out param_type [ , in_param in/out param_type ]…) ] is|as begin plsql_sentences; [ exception ] [ do_something_sentences; ] end [ pro_name ] 调用:</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/oracle/%E7%B4%A2%E5%BC%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/oracle/%E7%B4%A2%E5%BC%95/</guid><description>[toc]
1. 数据结构 索引结构与mysql的一样, 都是使用B+tree
只是oracle这里叫bTree
如果存储结构是索引组织表, 才和mysql一样
Oracle中索引的原理 - 想飞_毛毛虫 - 博客园 (cnblogs.com)
2. 失效场景 与mysql的基本一致
3. 执行计划 获取执行计划的方法
6种方法：
1. explain plan for获取（即PL/SQL中的F5）； 2. set autotrace on （跟踪性能统计）； 3. statistics_level=all（获取表访问次数）; 4. 通过dbms_xplan.display_cursor输入sql_id参数直接获取 5. 10046 trace跟踪 6. awrsqrpt.sql 各自的适用场景：
1.如果某SQL执行非常长时间才会出结果，甚至慢到返回不了结果，这时候看执行计划就只能用方法1，或者方法4调用现成的； 2.跟踪某条SQL最简单的方法是方法1，其次就是方法2； 3.如果想观察到某条SQL有多条执行计划的情况，只能用方法4和方法6； 4.如果SQL中含有多函数，函数中套有SQL等多层递归调用，想准确分析，只能使用方法5； 5.要想确保看到真实的执行计划，不能用方法1和方法2； 6.要想获取表被访问的次数，只能使用方法3； 重点讲一下第一种
EXPLAIN PLAN FOR命令不会运行 SQL 语句，因此创建的执行计划不一定与执行该语句时的实际计划相同。
该命令会将生成的执行计划保存到全局的临时表 PLAN_TABLE 中，然后使用系统包 DBMS_XPLAN 中的存储过程格式化显示该表中的执行计划。
使用步骤
步骤1：explain plan for &amp;ldquo;你的SQL&amp;rdquo;
步骤2：select * from table(dbms_xplan.display());
案例介绍
接下来，我们同样需要理解执行计划中各种信息的含义：
Plan hash value 是该语句的哈希值。SQL 语句和执行计划会存储在库缓存中，哈希值相同的语句可以重用已有的执行计划，也就是软解析；</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E4%BA%8B%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E4%BA%8B%E5%8A%A1/</guid><description>原子性(Atomicity)：事务中的全部操作在数据库中是不可分割的，要么全部完成，要么全部不执行。 [1] 一致性(Consistency)：几个并行执行的事务，其执行结果必须与按某一顺序 串行执行的结果相一致。 [1] 隔离性(Isolation)：事务的执行不受其他事务的干扰，事务执行的中间结果对其他事务必须是透明的。 [1] 持久性(Durability):对于任意已提交事务，系统必须保证该事务对数据库的改变不被丢失，即使数据库出现故障。 [1] 事务的ACID特性是由关系数据库系统(DBMS)来实现的，DBMS采用日志来保证事务的原子性、一致性和持久性。日志记录了事务对数据库所作的更新，如果某个事务在执行过程中发生错误，就可以根据日志撤销事务对数据库已做的更新，使得数据库回滚到执行事务前的初始状态。
对于事务的隔离性，DBMS是采用锁机制来实现的。当多个事务同时更新数据库中相同的数据时，只允许持有锁的事务能更新该数据，其他事务必须等待，直到前一个事务释放了锁，其他事务才有机会更新该数据。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%A0%86%E8%A1%A8%E5%92%8C%E7%B4%A2%E5%BC%95%E7%BB%84%E7%BB%87%E8%A1%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E5%A0%86%E8%A1%A8%E5%92%8C%E7%B4%A2%E5%BC%95%E7%BB%84%E7%BB%87%E8%A1%A8/</guid><description>堆表(heap table）和索引组织表（Index Oragnization Table，简称IOT)是两种数据表的存储结构
Oracle支持堆表，也支持索引组织表
PostgreSQL只支持堆表，不支持索引组织表
Innodb只支持索引组织表
MyIASM只支持堆表
堆表和索引组织表的区别 堆表 堆就是无序数据的集合,索引就是将数据变得有序,在索引中键值有序,数据还是无序的 数据存放在数据里面,索引存放在索引里 堆表中,主键索引和普通索引一样的,叶子节点存放的是指向堆表中数据的指针（可以是一个页编号加偏移量）,指向物理地址,没有回表的说法 堆表中,主键和普通索引基本上没区别,和非空的唯一索引没区别 mysql 的 myisam 引擎，oracle pg 都支持的是堆表 索引组织表 innodb 引擎支持的就是索引组织表 对于主键的索引,叶子节点存放了一整行所有数据,其他索引称为辅助索引(二级索引),它的叶子节点只是存放了键值和主键值 主键包含了一张表的所有数据,因为主键索引的页子节点中保存了每一行的完整记录,包括所有列。如果没有主键,MySQL会自动帮你加一个主键,但是对用户不可见 innodb中数据存放在聚集索引中,换言之,按照主键的方式来组织数据的 其他索引(唯一索引,普通索引)的叶子节点存放该索引列的键值和主键值 不管是什么索引非叶子节点存放的存放的就是键值和指针,不存数据,这个指针在innodb中是6个bit,键值就看数据大小了 在mysql中除了主键索引,其他都是二级索引 , 这些索引都是针对索引树做的二次索引树(因为只存真实数据都在主键里了), 这就是为什么它叫二级索引 以及会出现索引回表, 这全是由于数据存储结构引起的
深入理解Oracle表(6)：堆组织表(HOT)和索引组织表(IOT)的区别_linwaterbin的博客-CSDN博客_堆组织表 索引组织表
堆表 索引组织表 - mhabbyo - 博客园 (cnblogs.com)
数据库之堆表和索引组织表 - 墨天轮 (modb.pro)
如何理解oracle索引组织表 - 关系型数据库 - 亿速云 (yisu.com)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%8C%83%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E8%8C%83%E5%BC%8F/</guid><description>[toc]
1.前言 目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般来说，数据库只需满足第三范式(3NF）就行了。
2.范式 2.1 第一范式（1NF） 所谓第一范式（1NF）是指在关系模型中，对于添加的一个规范要求，所有的域都应该是原子性的，即数据库表的每一列都是不可分割的原子数据项，而不能是集合，数组，记录等非原子数据项。即实体中的某个属性有多个值时，必须拆分为不同的属性。在符合第一范式（1NF）表中的每个域值只能是实体的一个属性或一个属性的一部分。简而言之，第一范式就是无重复的域。
说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的设计基本要求，一般设计中都必须满足第一范式（1NF）。不过有些关系模型中突破了1NF的限制，这种称为非1NF的关系模型。换句话说，是否必须满足1NF的最低要求，主要依赖于所使用的关系模型。
简单理解: 表中一个字段应该是原子性的,不可分割的,
例如: 一个表,有学生名称和分数, 显然学生会有多门学科分数,此时应该把分数拆开成多门分数
2.2 第二范式（2NF） 在1NF的基础上，非码属性必须完全依赖于候选码（在1NF基础上消除非主属性对主码的部分函数依赖）
第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或记录必须可以被唯一地区分。选取一个能区分每个实体的属性或属性组，作为实体的唯一标识。
第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识。简而言之，第二范式就是在第一范式的基础上属性完全依赖于主键。
例如: 项目表中有经纬度和地址字段, 其实确认了地址字段,经纬度也就确认了,此时经纬度并不完成依赖于项目编码
2.3 第三范式（3NF） 在2NF基础上，任何非主属性不依赖于其它非主属性（在2NF基础上消除传递依赖）
第三范式（3NF）是第二范式（2NF）的一个子集，即满足第三范式（3NF）必须满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个关系中不包含已在其它关系已包含的非主关键字信息。
例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性，也就是在满足2NF的基础上，任何非主属性不得传递依赖于主属性。
2.4 巴斯-科德范式（BCNF） Boyce-Codd Normal Form（巴斯-科德范式）
在3NF基础上，任何非主属性不能对主键子集依赖（在3NF基础上消除对主码子集的依赖）
巴斯-科德范式（BCNF）是第三范式（3NF）的一个子集，即满足巴斯-科德范式（BCNF）必须满足第三范式（3NF）。通常情况下，巴斯-科德范式被认为没有新的设计规范加入，只是对第二范式与第三范式中设计规范要求更强，因而被认为是修正第三范式，也就是说，它事实上是对第三范式的修正，使数据库冗余度更小。这也是BCNF不被称为第四范式的原因。某些书上，根据范式要求的递增性将其称之为第四范式是不规范，也是更让人不容易理解的地方。而真正的第四范式，则是在设计规范中添加了对多值及依赖的要求。
2.5 第四范式 设关系R（X，Y，Z），其中X，Y，Z是成对的、不相交属性的集合。若存在非平凡多值依赖，则意味着对R中的每个属性
存在有函数依赖
（X必包含键）。那么
。
换句话说，当关系R的属性集合X是非平凡多值依赖的域，它就包含关系R的键。则
。这个定义和BCNF定义唯一的不同点是后者研究非平凡多值依赖的域。由于函数依赖是多值依赖的特定情况，因此，这直观地说明了4NF比BCNF更强的原因。
显然，若关系属于4NF，则它必属于BCNF；而属于BCNF的关系不一定属于4NF
2.6 第五范式 如果关系模式R中的每一个连接依赖均由R的候选码所隐含，则称此关系模式符合第五范式。
链接</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%B7%AE%E5%BC%82/mysql%E4%B8%8Eoracle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%B7%AE%E5%BC%82/mysql%E4%B8%8Eoracle/</guid><description>在使用上的差别:
mysql中 where 条件允许 1,0,true,false 条件,而Oracle只允许true/false,
例如 : select * from person where 1 ,相当于where 1=1 ,是个恒等式
事项 MySQL Oracle 注释 例子 where 支持1,0,true,false 支持true/false mysql : select * from person where 1Oracle: select * from person where 1=1 as 表和字段都能用,也可以不用 仅字段能用 都支持不写as,直接写别名就行 当前时间 SYSDATE() SYSDATE Oracle:SELECT SYSDATE FROM JBP_ACCOUNT 判空 IFNULL nvl oracle: nvl( LOCK_FLAG,&amp;rsquo;&amp;rsquo;) as lockFlag, 连接字符串 concat(str1.str2,str3,&amp;hellip;..) 不定参数 concat(str1,str2) 仅两个参数, 可以用|| 任意连接字符串 eg. &amp;lsquo;3&amp;rsquo;|| id || &amp;lsquo;4&amp;rsquo; 时间格式化 DATE_FORMAT(#{time} , %Y-%m-%d %H:%i:%s) TO_CHAR(yyyy-MM-dd hh24:mi:ss) 查找字符串 find_in_set(a,b) 可用替代 instr find_in_set查找a在b中的位置,b是又逗号隔开的字符串,用到了比特运算,速度很快 分组并合并 GROUP_CONCAT wm_concat</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%B7%AE%E5%BC%82/postgresql%E4%B8%8Emysql/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%B7%AE%E5%BC%82/postgresql%E4%B8%8Emysql/</guid><description>PostgreSQL与MySQL 的区别 特性 MySQL PostgreSQL 实例 通过执行 MySQL 命令（mysqld）启动实例。一个实例可以管理一个或多个数据库。一台服务器可以运行多个 mysqld 实例。一个实例管理器可以监视 mysqld 的各个实例。 通过执行 Postmaster 进程（pg_ctl）启动实例。一个实例可以管理一个或多个数据库，这些数据库组成一个集群。集群是磁盘上的一个区域，这个区域在安装时初始化并由一个目录组成，所有数据都存储在这个目录中。使用 initdb 创建第一个数据库。一台机器上可以启动多个实例。 数据库 数据库是命名的对象集合，是与实例中的其他数据库分离的实体。一个 MySQL 实例中的所有数据库共享同一个系统编目。 数据库是命名的对象集合，每个数据库是与其他数据库分离的实体。每个数据库有自己的系统编目，但是所有数据库共享 pg_databases。 数据缓冲区 通过 innodb_buffer_pool_size 配置参数设置数据缓冲区。这个参数是内存缓冲区的字节数，InnoDB 使用这个缓冲区来缓存表的数据和索引。在专用的数据库服务器上，这个参数最高可以设置为机器物理内存量的 80%。 Shared_buffers 缓存。在默认情况下分配 64 个缓冲区。默认的块大小是 8K。可以通过设置 postgresql.conf 文件中的 shared_buffers 参数来更新缓冲区缓存。 数据库连接 客户机使用 CONNECT 或 USE 语句连接数据库，这时要指定数据库名，还可以指定用户 id 和密码。使用角色管理数据库中的用户和用户组。 客户机使用 connect 语句连接数据库，这时要指定数据库名，还可以指定用户 id 和密码。使用角色管理数据库中的用户和用户组。 身份验证 MySQL 在数据库级管理身份验证。 基本只支持密码认证。 PostgreSQL 支持丰富的认证方法：信任认证、口令认证、Kerberos 认证、基于 Ident 的认证、LDAP 认证、PAM 认证 加密 可以在表级指定密码来对数据进行加密。还可以使用 AES_ENCRYPT 和 AES_DECRYPT 函数对列数据进行加密和解密。可以通过 SSL 连接实现网络加密。 可以使用 pgcrypto 库中的函数对列进行加密/解密。可以通过 SSL 连接实现网络加密。 审计 可以对 querylog 执行 grep。 可以在表上使用 PL/pgSQL 触发器来进行审计。 查询解释 使用 EXPLAIN 命令查看查询的解释计划。 使用 EXPLAIN 命令查看查询的解释计划。 备份、恢复和日志 InnoDB 使用写前（write-ahead）日志记录。支持在线和离线完全备份以及崩溃和事务恢复。需要第三方软件才能支持热备份。 在数据目录的一个子目录中维护写前日志。支持在线和离线完全备份以及崩溃、时间点和事务恢复。 可以支持热备份。 JDBC 驱动程序 可以从 参考资料 下载 JDBC 驱动程序。 可以从 参考资料 下载 JDBC 驱动程序。 表类型 取决于存储引擎。例如，NDB 存储引擎支持分区表，内存引擎支持内存表。 支持临时表、常规表以及范围和列表类型的分区表。不支持哈希分区表。 由于PostgreSQL的表分区是通过表继承和规则系统完成了，所以可以实现更复杂的分区方式。 索引类型 取决于存储引擎。MyISAM：BTREE，InnoDB：BTREE。 支持 B-树、哈希、R-树和 Gist 索引。 约束 支持主键、外键、惟一和非空约束。对检查约束进行解析，但是不强制实施。 支持主键、外键、惟一、非空和检查约束。 存储过程和用户定义函数 支持 CREATE PROCEDURE 和 CREATE FUNCTION 语句。存储过程可以用 SQL 和 C++ 编写。用户定义函数可以用 SQL、C 和 C++ 编写。 没有单独的存储过程，都是通过函数实现的。用户定义函数可以用 PL/pgSQL（专用的过程语言）、PL/Tcl、PL/Perl、PL/Python 、SQL 和 C 编写。 触发器 支持行前触发器、行后触发器和语句触发器，触发器语句用过程语言复合语句编写。 支持行前触发器、行后触发器和语句触发器，触发器过程用 C 编写。 系统配置文件 my.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A7%AF%E7%B4%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%A7%AF%E7%B4%AF/</guid><description>mysql改为oracle要改动的地方:
除去limit ​ ①.如果是做分页,直接去掉,用分页插件代替
​ ②.如果是取第一条,用 where ROWNUM = 1 代替
表别名写了as的,要去掉as
SYSDATE() / now() 改成 SYSDATE
where/and条件不能是数字,仅支持boolean, instr函数返回的就是数字, 在后面加上 &amp;gt;0 , =&amp;gt;INSTR(ORG_LINK, #{topOrgId})&amp;gt;0
CONCAT('%',#{noOrName},'%') 改成 CONCAT( CONCAT('%',#{noOrName}) ,'%') 或者 &amp;lsquo;%&amp;rsquo;||#{noOrName}||&amp;rsquo;%&amp;rsquo;
批量插入/修改时,sql前面加 begin ,结尾处加 end;
不能插空值,估计要加配置
IFNULL 改为 NVl
DATE_FORMAT 原(%Y-%m-%d)改为 TO_CHAR 时间格式: &amp;lsquo;yyyy-MM-dd hh24:mi:ss&amp;rsquo;
没有find_in_set函数,可以考虑用instr代替
没有GROUP_CONCAT函数,用wm_concat代替
不能使用连表删除/修改,借助 EXISTS 删除 =&amp;gt; https://blog.csdn.net/china_shrimp/article/details/78843256
case when中有取字段和自定义时,需要用 Translate(&amp;lsquo;根目录&amp;rsquo; USING NCHAR_CS) 替换 &amp;lsquo;根目录&amp;rsquo; 因为&amp;rsquo;根目录&amp;rsquo; 是varchar2类型,而目前数据中是nvarchar2类型
删除insert语句上的 useGeneratedKeys=&amp;ldquo;true&amp;rdquo;
暂时发现 : 如果是select ,语句后面不允许加 &amp;quot; ; &amp;quot;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%9D%A2%E8%AF%95/</guid><description>[toc]
1. sql优化 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。
应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：
select id from t where num is null 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：
select id from t where num=0 应尽量避免在 where 子句中使用!=或&amp;lt;&amp;gt;操作符，否则将引擎放弃使用索引而进行全表扫描。
应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描(如果or的每个条件都有索引,则or会走索引)，如：
select id from t where num=10 or num=20 可以这样查询：
select id from t where num=10 union all select id from t where num=20 in 和 not in 也要慎用，否则会导致全表扫描，如：
select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/docker-swarm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/docker-swarm/</guid><description>[toc]
1. 介绍 Docker Swarm 是一款用来管理多主机上的Docker容器的工具，可以负责帮你启动容器，监控容器状态，如果容器的状态不正常它会帮你重新帮你启动一个新的容器，来提供服务，同时也提供服务之间的负载均衡，而这些东西Docker-Compose 是做不到的
Kubernetes它本身的角色定位是和Docker Swarm 是一样的，也就是说他们负责的工作在容器领域来说是相同的部分，都是一个跨主机的容器管理平台，当然也有自己一些不一样的特点，k8s是谷歌公司根据自身的多年的运维经验研发的一款容器管理平台。而Docker Swarm则是由Docker 公司研发的。现在一般都用Kubernetes
docker、docker-compose、docker swarm和k8s的区别_似水流年的博客-CSDN博客</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/docker+springboot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/docker+springboot/</guid><description>[toc]
一.简易操作 1.安装docker,jdk 2.制作项目镜像 1.项目用mvn打包即可, (mvn install) 项目
2.在 项目.jar 包同一个目录下新建(放一起方便管理嘛)
3.Dockerfile文件,并写入:
# Docker image for springboot file run # VERSION 0.0.1 # 基础镜像使用java # 下载jdk8 ,没有才下载 FROM java:8 # VOLUME 指定了临时文件目录为/tmp。 # 其效果是在主机 /var/lib/docker 目录下创建了一个临时文件，并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名为app.jar ADD SpringBootTest-0.0.1-SNAPSHOT.jar app.jar # 运行jar包 RUN bash -c &amp;#39;touch /app.jar&amp;#39; ENTRYPOINT [&amp;#34;java&amp;#34;,&amp;#34;-Djava.security.egd=file:/dev/./urandom&amp;#34;,&amp;#34;-jar&amp;#34;,&amp;#34;/app.jar&amp;#34;] 来自 https://blog.csdn.net/Sirius_hly/article/details/83685256
编译镜像 docker build -t spring-boot-docker . #末尾的点代表文件所在目录执行,指上下文目录,应该是可以指定目录的,虽然没试过 docker images # 可以查看制作好的镜像
可以上传镜像,搭环境时只需要pull就行
3.安装Mysql docker run -it --rm --name mysql -v /usr/conf.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/docker-compose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/docker-compose/</guid><description>[toc]
1. 介绍 Docker-Compose 是用来管理你的容器的，有点像一个容器的管家，想象一下当你的Docker中有成百上千的容器需要启动，如果一个一个的启动那得多费时间。有了Docker-Compose你只需要编写一个文件，在这个文件里面声明好要启动的容器，配置一些参数，执行一下这个文件，Docker就会按照你声明的配置去把所有的容器启动起来，只需docker-compose up即可启动所有的容器，但是Docker-Compose只能管理当前主机上的Docker，也就是说不能去启动其他主机上的Docker容器
docker、docker-compose、docker swarm和k8s的区别_似水流年的博客-CSDN博客</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/%E4%BB%8B%E7%BB%8D/</guid><description>Docker 中包括三个基本的概念：
Image（镜像） Container（容器） Repository（仓库 ) 镜像是 Docker 运行容器的前提，仓库是存放镜像的场所，可见镜像更是 Docker 的核心。
来自链接&amp;gt;
Docker 面向对象 容器 对象 镜像 类 来自* &amp;lt;http://www.runoob.com/docker/docker-architecture.html&amp;gt;
Docker 镜像(Images) Docker 镜像是用于创建 Docker 容器的模板。 Docker 容器(Container) 容器是独立运行的一个或一组应用。 Docker 客户端(Client) Docker 客户端通过命令行或者其他工具使用 Docker API (https://docs.docker.com/reference/api/docker_remote_api) 与 Docker 的守护进程通信。 Docker 主机(Host) 一个物理或者虚拟的机器用于执行 Docker 守护进程和容器。 Docker 仓库(Registry) Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。Docker Hub(https://hub.docker.com) 提供了庞大的镜像集合供使用。 Docker Machine Docker Machine是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、 Digital Ocean、Microsoft Azure。 来自* &amp;lt;http://www.runoob.com/docker/docker-architecture.html&amp;gt;
Image（镜像）
那么镜像到底是什么呢？Docker 镜像可以看作是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。
镜像不包含任何动态数据，其内容在构建之后也不会被改变。镜像（Image）就是一堆只读层（read-only layer）的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助读者理解镜像的定义：
从左边我们看到了多个只读层，它们重叠在一起。除了最下面一层，其他层都会有一个指针指向下一层。这些层是 Docker 内部的实现细节，并且能够在主机的文件系统上访问到。
统一文件系统（Union File System）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/%E5%91%BD%E4%BB%A4/</guid><description>安装:
yum -y install docker
启动服务:
service docker start (最好用root用户启动,不然启动不了)
拉取一个 Docker 镜像：
docker pull centos:latest
注: cento：lastest 是镜像的名称以及版本(默认为最新)，Docker Daemon 发现本地没有我们需要的镜像，会自动去 Docker Hub 上去下载镜像，下载完成后，该镜像被默认保存到 /var/lib/docker 目录下。
来自* &amp;lt;https://mp.weixin.qq.com/s/x2zf854JJCsUz6DRhMVUTg&amp;gt;
测试运行:
docker run hello-world
docker run ubuntu:15.10 /bin/echo &amp;quot;Hello world&amp;quot;
docker: Docker 的二进制执行文件。 **run:**与前面的 docker 组合来运行一个容器。 ubuntu:15.10指定要运行的镜像，Docker首先从本地主机上查找镜像是否存在，如果不存在，Docker 就会从镜像仓库 Docker Hub 下载公共镜像。 /bin/echo &amp;ldquo;Hello world&amp;rdquo;: 在启动的容器里执行的命令 以上命令完整的意思可以解释为：Docker 以 ubuntu15.10 镜像创建一个新容器，然后在容器里执行 bin/echo &amp;ldquo;Hello world&amp;rdquo;，然后输出结果。
运行一个新的mysql容器:
docker run -p 3307:3306 --name mysql3 -e MYSQL_ROOT_PASSWORD=123456 -d mysql</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/%E5%AE%89%E8%A3%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/docker/%E5%AE%89%E8%A3%85/</guid><description>Docker 要求 CentOS7 系统的内核版本高于 3.10
来自* &amp;lt;http://www.runoob.com/docker/centos-docker-install.html&amp;gt;
uname -r 可查看内核版本
安装: yum -y install docker
启动服务: service docker start 或者 systemctl start docker.service (最好用root用户启动,不然启动不了)
更换镜像地址:
新版的 Docker 使用 /etc/docker/daemon.json（Linux） 请在该配置文件中加入（没有该文件的话，请先建一个）：
{ &amp;#34;registry-mirrors&amp;#34;: [&amp;#34;http://hub-mirror.c.163.com&amp;#34;] } 来自* &amp;lt;http://www.runoob.com/docker/centos-docker-install.html&amp;gt;
在安装k8s的文章中, 也有描述docker的安装教程
k8s安装.md</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/analysis/</guid><description>[toc]
1. 什么是analysis? analysis是Elasticsearch在文档发送之前对文档正文执行的过程，以添加到反向索引中（inverted index）。 在将文档添加到索引之前，Elasticsearch会为每个分析的字段执行许多步骤：
Character filtering (字符过滤器): 使用字符过滤器转换字符 Breaking text into tokens (把文字转化为标记): 将文本分成一组一个或多个标记 Token filtering：使用标记过滤器转换每个标记(大小写转化/删除无用词等等) Token indexing：把这些标记存于index中 文本分词会发生在两个地方：
创建索引:当索引文档字符类型为text时，在建立索引时将会对该字段进行分词。 搜索：当对一个text类型的字段进行全文检索时，会对用户输入的文本进行分词。 2. 主要组成 总体说来一个analyzer可以分为如下的几个部分：
0个或1个以上的character filter
接收原字符流，通过添加、删除或者替换操作改变原字符流。例如：去除文本中的html标签，或者将罗马数字转换成阿拉伯数字等。一个字符过滤器可以有零个或者多个。
1个tokenizer
简单的说就是将一整段文本拆分成一个个的词。例如拆分英文，通过空格能将句子拆分成一个个的词，但是对于中文来说，无法使用这种方式来实现。在一个分词器中,有且只有一个tokenizeer
0个或1个以上的token filter
将切分的单词添加、删除或者改变。例如将所有英文单词小写，或者将英文中的停词a删除等。在token filters中，不允许将token(分出的词)的position或者offset改变。同时，在一个分词器中，可以有零个或者多个token filters.
单词的过滤顺序按照 filter过滤器的顺序
3. 使用分析器 默认ES使用standard analyzer
3.1 Elasticsearch的内置分析器 Analyzer
Standard Analyzer - 默认分词器，按词切分，小写处理 Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理 Stop Analyzer - 小写处理，停用词过滤(the,a,is) Whitespace Analyzer - 按照空格切分，不转小写 Keyword Analyzer - 不分词，直接将输入当作输出 Patter Analyzer - 正则表达式，默认\W+(非字符分割) Language - 提供了30多种常见语言的分词器 Customer Analyzer 自定义分词器 参考链接</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/es%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/es%E4%BC%98%E5%8C%96/</guid><description>[toc]
1. 集群规划 集群中有两个主要角色，Master Node和Data Node，其它如Tribe Node等节点可根据业务需要另行设立
所以区分master和data节点,职责单一化(通过配置即可完成)
Master Node，整个集群的管理者，负有对index的管理、shards的分配，以及整个集群拓扑信息的管理等功能，众所周知，Master Node可以通过Data Node兼任
Data Node是数据的承载者，对索引的数据存储、查询、聚合等操作提供支持，这些操作严重消耗系统的CPU、内存、IO等资源
更多集群知识见 集群模式.md
2. 分片指定 因为es有分片的概念,即数据会分布到不同的分片上,存储到分片上时是根据id分配的,若我们能自定义这个存放规则,取值时就能精确查找了(默认情况下会全部遍历)(有点像自定义HBASE Rowkey)
例如: PUT my_index/my_type/1?routing=user1 { &amp;#34;title&amp;#34;: &amp;#34;This is a document&amp;#34; } GET my_index/my_type/1?routing=user1 在自定义routing的情况下,难免会出现量大的index:
一种解决办法是单独为这些数据量大的渠道创建独立的index，
另一种办法是指定index参数index.routing_partition_size，来解决最终可能产生群集不均衡的问题
索引中的mappings必须指定_routing为&amp;quot;required&amp;quot;: true，另外mappings不支持parent-child父子关系。
3. 索引拆分 当数据越来越多,则分片上的数据也越来越多,查询和存储的速度也越来越慢，更重要的是一个index其实是有存储上限的，如官方声明单个shard的文档数不能超过20亿, 可以借助 Rollover Api 和 Index Template,以时间维度拆分
大致如下: 存储数据的时候,存到具有类似名称的索引上,这些索引类属于同一个别名,则查询的时间可以根据这个规则范围查询
例如: logs-2020.04.12 , logs-2020.05.12 都类比于 logs-*
那如果你想查询3天内的数据可以通过日期规则来匹配索引名，
GET /&amp;lt;logs-{now/d}-*&amp;gt;,&amp;lt;logs-{now/d-1d}-*&amp;gt;,&amp;lt;logs-{now/d-2d}-*&amp;gt;/_search
4. Hot-Warm架构 冷热架构，为了保证大规模时序索引实时数据分析的时效性，可以根据资源配置不同将Data Nodes进行分类形成分层或分组架构，一部分支持新数据的读写，另一部分仅支持历史数据的存储，存放一些查询发生机率较低的数据，即Hot-Warm架构
思路:
将Data Node根据不同的资源配比打上标签，如：host、warm 定义2个时序索引的index template，包括hot template和warm template，hot template可以多分配一些shard和拥有更好资源的Hot Node 用hot template创建一个active index名为active-logs-1，别名active-logs，支持索引切割，插入一定数据后，通过roller over api将active-logs切割，并将切割前的index移动到warm nodes上，如active-logs-1，并阻止写入 通过Shrinking API收缩索引active-logs-1为inactive-logs-1，原shard为5，适当收缩到2或3，可以在warm template中指定，减少检索的shard，使查询更快 通过force-merging api合并inactive-logs-1索引每个shard的segment，节省存储空间 删除active-logs-1 来自 : https://www.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/painless-%E8%84%9A%E6%9C%AC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/painless-%E8%84%9A%E6%9C%AC/</guid><description>[toc]
1. 定义 ElasticStack在升级到5.0版本之后，带来了一个新的脚本语言，painless。这里说“新的“是相对与已经存在groove而言的。Groove脚本开启之后，如果被人误用可能带来各种漏洞，由于这些外部的脚本引擎太过于强大，用不好或者设置不当就会引起安全风险，基于安全和性能方面，所以elastic.co开发了一个新的脚本引擎，名字就叫Painless，和Groove的沙盒机制不一样，Painless使用白名单来限制函数与字段的访问，针对es的场景来进行优化，只做es数据的操作，更加轻量级，速度要快好几倍，并且支持Java静态类型，语法保持Groove类似，还支持Java的lambda表达式。
可以做很多事情,改字段值 / 加字段 / 查询时处理结果值等等
2. 语法 脚本变量
ctx._source.field：add, contains, remove, indexOf, length
ctx.op：应该对文档应用的操作：索引或删除
ctx._index：访问文档元数据字段
_score 只在script_score中有效
doc[‘field’], doc[‘field’].value: add, contains, remove, indexOf, length
1、常用数据类型： int、double、String、List、Map、bool (和java一致）
2、变量定义：
​ 定义变量有两种方式，动态类型和静态类型，建议用静态类型，静态类型的计算速度是动态类型的10倍
动态类型定义方式： def a = &amp;ldquo;abc&amp;rdquo; 静态类型定义方式： int a = 1; Sting b = &amp;ldquo;asdsd&amp;rdquo; 3、获取文档中某个字段的值的用法：
获取doc下字段： doc[&amp;rsquo;name&amp;rsquo;].value
nested字段取出来后就是数组，可用下标获取, 后面不用加value： doc['expect_jobs'][1]
doc以外的字段可直接使用，如下：
POST twitter/_update/1 { &amp;#34;script&amp;#34;: { &amp;#34;source&amp;#34;: &amp;#34;ctx._source.age = params.value&amp;#34;, &amp;#34;params&amp;#34;: { &amp;#34;value&amp;#34;: 34 } } } 如果params是其他类型,基本遵循java的语法</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%88%86%E9%A1%B5%E5%AE%9E%E7%8E%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%88%86%E9%A1%B5%E5%AE%9E%E7%8E%B0/</guid><description>[toc]
1. from + size 这是ES分页中最常用的一种方式，与MySQL类似，from指定起始位置，size指定返回的文档数。
GET kibana_sample_data_flights/_search { &amp;#34;from&amp;#34;: 10, &amp;#34;size&amp;#34;: 2, &amp;#34;query&amp;#34;: { &amp;#34;match&amp;#34;: { &amp;#34;DestWeather&amp;#34;: &amp;#34;Sunny&amp;#34; } }, &amp;#34;sort&amp;#34;: [ { &amp;#34;timestamp&amp;#34;: { &amp;#34;order&amp;#34;: &amp;#34;asc&amp;#34; } } ] } 使用简单，且默认的深度分页限制是1万，from + size 大于 10000会报错，可以通过index.max_result_window参数进行修改。
好处是可以灵活分页, 比如指定页码的跳转
这种分页方式，在分布式的环境下的深度分页是有性能问题的，一般不建议用这种方式做深度分页，可以用下面将要介绍的两种方式。(以下两种均不能指定页码跳转,只能下滑式翻页)
理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。
现在假设我们请求第 1000 页，结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。
可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。
2. scroll api 创建一个快照，有新的数据写入以后，无法被查到。每次查询后，输入上一次的 scroll_id</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%91%BD%E4%BB%A4/</guid><description>[toc]
前言 本文以命令行为主
其本质还是发送http请求去操作数据
1. 基本格式 es是以RESTFul风格来命名API的，其API的基本格式如下：
http://&amp;lt;ip&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;索引&amp;gt;/&amp;lt;类型&amp;gt;/&amp;lt;文档id&amp;gt;
这里需要注意的是，该格式从es7.0.0开始，移除Type（类型）这个概念，新的基本格式如下：
http://&amp;lt;ip&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;索引&amp;gt;/_doc/&amp;lt;文档id&amp;gt;
Type（类型）字段那里变为固定值 _doc
es的动作是以http方法来决定的: 常用的http方法: GET/PUT/POST/DELETE
post 主做修改, put主做创建
post也有创建的功能,特别是插入数据生成随机id时
2. 命令 2.1 创建索引 curl -XPUT http://localhost:9200/xkj_test 没有指定mappings,故为非结构化索引,也没有字段
// 结构化索引: 指定了mappings curl -X PUT &amp;#39;localhost:9200/accounts&amp;#39; -d &amp;#39; { &amp;#34;mappings&amp;#34;: { // mappings 映射 &amp;#34;man&amp;#34;: { // type &amp;#34;properties&amp;#34;: { // 具体属性 &amp;#34;name&amp;#34;: { // 字段名 &amp;#34;type&amp;#34;:&amp;#34;text&amp;#34; // 字段类型 // ....还有其他可以选,比如拆分规则,权重值 }, &amp;#34;country&amp;#34;: { &amp;#34;type&amp;#34;:&amp;#34;keyword&amp;#34; }, &amp;#34;age&amp;#34;: { &amp;#34;type&amp;#34;:&amp;#34;integer&amp;#34; }, &amp;#34;date&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&amp;#34; } } }, // &amp;#34;worman&amp;#34;: {} // 6.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%AD%98%E5%82%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E5%AD%98%E5%82%A8/</guid><description>[TOC]
在ElasticSearch 2.4版本中，文档存储的介质分为内存和硬盘：内存速度快，但是容量有限；硬盘速度较慢，但是容量很大。同时，ElasticSearch进程自身的运行也需要内存空间，必须保证ElasticSearch进程有充足的运行时内存。为了使ElasticSearch引擎达到最佳性能，必须合理分配有限的内存和硬盘资源。
一，倒排索引（Inverted Index） ElasticSearch引擎把文档数据写入到倒排索引（Inverted Index）的数据结构中，倒排索引建立的是分词（Term）和文档（Document）之间的映射关系，在倒排索引中，数据是面向词（Term）而不是面向文档的。
举个例子，文档和词条之间的关系如下图：
字段值被分析之后，存储在倒排索引中，倒排索引存储的是分词（Term）和文档（Doc）之间的关系，简化版的倒排索引如下图：
从图中可以看出，倒排索引有一个词条的列表，每个分词在列表中是唯一的，记录着词条出现的次数，以及包含词条的文档。实际上，ElasticSearch引擎创建的倒排索引比这个复杂得多。
1，段是倒排索引的组成部分 倒排索引是由段（Segment）组成的，段存储在硬盘（Disk）文件中。索引段不是实时更新的，这意味着，段在写入硬盘之后，就不再被更新。在删除文档时，ElasticSearch引擎把已删除的文档的信息存储在一个单独的文件中，在搜索数据时，ElasticSearch引擎首先从段中执行查询，再从查询结果中过滤被删除的文档，这意味着，段中存储着被删除的文档，这使得段中含有”正常文档“的密度降低。多个段可以通过段合并（Segment Merge）操作把“已删除”的文档将从段中物理删除，把未删除的文档合并到一个新段中，新段中没有”已删除文档“，因此，段合并操作能够提高索引的查找速度，但是，段合并是IO密集型操作，需要消耗大量的硬盘IO。
在ElasticSearch中，大多数查询都需要从硬盘文件（索引的段数据存储在硬盘文件中）中获取数据，因此，在全局配置文件elasticsearch.yml 中，把**结点的路径（Path）**配置为性能较高的硬盘，能够提高查询性能。默认情况下，ElasticSearch使用基于安装目录的相对路径来配置结点的路径，安装目录由属性path.home显示，在home path下，ElasticSearch自动创建config，data，logs和plugins目录，一般情况下不需要对结点路径单独配置。结点的文件路径配置项：
path.data：设置ElasticSearch结点的索引数据保存的目录，多个数据文件使用逗号隔开，例如，path.data: /path/to/data1,/path/to/data2； path.work：设置ElasticSearch的临时文件保存的目录； 2，分词和原始文本的存储 映射参数index决定ElasticSearch引擎是否对文本字段执行分析操作，也就是说分析操作把文本分割成一个一个的分词，也就是标记流（Token Stream），把分词编入索引，使分词能够被搜索到：
当index为analyzed时，该字段是分析字段，ElasticSearch引擎对该字段执行分析操作，把文本分割成分词流，存储在倒排索引中，使其支持全文搜索； 当index为not_analyzed时，该字段不会被分析，ElasticSearch引擎把原始文本作为单个分词存储在倒排索引中，不支持全文搜索，但是支持词条级别的搜索；也就是说，字段的原始文本不经过分析而存储在倒排索引中，把原始文本编入索引，在搜索的过程中，查询条件必须全部匹配整个原始文本； 当index为no时，该字段不会被存储到倒排索引中，不会被搜索到； 字段的原始值是否被存储到倒排索引，是由映射参数store决定的，默认值是false，也就是，原始值不存储到倒排索引中。
映射参数index和store的区别在于：
store用于获取（Retrieve）字段的原始值，不支持查询，可以使用投影参数fields，对stroe属性为true的字段进行过滤，只获取（Retrieve）特定的字段，减少网络负载； index用于查询（Search）字段，当index为analyzed时，对字段的分词执行全文查询；当index为not_analyzed时，字段的原始值作为一个分词，只能对字段的原始文本执行词条查询； 3，单个分词的最大长度 如果设置字段的index属性为not_analyzed，原始文本将作为单个分词，其最大长度跟UTF8 编码有关，默认的最大长度是32766Bytes，如果字段的文本超过该限制，那么ElasticSearch将跳过（Skip）该文档，并在Response中抛出异常消息：
operation[607]: index returned 400 _index: ebrite _type: events _id: 76860 _version: 0 error: Type: illegal_argument_exception Reason: &amp;ldquo;Document contains at least one immense term in field=&amp;ldquo;event_raw&amp;rdquo; (whose UTF8 encoding is longer than the max length 32766), all of which were skipped.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%89%93%E5%88%86%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%89%93%E5%88%86%E6%9C%BA%E5%88%B6/</guid><description>es 的默认打分机制依靠lucene的打分机制- TF/IDF(词频/逆向文档频率)
计算公式:
score(q,d) = queryNorm(q) //归一化因子 · coord(q,d) //协调因子 · ∑ ( tf(t in d) //词频 · idf(t)² //逆向文档频率 · t.getBoost() //权重 · norm(t,d) //字段长度归一值 ) (t in q) 来自:https://blog.csdn.net/paditang/article/details/79098
词频:
词在文档中出现的频度是多少？ 频度越高，权重 越高 。 5 次提到同一词的字段比只提到 1 次的更相关。词频的计算方式如下： tf(t in d) = √frequency , 词 t 在文档 d 的词频（ tf ）是该词在文档中出现次数的平方根。
将参数 index_options设置为docs可以禁用词频统计及词频位置，这个映射的字段不会计算词的出现次数，对于短语或近似查询也不可用。要求精确查询的not_analyzed 字符串字段会默认使用该设置。
逆向文档频率
词在集合所有文档里出现的频率是多少？频次越高，权重 越低 。 常用词如 and 或 the 对相关度贡献很少，因为它们在多数文档中都会出现，一些不常见词如 elastic 或 hippopotamus 可以帮助我们快速缩小范围找到感兴趣的文档。逆向文档频率的计算公式如下： idf(t) = 1 + log ( numDocs / (docFreq + 1)) 词 t 的逆向文档频率（idf）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</guid><description>[toc]
基于ElasticSearch 6.x 1 字段类型概述 一级分类 二级分类 具体类型 核心类型 字符串类型 string,text,keyword 整数类型 integer,long,short,byte 浮点类型 double,float,half_float,scaled_float 逻辑类型 boolean 日期类型 date 范围类型 range 二进制类型 binary 复合类型 数组类型 array 对象类型 object 嵌套类型 nested 地理类型 地理坐标类型 geo_point 地理地图 geo_shape 特殊类型 IP类型 ip 范围类型 completion 令牌计数类型 token_count 附件类型 attachment 抽取类型 percolator 2 字符串类型 （1）string string类型在ElasticSearch 旧版本中使用较多，从ElasticSearch 5.x开始不再支持string，由text和keyword类型替代。
（2）text 当一个字段是要被全文搜索的，比如Email内容、产品描述，应该使用text类型。设置text类型以后，字段内容会被分析，在生成倒排索引以前，字符串会被分析器分成一个一个词项。text类型的字段不用于排序，很少用于聚合。
（3）keyword keyword类型适用于索引结构化的字段，比如email地址、主机名、状态码和标签。如果字段需要进行过滤(比如查找已发布博客中status属性为published的文章)、排序、聚合。keyword类型的字段只能通过精确值搜索到。
以下 基于 ElasticSearch 7.x :
关键字族包括以下字段类型：
keyword，用于结构化内容，例如 ID、电子邮件地址、主机名、状态代码、邮政编码或标签。 constant_keyword 用于始终包含相同值的关键字字段。 wildcard 为非结构化内容提供。wildcard类型针对具有大值或高基数的字段进行了优化。 Keyword字段通常用于排序、聚合和术语级别的查询，例如术语。
简单的说,wildcard 相比keyword做了优化,不过也牺牲了一些场景和功能</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/</guid><description>一些基本概念 接近实时（NRT）
Elasticsearch 是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个很小的延迟（通常是 1 秒）。 集群（cluster）
代表一个集群，集群中有多个节点（node），其中有一个为主节点，这个主节点是可以通过选举产生的，主从节点是对于集群内部来说的。es的一个概念就是去中心化，字面上理解就是无中心节点，这是对于集群外部来说的，因为从外部来看es集群，在逻辑上是个整体，你与任何一个节点的通信和与整个es集群通信是等价的。 索引（index）
ElasticSearch将它的数据存储在一个或多个索引（index）中。用SQL领域的术语来类比，索引就像数据库，可以向索引写入文档或者从索引中读取文档，并通过ElasticSearch内部使用Lucene将数据写入索引或从索引中检索数据。 ​ 索引分为结构化创建与非结构化创建 其划分依据是 是否有mappings(有则为结构化),创建索引时可以指定. 结构化的好处,es知道了字段的相关信息,可以做一些预处理,提高性能
来源: https://blog.csdn.net/liuxiao723846/article/details/78444472
https://www.cnblogs.com/hong-fithing/p/11221020.html
文档（document）
文档（document）是ElasticSearch中的主要实体。对所有使用ElasticSearch的案例来说，他们最终都可以归结为对文档的搜索。文档由字段构成。 映射（mapping）
所有文档写进索引之前都会先进行分析，如何将输入的文本分割为词条、哪些词条又会被过滤，这种行为叫做映射（mapping）。一般由用户自己定义规则。 类型（type）
每个文档都有与之对应的类型（type）定义。这允许用户在一个索引中存储多种文档类型，并为不同文档提供类型提供不同的映射。 分片（shards）
代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引拆分成多个，分布到不同的节点上。构成分布式搜索。分片的数量只能在索引创建前指定，并且索引创建后不能更改。5.X默认不能通过配置文件定义分片
shard数量过多，则批量写入/查询请求被分割为过多的子写入/查询，导致该index的写入、查询拒绝率上升；
对于数据量较大的inex，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高 或 不均衡，影响写入/查询的效率。
ElasticSearch 7.x 不需要优化分片的数量了吗？ - Elastic 中文社区
副本（replicas）
代表索引副本，es可以设置多个索引的副本，副本的作用一是提高系统的容错性，当个某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负载均衡。
其本质是对分片进行备份操作, 副本会接收读请求
扩展阅读: ElasticSearch 如何保证数据一致性,实时性 - 简书 (jianshu.com)
数据恢复（recovery）
代表数据恢复或叫数据重新分布，es在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。 GET /_cat/health?v #可以看到集群状态 数据源（River）
代表es的一个数据源，也是其它存储方式（如：数据库）同步数据到es的一个方法。它是以插件方式存在的一个es服务，通过读取river中的数据并把它索引到es中，官方的river有couchDB的，RabbitMQ的，Twitter的，Wikipedia的，river这个功能将会在后面的文件中重点说到。 网关（gateway）
代表es索引的持久化存储方式，es默认是先把索引存放到内存中，当内存满了时再持久化到硬盘。当这个es集群关闭再重新启动时就会从gateway中读取索引数据。es支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和amazon的s3云存储服务。 自动发现（discovery.zen）
代表es的自动发现节点机制，es是一个基于p2p的系统，它先通过广播寻找存在的节点，再通过多播协议来进行节点之间的通信，同时也支持点对点的交互。 5.X关闭广播，需要自定义 通信（Transport）
代表es内部节点或集群与客户端的交互方式，默认内部是使用tcp协议进行交互，同时它支持http协议（json格式）、thrift、servlet、memcached、zeroMQ等的传输协议（通过插件方式集成）。 节点间通信端口默认：9300-9400 分片和复制（shards and replicas）
一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点可能没有这样大的磁盘空间来存储或者单个节点处理搜索请求，响应会太慢。
为了解决这个问题，Elasticsearch提供了将索引划分成多片的能力，这些片叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引” 可以被放置到集群中的任何节点上。
分片之所以重要，主要有两方面的原因：
允许你水平分割/扩展你的内容容量 允许你在分片（位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/elasticsearch/%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/</guid><description>[toc]
前言 本文档基于elasticsearch 6.4.2 , jdk 1.8 ,三台centos6 服务器,以下所有操作均在jht用户下完成,以10.10.204.167为主服务器
不要使用root账号安装!!! 使用额外账号安装,例如:jht
中文官网
1. 节点角色 ES 支持的功能较多（譬如机器学习），为了更好的支持这些功能，es 定义了多种节点角色，一个节点可以同时担任多种角色，默认具有以下角色：
master data data_content data_hot data_warm data_cold data_frozen ingest ml remote_cluster_client transform ES 集群中的节点大致可以分为管理节点、数据节点和任务节点。
管理节点 ：
角色配置为 master，能够参与 leader 投票、可以被选为 leader 的节点 被选为 leader 的节点负责集群的管理工作 如果同时配置了 voting_only 角色，该节点只参与选举投票，不做候选人 数据节点：
角色配置为 data，负责存储数据和提供数据查询、聚合处理的节点 数据节点可以进一步细分：data_content，data_hot，data_warm，data_cold，data_frozen data_content: 存放 document 数据 data_hot： 存放刚写入的时间序列数据（热点数据），要能够快速读写 data_warm：不再经常更新、低频查询的数据 data_cold：极少访问的只读数据 data_frozen：缓存从快照中查询出的数据，如果不存在从快照读取后缓存， 任务节点 类型较多，每个角色负责专门的任务：
[]： 角色为空，coordinating node，没有任何角色，只负责将收到的请求转发到其它节点 ingest：承担 pipeline 处理任务的节点 remote_cluster_client：跨集群操作时，与其它机器进行通信的节点 ml：执行机器学习任务和处理机器学习 api 的节点，通常建议同时配置角色 remote_cluster_client transform：数据处理节点，对文档数据进行再次加工，通常建议同时配置角色 remote_cluster_client ElasticSearch 零基础入门（1）：基本概念、集群模式、查询语法和聚合语法By李佶澳 (lijiaocn.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/logstash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/logstash/</guid><description>[toc]
logstash全是插件,从input到output,幸运的是官方文档都有些,https://www.elastic.co/guide/en/logstash/6.5/index.html ,
总感觉logstash和flume很相似,他们都有三个级别,前,中,后,但是他们还是区别,logstash更倾向于日志收集,比较轻量级,flume更倾向于链路路由,比较重量级,https://blog.csdn.net/jek123456/article/details/65658790
1.input 能从许多地方读取信息,从beats(专门读文件(log)的)工具,elasticsearch,shell,文件,jdbc,kafka,redis,rabbitmp,tcp/udp等等
1.1 file插件 input{ file{ path =&amp;gt; [&amp;#34;/var/log/nginx/access.log&amp;#34;, &amp;#34;/var/log/nginx/error.log&amp;#34;] #处理的文件的路径, 可以定义多个路径 exclude =&amp;gt; &amp;#34;*.zip&amp;#34; #匹配排除 sincedb_path =&amp;gt; &amp;#34;/data/&amp;#34; #sincedb数据文件的路径, 默认&amp;lt;path.data&amp;gt;/plugins/inputs/file codec =&amp;gt; &amp;#34;plain&amp;#34; #默认是plain,可通过这个参数设置编码方式 # codec =&amp;gt; multiline { # 管理多线事件,将java的异常归纳到一条数据中 # pattern =&amp;gt;“^\s” # what =&amp;gt;“previous” # } tags =&amp;gt; [&amp;#34;nginx&amp;#34;] #添加标记 type =&amp;gt; &amp;#34;nginx&amp;#34; #添加类型 discover_interval =&amp;gt; 2 #每隔多久去查一次文件, 默认15s stat_interval =&amp;gt; 1 #每隔多久去查一次文件是否被修改过, 默认1s start_position =&amp;gt; &amp;#34;beginning&amp;#34; #从什么位置开始读取文件数据, beginning和end, 默认是结束位置end } } 原文：https://blog.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/%E4%BB%8B%E7%BB%8D/</guid><description>[TOC]
一.概念 日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。
通常，日志被分散在储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。
集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。
通过我们需要对日志进行集中化管理，将所有机器上的日志信息收集、汇总到一起。完整的日志数据具有非常重要的作用：
1）信息查找。通过检索日志信息，定位相应的bug，找出解决方案。
2）服务诊断。通过对日志信息进行统计、分析，了解服务器的负荷和服务运行状态，找出耗时请求进行优化等等。
3）数据分析。如果是格式化的log，可以做进一步的数据分析，统计、聚合出有意义的信息，比如根据请求中的商品id，找出TOP10用户感兴趣商品。
开源实时日志分析ELK平台能够完美的解决我们上述的问题，ELK由ElasticSearch、Logstash和Kiabana三个开源工具组成：
1）ElasticSearch是一个基于Lucene的开源分布式搜索服务器。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是第二流行的企业搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。在elasticsearch中，所有节点的数据是均等的。
Elasticsearch本身就是分布式的，因此即便你只有一个节点，Elasticsearch默认也会对你的数据进行分片和副本操作，当你向集群添加新数据时，数据也会在新加入的节点中进行平衡。
2）Logstash是一个完全开源的工具，它可以对你的日志进行收集、过滤、分析，支持大量的数据获取方法，并将其存储供以后使用（如搜索）。说到搜索，logstash带有一个web界面，搜索和展示所有日志。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。
3）Kibana 是一个基于浏览器页面的Elasticsearch前端展示工具，也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。
ELK工作原理展示图：
如上图：Logstash收集AppServer产生的Log，并存放到ElasticSearch集群中，而Kibana则从ES集群中查询数据生成图表，再返回给Browser。
Logstash工作原理：
Logstash事件处理有三个阶段：inputs → filters → outputs。是一个接收，处理，转发日志的工具。支持系统日志，webserver日志，错误日志，应用日志，总之包括所有可以抛出来的日志类型。
Input：输入数据到logstash。
一些常用的输入为：
file：从文件系统的文件中读取，类似于tail -f命令
syslog：在514端口上监听系统日志消息，并根据RFC3164标准进行解析
redis：从redis service中读取
beats：从filebeat中读取
Filters：数据中间处理，对数据进行操作。
一些常用的过滤器为：
grok：解析任意文本数据，Grok 是 Logstash 最重要的插件。它的主要作用就是将文本格式的字符串，转换成为具体的结构化的数据，配合正则表达式使用。内置120多个解析语法。
mutate：对字段进行转换。例如对字段进行删除、替换、修改、重命名等。
drop：丢弃一部分events不进行处理。
clone：拷贝 event，这个过程中也可以添加或移除字段。
geoip：添加地理信息(为前台kibana图形化展示使用)
Outputs：outputs是logstash处理管道的最末端组件。一个event可以在处理过程中经过多重输出，但是一旦所有的outputs都执行结束，这个event也就完成生命周期。
一些常见的outputs为：
elasticsearch：可以高效的保存数据，并且能够方便和简单的进行查询。
file：将event数据保存到文件中。
graphite：将event数据发送到图形化组件中，一个很流行的开源存储图形化展示的组件。
Codecs：codecs 是基于数据流的过滤器，它可以作为input，output的一部分配置。Codecs可以帮助你轻松的分割发送过来已经被序列化的数据。
一些常见的codecs：
json：使用json格式对数据进行编码/解码。
multiline：将汇多个事件中数据汇总为一个单一的行。比如：java异常信息和堆栈信息。
这里，需要重点关注filter部分，下面列举几个常用的插件，实际使用中根据自身需求从官方文档中查找适合自己业务的插件并使用即可，当然也可以编写自己的插件。
**grok：**是Logstash最重要的一个插件，用于将非结构化的文本数据转化为结构化的数据。grok内部使用正则语法对文本数据进行匹配，为了降低使用复杂度，其提供了一组pattern，我们可以直接调用pattern而不需要自己写正则表达式，参考源码grok-patterns。grok解析文本的语法格式是%{SYNTAX:SEMANTIC}，SYNTAX是pattern名称，SEMANTIC是需要生成的字段名称，使用工具Grok Debugger可以对解析语法进行调试。例如，在下面的配置中，我们先使用grok对输入的原始nginx日志信息（默认以message作为字段名）进行解析，并添加新的字段request_path_with_verb（该字段的值是verb和request_path的组合），然后对request_path字段做进一步解析。
**kv：**用于将某个字段的值进行分解，类似于编程语言中的字符串Split。在下面的配置中，我们将request_args字段值按照“&amp;amp;”进行分解，分解后的字段名称以“request_args_”作为前缀，并且丢弃重复的字段。
**geoip：**用于根据IP信息生成地理位置信息，默认使用自带的一份GeoLiteCity database，也可以自己更换为最新的数据库，但是需要数据格式需要遵循Maxmind的格式（参考GeoLite），似乎目前只能支持legacy database，数据类型必须是.dat。下载GeoLiteCity.dat.gz后解压， 并将文件路径配置到source中即可。
**translate：**用于检测某字段的值是否符合条件，如果符合条件则将其翻译成新的值，写入一个新的字段，匹配pattern可以通过YAML文件来配置。例如，在下面的配置中，我们对request_api字段翻译成更加易懂的文字描述。
filter { grok { match =&amp;gt; {&amp;#34;message&amp;#34; =&amp;gt; &amp;#34;%{IPORHOST:client_ip} \&amp;#34;%{TIMESTAMP_ISO8601:timestamp}\&amp;#34; \&amp;#34;%{WORD:verb} %{NOTSPACE:request_path} HTTP/%{NUMBER:httpversion}\&amp;#34; %{NUMBER:response_status:int} %{NUMBER:response_body_bytes:int} \&amp;#34;%{DATA:user_agent}\&amp;#34; \&amp;#34;%{DATA:http_referer}\&amp;#34; \&amp;#34;%{NOTSPACE:http_x_forwarder_for}\&amp;#34; \&amp;#34;%{NUMBER:request_time:float}\&amp;#34; \&amp;#34;%{DATA:upstream_resopnse_time}\&amp;#34; \&amp;#34;%{DATA:http_cookie}\&amp;#34; \&amp;#34;%{DATA:http_authorization}\&amp;#34; \&amp;#34;%{DATA:http_token}\&amp;#34;&amp;#34;} add_field =&amp;gt; {&amp;#34;request_path_with_verb&amp;#34; =&amp;gt; &amp;#34;%{verb} %{request_path}&amp;#34;} } grok { match =&amp;gt; {&amp;#34;request_path&amp;#34; =&amp;gt; &amp;#34;%{URIPATH:request_api}(?</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/%E5%9F%BA%E6%9C%AC%E6%90%AD%E5%BB%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/%E5%9F%BA%E6%9C%AC%E6%90%AD%E5%BB%BA/</guid><description>[toc]
ElasticSearch、Logstash和Kiabana均有开箱即用的版本, 也可以使用docker,就不用下载具体的包了
官网下载:https://www.elastic.co/cn/downloads/
1. ElasticSearch: 在config中 增加elasticsearch.yml文件如下内容:
`network.host: 0.0.0.0 # 网络设置,表示大家都能连``
执行 bin/elasticsearch 即可
在浏览器中输入 http://localhost:9200/
返回如下json表示成功:
{ &amp;#34;name&amp;#34;: &amp;#34;node-1&amp;#34;, &amp;#34;cluster_name&amp;#34;: &amp;#34;elasticsearch&amp;#34;, &amp;#34;cluster_uuid&amp;#34;: &amp;#34;qgDoFT0_Sa66sYTd_5ETug&amp;#34;, &amp;#34;version&amp;#34;: { &amp;#34;number&amp;#34;: &amp;#34;5.5.3&amp;#34;, &amp;#34;build_hash&amp;#34;: &amp;#34;9305a5e&amp;#34;, &amp;#34;build_date&amp;#34;: &amp;#34;2017-09-07T15:56:59.599Z&amp;#34;, &amp;#34;build_snapshot&amp;#34;: false, &amp;#34;lucene_version&amp;#34;: &amp;#34;6.6.0&amp;#34; }, &amp;#34;tagline&amp;#34;: &amp;#34;You Know, for Search&amp;#34; } 2.Kiabana: 在config中修改 kibana.yml ,如下内容:
elasticsearch.url: &amp;quot;http://localhost:9200&amp;quot;
执行 bin/kibana.bat
在浏览器中访问 http://localhost:5601 会出现页面,说明成功了
3.Logstash (有点像Flume,有接收数据,处理数据,输出数据.输入/输出都有各种选择)
直接运行 start.bat 即可
在config文件夹下,创建logstash.conf文件,写入以下内容:
input {// logstash的数据来源 # 从控制台接收, 类型是test(可以不写), stdin{ type =&amp;gt; &amp;#34;test&amp;#34; } #从文件中接收, file{ path=&amp;gt;&amp;#34;/home/jht/jportal-license/jportal/logs/jportal/jportal.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%85%88%E7%BA%A7%E5%AF%BC%E8%AE%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/elk/%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%85%88%E7%BA%A7%E5%AF%BC%E8%AE%BA/</guid><description>我建议 Elasticsearch 为第一优先级。需要掌握的内容如下。
（1）掌握 Elasticsearch 的基本概念，主要包括：
索引（index） 类型（type） 映射（mapping） 文档（document） 倒排索引原理 文档打分机制 集群（cluster）——单节点、集群安装与部署 健康状态（red/yellow/green） 数据存储 数据类型（long/date/text、keyword/nested等） 数据展示（结合Head插件的基础可视化） …… （2）掌握 Elasitcsearch 的基本操作，主要包括：
新增（insert） 删除（delete/deletebyquery） 修改（update/updatebyquery） 查找（search） 精确匹配检索（term、terms、range、exists） 模糊匹配检索（wildcard、prefix、negix正则） 分词全文检索（match/match_phrase等） 多条件 bool 检索（must/must_not/should多重组合） 分词（英文分词、拼音分词、中文分词） 高亮 分页查询 指定关键词返回 批量操作 bulk scroll 查询 reindex 操作 …… （3）掌握 Elasticsearch 高级操作，主要包括：
聚合统计（数量聚合、最大值、最小值、平均值、求和等聚合操作） 图像化展示（hisgram 按照日期等聚合） 聚合后分页 父子文档 数组类型 nested 嵌套类型 ES 插件错误排查（集群问题、检索问题、性能问题） ES 性能调优（配置调优、集群调优等） …… （4）掌握 Elasticsearch Java/Python 等API，主要包括：
Elasticsearch 原生自带 API、JEST、Springboot 等 API 选型 Elasticsearch 多条件 bool 复杂检索 API Elasticsearch 分页 API Elasticsearch 高亮 API Elasticsearch 聚合 API Elasticsearch 相关 JSON 数据解析 …… （5）Elasticsearch 结合场景开发实战，主要包括：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/kubernetes/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/kubernetes/%E4%BB%8B%E7%BB%8D/</guid><description>[toc]
一、Kubernetes是什么？ 官方文档中描述为:
Kubernetes一个用于容器集群的自动化部署、扩容以及运维的开源平台。通过Kubernetes,你可以快速有效地响应用户需求;快速而有预期地部署你的应用;极速地扩展你的应用;无缝对接新应用功能;节省资源，优化硬件资源的使用。为容器编排管理提供了完整的开源方案。
介绍一下其中提到的几个词:
容器
我们现在常说的容器一般是指Docker容器，通过容器隔离的特性和宿主机进行解耦，使我们的服务不需要依赖于宿主机而运行，与宿主机互不影响，Docker容器十分轻量。而kubernetes则负责管理服务中所有的Docker容器，创建、运行、重启与删除容器。
快速响应
个人理解为两个方面。
一、新增或者修改需求时，可以快速进行部署测试(CICD)；
二、kubernetes可以根据不同条件进行动态扩缩容，举个栗子，用户访问量突然由1000人上升到100000人时，现有的服务已经无法支撑，kubernetes会自动将用户服务模块增加更多实例以保证当前的系统访问量。
扩展
在快速响应的特点中已经有所提及，这里再补充一点: Kubernetes内部有完善的注册发现机制，当某个服务的实例增加时，kubernetes会自动将其加入服务列表中，免除在传统运维中需要人工维护服务列表的问题。
对接新应用
kubernetes是一个通用的容器编排框架，支持不同类型的语言，或者是语言无关的，新增加的应用都会以一个新的对象进行接入。
硬件资源
这一点我觉得是kubernetess很基本但是非常重要的一个优点了，kubernetes在部署应用时会自动检查各个服务器的cpu与内存使用量，同时会根据服务申请的cpu与内存资源，将服务部署到最合适的服务器。(其实这就是容器调度的核心功能了)
小知识: 因kubernetes名字过长，一般简称为k8s，因为k与s之间有8个字母，故而称之。
二、Kubernetes解决了什么问题？ 下面以几个case进行阐述，便于理解。
服务器环境
kubernetes是使用Docker进行容器管理的，所以天生具备Docker的所有特性，只需要使用相应环境的Docker镜像就可以运行服务，还需要关心宿主机是redhat、centos还是ubuntu，只要在宿主机上安装Docker环境即可，相比传统运维，减少了各种依赖环境的冲突，降低运维成本，也方便整体服务的迁移。
服务器资源管理
对于kubernetes来说，是不关心有几台服务器的，每个服务器都是一个资源对象(Node)，kubernetes关心的是这个Node上有多少可用的cpu和内存。例如现在有两台服务器
server01 (4c16g), 已用(2c7.5G)
server02 (4c16g), 已用(3c13G)
现在有一个服务ServiceA需要部署，ServiceA申明自己运行需要至少3G内存，这时kubernetes会根据调度策略将其部署到server01上，很明显server01的资源是更加充足的。实际上kubernetes的调度策略要复杂的多，kubernetes会监控整体服务器资源的状态进行调度，而以前的运维方式只能由人工判断资源使用。
服务容灾恢复
说简单点，就是服务挂了之后，能够自动恢复。例如现在有一个ServiceA，运行在server01上，kubernetes会通过内部的kubelet组件监控ServiceA服务进程的状态，一旦发现进程丢失(服务本身挂掉或者整个server01的服务器挂掉)，就会尝试换一台资源充足的服务器重新部署ServiceA并启动，这样就可以确保我们的服务一直是可用状态，而不需要人工维护。
硬件资源利用
前面已经说过，kubernetes会根据节点(Node)的CPU与内存资源的可用量对服务进行部署调度，在调度策略中，可以配置不同的调度策略。例如现在有两台服务器：
server01 (4c16g), 已用(3c7.5G) server02 (4c16g), 已用(1c13G) 需要部署两个服务
serviceA-Java, 申请2G内存，0.5CPU单位 ServiceB-Nginx, 申请200M内存,申请1CPU单位 这里kubernetes如果讲道理的话，会将ServiceA-Java部署到server01，将serviceB-Nginx部署到server02。这里server01的内存和server02的CPU资源都得到了充分的利用。经过个人实践，相比之前的部署方式，kubernetes节省了很多资源，资源利用是非常高效的。
原文：https://blog.csdn.net/kingboyworld/article/details/80966107
版本管理与滚动升级
版本管理 kubernetes在部署服务时，会记录部署服务的版本，我们可以很容易的进行上次版本或跨版本回退。
滚动升级 kubernetes在进行服务升级时，采用的默认策略是先将一部分新的服务启动，确定服务正常后，停止一部分旧服务，进行新老服务的替换，之后再启动一些新的服务，停止一部分旧服务，直到旧服务全部停止，即切换完成。滚动省级的过程中，极大的减少了服务切换的间隔时间。
其它
上面所说的是kubernetes的主体功能，kubernetes还有很多其他重要的特性解决了之前运维的痛点，例如DNS解析、自动负载、存储声明等等。
原文：https://blog.csdn.net/kingboyworld/article/details/80966107
三、kubernetes特点 网络模型
kubernetes采用了三层网络模型，分为PodIP,ClusterIP,NodeIP。用简单的话来说，kubernetes在内部使用自己的网络进行通讯，这样做一个最直接的好处是我们不用再担心端口冲突的问题。
举个栗子: 我们在server01上部署两个一样的服务serviceA-1,serviceA-2,两个服务的端口都是8080，这个时候有一个服务是无法启动的，因为端口被占用了，而在kubernetes中，两个服务在不同的Docker容器中,每个Docker容器都有自己的IP,这时就不会出现端口占用的问题了。
为什么要有三层网络有三个IP呢？其实每个IP的作用是不一样的：
NodeIP NodeIP是最好理解的，就是每个服务器的IP。例如server01的IP是192.168.1.2，有一个服务实例的IP申明类型为NodeIP，端口申明为30222,那么我们就可以通过192.168.1.2:30222访问到这个服务实例。
PodIP PodIP的作用可以简单理解为每个服务自己特有的IP,就像上面说的可以解决端口冲突的问题，同时也是每个服务的唯一标识。PodIP是无法通过外网访问的，只能在服务内部进行访问。
ClusterIP(可以按照下面访问的进行理解，但实际有所区别) 中文叫集群IP。集群IP可以简单理解为是对同一个服务的多个实例(每个实例有自己的PodIP)组成的集群的入口IP，换句话说，是对多个实例的负载IP。举个栗子：</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/kubernetes/%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/kubernetes/%E5%91%BD%E4%BB%A4/</guid><description>[toc]
1. kubectl 使用指南 kubectl 是 Kubernetes 自带的客户端，可以用它来直接操作 Kubernetes集群。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数&amp;ndash;kubeconfig 来指定其他位置的 kubeconfig 文件。
kubectl 并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。
从用户角度来说，kubectl 就是控制 Kubernetes 的驾驶舱，它允许你执行所有可能的 Kubernetes 操作；从技术角度来看，kubectl 就是 Kubernetes API 的一个客户端而已。
使用以下语法 kubectl 从终端窗口运行命令： kubectl [command] [TYPE] [NAME] [flags]
command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 TYPE：指定**资源类型**。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果: kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAME：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息 kubectl get pods。
flags: 指定可选的参数。例如，可以使用 -s 或 -server 参数指定 Kubernetes API 服务器的地址和端口。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/kubernetes/%E5%AE%89%E8%A3%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/kubernetes/%E5%AE%89%E8%A3%85/</guid><description>[toc]
K8s搭建流程-使用kubeadm
1 准备环境 1.1 服务器要求： 建议最小硬件配置：2核CPU、2G内存、20G硬盘，服务器最好可以访问外网，会有从网上拉取镜像需求，如果服务器不能上网，需要提前下载对应镜像并导入节点。
1.2 软件环境： 软件 版本 操作系统 CentOS 7.7.1908 (Core) Docker 24.0.4 (当前最新版) Kubernetes 1.23 1.3 服务器规划： 服务器名称 服务器IP 备注 master 10.39.40.1 24u/93G/755G 同时作为node节点 node1 10.39.40.2 32u/132G/3.6T node2 10.39.40.3 64u/141G/1.1T 部署使用root账号, 非root账号很多命令相对麻烦
2. 主机名解析 编辑所有节点服务器的 /etc/hosts 文件 ,添加主机名, 执行命令：
cat &amp;gt;&amp;gt; /etc/hosts &amp;lt;&amp;lt; EOF 10.39.97.19 master 10.39.97.20 node1 EOF 用sudo cat追加文件出错_iteye_2225的博客-CSDN博客
设置每台节点的hostname, 分别执行
master节点:
hostnamectl set-hostname master
node1节点:
hostnamectl set-hostname node1
3.时间同步 启动chronyd服务
systemctl start chronyd systemctl enable chronyd date 如果没有安装 chronyd , 可以使用 yum install chrony -y 安装</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</guid><description>[toc]
前言 Redis目前基本的数据类型有String、List、Set、ZSet、Hash五种，首先Redis是C语言开发的，所以底层就是用C语言封装数据结构或者C语言本身提供的数据结构来存储。
redis内部的主要数据结构主要有简单字符串（SDS）、双端链表、字典、压缩列表、跳跃表、整数集合。
Redis内部并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统-redisObject，这个对象系统包含了我们所熟知的五种基本类型数据，也就是字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象。redisObject 是 Redis 类型系统的核心, 数据库中的每个键、值, 以及 Redis 本身处理的参数, 都表示为这种数据类型。
而它们每一种对象都使用到了至少一种前面所介绍的数据结构。下面介绍一下redis内部的主要几个数据结构简单字符串（SDS）、双端链表、压缩列表、跳跃表的定义。然后再介绍一下redis基本的五种数据类型，也就是五种类型的对象用到了上面的哪些数据结构。
Redis相关知识&amp;mdash;-对象机制_小舟~的博客-CSDN博客
1. redis的底层数据结构 1.1 SDS（Simple Dynamic String）简单字符串 1、redis定义：
//定义了一个char 指针 typedef char *sds; /* Note: sdshdr5 is never used, we just access the flags byte directly. * sdshdr5 已经不用了, 在处理时会自动转为 sdshdr8 * However is here to document the layout of type 5 SDS strings. */ struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 前 3 位保存类型，后 5 位保存字符串长度 */ char buf[]; }; //lsb 代表有效位的意思， //__attribute__ ((__packed__)) 代表structure 采用手动对齐的方式。 struct __attribute__ ((__packed__)) sdshdr8 { //buf 已经使用的长度 uint8_t len; /* used */ //buf 分配的长度，等于buf[]的总长度-1，因为buf有包括一个/0的结束符 uint8_t alloc; /* excluding the header and null terminator */ //只有3位有效位，因为类型的表示就是0到4，所有这个8位的flags 有5位没有被用到 unsigned char flags; /* 3 lsb of type, 5 unused bits */ //实际的字符串存在这里 char buf[]; }; // 还定义了其他长度的类型, 与上面的变化只有len和alloc， 就是长度不同而已 struct __attribute__ ((__packed__)) sdshdr16 { }; struct __attribute__ ((__packed__)) sdshdr32 { }; struct __attribute__ ((__packed__)) sdshdr64 { }; 这么多类型, 是为了针对不同的数据大小, 因为统一用int的话, 就占太多空间了(int 是 4字节)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/redis%E7%9A%84%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5lru%E4%B8%8Elfu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/redis%E7%9A%84%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5lru%E4%B8%8Elfu/</guid><description>[toc]
释放内存其实在每次处理命令时都会执行, 只是满足判断条件才执行 , 例如内存满了, 需要淘汰key等等条件, 若发现已用内存超出maxmemory，会计算需释放的内存量。这个 释放内存大小=已使用内存量-maxmemory。
redis中的LRU实现 redis没有使用标准的LRU算法, 只是近似的LRU算法, 因为嫌LinkedList占用的空间太大了(因为起码要记录头尾指针)
简述: redis通过计算每个key的闲置时间来决定是否要选它淘汰(全局时钟 减去 当前key的访问时钟), redis会随机选几个key, 它们的闲置时间都要大于一个阈值(其实会存入一个pool, 这个阈值就是pool中最小的闲置时间), 当内存不够时, 就从这几个key中淘汰闲置时间最大的值
首先看一下全局时钟定义
#define LRU_BITS 24 struct redisServer { pid_t pid; /* Main process pid. */ char *configfile; /* Absolute config file path, or NULL */ ….. unsigned lruclock:LRU_BITS; /* Clock for LRU eviction */ ... }; redisServer 中包含了redis服务器启动之后的基本信息(PID,配置文件路径,serverCron运行频率hz等),外部可调用模块信息，网络信息，RDB/AOF信息，日志信息，复制信息等等。
上述结构体中lruclock:LRU_BITS,其中存储了服务器自启动之后的lru时钟，该时钟是全局的lru时钟。该时钟100ms更新一次。
可以通过hz来调整,默认情况hz=10,因此每1000ms/10=100ms执行一次定时任务
因此lrulock最大能到(2**24-1)/3600/24 = 194天,如果超过了这个时间，lrulock重新开始。对于redis server来说，server.lrulock表示的是一个全局的lrulock，
如果全局时钟 小于 对象时钟, 则会再加上时钟最大值 REDIS_LRU_CLOCK_MAX , 也就是 194天 的秒数, 相当于是第二轮了</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E4%B8%89%E7%A7%8D%E9%9B%86%E7%BE%A4%E6%96%B9%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E4%B8%89%E7%A7%8D%E9%9B%86%E7%BE%A4%E6%96%B9%E5%BC%8F/</guid><description>[toc]
主从复制 复制过程:
从服务器连接主服务器，发送SYNC（同步）命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令；（从服务器初始化完成） 主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令（从服务器初始化完成后的操作） 优点:
为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成 Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。 Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据 缺点
Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。 链接：https://www.jianshu.com/p/7967f95655b2
主从复制是基础,后面的哨兵和Cluster都是基于这种方式,但是这种方式太垃圾了,master死了就不能写操作了,所以一般会用后面两种 使用主从复制很简单,在配置文件中指定master/slave,并设置密码(必须使用)
fork耗时严重问题 我们可能会开启后台定时 RDB 和 AOF rewrite 功能。但如果你发现，操作 Redis 延迟变大，都发生在 Redis 后台 RDB 和 AOF rewrite 期间，在这期间有可能导致变慢的情况。
当 Redis 开启了后台 RDB 和 AOF rewrite 后，在执行时，它们都需要主进程创建出一个子进程进行数据的持久化。
主进程创建子进程，会调用操作系统提供的 fork 函数。
而 fork 在执行过程中，主进程需要拷贝自己的内存页表给子进程，如果这个实例很大，那么这个拷贝的过程也会比较耗时。
而且这个 fork 过程会消耗大量的 CPU 资源，在完成 fork 之前，整个 Redis 实例会被阻塞住，无法处理任何客户端请求。
如果此时你的 CPU 资源本来就很紧张，那么 fork 的耗时会更长，甚至达到秒级，这会严重影响 Redis 的性能。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</guid><description>[toc]
1. 高效分布式锁 互斥 在分布式高并发的条件下，我们最需要保证，同一时刻只能有一个线程获得锁，这是最基本的一点。
防止死锁 在分布式高并发的条件下，比如有个线程获得锁的同时，还没有来得及去释放锁，就因为系统故障或者其它原因使它无法执行释放锁的命令,导致其它线程都无法获得锁，造成死锁。
所以分布式非常有必要设置锁的有效时间，确保系统出现故障后，在一定时间内能够主动去释放锁，避免造成死锁的情况。
性能 对于访问量大的共享资源，需要考虑减少锁等待的时间，避免导致大量线程阻塞。
所以在锁的设计时，需要考虑两点。
锁的颗粒度要尽量小。比如你要通过锁来减库存，那这个锁的名称你可以设置成是商品的ID,而不是任取名称。这样这个锁只对当前商品有效,锁的颗粒度小。
锁的范围尽量要小。比如只要锁2行代码就可以解决问题的，那就不要去锁10行代码了。
重入
我们知道ReentrantLock是可重入锁，那它的特点就是：同一个线程可以重复拿到同一个资源的锁。重入锁非常有利于资源的高效利用。关于这点之后会做演示。
2. 单机锁 使用setnx()方法获得锁 用eval执行lua脚本删除锁 用lua脚本可以做到原子操作
详细使用: https://www.cnblogs.com/linjiqin/p/8003838.html redisson也支持单机部署,而且使用更简单
3. redisson锁 3.1 原理 Redisson是一个基于java编程框架netty进行扩展了的redis。 Redisson是架设在Redis基础上的一个Java驻内存数据网格（In-Memory Data Grid）。充分的利用了Redis键值数据库提供的一系列优势，基于Java实用工具包中常用接口，为使用者提供了一系列具有分布式特性的常用工具类。使得原本作为协调单机多线程并发程序的工具包获得了协调分布式多机多线程并发系统的能力，大大降低了设计和研发大规模分布式系统的难度。同时结合各富特色的分布式服务，更进一步简化了分布式环境中程序相互之间的协作其底层是实现Lock接口实现的。
加锁和解锁都是通过lua脚本来执行,
3.1.1 可重入加锁机制 对同一个锁,可以lock多次,对应的也需要unlock多次.
Redisson可以实现可重入加锁机制的原因，我觉得跟两点有关：
Redis存储锁的数据类型是 Hash类型 Hash数据类型的key值包含了当前线程信息。 具体值: 这里表面数据类型是Hash类型,Hash类型相当于我们java的 &amp;lt;keyName,&amp;lt;field,value&amp;raquo; 类型,
这里keyName是指 &amp;lsquo;redisson&amp;rsquo;，
field值它的组成是:uuid + 当前线程的ID,
value是重入次数
uuid是客户端实例化时就创建好了,它是客户端的标识
重入过程:
Redisson实现分布式锁(1)&amp;mdash;原理 - 雨点的名字 - 博客园 (cnblogs.com)
3.1.2 (watchDog)看门狗 在持有锁的时间内,业务没有执行完,怎么办?(应该继续拥有锁,知道业务执行完成),所以需要一个线程去监听.
原理: 额外启动一个线程,每隔10s检测业务是否完成,未完成则续期
使用: 不设置锁的失效时间,看门狗则自动生效
只有获得锁时没有指定失效时间,看门狗才会生效,默认失效时间为30s,
不过要注意,如果业务(比如数据库)出现死锁,导致看门狗一直续期,整个程序就会死锁,这种情况要好生处理,不要让业务出现死锁
https://blog.csdn.net/ice24for/article/details/86177152
https://www.oschina.net/question/1255119_2313008
3.1.3 加锁过程 先竞争锁,成功后设置看门狗,</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E7%AE%80%E4%BB%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E7%AE%80%E4%BB%8B/</guid><description>[toc]
常用参数说明 配置项 说明 daemonize no Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程（Windows 不支持守护线程的配置为 no ） pidfile /var/run/redis.pid 当 Redis 以守护进程方式运行时，Redis 默认会把 pid 写入 /var/run/redis.pid 文件，可以通过 pidfile 指定 port 6379 指定 Redis 监听端口，默认端口为 6379 bind 127.0.0.1 绑定的主机地址,默认只允许本地访问,将此配置删除,则允许外网访问(但是不建议,因为redis处理快,可能被暴力破解,而且以前也出现过redis漏洞) logfile stdout 日志记录方式，默认为标准输出，也可以写文件路径,如果配置 Redis 为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 /dev/null requirepass password 设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH 命令提供密码，默认关闭 https://www.runoob.com/redis/redis-conf.html
数据类型 Redis支持五种数据类型：
string（字符串）， hash（哈希）， list（列表）， set（集合） zset(sorted set：有序集合)。 String string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。
string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%A9%BF%E9%80%8F%E5%92%8C%E5%87%BB%E7%A9%BF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%A9%BF%E9%80%8F%E5%92%8C%E5%87%BB%E7%A9%BF/</guid><description>[toc]
缓存穿透 缓存穿透，是指频繁查询一个数据库一定不存在的数据,这种redis一般不会保存,这样redis被穿过了,直接去访问了数据库,这种穿透多了,数据库就压力大。正常的使用缓存流程大致是，数据查询先进行缓存查询，如果key不存在或者key已经过期，再对数据库进行查询，并把查询到的对象
缓存被穿透了,因为总是查询一些在redis中查不到的值
解决方案:
使用锁,将这次请求锁住,等数据库返回后才释放(分布式环境用分布式锁,单机用普通锁(synchronized等)) 接口限流与熔断、降级 布隆过滤器(java的guava包中默认误判率3%(最合适)) (将全部&amp;quot;可能&amp;quot;数据存到布隆过滤器中,当redis中找不到时,先经过布隆,如果过滤器说不存在,就不用访问数据库了,直接返回) 将这种key存到redis,设置较短的过期时间,比如1分钟/5分钟 缓存雪崩 缓存雪崩，是指在某一个时间段，缓存集中过期失效,大批量key失效,命中redis的概率就小了,只能去查数据库了.
其实集中过期，倒不是非常致命，比较致命的缓存雪崩，是缓存服务器某个节点宕机或断网。因为自然形成的缓存雪崩，一定是在某个时间段集中创建缓存，那么那个时候数据库能顶住压力，这个时候，数据库也是可以顶住压力的。无非就是对数据库产生周期性的压力而已。
解决方案:
也是像解决缓存穿透一样加锁排队
建立备份缓存, 采用哨兵或者集群 ,多起几个节点
给同时添加key时设置不同过期时间
事前：确保Redis本身的高可用性，数据恢复备份、主从架构+哨兵，Redis cluster，一旦主节点挂了，从节点跟上
事中：当redis不可用时，少量请求可以走缓存生产服务的本地缓存ehcache获取数据，基于hystrix对商品服务和redis操作做限流保护 配置降级、超时、熔断策略；从而保证发生缓存雪崩时缓存生产服务不会被拖死
事后：基于redis的数据备份，快速将redis重新跑起来对外提供服务
缓存击穿 缓存击穿，热点key在请求高峰期失效了，瞬间大量请求落到数据库，就像在一个屏障上凿开了一个洞。
解决方案:
本地缓存 + redis缓存的多级缓存 也是像解决缓存穿透一样加锁排队 链接:https://baijiahao.baidu.com/s?id=1619572269435584821&amp;amp;wfr=spider&amp;amp;for=pc
https://blog.csdn.net/fanrenxiang/article/details/80542580
如何解决redis缓存击穿问题_Sebastian Xia的博客-CSDN博客_redis缓存击穿怎么解决</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/redis/%E9%9D%A2%E8%AF%95/</guid><description>[toc]
Redis的单线程指什么 Redis 单线程主要指的是网络IO和键值对读写是由一个线程来完成的，Redis在处理客户端的请求的时候包括获取(Socket)，解析，执行，内容返回(Socket写)等由一个顺序串行的主线程处理，这即为单线程
但Redis的其它功能，比如持久化，异步删除，集群数据同步等等，都是由额外的线程执行的，是多线程
即Redis工作线程是单线程的，但是，整个Redis来说是多线程的
Redis单线程为什么很快 1、基于内存操作：Redis的所有数据都存在内存中，因此所有的运算都是内存级别
2、数据结构简单：Redis的数据结构简单，查找和操作时间大部分时间复杂度是O(1)
3、多路复用和非阻塞：Redis使用多路复用功能来监听多个Socket链接客服端，这样可以一个线程处理多个请求，减少线程切换带来的开销，同时也避免了IO阻塞操作
4、避免上下文切换：因为是单线程模型，因此避免了多线程切换和多线程竞争，减少时间和性能的消耗
Redis系列第一讲：Redis是单线程吗 - 掘金 (juejin.cn)
谈谈redis的对象机制（redisObject) Redis的5种基础数据类型，在底层是采用对象机制实现的。
Redis的每种对象其实都由对象结构(redisObject) 与 对应编码的数据结构组合而成，而每种对象类型对应若干编码方式，不同的编码方式所对应的底层数据结构是不同的。
redisObject 是 Redis 类型系统的核心, 数据库中的每个键、值, 以及 Redis 本身处理的参数, 都表示为这种数据类型。
Redis相关知识&amp;mdash;-对象机制_小舟~的博客-CSDN博客
redis 优化措施 避免大key的存在
设置合理的淘汰策略
内存尽可能大; 淘汰策略改为随机淘汰，随机淘汰比 LRU 要快很多（视业务情况调整）
谨慎fork操作带来的阻塞
控制 Redis 实例的内存：尽量在 10G 以下，执行 fork 的耗时与实例大小有关，实例越大，耗时越久
关闭操作系统的内存大页
应用在申请内存时,linux会给一个完整的内存页大小, 申请的多了, 分配内存也就大了 , 耗时也就长了
对于 Redis 这种对性能和延迟极其敏感的数据库来说，我们希望 Redis 在每次申请内存时，耗时尽量短
开启AOF后设置 重写时不刷盘
# AOF rewrite 期间，AOF 后台子线程不进行刷盘操作 # 相当于在这期间，临时把 appendfsync 设置为了 none no-appendfsync-on-rewrite yes 建议使用 固态硬盘, 加快数据落磁盘</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/</guid><description>[toc]
前言 RocketMQ为我们提供了事务消息的功能，它使得我们投放消息和其他的一些操作保持一个整体的原子性。比如：向数据库中插入数据，再向MQ中投放消息，把这两个动作作为一个原子性的操作。貌似其他的MQ是没有这种功能的。
RocketMQ系列（七）事务消息-阿里云开发者社区 (aliyun.com)
1. 事务消息实现思想 RocketMQ 事务消息的实现原理基于两阶段提交和定时事务状态回查来决定消息最终是提交还是回滚
RocketMQ实现事务消息主要分为两个阶段：正常事务的发送及提交、事务信息的补偿流程
整体流程为：
正常事务发送与提交阶段
生产者发送一个半消息给MQServer（半消息是指消费者暂时不能消费的消息）
服务端响应消息写入结果，半消息发送成功
开始执行本地事务
根据本地事务的执行状态执行Commit或者Rollback操作
总结: 先发送MQ,再操作数据库, MQ发送半消息, 等数据库操作完成后,再决定那条半消息是成功还是失败
事务信息的补偿流程
如果MQServer长时间没收到本地事务的执行状态会向生产者发起一个确认回查的操作请求
生产者收到确认回查请求后，检查本地事务的执行状态
根据检查后的结果执行Commit或者Rollback操作
补偿阶段主要是用于解决生产者在发送Commit或者Rollback操作时发生超时或失败的情况。
2. RocketMQ事务流程关键 事务消息在一阶段对用户不可见 事务消息相对普通消息最大的特点就是一阶段发送的消息对用户是不可见的，也就是说消费者不能直接消费。这里RocketMQ的实现方法是原消息的主题与消息消费队列，然后把主题改成 RMQ_SYS_TRANS_HALF_TOPIC (消息体中存储了原来的主题和队列id)，这样由于消费者没有订阅这个主题，所以不会被消费。
如何处理第二阶段的失败消息？
在本地事务执行完成后会向MQServer发送Commit或Rollback操作，此时如果在发送消息的时候生产者出故障了，那么要保证这条消息最终被消费，MQServer会像服务端发送回查请求，确认本地事务的执行状态。当然了rocketmq并不会无休止的的信息事务状态回查，默认回查15次, 每60s发一次，如果15次回查还是无法得知事务状态，RocketMQ默认回滚该消息。
消息状态 事务消息有三种状态： TransactionStatus.CommitTransaction：提交事务消息，消费者可以消费此消息 TransactionStatus.RollbackTransaction：回滚事务，它代表该消息将被删除，不允许被消费。 TransactionStatus.Unknown ：中间状态，它代表需要检查消息队列来确定状态。
配置文件 : org.apache.rocketmq.common.BrokerConfig
/** * The minimum time of the transactional message to be checked firstly, one message only exceed this time interval * that can be checked. */ @ImportantField private long transactionTimeOut = 6 * 1000; /** * The maximum number of times the message was checked, if exceed this value, this message will be discarded.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E4%BB%8B%E7%BB%8D/</guid><description>[toc]
1. 简介 RocketMQ是一个纯Java、分布式、队列模型的开源消息中间件，前身是MetaQ，是阿里参考Kafka特点研发的一个队列模型的消息中间件，后开源给apache基金会成为了apache的顶级开源项目，具有高性能、高可靠、高实时、分布式特点
2. 组成 他主要有四大核心组成部分：NameServer、Broker、Producer以及Consumer四部分。
RocketMQ架构上主要分为四部分，如上图所示:
Producer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳
Consumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳. Consumer既可以从Master订阅消息，也可以从Slave订阅消息. Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。
NameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。
​ 主要包括两个功能：
Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；
路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。
NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。
BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。
Remoting Module：整个Broker的实体，负责处理来自clients端的请求。
Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息
Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。
HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。
Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。
集群工作流程：
启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。
Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。
收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。
如果是自动创建主题, 则会沿用默认主题TBW102的配置,它在哪,自动创建的主题就在哪
RocketMQ自动创建topic - 简书 (jianshu.com)
Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。
Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。
NameServer：
主要负责对于源数据的管理，包括了对于Topic和路由信息的管理。
NameServer压力不会太大，平时主要开销是在维持心跳和提供Topic-Broker的关系数据。
但有一点需要注意，Broker向NameServer发心跳时， 会带上当前自己所负责的所有Topic信息，如果Topic个数太多（万级别），会导致一次心跳中，就Topic的数据就几十M，网络情况差的话， 网络传输失败，心跳失败，导致NameServer误认为Broker心跳失败。
NameServer 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。
每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2/</guid><description/></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E9%9D%A2%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E9%9D%A2%E8%AF%95/</guid><description>[toc]
RocketMQ如何保证消息不丢失？ Producer端如何保证消息不丢失 采取send()「同步发消息」，发送结果是同步感知的。 发送失败后可以**「重试」**，设置重试次数。默认3次。 producer.setRetryTimesWhenSendFailed(10); 「集群部署」，比如发送失败了的原因可能是当前Broker宕机了，重试的时候会发送到其他Broker上。 Broker端如何保证消息不丢失 修改刷盘策略为同步刷盘。默认情况下是异步刷盘的。 flushDiskType = SYNC_FLUSH 集群部署，主从模式，高可用。 Consumer端如何保证消息不丢失 完全消费正常后在进行手动ack确认。 RocketMQ如果订阅关系不一致会怎样？ 订阅关系一致指的是同一个消费者Group ID下所有Consumer实例所订阅的Topic、Tag必须完全一致。如果订阅关系不一致，消息消费的逻辑就会混乱，甚至导致消息丢失。问题如下:
第一个问题: 订阅以集群为单位，如果集群中消费者1订阅了topicA， 消费者2订阅topicB，那么会覆盖，二者只有一个成功订阅。
更新订阅信息时，订阅信息是按照消费组存放的，这步骤就会导致同一个消费组内的各个消费者客户端的订阅信息相互被覆盖。
第二个问题: 就是集群中会进行分摊，比如说消费者1 订阅了topicA ，来了100个topicA的消息， 也会分50个给消费者2 。
因为在同一个消费组中, 所以会分给其他消费者, 但是其他消费者又没订阅, 所以会消费失败, 进而消息重试
kafka就没有这个毛病, 因为kafka在消费架构上简单, 不像RocketMQ一样有 消费组 和 集群/广播消费 的维度
RocketMQ 可以在同一个消费组中 设置集群消费还是广播消费, (当然组不同的话, 也能实现广播消费)
kafka只有消费组, 而组里的消费者只能是竞争关系(一个分区只能被一个消费者消费,对应 RocketMQ的集群消费), 想要 广播消费 就用不同的组来消费
订阅关系一致 (alibabacloud.com)
RocketMQ的订阅关系(topic tag和GID)
RocketMQ为什么要保证订阅关系的一致性？ - 简书 (jianshu.com)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E9%AB%98%E6%80%A7%E8%83%BD%E4%BF%9D%E8%AF%81/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/rocketmq/%E9%AB%98%E6%80%A7%E8%83%BD%E4%BF%9D%E8%AF%81/</guid><description>[toc]
1. 整体架构 RocketMQ 主要由 Broker、NameServer、Producer 和 Consumer 组成的一个集群。
NameServer：整个集群的注册中心和配置中心，管理集群的元数据。包括 Topic 信息和路由信息、Producer 和 Consumer 的客户端注册信息、Broker 的注册信息。 Broker：负责接收消息的生产和消费请求，并进行消息的持久化和消息的读取。 Producer：负责生产消息。 Consumer：负责消费消息。 broker有很多台,可以分散存储和被访问的压力, 且每个broker可以存在多个slave,当消费者从slave获取消息时,还能分担broker压力,进一步提高消费性能
生产和消费直连nameServer,可以快速定位到主题在哪个broker上,而且broker上还有这些数据(meta)的缓存, 甚至连nameServer的访问都省略了
消费集群可以增加数量, 以增加消费性能
1.1. 网络模型 RocketMQ 使用 Netty 框架实现高性能的网络传输。
Netty 的高性能传输的体现
非阻塞 IO Ractor 线程模型 零拷贝。使用 FileChannel.transfer 避免在用户态和内核态之间的拷贝操作；通过 CompositeByteBuf 组合多个 ByteBuffer；通过 slice 获取 ByteBuffer 的切片；通过 wrapper 把普通 ByteBuffer 封装成 netty.ByteBuffer。 RocketMQ 网络模型
RocketMQ 的 Broker 端基于 Netty 实现了主从 Reactor 模型。架构如下：
具体流程：
eventLoopGroupBoss 作为 acceptor 负责接收客户端的连接请求 eventLoopGroupSelector 负责 NIO 的读写操作 NettyServerHandler 读取 IO 数据，并对消息头进行解析 disatch 过程根据注册的消息 code 和 processsor 把不同的事件分发给不同的线程。由 processTable 维护（类型为 HashMap） 1.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/javaee/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/javaee/</guid><description>[toc]
1. java 历程 java8:增加lambda,流 java9:增加module系统(要用什么先声明) java10: 引入var,局部变量类型判断,不能用于方法上的参数 java11: 允许在lambda中使用var,增加了ZGC,(长期支持版本) java12: swtch增加,可以写表达式,可以(用break)写返回值 java13: swtch改变,用yield 返参,break不返,可以使用&amp;quot;&amp;ldquo;&amp;ldquo;code&amp;rdquo;&amp;ldquo;&amp;ldquo;&amp;ldquo;包含代码块(和py一样) 2. 网站如何提高并发量? 答:无法容纳高访问,是因为无法快速处理请求,大量请求被堆积,导致服务器崩溃.
目前最耗时的是对数据库的访问,所有优化sql是必要的.
还可以用各种缓存(Redis),各种服务器(CDN,文件服务器),各种分布式操作,加快对请求的处理
3. 对象深拷贝与浅拷贝的区别 浅拷贝(影子克隆):只复制对象的基本类型,而对象类型仍属于原来的引用. 深拷贝(深度克隆):不仅复制对象的基本类型,同时也复制原对象中的对象.就是说完全是新对象产生的 4. multipart/form-data 与 application/x-www-form-urlencoded x-www-form-urlencoded
传递数据是文本格式,form表单提交 默认就是这种格式
form-data
一般用于图片,视频等数据,传递数据使用二进制流传递,所以后台是不能接收到值的,所以需要用方面的类来处理,在jfw-front中,引入了文件上传的包,spring就自动解析了,所以jfw-front这两种格式都支持,当然也可以用特定方式来取form-data格式的值
&amp;lt;!-- 上传文件拦截，设置最大上传文件大小 10M=10*1024*1024(B)=10485760 bytes --&amp;gt; &amp;lt;bean id=&amp;#34;multipartResolver&amp;#34; class=&amp;#34;org.springframework.web.multipart.commons.CommonsMultipartResolver&amp;#34;&amp;gt; &amp;lt;property name=&amp;#34;maxUploadSize&amp;#34; value=&amp;#34;10485760&amp;#34; /&amp;gt; &amp;lt;/bean&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-web&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.1.0.RELEASE&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; 这些格式是用于 http请求或者响应, 用于指明数据的格式, 称为 内容类型 (Content-Type)
Content-Type 详解_leoss.H的博客-CSDN博客_content-type
5. jwt JSON Web Token（缩写 JWT）是目前最流行的跨域认证解决方案
JWT 的原理是，服务器认证以后，生成一个 JSON 对象(唯一标识(用户id),到期时间,其他信息等)，发回给用户.
以后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名（详见后文）。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/java%E5%9F%BA%E7%A1%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/java%E5%9F%BA%E7%A1%80/</guid><description>1.java是值传递还是引用传递 ​ 答: 全部均为值传递!!!!
以前我认为基本类型是值传递,而对象是引用传递,这是错误的!!!!
有时候我会觉得传对象时里面的属性值变化了,
(瞎猜:)其实是属性值的地址变化了,值也就改变了,而原来的值还在,指向原来的地址,只是没人用而已(等待被回收)
来自* &amp;lt;https://blog.csdn.net/bjweimengshu/article/details/79799485&amp;gt;
2.尾递归 有一种特殊的递归方式叫尾递归。如果函数中的递归调用都是尾调用，则该函数是尾递归函数。尾递归的特性使得递归调用不需要额外的空间 . 不能有任何操作, 如果有f(n)+1这类的操作,都不算尾递归(因为还是保存了函数的返回值)
来自:https://www.ibm.com/developerworks/cn/java/j-understanding-functional-programming-2/index.html?ca=drs-#icomments
https://www.cnblogs.com/bellkosmos/p/5280619.html
// 实现斐波那契数列 // 普通递归 int FibonacciRecursive(int n) { if( n &amp;lt; 2) return n; return FibonacciRecursive(n-1)+FibonacciRecursive(n-2); } // 尾递归 int FibonacciTailRecursive(int n,int ret1,int ret2) { if(n==0) return ret1; return FibonacciTailRecursive(n-1,ret2,ret1+ret2); } 来自: https://blog.csdn.net/mengxiangjia_linxi/article/details/78158819
尾递归效率高的原理:
尾递归就是从最后开始计算, 每递归一次就算出相应的结果, 也就是说, 函数调用出现在调用者函数的尾部, 因为是尾部, 所以根本没有必要去保存任何局部变量. 直接让被调用的函数返回时越过调用者, 返回到调用者的调用者去。精髓：尾递归就是把当前的运算结果（或路径）放在参数里传给下层函数 , 也不用开辟新的栈空间,直接用上一个栈
尾递归优化得益于编译器的支持,恰巧java不支持尾递归优化,但是上面那种省栈省开销还是有的(应该吧).一般函数式语言都是支持的,比如scala
但是可以利用lambda的懒加载来实现尾递归优化,
详情: https://www.cnblogs.com/invoker-/p/7723420.html#autoid-3-0-0
3. java中的CAS和AQS CAS : Conmpare And Swap (比较和交换) 是用于实现多线程同步的原子指令。 它将内存位置的内容与给(期望)定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。 这是作为单个原子操作完成的。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/netty/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/netty/</guid><description>Netty是由JBOSS提供的一个java开源框架，现为 Github上的独立项目。Netty提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序。 也就是说，Netty 是一个基于NIO的客户、服务器端的编程框架，使用Netty 可以确保你快速和简单的开发出一个网络应用，例如实现了某种协议的客户、服务端应用。Netty相当于简化和流线化了网络应用的编程开发过程，例如：基于TCP和UDP的socket服务开发。
简单的说,可以用来做网络编程,取代websocket, 其底层使用NIO(new IO:同步非阻塞IO, 这玩意听说贼难)
netty能够受到青睐的原因有三：
并发高 传输快 封装好 并发高是因为非阻塞接收,使用多线程接收
传输快是因为 使用了零拷贝
封装好是因为 封装了NIO,让网络编程变得简单(但还是太难&amp;hellip;)
来源: https://www.jianshu.com/p/b9f3f6a16911
NIO的零拷贝和kafka的零拷贝应该是一样的,零拷贝其实是分类型的,总的来说是减少了数据拷贝的次数
零拷贝分类:https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E5%BE%AE%E6%9C%8D%E5%8A%A1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E5%BE%AE%E6%9C%8D%E5%8A%A1/</guid><description>1.RPC: RPC（Remote Procedure Call）—
远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。
客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。
运行时,一次客户机对服务器的RPC调用,其内部操作大致有如下十步：
1.调用客户端句柄；执行传送参数
2.调用本地系统内核发送网络消息
3.消息传送到远程主机
4.服务器句柄得到消息并取得参数
5.执行远程过程
6.执行的过程将结果返回服务器句柄
7.服务器句柄返回结果，调用远程系统内核
8.消息传回本地主机
9.客户句柄由内核接收消息
10.客户接收句柄返回的数据
来自链接&amp;gt;
2.Hessian:
Hessian是一个轻量级的remoting onhttp工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。
Dubbo大多使用的是Hessian协议
来自* &amp;lt;https://baike.baidu.com/item/Hessian&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E6%9E%B6%E6%9E%84/saas%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9A%94%E7%A6%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E6%9E%B6%E6%9E%84/saas%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9A%94%E7%A6%BB/</guid><description>[TOC]
1. 每个租户提供独立的数据库系统 这种方案的实现方式是所有租户共享同一个应用，但应用后端会连接多个数据库系统，一个租户单独使用一个数据库系统。这种方案的用户数据隔离级别最高，安全性最好，租户间的数据能够实现物理隔离。但成本较高。
2. 每个租户提供独立的表空间 这种方案的实现方式，就是所有租户共享同一个应用，应用后端只连接一个数据库系统，所有租户共享这个数据库系统，每个租户在数据库系统中拥有一个独立的表空间。
3. 按租户id字段区分租户 这种方案是多租户方案中最简单的设计方式，即在每张表中都添加一个用于区分租户的字段（如租户id或租户代码）来标识每条数据属于哪个租户，其作用很像外键。当进行查询的时候每条语句都要添加该字段作为过滤条件，其特点是所有租户的数据全都存放在同一个表中，数据的隔离性是最低的，完全是通过字段来区分的。
三种数据隔离方案的优劣势分析 隔离方案 成本 支持租户数量 优点 不足 独立数据库系统 高 少 隔离级别最高，安全性最好，能够满足不同租户的独特需求，出现故障时恢复数据比较容易 维护成本和购置成本高, 数据收集分析成本高 共享数据库，独立表空间 中 较多 提供了一定程度的逻辑数据隔离，一个数据库系统可支持多个租户 统一数据库支持的Schema有限, 夸租户统计数据复杂 按租户id字段区分 低 非常多 维护和购置成本最低，每个数据库能够支持的租户数量最多 隔离级别最低，安全性也最低，数据备份和恢复非常复杂，需要逐表逐条备份和还原 saas系统多租户数据隔离的实现（一）数据隔离方案 - 湖畔清茶杨柳飘 - 博客园 (cnblogs.com)
SAAS服务多租户数据隔离架构 - 知乎 (zhihu.com)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E6%9E%B6%E6%9E%84/soa%E6%9E%B6%E6%9E%84%E5%92%8C%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E7%9A%84%E5%8C%BA%E5%88%AB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E6%9E%B6%E6%9E%84/soa%E6%9E%B6%E6%9E%84%E5%92%8C%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E7%9A%84%E5%8C%BA%E5%88%AB/</guid><description>[toc]
SOA Service-Oriented Architecture 面向服务的架构
SOA（面向服务的架构）定义了一种可通过服务接口复用软件组件的方法。 此类接口会使用通用的通信标准，这些标准能够快速合并到新应用程序中，而不必每次都执行深度集成。
SOA架构和微服务架构的区别 首先SOA和微服务架构一个层面的东西，而对于ESB和微服务网关是一个层面的东西，一个谈到是架构风格和方法，一个谈的是实现工具或组件。
SOA（Service Oriented Architecture）“面向服务的架构”:他是一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。一个服务 通常以独立的形式存在与操作系统进程中。各个服务之间 通过网络调用。
微服务架构:其实和 SOA 架构类似,微服务是在 SOA 上做的升华，微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。
微服务架构 = 80%的SOA服务架构思想 + 100%的组件化架构思想 + 80%的领域建模思想
原文链接：https://blog.csdn.net/zpoison/java/article/details/80729052
很多人说微服务是 SOA 的延续，都强调松耦合，只是 SOA 高度依赖服务总线（ESB），而微服务不需要。
从理论上讲，您可以在不使用 ESB 的情况下实施 SOA，但每个应用程序所有者都必须通过自己独有的方式来公开服务接口，这就需要完成大量工作（即使接口最终可复用也不例外），并且在未来会带来巨大的维护挑战。 实际上，ESB 最终被认为是所有 SOA 实施都包含的事实元素，以致于有时将这两个术语作为同义词使用，从而造成了混乱。
https://www.zhihu.com/question/37808426/answer/536875376
捷顺的天启系统更像是SOA系统,其划分不完全隔离,主要体现在数据库的不分离,存在公共业务,天启和其他业务系统一起更像SOA,天启平台像那个总线
什么是 SOA（面向服务的架构）？ - 中国 | IBM</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E7%A7%92%E6%9D%80/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/java/%E7%A7%92%E6%9D%80/</guid><description>秒杀系统注意点:
严格防止超卖：库存100件你卖了120件，凉凉 事物性: 用户的金额/优惠券,库存要保持一致 防止黑产：防止不怀好意的人群通过各种技术手段把你本该下发给群众的利益全收入了囊中。 保证用户体验：高并发下，别网页打不开了，支付不成功了，购物车进不去了 抗住并发量,使用CDN节点,减轻前端压力
尽量把流量控制在上游,因为抢单都是疯狂点击的或者直接爬虫类,这样对系统来说,很多都是重复的请求,徒增系统压力
对于前端层,界面操作的,前端可以控制下来许多,禁止重点,或者多次点击只发生一次
对于网关层,对url做去重处理,用一些关键参数(如用户id)做hash,可利用布隆过滤器增加效率; 也可以做页面缓存,对同一个ip段数量多的返回同一个页面; 加上熔断机制,对于严重超额的请求直接拒绝
对于服务层,请求不直接进入数据层,先经过缓存,确认当前数据,再经过消息队列,一批一批的下放到数据层
对于数据层, 采用分库分表,对热门区域/热门商品等维度进行拆分
事物保证, 可以利用redis单线程速度快的特性来控制库存等问题; 可以用消息队列来保证事物的最终一致性</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E4%BB%A3%E7%A0%81%E7%B1%BB/java%E6%93%8D%E4%BD%9Cword%E6%96%87%E6%A1%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E4%BB%A3%E7%A0%81%E7%B1%BB/java%E6%93%8D%E4%BD%9Cword%E6%96%87%E6%A1%A3/</guid><description>介绍: 该代码支持:
填充work中的占位符(仅是填充,不能动态加格式之类) 将work转化为html 使用 pom.xml
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.poi&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;poi-ooxml&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${poi.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.poi&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;poi-ooxml-schemas&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${poi.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.poi&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;ooxml-schemas&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.4&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.poi&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;poi-scratchpad&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${poi.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;fr.opensagres.xdocreport&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;xdocreport&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.0.6&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; wordUtils
public static HWPFDocument replaceTables2003(String filePath, Map&amp;lt;String, Object&amp;gt; map) throws Exception { //logger.info(&amp;#34;替换word2003文档表格内容，文档路径:{}，要替换的内容:{}&amp;#34;, filePath, JsonUtil.toJSONString(map)); if (StringUtils.isBlank(filePath) || MapUtils.isEmpty(map)) { return null; } try (FileInputStream is = new FileInputStream(filePath)) { HWPFDocument document = new HWPFDocument(is); Range range = document.getRange(); for (Entry&amp;lt;String, Object&amp;gt; e : map.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/</guid><description>hive on spark/yarn使用与区别 使用:
默认是使用yarn,通过设置 配置文件或者启动时设置可以指定spark, 但使用spark计算引擎很麻烦,要对应版本的spark,且不含hive jar包
由于官方的spark都自带hive jar包,所以需要自己编码
是不是cdh不需要?待检验
https://www.jianshu.com/p/339da2b6d480
区别:
其本质是替换了计算引擎,一个基于MapReduce,一个基于spark,于此他们计算/资源等等方式都有所不同
小文件的场景,危害,怎么处理 场景:
接收的就是小文件,需要自行处理 自己的代码造成了很多小文件 危害:
占用过多空间(一个块存不满也占用那么多)
文件越多,map也多,划不来
处理:
总得来说合并小文件
通过参数方式,再写入一次做合并
存储时指定可以压缩的文本格式(sequenceFile),还可以指定压缩格式
对于不常用的数据,可以使用hadoop的archive归档
详情见hive的&amp;laquo;小文件&amp;raquo;
Hive中order by，sort by，distribute by，cluster by的区别 一：order by
order by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，然而只有一个Reducer，会导致当输入规模较大时，消耗较长的计算时间。关于order by的详细介绍请参考这篇文章：Hive Order by操作。
二：sort by
sort by不是全局排序，其在数据进入reducer前完成排序，因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&amp;gt;1，则sort by只会保证每个reducer的输出有序，并不保证全局有序。sort by不同于order by，它不受hive.mapred.mode属性的影响，sort by的数据只能保证在同一个reduce中的数据可以按指定字段排序。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定)，对输出的数据再执行归并排序，即可得到全部结果。
三：distribute by
distribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。
注：Distribute by和sort by的使用场景
1.Map输出的文件大小不均。
2.Reduce输出文件大小不均。
3.小文件过多。
4.文件超大。
四：cluster by
cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒叙排序，不能指定排序规则为ASC或者DESC。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/spark/</guid><description>[toc]
1. Mesos: 一个资源统一管理和调度平台,类似YARN,个人感觉不如YARN好
出自:http://dongxicheng.org/mapreduce-nextgen/mesos_vs_yarn/
2. DGA: 在spark里每一个操作生成一个RDD，RDD之间连一条边，最后这些RDD和他们之间的边组成一个有向无环图，这个就是DAG。(一般来说,生成新的RDD后,旧的不会使用,会等着被JVM回收)
来自 &amp;lt;http://blog.csdn.net/sinat_31726559/article/details/51738155&amp;gt; 3. yarn和spark: spark可以运行在yarn上,做资源管理,但是spark有自己的资源管理平台(可以深究,TaskScheduler)
4. Spark core: Spark Core实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中还包含了对弹性分布式数据集（resilient distributed dataset，简称RDD）的API定义。 Spark Core提供了创建和操作这些集合的多个API。
来自 &amp;lt;https://www.douban.com/note/536766108/?from=tag&amp;gt; 5. RDD类型 6. PageRank算法 7. Spark Shuffle 以reduceByKey为例,涉及按key对于RDD成员进行重组。将具有相同key但分布在不同节点上的成员聚会在一个节点上,以便对他们的value进行操作,这个重组过程就是shuffle操作。因为shuffle操作会涉及数据的传输，所以成本特别高，而且过程复杂。
简单的说就是需要操作所有数据时,整合数据的过程叫shuffle
8. RDD不可变,那RDD会不会存在回收? RDD 使用的是java内存,每生成一个RDD,就生成了一个对象(即使不接收),当RDD不再使用时(可能是标记法吧),又JVM回收
9. 为什么Flink比spark快? Flink所用事物类型更底层,在运行时就经过了优化 flink提供了基于每个事件的流式处理机制,而spark是用小批量来模拟流式，也就是多个事件的集合 以上只是简述,并非完全.来自: https://www.jianshu.com/p/905ca3a7edb9
10.checkpoint checkpoint在spark中主要有两块应用：
一块是在spark core中对RDD做checkpoint，可以切断做checkpoint RDD的依赖关系，将RDD数据保存到可靠存储（如HDFS）以便数据恢复；
另外一块是应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理（如之前waiting batch的job会在重启后继续处理）。
注: checkpoint着重的是DAG图,会保存当前RDD,比如:RDD经过很多转换,最后才触发action;
cache着重的是数据,比如:需要重复用到该数据 https://www.cnblogs.com/superhedantou/p/9004820.html https://blog.csdn.net/j904538808/article/details/80104525
11.SparkSQL 和 impala SparkSQL优势是能做业务处理,因为是RDD,能和其他RDD无缝连接,
impala优势是查询速度够快,(感觉能用web系统的操作)
12.springStream中的 updatestateByKey函数和Mapwithstate函数 SparkStreaming 7*24 小时不间断的运行，有时需要管理一些状态，比如wordCount，每个batch的数据不是独立的而是需要累加的，这时就需要sparkStreaming来维护一些状态(这些状态可以是任意类型)，目前有两种方案updateStateByKey和mapWithState，(后者性能比前者好)
updateStateByKey
ssc.checkpoint(&amp;#34;.&amp;#34;) // 需打开checkpiont def updateFunction(currValues:Seq[Int],preValue:Option[Int]): Option[Int] = { val currValueSum = currValues.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8C%BA%E5%9D%97%E9%93%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%8C%BA%E5%9D%97%E9%93%BE/</guid><description>**1.**为什么说比特币是高度透明和隐秘的?使用比特币交易后可不可以查看交易双方?通过比特币地址为什么不能查找到人?
答:(1) 透明是说所有的交易信息每人都能查看,隐秘是说:交易双方的信息都是加密的,
​ (2)可以查看,只能查看最近的交易记录,双方的标识id会变化(加密)
(3)区块链技术作为比特币的底层支撑，可以清晰的记录并查到所有比特币地址、支付以及交易路径，通过分析交易模式，追查到资金的走向以及公钥背后实际的当事人是很有可能的。黑客如果不会洗钱(币),钱就等于是死钱了
来自 http://www.wanbizu.com/news/201705229806.html ,
**2.**每产生新的比特币,新的比特币会记录以前的记录吗?如果记录,随着记录的增多,以后产生的比特币会不会疲于更新?如果不同步,也就说明区块链的内容不一样?
答:(1)当前块只会记录(从生产开始算)10分钟内的交易内容
(2)内容的确不一样,但是都是承接的
&amp;ldquo;可以这么理解，比特币系统是一个巨大的、不断更新的账本。每一页都叫做一个区块，按照时间顺序连起来，就叫做比特币的区块链。每10分钟新增一个区块，里面的内容是过去10分钟系统内发生的一些交易。每一笔交易都会完完整整记录在这个账本里，比特币就是账本里记录的钱&amp;rdquo;
来自* &amp;lt;http://tieba.baidu.com/p/5606568292&amp;gt;
3.51%算力攻击
答:所谓51%攻击，就是利用比特币使用算力作为竞争条件的特点，使用算力优势撤销自己已经发生的付款交易。如果有人掌握了50%以上的算力，他能够比其他人更快地找到开采区块需要的那个随机数，因此他实际上拥有了绝对哪个一区块的有效权利
来自* &amp;lt;http://8btc.com/article-1949-1.html&amp;gt;
交易生效是需要下一个块生成才确定的,如果有人掌握50%算力,就有能力算出另一个(并列)块,因为算力够强,继续生成下一块,而区块链会选择最长的链作为主链,从而虚假块成为主链,真正的交易块被遗弃
但是基本拥有51%算力基本不可能,目前算力是 236万万亿次哈希碰撞每秒 ,而世界第一的神威太湖之光 仅9.3亿亿次每秒,
**4.**比特币同时多处交易,怎么保证数据的同步?(等6个块)
答:每次交易只部分块记录(块会记录块产生后的10分钟内的所有交易记录),发生一次就在块中记录一次,然后hash不停在计算,更新后面的块,等6个块就完全确定了交易
**5.**如果比特币只记录10分钟内的记录,那就意味着有些块记录特别少,有些块记录特别多?
答: 是先有交易才会有块,等达到一定的交易量时才会被矿工拿着这些交易记录去挖矿,所以交易记录不会有重复
一次交易流程大致如此:
1.产生新交易: 我产生一个交易A
2.签名加密: 验证这个交易是不是我发起的,钱是不是我的
3.交易在比特币网络中传播: 验证完成后,要别人验证,是不是我的钱,钱够不够之类的
4.整合交易&amp;amp;构建新区块: 验证交易后，每个比特币网络节点会将这些交易添加到自己的内存池中，内存池也称作交易池，用来暂存尚未被加入到区块的交易记录。而挖矿节点除了收集和验证交易以外，还会将这些交易打包到一个候选的区块中,会把交易A连同其它一些近期被创建的交易整合,打包.挖矿节点需要为内存池中的每笔交易分配一个优先级，并选择较高优先级的交易记录来构建候选区块，在区块被填满后，内存池中的剩余交易会成为下一个区块的候选交易。然后挖矿节点就准备拿候选区块来挖矿
5.挖矿: 猜测一个数值(nonce),进行计算(相当复杂的),猜出一个小于nonce值就是正确的hash,即正确的块,获得&amp;quot;记账权&amp;quot;,那些交易记录将存在这个块中,节点(矿工)将获得奖励(网络中还有比特币时奖励比特币和交易费,没有比特币就奖励交易费)
6.新区块连接到区块链:比特币交易生命周期的最后一步是将新区块连接至有最大工作量证明的链中。一个节点一旦验证了一个新的区块，它将尝试将新的区块连接到到现存的区块链组装起来。
我有一个交易记录A,交易 A 会被一个或者多个签名加密(这些签名用来说明交易 A 是我发起的,不是别人)。而后，交易 A 被广播到比特币网络中，最快收到广播信息的是相邻的2- 3 个节点，这些节点都会参与验证这笔交易，于此同时将交易在网络中再次进行广播，直到这笔交易 A 被网络中大多数节点(所有下载比特币客户端的设备都有可能成为这样的节点)接收,最终，交易 A 被一个正在参与挖矿的节点验证，交易 A 连同其它一些近期被创建的交易一起被打包到一个区块 B ()中，并被添加到区块链上，这时整个区块链就被延长并新增了一个区块 B 。区块 B 获得 6 次以上的“确认”时就被认为是不可撤销的，
来自* &amp;lt;http://www.yixieshi.com/110742.html&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%BB%BC%E5%90%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%BB%BC%E5%90%88/</guid><description>[toc]
1. Exactly-once 字面意思是&amp;quot;精确一次&amp;quot;;
只要在flink和kafka中出现,描述一下kafka中的含义:
kafka有生产和消费,同一个消息有生产的次数和消费的次数,正确来说,应该都只有一次才对,但是在某些情况下由于各种原因导致出现不止一次(因为网络问题导致同一消息生产多次或者消费多次),这就出现了解决++幂等性++的问题,在kafka的高端应用中就变得格外重要(所以总有文章提到)
2.Parquet列式存储格式 parquet只是一种存储格式，与上层语言无关(数据不是完整可看性)
把IO只给查询需要用到的数据，只加载需要被计算的列 列式的压缩效果更好，节省空间 适配通用性 存储空间优化 计算时间优化 一个Parquet文件是由一个header以及一个或多个block块组成，以一个footer结尾。
header中只包含一个4个字节的数字PAR1用来识别整个Parquet文件格式。
文件中所有的metadata都存在于footer中。footer中的metadata包含了格式的版本信息，schema信息、key-value paris以及所有block中的metadata信息。
footer中最后两个字段为一个以4个字节长度的footer的metadata,以及同header中包含的一样的PAR1。
在Parquet文件中，每一个block都具有一组Row group,它们是由一组Column chunk组成的列数据。继续往下，每一个column chunk中又包含了它具有的pages。每个page就包含了来自于相同列的值
常用parquet文件读写的几种方式:
用spark的hadoopFile api读取hive中的parquet格式 用sparkSql读写hive中的parquet 用新旧MapReduce读写parquet格式文件 https://www.cnblogs.com/tonglin0325/p/10244676.html https://parquet.apache.org/documentation/latest/ https://www.cnblogs.com/windliu/p/10942252.html</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/eclipse/%E4%BD%BF%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/eclipse/%E4%BD%BF%E7%94%A8/</guid><description>[TOC]
一. 添加反编译插件 关键东西:
jd-eclipse http://jd.benow.ca/jd-eclipse/update 二. 不让控制台总是弹出来 window -&amp;gt; preferences -&amp;gt; run/debug -&amp;gt; console 在右边面板去掉
&amp;#34;Show when program writest to standard out&amp;#34;和 &amp;#34;Show when program writes to standard error&amp;#34; 两个多选框，然后重启Eclipse。
或者:
直接选择下图中的按钮,第一个选中时表示正常输出时展现控制台(看英文), 第二个是错误输出时展示
来自 https://blog.csdn.net/xingxiupaioxue/article/details/46647433</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/eclipse/%E9%94%99%E8%AF%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/eclipse/%E9%94%99%E8%AF%AF/</guid><description>[TOC]
一.tomcat找不到web项目(未成功): 新下载的Eclipse, 将项目导入后安装tomcat后部署项目出现There are no resources that can be added or removed from the server. 在网上查来很多方法，说在Properties&amp;ndash;project facets中添加Dynamic Web Module。 但是在我的project中没有Dynamic Web Module,于是又在网上查找相关解决方案，如修改.setting中的文件和.project、 .classpath等方法均无效。 最终在Help&amp;ndash;install new software 中的 Work with 输入Eclipse Repository - http://download.eclipse.org/releases/neon
（最后的neon为Eclipse版本） &amp;ndash; Web, XML, Java EE and OSGi Enterprise Development 下 安装有关JAVA EE的插件 （具体名称没有记录）之后在project facets 中就会出现Dynamic Web Module 了。 原文：&amp;lt; https://blog.csdn.net/TnkTechSHL/article/details/81259798&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/idea/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/idea/%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/</guid><description>在启动的配置参数上加
-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=${debug_port}
其中${debug_port}是用户自定义的，为debug端口,不能是被用的端口(自己用个新的)
打开idea，在右上角点击edit configurations,进去之后点击+号，选择remote，host处填写远程服务器的iP,端口填写debug 端口，如果包含多个module，可以执行要运行的module的名字，然后点击apply按钮。
在name那里给配置起一个名字：本地debug
然后debug模式启动,然后打上断点()
在远程服务的界面上点击相应操作,就可以在本地debug了</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</guid><description>1：sublime下载安装
​ https://www.sublimetext.com/3
2:安装插件Package Control(注意版本问题 2/3)
​ crtl+` 调出shell，粘贴下列代码：
import urllib.request,os,hashlib; h = &amp;#39;6f4c264a24d933ce70df5dedcf1dcaee&amp;#39; + &amp;#39;ebe013ee18cced0ef93d5f746d80ef60&amp;#39;; pf = &amp;#39;Package Control.sublime-package&amp;#39;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &amp;#39;http://packagecontrol.io/&amp;#39; + pf.replace(&amp;#39; &amp;#39;, &amp;#39;%20&amp;#39;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&amp;#39;Error validating download (got %s instead of %s), please try manual install&amp;#39; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &amp;#39;wb&amp;#39; ).write(by) 3:退出重启sublime
在菜单-&amp;gt;preferences-&amp;gt;Package Settings和package control选项，就说明安装package control成功了
4: crtl+shift+p： 输入install ，分别安装以下插件(存在无法下载错误)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/%E9%94%99%E8%AF%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%B7%A5%E5%85%B7%E7%AF%87/python%E6%90%AD%E5%BB%BAidle%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/%E9%94%99%E8%AF%AF/</guid><description>搭建环境的第4步中,会有错误,如图:
原因:是因为要访问的网站在国外,特别慢
解决:
方案一: 在hosts文件中加入如下代码:
​ 50.116.34.243 sublime.wbond.net
方案二:
1. 下载这个网站的内容并保存到本地: https://packagecontrol.io/channel_v3.json注意sublime的版本,这个文件默认是3.0.0关键字为: schema_version版本:2.0 2. 打开配置文件,(添加下好的文件路径)Preferences &amp;gt; Package Settings &amp;gt; Package Control &amp;gt; Settings - User添加如下内容(注意文本格式):&amp;quot;channels&amp;quot;: [ &amp;quot;E:/software/sublime/channel_v3.json&amp;quot; ],</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8/openresty/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8/openresty/</guid><description>OpenResty(又称：ngx_openresty) 是一个基于 NGINX 的可伸缩的 Web 平台，由中国人章亦春发起，提供了很多高质量的第三方模块。
OpenResty 是一个强大的 Web 应用服务器，Web 开发人员可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块,更主要的是在性能方面，OpenResty可以 快速构造出足以胜任 10K 以上并发连接响应的超高性能 Web 应用系统。
360，UPYUN，阿里云，新浪，腾讯网，去哪儿网，酷狗音乐等都是 OpenResty 的深度用户。
(我目前还是nginx都还不会用,哪懂写模块&amp;hellip;.)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8/tomcat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9C%8D%E5%8A%A1%E5%99%A8/tomcat/</guid><description>关于Https: https=ssl+http
证书:
SSL证书需要向国际公认的证书证书认证机构（简称CA，Certificate Authority）申请。 CA机构颁发的证书有3种类型：
域名型SSL证书（DV SSL）：信任等级普通，只需验证网站的真实性便可颁发证书保护网站； 企业型SSL证书（OV SSL）：信任等级强，须要验证企业的身份，审核严格，安全性更高； 增强型SSL证书（EV SSL）：信任等级最高，一般用于银行证券等金融机构，审核严格，安全性最高，同时可以激活绿色网址栏。 来自 https://www.zhihu.com/question/19578422
也有免费的证书,比如腾讯云的DV SSL 也能自己生产证书,只是不会被其他人承认(也就是说别人访问你的网站会提示证书危险并且浏览器默认不会加载非HTTPS域名下的javascript ),jdk能生成证书,既然别人访问你都会爆红,自己做个证书也没啥用,不如申请免费的玩玩
Tomcat配置Https:
打开 Tomcat 配置文件 conf\server.xml。取消注释，并添加三个属性 keystoreFile，keystoreType，keystorePass。 来自 https://blog.csdn.net/gane_cheng/article/details/53001846 http://lixor.iteye.com/blog/1532655
&amp;lt;Connector port=&amp;#34;8443&amp;#34; protocol=&amp;#34;HTTP/1.1&amp;#34; SSLEnabled=&amp;#34;true&amp;#34; maxThreads=&amp;#34;150&amp;#34; scheme=&amp;#34;https&amp;#34; secure=&amp;#34;true&amp;#34; clientAuth=&amp;#34;false&amp;#34; sslProtocol=&amp;#34;TLS&amp;#34; keystoreFile=&amp;#34;/你的磁盘目录/订单号.pfx&amp;#34; keystoreType=&amp;#34;PKCS12&amp;#34; keystorePass=&amp;#34;订单号&amp;#34; /&amp;gt; 注:还未实践,暂时只为记录!</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/iaaspaassaascaas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/iaaspaassaascaas/</guid><description>很明显，这五者之间主要的区别在于第一个单词，而aaS都是as-a-service（即服务）的意思，这五个模式都是近年来兴起的，且这五者都是云计算的落地产品.
IaaS
=&amp;gt; Infrastructure-as-a-Service 基础设施即是服务, 简单的不科学说法就是，人家买了一堆电脑租给你用。这是第一层。解决了之前企业要买服务器，装修机房，还要随时防止你没交物业给你停电的问题。服务器都在供应商那， 你不用管啥时候维护。牛逼的供应商很多，阿里云，网易，Azure，AWS-EC2，AWS中国（中国的AWS跟米国的不是一回事，很多服务没有哦）
PaaS
=&amp;gt;这个词不好理解，Platform-as-a-Service 平台即是服务, 简单的不科学说法就是，人家帮你装好操作系统了和基础软件啦。供应商不仅帮你装好了操作系统还有很多基础软件，例如JAVA/Python的环境，让你上传代码就可以启动服务了。 牛逼的供应商很多，GAE/BAE/SAE，Heroku，JCS（oracle JAVA-Cloud-Service）。PaaS 系统提供很多OOTB的功能，例如编译环境，CI/CD，SCM，LB，DNS，Log， DB， Cache。其实我发现很多时候SAAS和PAAS大家分也分不清楚，随缘啦。
SaaS
=&amp;gt; Software-as-a-Service 软件即是服务, 简单的不科学说法就是，人家帮你装好word啦.这里是Application层，相当发达，咱们小公司，还是在这里机会更多。例如你想要短信服务，阿里云去买。你想要直播服务，优酷去买。你想要CDN服务，各个大厂商都在卖（七牛，阿里云，AWS-S3）。
CaaS
=&amp;gt;这个次比较新，Container-as-a-Service 容器即是服务, 简单的不科学且不形象的说法（我也不知道怎么比喻）就是，人家帮你在你mac上装了10个IE（注意，CaaS 有很多中说法，有人说是Communications，有人说是Commerce还有人说是Cloud, 这里只讨论容器服务)。随着容器的迅速发展，开始有大量厂商提供容器服务，让企业成本（现金成本和管理成本）更低，他们一般都提供容器编排服务。流行度还没那么高，目测很多企业还是在ISSA的基础上使用容器编排软件来搞。比较流行的是Docker,Rancher,Kubernetes和Mesos。
IaaS VS PaaS
简单点说IaaS提供虚拟机，而PaaS提供了IDE和测试环境。用了IaaS你就可以装很多乱七八糟的软件了，而PaaS可能对你的行为进行限制，例如你在BAE上部署一个spring项目，你想读取磁盘的一个文件。PaaS会说NO，它已经帮你弄好了编译/运行时环境，限制也大，但是简单。如果你用IaaS，这是你的虚拟机，你可做任何你想要的，rm -rf / 也可以。应用级别的软件就要自己装自己搞了。
PaaS VS CaaS
PaaS针对特定的应用，提供了整套的解决方案，但是对于个性化需求限制太大，随着DevOps/Micro-Service的迅速发展，越来越多的公司期望通过环境的统一解决Dev/Test/Production的一致性问题，让部署更加自动化与统一化。容器来了，特别随着Docker的牛逼，这个神器让复杂的Micro-service问题降低了很大的门槛。当然CaaS也更便宜。
听起来，Xxxx-as-a-Service 似乎感觉是继承关系。确实是，但不是绝对的。
链接：https://www.jianshu.com/p/b27a9f4686f6
更通俗的解释: https://www.zhihu.com/question/21641778/answer/62523535?from=groupmessage</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/kong/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/kong/</guid><description>API 网关，即API Gateway，是大型分布式系统中，为了保护内部服务而设计的一道屏障，可以提供高性能、高可用的 API托管服务，从而帮助服务的开发者便捷地对外提供服务，而不用考虑安全控制、流量控制、审计日志等问题，统一在网关层将安全认证，流量控制，审计日志，黑白名单等实现。网关的下一层，是内部服务，内部服务只需开发和关注具体业务相关的实现。网关可以提供API发布、管理、维护等主要功能。开发者只需要简单的配置操作即可把自己开发的服务发布出去，同时置于网关的保护之下。
对于 API Gateway，常见的选型有:
基于 Openresty 的 Kong
基于 Go 的 Tyk
基于 Java 的 Zuul
这样就很清楚了,只是技术选型不一样,功能都是一样的,都是用来做网关,各有优缺点</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/oauth2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/oauth2/</guid><description>[toc]
(OAuth1和OAuth2差别较大,这里讲解的是OAuth2)
1.介绍 简单说，OAuth 就是一种授权机制。数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（token），用来代替密码，供第三方应用使用
令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。
（1）令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。
（2）令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。
（3）令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。
上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。
注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。 这也是为什么令牌的有效期，一般都设置得很短的原因。
OAuth 2.0 对于如何颁发令牌的细节，规定得非常详细。具体来说，一共分成四种授权类型（authorization grant），即四种颁发令牌的方式，适用于不同的互联网场景。
授权码（authorization-code） 隐藏式（implicit） 密码式（password）： 客户端凭证（client credentials） 注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。
1.1 授权码方式 授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。
这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。
第一步，A 网站提供一个链接，用户点击后就会跳转到 B 网站，授权用户数据给 A 网站使用。下面就是 A 网站跳转 B 网站的一个示意链接。
https://b.com/oauth/authorize? response_type=code&amp;amp; client_id=CLIENT_ID&amp;amp; redirect_uri=CALLBACK_URL&amp;amp; scope=read 上面 URL 中，response_type 参数表示要求返回授权码（code），
client_id 参数让 B 知道是谁在请求，
redirect_uri 参数是 B 接受或拒绝请求后的跳转网址，
scope 参数表示要求的授权范围（这里是只读）。
第二步，用户跳转后，B 网站会要求用户登录，然后询问是否同意给予 A 网站授权。用户表示同意，这时 B 网站就会跳回redirect_uri参数指定的网址。跳转时，会传回一个授权码，就像下面这样。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/serviceless/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/serviceless/</guid><description>云计算机经过这么多年的发展，逐渐进化到用户仅需关注业务和所需的资源。通过Swarm、K8S这些编排工具，容器服务让开发者的体验达到很完美的境界。我曾经觉得Docker可以替代虚机，用户只要关注自己的计算和需要的资源就行，不需要操心到机器这一层。但是因为Docker对资源的隔离不够好，各大云厂商的做法还是一个Docker对应一台虚机，不仅成本高，给用户暴露虚机也多余了。
用户为什么需要关注业务运行所需要的CPU、内存、网络情况？还有没有更好的解决方案？Serverless架构应运而生，让人们不再操心运行所需的资源，只需关注自己的业务逻辑，并且为实际消耗的资源付费。可以说，随着Serverless架构的兴起，真正的云计算时代才算到来了。(其实就是把这部分管理托管出去,就像现在的买阿里服务器一样,不要维护服务器,只管用就行)
容器在开发模式方面并没有提出新的想法，大家还是在用传统的那一套开发模式，需要写一个大而全的后端服务。与之对比，Serverless架构是事件驱动的，这样让后端的开发体验变得跟前端和移动端很类似了。针对不同客户的需求，先让其购买好相关的资源，然后一个个填坑，给不同的产品添加各种事件处理逻辑就行。这就跟iOS开发一样，界面写出来，然后处理一个个事件就好了，大家都很容易理解这种开发模式。
Service Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。
来自:https://blog.csdn.net/weixin_33845477/article/details/87941863
Serverless的主要优点
开发者更加专注于业务逻辑，开发效率更高。开发一个典型的服务器端项目，需要花很多时间处理依赖、线程、日志、发布和使用服务、部署及维护等相关的工作，基于Serverless架构则不需要操心这些工作。 用户为实际使用的资源付费。用户购买的ECS使用时间一般不到5成，但是为另外5成闲置时间付费了。Lambda按照运行的时间收费，成本会低很多。 NO Architecture，NO Ops。架构师的责任是设计一个高可用、高扩展的架构。运维负责整个系统稳定可靠地运行，适当缩减和增加资源。大型云厂商能保证产品的高可用，Serverless架构本身就是高扩展的。Serverless不再需要服务器端的工作人员，给客户节省了大量的资源。架构师和运维的同学应该好好思考一下未来的出路了。架构师可以转型去做销售，整理用户的需求，然后写写CloudFormation的template就好了。 还是成本。IT行业一些领先的公司基础设施非常完善，开发工程师写好代码，然后通过发布平台发布，感觉也是挺方便的。比起Serverless的架构，成本还是要高不少。 机器成本。日常、预发、线上，1+1+2=4台服务器少不了。 时刻要关注业务数据，盘点资源，看看是否需要扩容和缩减资源。扩容容易，缩减难，造成大量资源闲置。 全链路压测是不是很烦？ Serverless的主要缺点
排查问题困难，因为逻辑散落在各处，一个操作可能触发成百上千个Lambda执行。AWS的X-Ray和CloudWatch等产品可以帮助用户排查问题。 准备runtime需要时间，流量瞬间爆发容易导致超时。 带状态的Lambda写起来很困难。 Lambda运行有诸多资源限制，比如运行时长、内存、磁盘、打开的文件数量等。 厂商锁定。云计算是赢者通吃的行业，大而全的云厂商优势巨大，Serverless加剧了这种趋势。以前用户还需要自己写很多服务器端的逻辑，迁移的时候，把服务器端代码重新部署一下。采用Serverless架构之后，代码都是各个平台的Lambda代码片段，没法迁移。从客户的角度来看，是不希望自己被某家云厂商所绑架的。所以云计算行业需要做很多标准化的工作，方便用户无缝在各种云之间迁移。 链接：https://www.jianshu.com/p/51a19ef5f8cf</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/timingwheel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/timingwheel/</guid><description>TimingWheel基本原理:
众所周知寻常的定时器大概有两种，一种是开阻塞线程，另一种是开一个任务队列然后定期扫描。显而易见这两种方式的弊端很明显，前者对线程消耗过大，后者对时间消耗过大（很多未到时间的任务会被多次重复扫描消耗性能）
为了解决以上两个问题就可以使用TimingWheel数据结构。
很明显时间轮算法是基于循环链表数据结构。那么他的工作原理具体是怎样的呢？
能看到图中有个指针，我们假设指针没跳动一下需要10秒，然后现在我们有一个50秒后执行的任务A，由此推断在当前指针指向2的时候，任务A会被存放在槽格7中。当指针跳动到7后取出槽格中的任务队列，此时任务A将会被执行。但是当如果有100秒后的任务需要执行，依然超出了槽格数量是该怎么办呢？很简单，我们为任务实例添加一个属性“圈数”就可以解决。就是说每次装载任务时算出此任务需要指针转动几圈才能够被执行。
来源</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E4%B8%8B%E8%BD%BDsvg%E5%9B%BE%E7%89%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E4%B8%8B%E8%BD%BDsvg%E5%9B%BE%E7%89%87/</guid><description>在项目中,下载普通图片(png,jpg等)都正常,但是svg图片不能下载,主要问题是返回的请求头不对,要写成如下:
&amp;#34;Content-Type&amp;#34;, &amp;#34;image/svg+xml;charset=UTF-8&amp;#34; 完整代码:
@RequestMapping(&amp;#34;/downLoadImage&amp;#34;) public BaseResponse downLoadImage(@Param(&amp;#34;imageName&amp;#34;) String imageName, HttpServletResponse resp) { logger.info(&amp;#34;下载图片开始-入参：{}&amp;#34;, imageName); BaseResponse response = new BaseResponse(); FileImageInputStream fs = null; ServletOutputStream os = null; try { if (StringUtil.isNullOrEmpty(imageName)) { logger.info(&amp;#34;图片名称为空&amp;#34;); setResponse(response, ResponseCode.JIESHUN0003.getCode()); return response; } File file = ImageUtil.downLoadImage(imageName, response); if (file == null) { logger.info(&amp;#34;没有找到图片..&amp;#34;); return response; } // 如果是svg格式后缀的头信息不一样 String suffix = imageName.substring(imageName.lastIndexOf(&amp;#34;.&amp;#34;)); fs = new FileImageInputStream(file); int len = (int) fs.length(); byte[] bs = new byte[len]; fs.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/</guid><description>当用户第一次访问应用系统的时候，因为还没有登录，会被引导到认证系统中进行登录；根据用户提供的登录信息，认证系统进行身份校验，如果通过校验，应该返回给用户一个认证的凭据－－ticket；用户再访问别的应用的时候，就会将这个ticket带上，作为自己认证的凭据，应用系统接受到请求之后会把ticket送到认证系统进行校验，检查ticket的合法性。如果通过校验，用户就可以在不用再次登录的情况下访问应用系统2和应用系统3了。
要实现SSO，需要以下主要的功能：
所有应用系统共享一个身份认证系统。 统一的认证系统是SSO的前提之一。认证系统的主要功能是将用户的登录信息和用户信息库相比较，对用户进行登录认证；认证成功后，认证系统应该生成统一的认证标志（ticket），返还给用户。另外，认证系统还应该对ticket进行效验，判断其有效性。
所有应用系统能够识别和提取ticket信息 要实现SSO的功能，让用户只登录一次，就必须让应用系统能够识别已经登录过的用户。应用系统应该能对ticket进行识别和提取，通过与认证系统的通讯，能自动判断当前用户是否登录过，从而完成单点登录的功能。
总结:做一个认证系统,(所有的访问要经过这个认证系统,除去登录,获取验证码这类的请求),当用户在一个系统登录后,会得到一个ticket(或者说一个唯一标识),以后请求带着这个ticket去访问,由认证系统验证,通过了就正常放行
来自* &amp;lt;https://baike.baidu.com/item/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95#2&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E5%A4%9A%E7%A7%9F%E6%88%B7%E6%A8%A1%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%9D%82%E9%A1%B9/%E5%A4%9A%E7%A7%9F%E6%88%B7%E6%A8%A1%E5%BC%8F/</guid><description>简单的说: 一个虚拟机上,有很多用户在使用.也就是说,一个机器能满足好几个用户.感觉还不错哈.但这是有问题的,
1.其物理资源还是共享的,一旦某个用户的操作占用大量资源,那其他用户就会受到很大影响
2.这里的用户也可以是应用(像k8s),每个应用在一个虚拟机上执行,总有先后,如何保证重要应用优先执行
3.资源共享,也存在安全问题
在HBase1.1.0发布之前，HBase同一集群上的用户、表都是平等的，没有优劣之分。这种’大同’社会看起来完美，实际上有很多问题。最棘手的主要有这么两个，其一是某些业务较其他业务重要，需要在资源有限的情况下优先保证核心重要业务的正常运行，其二是有些业务在某些场景下会时常’抽风’，QPS常常居高不下，严重消耗系统资源，导致其他业务无法正常运转。
这实际上是典型的多租户问题，
来自: https://blog.csdn.net/lw_ghy/article/details/60779482
k8s
我认为这也可能成为容器崩溃的核心原因——多租户机制。
Linux容器在设计之初并没有考虑到安全的隔离沙箱（例如Solaris Zones或者FreeBSD Jails）。相反，容器采用的是共享内核模式，其利用内核功能提供基础性的进程隔离功能。正如Jessie Frazelle在文章中指出，“容器并不真实存在。”
更麻烦的是，大多数Kubernetes组件无法感知到租户。虽然我们可以使用命名空间以及Pod安全策略，但API本身确实不具备租户感知能力。此外， kubelet或者kube-proxy等内部组件也存在同样的问题。这意味着Kubernetes能够提供的只是一种“软租户”模式。
抽象泄漏。建立在容器之上的平台会继承容器技术的诸多软租户因素。正如建立在硬多租户虚拟机基础之上的平台（包括VMware、Amazon Web Srevices以及OpenStack等），也都继承了这种硬租户机制一样。
Kubernetes集群本身成了“硬租户”面临的第一道坎，也因此导致大部分用户只能使用“多集群”这一新兴模式，而非“单一共享”集群。相信很多朋友都发现了，谷歌GKE Service的用户往往需要面向多个团队部署数十个Kubernetes集群，有时候每一位开发者都拥有自己的集群。这类作法最终导致了严重的Kube泛滥问题。
“这类作法最终导致了严重的Kube泛滥问题。”
原文：https://blog.csdn.net/M2l0ZgSsVc7r69eFdTj/article/details/85333601</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E6%A1%88%E4%BE%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E6%A1%88%E4%BE%8B/</guid><description>例子:
将所有方法foo(a,b,c)的实例改为foo(b,a,c)
`` :%s/foo(([^,]),([^,]),([^,)]*))/foo(\2,\1,\3)/g`
1.零宽断言
就是断言匹配字符串的前后规则
(?=exp)也叫零宽度正预测先行断言[4] ，它断言自身出现的位置的后面能匹配表达式exp。比如\b\w+(?=ing\b)，匹配以ing结尾的单词的前面部分(除了ing以外的部分)，如查找I&amp;rsquo;m singing while you&amp;rsquo;re dancing.时，它会匹配sing和danc。
(?&amp;lt;=exp)也叫零宽度正回顾后发断言[4] ，它断言自身出现的位置的前面能匹配表达式exp。比如(?&amp;lt;=\bre)\w+\b会匹配以re开头的单词的后半部分(除了re以外的部分)，例如在查找reading a book时，它匹配ading。
例子:
字符串 45678.002017.12.8 正则 \d+.\d{2}(?=.) 匹配结果: 2017.12.8 2.负向零宽
(感觉有点像零宽度正回顾后发断言)
零宽度负预测先行断言(?!exp)，断言此位置的后面不能匹配表达式exp。例如：\d{3}(?!\d)匹配三位数字，而且这三位数字的后面不能是数字；\b((?!abc)\w)+\b匹配不包含连续字符串abc的单词。
例子:
字符串 45678.002017.12.8 正则 \d+.\d{2}(?!.) 匹配结果: 45678.00 字符串 /jp/xkj/jportal/xkj-jportal 正则 /jp/(?.*) 匹配结果 /jp/xkj 替换字符串 /jpxkj/${name} 替换结果 /jpxkj/xkj</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E6%AD%A3%E5%88%99%E5%BC%95%E6%93%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E6%AD%A3%E5%88%99%E5%BC%95%E6%93%8E/</guid><description>主流的正则引擎又分为3类：
DFA， 传统型NFA， POSIX NFA 使用DFA引擎的程序主要有：awk,egrep,flex,lex,MySQL,Procmail等；
使用传统型NFA引擎的程序主要有：GNU Emacs,Java,ergp,less,more,.NET语言,PCRE library,Perl,PHP,Python,Ruby,sed,vi；
使用POSIX NFA引擎的程序主要有：mawk,Mortice Kern Systems’ utilities,GNU Emacs(使用时可以明确指定)；
也有使用DFA/NFA混合的引擎：GNU awk,GNU grep/egrep,Tcl。
DFA
引擎在线性时状态下执行，因为它们不要求回溯（并因此它们永远不测试相同的字符两次）。DFA 引擎还可以确保匹配最长的可能的字符串。但是，因为 DFA 引擎只包含有限的状态，所以它不能匹配具有反向引用的模式；并且因为它不构造显示扩展，所以它不可以捕获子表达式。
NFA
传统的 NFA 引擎运行所谓的“贪婪的”匹配回溯算法，以指定顺序测试正则表达式的所有可能的扩展并接受第一个匹配项。因为传统的 NFA 构造正则表达式的特定扩展以获得成功的匹配，所以它可以捕获子表达式匹配和匹配的反向引用。但是，因为传统的 NFA 回溯，所以它可以访问完全相同的状态多次（如果通过不同的路径到达该状态）。因此，在最坏情况下，它的执行速度可能非常慢。因为传统的 NFA 接受它找到的第一个匹配，所以它还可能会导致其他（可能更长）匹配未被发现。
举例简单说明NFA与DFA工作的区别：
比如有字符串this is yansen’s blog，正则表达式为 /ya(msen|nsen|nsem)/ (不要在乎表达式怎么样，这里只是为了说明引擎间的工作区别)。
NFA工作方式如下，先在字符串中查找 y 然后匹配其后是否为 a ，如果是 a 则继续，查找其后是否为 m 如果不是则匹配其后是否为 n (此时淘汰msen选择支)。然后继续看其后是否依次为 s,e，接着测试是否为 n ，是 n 则匹配成功，不是则测试是否为 m 。为什么是 m ？因为 NFA 工作方式是以正则表达式为标准，反复测试字符串，这样同样一个字符串有可能被反复测试了很多次！
而DFA则不是如此，DFA会从 this 中 t 开始依次查找 y，定位到 y ，已知其后为 a ，则查看表达式是否有 a ，此处正好有 a 。然后字符串 a 后为 n ，DFA依次测试表达式，此时 msen 不符合要求淘汰。nsen 和 nsem 符合要求，然后DFA依次检查字符串，检测到sen 中的 n 时只有nsen 分支符合，则匹配成功！</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E7%AC%A6%E5%8F%B7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%AD%A3%E5%88%99/%E7%AC%A6%E5%8F%B7/</guid><description>元字符 描述 \ 将下一个字符标记符、或一个向后引用、或一个八进制转义符。例如，“\n”匹配\n。“\n”匹配换行符。序列“\”匹配“\”而“(”则匹配“(”。即相当于多种编程语言中都有的“转义字符”的概念。 ^ 匹配输入字行首。如果设置了RegExp对象的Multiline属性，^也匹配“\n”或“\r”之后的位置。 $ 匹配输入行尾。如果设置了RegExp对象的Multiline属性，$也匹配“\n”或“\r”之前的位置。 * 匹配前面的子表达式任意次。例如，zo*能匹配“z”，也能匹配“zo”以及“zoo”。*等价于o{0,} + 匹配前面的子表达式一次或多次(大于等于1次）。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配“do”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n&amp;lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o为一组，后三个o为一组。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+”将尽可能多的匹配“o”，得到结果[“oooo”]，而“o+?”将尽可能少的匹配“o”，得到结果 [&amp;lsquo;o&amp;rsquo;, &amp;lsquo;o&amp;rsquo;, &amp;lsquo;o&amp;rsquo;, &amp;lsquo;o&amp;rsquo;] .点 匹配除“\r\n”之外的任何单个字符。要匹配包括“\r\n”在内的任何字符，请使用像“[\s\S]”的模式。 (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)”。 (?:pattern) 非获取匹配，匹配pattern但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分时很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。 (?=pattern) 非获取匹配，正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如，“Windows(?=95|98|NT|2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?!pattern) 非获取匹配，正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如“Windows(?!95|98|NT|2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。 (?&amp;lt;=pattern) 非获取匹配，反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?&amp;lt;=95|98|NT|2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。 (?&amp;lt;!pattern) 非获取匹配，反向否定预查，与正向否定预查类似，只是方向相反。例如“(?&amp;lt;!95|98|NT|2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。这个地方不正确，有问题此处用或任意一项都不能超过2位，如“(?&amp;lt;!95|98|NT|20)Windows正确，“(?&amp;lt;!95|980|NT|20)Windows 报错，若是单独使用则无限制，如(?&amp;lt;!2000)Windows 正确匹配 x|y 匹配x或y。例如，“z|food”能匹配“z”或“food”(此处请谨慎)。“[zf]ood”则匹配“zood”或“food”。 [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配“plain”中的“plin”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。注意:只有连字符在字符组内部时,并且出现在两个字符之间时,才能表示字符的范围; 如果出字符组的开头,则只能表示连字符本身. [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在“a”到“z”范围内的任意字符。 \b 匹配一个单词边界，也就是指单词和空格间的位置（即正则表达式的“匹配”有两种概念，一种是匹配字符，一种是匹配位置，这里的\b就是匹配位置的）。例如，“er\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。 \B 匹配非单词边界。“er\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。 \cx 匹配由x指明的控制字符。例如，\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。 \d 匹配一个数字字符。等价于[0-9]。grep 要加上-P，perl正则支持 \D 匹配一个非数字字符。等价于[^0-9]。grep要加上-P，perl正则支持 \f 匹配一个换页符。等价于\x0c和\cL。 \n 匹配一个换行符。等价于\x0a和\cJ。 \r 匹配一个回车符。等价于\x0d和\cM。 \s 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。 \S 匹配任何可见字符。等价于[^ \f\n\r\t\v]。 \t 匹配一个制表符。等价于\x09和\cI。 \v 匹配一个垂直制表符。等价于\x0b和\cK。 \w 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的&amp;quot;单词&amp;quot;字符使用Unicode字符集。 \W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。 \xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\x41”匹配“A”。“\x041”则等价于“\x04&amp;amp;1”。正则表达式中可以使用ASCII编码。 \num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%B5%8B%E8%AF%95/%E6%B5%8B%E8%AF%95%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84ie%E6%B5%8F%E8%A7%88%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E6%B5%8B%E8%AF%95/%E6%B5%8B%E8%AF%95%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84ie%E6%B5%8F%E8%A7%88%E5%99%A8/</guid><description>新版IE有切换版本的功能:
可以配合虚拟机使用,装最低版本,升级一个版本做个快照,然后就可以慢慢测了
或者用IETester软件和DebugBar软件,
来自* &amp;lt;https://blog.csdn.net/qq_27709465/article/details/76972707&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/http/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/http/</guid><description>[toc]
http 2.0 新特性 二进制分帧
在应用层(HTTP2.0)和传输层(TCP、UDP)新增的二进制分帧层。在这层中,数据会被分割成更小的消息和帧,然后可以无序发,最后组装就行
head压缩
多路复用
做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。
因为请求都在同一个tcp连接上完成
服务器推送
服务器可以对一个客户端请求发送多个响应,之前是一个请求一个响应
实现原理大致: 客户端发送一次请求,服务端的请求并不会关闭,发完第一次,接着发第二次
目前NGINX的V1.13.9和tomcat已经支持,后端也能实现
关键字是link
服务器推送有一个很麻烦的问题。所要推送的资源文件，如果浏览器已经有缓存，推送就是浪费带宽。即使推送的文件版本更新，浏览器也会优先使用本地缓存。
一种解决办法是，只对第一次访问的用户开启服务器推送。
来源: 服务器推送实现
http2.0新特性
http各版本之间的区别
HTTP协议的Keep-Alive 可以看到里面的请求头部和响应头部都有一个key-valueConnection: Keep-Alive，这个键值对的作用是让HTTP保持连接状态(就是俗称的长链接)，因为HTTP 协议采用“请求-应答”模式，当使用普通模式，即非 Keep-Alive 模式时，每个请求/应答客户和服务器都要新建一个连接，完成之后立即断开连接（HTTP 协议为无连接的协议）；当使用 Keep-Alive 模式时，Keep-Alive 功能使客户端到服务器端的连接持续有效。
在HTTP 1.1版本后，默认都开启Keep-Alive模式，只有加入加入 Connection: close才关闭连接，当然也可以设置Keep-Alive模式的属性，例如 Keep-Alive: timeout=5, max=100，表示这个TCP通道可以保持5秒，max=100，表示这个长连接最多接收100次请求就断开。
Keep-Alive模式下如何知道某一次数据传输结束
如果不是Keep-Alive模式，HTTP协议中客户端发送一个请求，服务器响应其请求，返回数据。服务器通常在发送回所请求的数据之后就关闭连接。这样客户端读数据时会返回EOF（-1），就知道数据已经接收完全了。 但是如果开启了 Keep-Alive模式，那么客户端如何知道某一次的响应结束了呢？
以下有两个方法
如果是静态的响应数据，可以通过判断响应头部中的Content-Length 字段，判断数据达到这个大小就知道数据传输结束了。 但是返回的数据是动态变化的，服务器不能第一时间知道数据长度，这样就没有 Content-Length 关键字了。这种情况下，服务器是分块传输数据的，Transfer-Encoding：chunk，这时候就要根据传输的数据块chunk来判断，数据传输结束的时候，最后的一个数据块chunk的长度是0。 使用HTTP建立长连接
当需要建立 HTTP 长连接时，HTTP 请求头将包含如下内容：
Connection: Keep-Alive
如果服务端同意建立长连接，HTTP 响应头也将包含如下内容：
Connection: Keep-Alive
当需要关闭连接时，HTTP 头中会包含如下内容：
Connection: Close
慢速攻击：Http协议中规定，HttpRequest以\r\n\r\n结尾来表示客户端发送结束。攻击者打开一个Http 1.1的连接，将Connection设置为Keep-Alive， 保持和服务器的TCP长连接。然后始终不发送\r\n\r\n， 每隔几分钟写入一些无意义的数据流， 拖死机器。
cc攻击</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/https/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/https/</guid><description>[toc]
介绍 HTTPS（全称：Hyper Text Transfer Protocol over Secure Socket Layer 或 Hypertext Transfer Protocol Secure，超文本传输安全协议），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 它是一个URI scheme（抽象标识符体系），句法类同http:体系。用于安全的HTTP数据传输。https:URL表明它使用了HTTP，但HTTPS存在不同于HTTP的默认端口及一个加密/身份验证层（在HTTP与TCP之间）。
基础知识 非对称加密 加密使用的密钥和解密使用的密钥是不相同的，分别称为：公钥、私钥，公钥和算法都是公开的，私钥是保密的。非对称加密算法性能较低，但是安全性超强，由于其加密特性，非对称加密算法能加密的数据长度也是有限的。 例如：RSA、DSA、ECDSA、 DH、ECDHE。
既可以用公钥加密私钥解密（传输敏感信息场景），也可以用私钥加密公钥解密（用户认证场景）
例如
非对称加密利用成对的两个秘钥：K1 和 K2。小红用其中一个加密文本，小明可 以用另一个解密文本。比如，小红用 K1 加密，小明用 K2 解密：
小红 : C = E(M, K1) 小明 : M = D(C, K2) 这样一来，双方中的一方（比如小红）可以生成 K1和K2，然后把其中一个秘钥 （比如K1）私藏，称为私钥；另一个（比如K2）公开，称为公钥。另一 方（比如小明）得到公钥之后，双方就可以通信。
因为加密和解密的 秘钥值不一样, 所以是不可逆吧
RSA 算法：该算法的命名以三位科学家的姓氏缩写组合得来，在计算机网络世界，一直是最广为使用的 “非对称加密算法”。
ECC 是非对称加密里的 “后起之秀”，它基于 “椭圆曲线离散对数” 的数学难题，使用特定的曲线方程和基点生成公钥和私钥，子算法 ECDHE 用于密钥交换，ECDSA 用于数字签名。
对称加密 有流式、分组两种，加密和解密都是使用的同一个密钥。 例如：DES、AES-GCM、ChaCha20-Poly1305等。
因为加密和解密的 秘钥值是一样的, 所以也是可逆
证书 证书是用来认证公钥持有者的身份的电子文档，防止第三方进行冒充。一个证书中包含了公钥、持有者信息、证明证书内容有效的签名以及证书有效期，还有一些其他额外信息。操作系统、浏览器本身会预装一些它们信任的根证书，如果其中有该CA机构的根证书，那就可以拿到它对应的可信公钥了。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/io%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/io%E6%A8%A1%E5%9E%8B/</guid><description>[toc]
同步阻塞IO (BIO) 假设应用程序的进程发起IO调用，但是如果内核的数据还没准备好的话，那应用程序进程就一直在阻塞等待，一直等到内核数据准备好了，从内核拷贝到用户空间，才返回成功提示，此次IO操作，称之为阻塞IO。
阻塞IO比较经典的应用就是阻塞socket、Java BIO。 阻塞IO的缺点就是：如果内核数据一直没准备好，那用户进程将一直阻塞，浪费性能，可以使用非阻塞IO优化。 同步非阻塞IO (NIO) 如果内核数据还没准备好，可以先返回错误信息给用户进程，让它不需要等待，而是通过轮询的方式再来请求。这就是非阻塞IO，流程图如下：
非阻塞IO模型，简称NIO，Non-Blocking IO / New IO。它相对于阻塞IO，虽然大幅提升了性能，但是它依然存在性能问题，即频繁的轮询，导致频繁的系统调用，同样会消耗大量的CPU资源。可以考虑IO复用模型，去解决这个问题。
IO多路复用模型 (NIO) IO复用模型核心思路：系统给我们提供一类函数（如我们耳濡目染的select、poll、epoll函数），它们可以同时监控多个fd的操作，任何一个返回内核数据就绪，应用进程再发起recvfrom系统调用。
文件描述符fd(File Descriptor),它是计算机科学中的一个术语，形式上是一个非负整数。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。
epoll函数的调用流程
更多详见 - IO多路复用.md
IO模型之信号驱动模型 信号驱动IO不再用主动询问的方式去确认数据是否就绪，而是向内核发送一个信号（调用sigaction的时候建立一个SIGIO的信号），然后应用用户进程可以去做别的事，不用阻塞。当内核数据准备好后，再通过SIGIO信号通知应用进程，数据准备好后的可读状态。应用用户进程收到信号之后，立即调用recvfrom，去读取数据。
信号驱动IO模型，在应用进程发出信号后，是立即返回的，不会阻塞进程。它已经有异步操作的感觉了。但是你细看上面的流程图，发现数据复制到应用缓冲的时候，应用进程还是阻塞的。回过头来看下，不管是BIO，还是NIO，还是信号驱动，在数据从内核复制到应用缓冲的时候，都是阻塞的
IO 模型之异步IO(AIO) 前面讲的BIO，NIO和信号驱动，在数据从内核复制到应用缓冲的时候，都是阻塞的，因此都不是真正的异步。AIO实现了IO全流程的非阻塞，就是应用进程发出系统调用后，是立即返回的，但是立即返回的不是处理结果，而是表示提交成功类似的意思。等内核数据准备好，将数据拷贝到用户进程缓冲区，发送信号通知用户进程IO操作执行完毕。
看一遍就理解：IO模型详解 - 掘金 (juejin.cn)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/tcp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C/tcp/</guid><description>[toc]
滑动窗口 我们先来看发送方与接收方之间，两者之间的交互
如图，这个就是我们把两个包一起发送，然后一起确认。我们改善了吞吐量的问题
问题：我们每次需要发多少个包过去呢？发送多少包是最优解呢？
我们能不能把第一个和第二个包发过去后，收到第一个确认包就把第三个包发过去呢？而不是去等到第二个包的确认包才去发第三个包。这样就很自然的产生了我们&amp;quot;滑动窗口&amp;quot;的实现。
在图中，我们可看出灰色1号2号3号包已经发送完毕，并且已经收到Ack。这些包就已经是过去式。4、5、6、7号包是黄色的，表示已经发送了。但是并没有收到对方的Ack，所以也不知道接收方有没有收到。8、9、10号包是绿色的。是我们还没有发送的。这些绿色也就是我们接下来马上要发送的包。 可以看出我们的窗口正好是11格。后面的11-16还没有被读进内存。要等4号-10号包有接下来的动作后，我们的包才会继续往下发送。
正常情况 可以看到4号包对方已经被接收到，所以被涂成了灰色。“窗口”就往右移一格，这里只要保证“窗口”是7格的。 我们就把11号包读进了我们的缓存。进入了“待发送”的状态。8、9号包已经变成了黄色，表示已经发送出去了。接下来的操作就是一样的了，确认包后，窗口往后移继续将未发送的包读进缓存，把“待发送“状态的包变为”已发送“。
丢包情况 有可能我们包发过去，对方的Ack丢了。也有可能我们的包并没有发送过去。从发送方角度看就是我们没有收到Ack。
发生的情况：一直在等Ack。如果一直等不到的话，我们也会把读进缓存的待发送的包也一起发过去。但是，这个时候我们的窗口已经发满了。所以并不能把12号包读进来，而是始终在等待5号包的Ack。
超时重发 这里有一点要说明：这个Ack是要按顺序的。必须要等到5的Ack收到，才会把6-11的Ack发送过去。这样就保证了滑动窗口的一个顺序。
这时候可以看出5号包已经接受到Ack，后面的6、7、8号包也已经发送过去已Ack。窗口便继续向后移动。
这个与gateway的滑动窗口限流有点不一样 , gateway的限流是会丢弃流量的, tcp的滑动窗口不会丢弃, 都在后面排队
一篇带你读懂TCP之“滑动窗口”协议 - Coder编程 - 博客园 (cnblogs.com)
TCP滑动窗口 - alifpga - 博客园 (cnblogs.com)
粘包和拆包 TCP 协议是流式协议。所谓流式协议，即协议的内容是像流水一样的字节流，内容与内容之间没有明确的分界标志，需要我们人为地去给这些协议划分边界。
A 与 B 进行 TCP 通信，A 先后给 B 发送了一个 100 字节和 200 字节的数据包，B 可能收到以一次或者多次任意形式的总数为 300 字节, 对于 B 来说，如果不人为规定多少字节作为一个数据包，B 每次是不知道应该把收到的数据中多少字节作为一个有效的数据包的，而规定每次把多少数据当成一个包就是协议格式定义的内容之一。
粘包 有的面试官可能会这么问：网络通信时，如何解决粘包、丢包或者包乱序问题？
如果是 TCP 协议，在大多数场景下，是不存在丢包和包乱序问题的，TCP 通信是可靠通信方式，TCP 协议栈通过序列号和包重传确认机制保证数据包的有序和一定被正确发到目的地；
如果是 UDP 协议，如果不能接受少量丢包，那就要自己在 UDP 的基础上实现类似 TCP 这种有序和可靠传输机制了（例如 RTP 协议、RUDP 协议）。所以，问题拆解后，只剩下如何解决粘包的问题。</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E9%87%91%E8%9E%8D/%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E7%9F%A5%E8%AF%86/%E9%87%91%E8%9E%8D/%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87/</guid><description>存款准备金率: 存款准备金利率（英文：Deposit Reserve Rate）是央行支付给金融机构缴存的 存款准备金（法定存款准备金和超额存款准备金）所支付的利率。
来自:链接&amp;gt;
解释: 我们会往银行存钱,为了保证你来取钱时银行仓库里还有钱,银行要给中央银行一些钱,作为最低存款,这些钱不能用于贷款,
如果比率下降了,则上交国家的钱就减少了(存的钱基本不会变),银行就有更多的钱用来贷款(赚钱),流通在市面上的钱就多了(王健林就能贷到更多的钱(反正你贷不到))
https://baijiahao.baidu.com/s?id=1613737512299536546&amp;amp;wfr=spider&amp;amp;for=pc</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E6%9C%8D%E5%8A%A1%E5%99%A8/nginx/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E6%9C%8D%E5%8A%A1%E5%99%A8/nginx/</guid><description>配置: ~ 为区分大小写匹配 ~* 为不区分大小写匹配 !~和!~*分别为区分大小写不匹配及不区分大小写不匹配 来自* &amp;lt;https://www.cnblogs.com/xuey/p/7631690.html&amp;gt;
location ~ .*\.(sh|bash)?$ { # 拦截sh和bash结尾的访问,并返回403状态码 return 403; } # 如果localhost 后的地址有冲突 ,比如还有以下,则匹配最长的(也就是最精准的),所以上面这个location会起作用 location / { # 拦截所有的访问,并返回200状态码 return 200; } # 用等号是最精准的 location = /index.html { # 拦截index的访问,并返回200状态码 return 200; } 主流软件负载均衡器对比(LVS、Nginx、HAproxy) - 知乎 (zhihu.com)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E6%9C%8D%E5%8A%A1%E5%99%A8/nginx%E5%92%8Cuwsgi/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E6%9C%8D%E5%8A%A1%E5%99%A8/nginx%E5%92%8Cuwsgi/</guid><description>以下是基于ucap(Django,contos7)项目:
安装NIGNX:
安装uwsgi: pip install uwsgi
#配置NGINX
配置文件: vim /etc/nginx/conf.d/nginx_mas_ucap.conf
\# nginx_mas_ucap.conf \# the upstream component nginx needs to connect to upstream mas_ucap { server unix:///opt/mas_ucap/unix.sock; # for a file socket \#server 192.168.153.128:8080; } \# configuration of the server server { \# the port your site will be served on listen 8000; \# the domain name it will serve for, substitute your machine&amp;#39;s IP address or FQDN server_name 192.168.153.128; charset utf-8; \# max upload size client_max_body_size 75M; \# adjust to taste client_header_buffer_size 32k; large_client_header_buffers 4 32k; access_log /var/log/nginx/mas_ucap_test_access.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E5%91%BD%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E5%91%BD%E4%BB%A4/</guid><description>[toc]
工作区（working tree）： 本地编辑器 暂存区（index）：git add操作后进入暂存区，可用git status查看 本地仓库（repository）：git commit 后进入本地仓库 git commit回退三种姿势_DiuDiu_yang的博客-CSDN博客_git 回退commit
1. 初始化git和github 先在github上新建一个项目
ssh-keygen -t rsa -C &amp;quot;1767822853@qq.com&amp;quot; //123 是你自己注册GitHub的邮箱 (一路回车即可)
用户主目录里找到.ssh文件夹，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露，id_rsa.pub是公钥，可以公开。
把公钥配置到github上
验证
ssh -T git@github.com
配置user和email
git config --global user.name &amp;#34;xkj&amp;#34; # 用户名 git config --global user.email &amp;#34;1767822853@qq.com&amp;#34; # 用户邮箱地址 连接git和github
git remote add origin git@github.com:flora0103/example.git
剩下的就是走代码提交流程了
连接流程
push时问题
2. 代码提交流程: git add .（后面有一个点，意思是将你本地所有修改了的文件添加到暂存区）
git commit -m &amp;quot;&amp;quot;
(引号里面是你的介绍，就是你的这次的提交是什么内容，便于你以后查看，这个是将索引的当前内容与描述更改的用户和日志消息一起存储在新的提交中)
git pull origin master 这是下拉代码，将远程最新的代码先跟你本地的代码合并一下，如果确定远程没有更新，可以不用这个，最好是每次都执行以下，完成之后打开代码查看有没有冲突，并解决，如果有冲突解决完成以后再次执行1跟2的操作</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E5%92%8Csvn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E5%92%8Csvn/</guid><description>区别:
关于切换分支
Git 的切换分支更像(或者说就是)切换快照(head),因为可以在同一个文件夹下使用 git checkout 来切换分支(本地代码就切换了)
svn的切换分支更像是切换文件,(好像也可以在同一个文件夹下) 关于提交代码
Git 是要先add,再commit最后push 才能把代码推送至远程
svn是直接commit就能把代码推送至远程(也可以先add再commit) svn提交代码后，本地的版本不是最新的，需要update才是最新版本；而git是提交后，本地和远程均为最新版本 关于删除/新增文件
Git 删除/新增文件需要指明某一个文件删除/新增( 新增:git add 删除:git rm)
SVN 只需要将删除/新增文件直接提交即可 关于优/劣势: 版本最重要的是历史版本,而不是当前版本(文件). svn一旦代码服务器数据丢失,将失去所有历史版本,(除非你本地有每个版本的文件) git是分布式的,每次pull或者生成新的分支都会保留历史版本,也就是说代码服务器数据丢失,历史版本也会保留(只要有人pull过就会保存) git是每个历史版本都存储完整的文件,便于恢复,svn是存储差异文件,历史版本不可恢复 git有一个本地仓库,所以即使你没有网络(包括局域网),你也能提交代码(到本地仓库),等有网了你再提交到代码服务器;而svn是保存文件,没有本地仓库的概念,所以没网的情况下不能提交 关于解决冲突: 当出现冲突后,均需要解决冲突,冲突上面部分是自己的代码,但是不同的是, git解决完冲突后直接commit就行; svn需要标记为解决(mark as merge)才能commit</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E6%8B%93%E5%B1%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/git%E6%8B%93%E5%B1%95/</guid><description>git 的缺点：
Git 没有严格的权限管理控制，一般通过系统设置文件读写权限的方式来做权限控制。
就是那种,一个项目,前段只能看到前段代码,后端只能看到后端代码,git是做不到这种按路径授权, 如果允许按照路径授权，则各个克隆的关系将不再是平等的关系，有的内容多，有的内容少，分布式的理念被破坏
工作目录只能是整个项目。比如 checkout，建分支，都是基于整个项目的。而 svn 可以基于项目中的某一个目录
来自* &amp;lt;https://blog.csdn.net/hellow__world/article/details/72529022&amp;gt;
https://www.zhihu.com/question/22363313/answer/142703190
关于git 密钥:(主要用于免密登录)
git用的是密钥,由公钥和私钥组成( 公钥加密,私钥解密,所以说公钥给别人,私钥自己留 =&amp;gt; 两个人想通信,则公钥要互给 )
我们要和github传代码,就要互相通信,我们把公钥上传到github上,就要能访问github了,第一次访问时github就将指纹系统保存下来了(仿佛就像将github的公钥保存在自己电脑上),以后就能互相通信了
登录过程和使用 rlogin 或 telnet 建立的会话非常类似。 在连接时， SSH 会利用一个密钥指纹系统来验证服务器的真实性。 只有在第一次连接时， 用户会被要求输入 yes。 之后的连接将会验证预先保存下来的密钥指纹。 如果保存的指纹与登录时接收到的不符， 则将会给出警告。 指纹保存在 ~/.ssh/known_hosts 中， 对于 SSH v2 指纹， 则是 ~/.ssh/known_hosts2。
来自 https://www.freebsd.org/doc/zh_CN/books/handbook/openssh.html
来自 https://www.cnblogs.com/dzblog/p/6930147.html
误解:以前以为git中的密钥是用来加密代码传输的,其实是用来验证电脑和github间的信任
认证流程：
Client端用户TopGun将自己的公钥存放在Server上，追加在文件authorized_keys中。
Server收到登录请求后，随机生成一个字符串str1，并发送给Client。
Client用自己的私钥对字符串str1进行加密。
将加密后字符串发送给Server。
Server用之前存储的公钥进行解密，比较解密后的str2和str1。
根据比较结果，返回客户端登陆结果。
来自* &amp;lt;https://www.cnblogs.com/dzblog/p/6930147.html&amp;gt;</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/lfu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/lfu/</guid><description>[toc]
介绍 LFU全称是最不经常使用算法（Least Frequently Used），LFU算法的基本思想和所有的缓存算法一样，都是基于locality假设（局部性原理）：
如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。
LFU是基于这种思想进行设计：一定时期内被访问次数最少的页，在将来被访问到的几率也是最小的。
相比于LRU（Least Recently Use）算法，LFU更加注重于使用的频率。
原理 LFU将数据和数据的访问频次保存在一个容量有限的容器中，当访问一个数据时：
该数据在容器中，则将该数据的访问频次加1。
该数据不在容器中，则将该数据加入到容器中，且访问频次为1。
当数据量达到容器的限制后，会剔除掉访问频次最低的数据。下图是一个简易的LFU算法示意图。
上图中的LRU容器是一个链表，会动态地根据访问频次调整数据在链表中的位置，方便进行数据的淘汰，可以看到，在第四步时，因为需要插入数据F，而淘汰了数据E。
LFU实现 和LRU类似, 都可以用数组或者链表来实现, 只不过要遍历全链表, 性能都不太好, 因为数据和频率在一起,
基于双哈希表实现 所以换为用 map 键值对来维护，用频次作为键，用当前频次对应的一条具有先后访问顺序的链表来作为值; 再一个map存 key和数据的映射
class LFUCache { private int capacity; // 容量限制 private int size; // 当前数据个数 private int minFreq; // 当前最小频率 private Map&amp;lt;Integer, Node&amp;gt; map; // key和数据的映射 private Map&amp;lt;Integer, LinkedHashSet&amp;lt;Node&amp;gt;&amp;gt; freqMap; // 数据频率和对应数据组成的链表 public LFUCache(int capacity) { this.capacity = capacity; this.size = 0; this.minFreq = 1; this.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/lru/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/lru/</guid><description>[toc]
前言 LRU算法全称是最少最近使用算法（Least Recently Use），广泛的应用于缓存机制中。当缓存使用的空间达到上限后，就需要从已有的数据中淘汰一部分以维持缓存的可用性，而淘汰数据的选择就是通过LRU算法完成的。
LRU算法的基本思想是基于局部性原理的时间局部性：
如果一个信息项正在被访问，那么在近期它很可能还会被再次访问。
所以顾名思义，LRU算法会选出最近最少使用的数据进行淘汰。
原理 一般来讲，LRU将访问数据的顺序或时间和数据本身维护在一个容器当中。当访问一个数据时：
该数据不在容器当中，则设置该数据的优先级为最高并放入容器中。该数据在容器当中，则更新该数据的优先级至最高。当数据的总量达到上限后，则移除容器中优先级最低的数据。下图是一个简单的LRU原理示意图：
如果我们按照7 0 1 2 0 3 0 4的顺序来访问数据，且数据的总量上限为3，则如上图所示，LRU算法会依次淘汰7 1 2这三个数据。
由图感觉, 没有体现出最少的概念, 更贴切的描述应该是: 最近使用算法,
朴素的LRU算法 那么我们现在就按照上面的原理，实现一个朴素的LRU算法。下面有三种方案：
基于数组 方案：为每一个数据附加一个额外的属性——时间戳，当每一次访问数据时，更新该数据的时间戳至当前时间。当数据空间已满后，则扫描整个数组，淘汰时间戳最小的数据。
不足：维护时间戳需要耗费额外的空间，淘汰数据时需要扫描整个数组。
基于长度有限的双向链表 方案：访问一个数据时，当数据不在链表中，则将数据插入至链表头部，如果在链表中，则将该数据移至链表头部。当数据空间已满后，则淘汰链表最末尾的数据。
不足：插入数据或取数据时，需要扫描整个链表。
基于双向链表和哈希表 方案：为了改进上面需要扫描链表的缺陷，配合哈希表，将数据和链表中的节点形成映射，HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1) , 而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点，如图所示。
重点是hash表, 以hash表为维度操作
基于LinkedHashMap实现的LRU JDK给我们提供的LinkedHashMap直接实现LRU。因为LinkedHashMap的底层即为双向链表和哈希表的组合，所以可以直接拿来使用。
public class LRUCache extends LinkedHashMap { private int capacity; public LRUCache(int capacity) { // true表示 访问的顺序 ， false表示插入的顺序 // 注意这里将LinkedHashMap的accessOrder设为true super(16, 0.</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/morris%E9%81%8D%E5%8E%86%E8%8E%AB%E9%87%8C%E6%96%AF%E9%81%8D%E5%8E%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/morris%E9%81%8D%E5%8E%86%E8%8E%AB%E9%87%8C%E6%96%AF%E9%81%8D%E5%8E%86/</guid><description>[toc]
1. 什么是Morris遍历(莫里斯遍历) 普通的二叉树遍历大多需要栈在存储节点,最差的情况下需要存储整棵树,Morris遍历则是将空间复杂度降到了O(1)级别。Morris遍历用到了“线索二叉树”的概念，其实就是利用了叶子节点的左右空指针来存储某种遍历前驱节点或者后继节点。因此没有使用额外的空间。
morris遍历的特点: 无栈;空间复杂度为O(1);无递归(用while代替了)
来源: https://blog.csdn.net/danmo_wuhen/article/details/104339630
https://blog.csdn.net/woshinannan741/article/details/52839946
2. Morris遍历的算法思想 假设当前节点为cur，并且开始时赋值为根节点root。
判断cur节点是否为空
如果不为空
1）如果cur没有左孩子，cur向右更新，即（cur = cur.right）
2）如果cur有左孩子，则从左子树找到最右侧节点pre
如果pre的右孩子为空，则将右孩子指向cur。pre.right = cur 如果pre的右孩子为cur，则将其指向为空。pre.right = null。（还原树结构） cur为空时，停止遍历
算法核心是 : 把右叶子节点指向上级(或者上面节点),这样就不用回归(达到了栈的效果),此时树就变成了一个无向环形图,把右叶子节点取消后继节点,树就恢复了,此时也到了输出条件,如此往复直到遍历整棵树
来源: https://blog.csdn.net/danmo_wuhen/article/details/104339630
https://www.cnblogs.com/AnnieKim/archive/2013/06/15/morristraversal.html
https://blog.csdn.net/woshinannan741/article/details/52839946
以后遇到了再详细看吧</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/</guid><description>倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引(inverted index)。带有倒排索引的文件我们称为倒排索引文件，简称倒排文件(inverted file)。
例如: 先将很多文章切词, 得到切后的词与来源文章的编号维护成列表,这样根据某个词就能快速找到文章.这就是核心
列表中也可以保存单词出现的次数,来源文章等等信息, (百度不知道都存了什么,反正肯定很牛逼)
以单词“拉斯”为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;&amp;lt;4&amp;gt;)，(5;1;&amp;lt;4&amp;gt;)},其含义为在文档3和文档5出现过这个单词，单词频率都为1，单词“拉斯”在两个文档中的出现位置都是4，即文档中第四个单词是“拉斯”。
原文：https://blog.csdn.net/u012965373/article/details/39118483</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</guid><description>[toc]
介绍 一种过滤器,用于判断一个元素在一个集合中是否存在.
它实际上是一个很长的二进制矢量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。
来自* &amp;lt;https://blog.csdn.net/iam333/article/details/38084137&amp;gt;
原理 一个元素通过K个不同的hash函数随机散列到bit数组的K个位置上，
Bloom Filter的一个例子集合S｛x，y，z｝。带有颜色的箭头表示元素经过k（k＝3）hash函数的到在M（bit数组）中的位置。元素W不在S集合中，因为元素W经过k个hash函数得到在M（bit数组）的k个位置中存在值为0的位置。
向集合S中添加元素x：x经过k个散列函数后，在M中得到k个位置，然后，将这k个位置的值设置为1。
判断x元素是否在集合S中：x经过k个散列函数后，的到k个位置的值，如果这k个值中间存在为0的，说明元素x不在集合中——元素x曾经插入到过集合S，则M中的k个位置会全部置为1；如果M中的k个位置全为1，则有两种情形。
情形一：这个元素在这个集合中；
情形二：曾经有元素插入的时候将这k个位置的值置为1了（第一类错误产生的原因FalsePositive）
来自* &amp;lt;https://blog.csdn.net/lvsaixia/article/details/51503231&amp;gt;
来自* &amp;lt;https://blog.csdn.net/lvsaixia/article/details/51503231&amp;gt;
总结 优点
存储空间和插入/查询时间都是常数，远远超过一般的算法 Hash函数相互之间没有关系，方便由硬件并行实现 不需要存储元素本身，在某些对保密要求非常严格的场合有优势 缺点
有一定的误识别率 删除困难 来自* &amp;lt;https://www.cnblogs.com/Jack47/p/bloom_filter_intro.html&amp;gt;
特性:
过滤器说不存在,那这个元素一定不存在;
过滤器说存在,这个元素可能存在(有错误率)
A&amp;amp;Q 1.为什么有错误率?
由原理决定的,一个元素被映射成多个值,需比较的值也会被映射成多个值,这个两个元素映射后的值可能会有重合,导致误判,但是需比较映射的值有个为0,则该值一定不存在.也可以说,你的集合越大,误判的次数就越大,按数学推理来算大约是0.0005
2.为什么删除困难?
​ 映射后的值可能有冲突,你删除一个元素,就必须连同映射后的值一起删除,这就可能影响到其他值(也用到了这个映射值),而且布隆本身就存在误判,万一你要删除的它说不存在呢(实际上存在的)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</guid><description>[toc]
快速排序 排序流程:
快速排序算法通过多次比较和交换来实现排序，其排序流程如下：
首先设定一个分界值，通过该分界值将数组分成左右两部分。
将大于或等于分界值的数据集中到数组右边，小于分界值的数据集中到数组的左边。此时，左边部分中各元素都小于或等于分界值，而右边部分中各元素都大于或等于分界值。
然后，左边和右边的数据可以独立排序。对于左侧的数组数据，又可以取一个分界值，将该部分数据分成左右两部分，同样在左边放置较小值，右边放置较大值。右侧的数组数据也可以做类似处理。
重复上述过程，可以看出，这是一个递归定义。通过递归将左侧部分排好序后，再递归排好右侧部分的顺序。当左、右两个部分各数据排序完成后，整个数组的排序也就完成了。
时间复杂度和空间复杂度平均值都是O(log2n)
计数排序 它的优势在于在对一定范围内的整数排序时，它的复杂度为Ο(n+k)（其中k是整数的范围）,此时快于任何比较排序算法,计数排序算法是一个稳定的排序算法,快排不是
排序过程(十分巧妙运用数组):
假设输入的线性表L的长度为n，L=L1,L2,..,Ln；线性表的元素属于有限偏序集S，|S|=k且k=O(n)，S={S1,S2,..Sk}；则计数排序可以描述如下：
1、扫描整个集合S，对每一个Si∈S，找到在线性表L中小于等于Si的元素的个数T(Si)；
2、扫描整个线性表L，对L中的每一个元素Li，将Li放在输出线性表的第T(Li)个位置上，并将T(Li)减1。
举例:
​ 假设要排序的数组为 A = {1,0,3,1,0,1,1}
这里最大值为3，最小值为0，那么我们创建一个数组C，长度为4.
然后一趟扫描数组A，得到A中各个元素的总数，并保持到数组C的对应单元中。
比如0 的出现次数为2次，则 C[0] = 2;1 的出现次数为4次，则C[1] = 4, (求出现次数也不麻烦,因为值成为了下标,那对下标对应的值累计就行了) 最终c数组如图:
由于C 是以A的元素为下标的，所以这样一做，A中的元素在C中自然就成为有序的了，这里我们可以知道 顺序为 0,1,3 (2 的计数为0),
然后我们把这个在C中的记录按每个元素的计数展开到输出数组B中，排序就完成了。
也就是 B[0] 到 B[1] 为0 B[2] 到 B[5] 为1 这样依此类推。(累加就行)
时间复杂度为: O(nlogn)</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E7%BA%A2%E9%BB%91%E6%A0%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E7%AE%97%E6%B3%95/%E7%BA%A2%E9%BB%91%E6%A0%91/</guid><description>红黑树是一种特定类型的二叉树，它是在计算机科学中用来组织数据比如数字的块的一种结构。若一棵二叉查找树是红黑树，则它的任一子树必为红黑树. [4]
红黑树是一种平衡二叉查找树的变体，它的左右子树高差有可能大于 1，所以红黑树不是严格意义上的平衡二叉树（AVL），但 对之进行平衡的代价较低， 其平均统计性能要强于 AVL 。 [2]
由于每一颗红黑树都是一颗二叉排序树，因此，在对红黑树进行查找时，可以采用运用于普通二叉排序树上的查找算法，在查找过程中不需要颜色信息。 [5]
恢复红黑树的属性需要少量(O(log n))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。 虽然插入和删除很复杂，但操作时间仍可以保持为 O(log n) 次 。
特征:
节点是红色或黑色。 [3]
根节点是黑色。 [3]
所有叶子都是黑色。（叶子是NUIL节点） [3]
每个红色节点的两个子节点都是黑色。（从每个叶子到根的所有路径上不能有两个连续的红色节点）
从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 [3]
https://www.cnblogs.com/gofighting/p/5437998.html
https://baijiahao.baidu.com/s?id=1641940303518144126&amp;amp;wfr=spider&amp;amp;for=pc
https://hacpai.com/article/1578230896592
标准的二叉树: 右子节点比父节点大,左子节点比父节点小,查找的时候就实现了折半查找了
每次插入和删除时,都会改变树的结构,此时可能会破坏红黑树的规则,则有两种操作来继续保持规则&amp;mdash;-变色,(左/右)旋转
变色: 把红变黑, 或者黑变红,(尽可能的使其符合规则)
旋转: 变色不能满足时,则通过旋转,看图
左旋:
变成了
右旋转:
变成了</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/io%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/</guid><description>[toc]
前言 操作系统的一次IO过程
应用程序发起的一次IO操作包含两个阶段：
IO调用：应用程序进程向操作系统内核发起调用。
IO执行：操作系统内核完成IO操作。
操作系统内核完成IO操作还包括连个两个过程：
准备数据阶段：数据从硬件拷贝到内核缓冲区，这里的硬件可以是磁盘，网卡等设备。
拷贝数据阶段：将数据从内核缓冲区拷贝到用户空间缓冲区
根据step1是否阻塞可以把IO操作划分为：
阻塞IO
非阻塞IO
根据step2是否是否阻塞可以划分为：
同步IO 异步IO 同步阻塞 IO (BIO) 服务端为了处理客户端的连接和请求的数据，写了如下代码。
listenfd = socket(); // 打开一个网络通信端口 bind(listenfd); // 绑定 listen(listenfd); // 监听 while(1) { connfd = accept(listenfd); // 阻塞建立连接 int n = read(connfd, buf); // 阻塞读数据 doSomeThing(buf); // 利用读到的数据做些什么 close(connfd); // 关闭连接，循环等待下一个连接 } 这段代码会执行得磕磕绊绊，就像这样。
可以看到，服务端的线程阻塞在了两个地方，一个是 accept 函数，一个是 read 函数。
如果再把 read 函数的细节展开，我们会发现其阻塞在了两个阶段。
这就是传统的阻塞 IO。
整体流程如下图。
所以，如果这个连接的客户端一直不发数据，那么服务端线程将会一直阻塞在 read 函数上不返回，也无法接受其他客户端连接。
这肯定是不行的。
文件描述符:
既然在Linux操作系统中，你将一切都抽象为了文件，那么对于一个打开的文件，我应用程序怎么对应上呢？
文件描述符应运而生</description></item><item><title/><link>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaokunji.github.io/myBlog/post/%E7%BB%BC%E5%90%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>[toc]
位运算 符号 描述 运算规则 &amp;amp; 与 两个位都为1时，结果才为1 | 或 两个位都为0时，结果才为0 ^ 异或 两个位相同为0，相异为1 ~ 取反 0变1，1变0 &amp;laquo; 左移 各二进位全部左移若干位，高位丢弃，低位补0 &amp;raquo; 右移 各二进位全部右移若干位，对无符号数，高位补0，有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补0（逻辑右移） 位运算（&amp;amp;、|、^、~、&amp;raquo;、&amp;laquo;） - 五色风车 - 博客园 (cnblogs.com)</description></item></channel></rss>